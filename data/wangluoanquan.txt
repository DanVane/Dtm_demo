124,  67 , 316,  438,  572,  428, 234,  328,  363,  154, 26
This paper presents a methodology to assess large-scale wind generation projects that considers their effect on network security. The proposed method is based on contingency analysis, including temporal study. Inputs to the simulation are grid model, forecasted load, conventional generation profiles, and wind variability of proposed projects. A time-step simulation is run for the time horizon to produce benefit indices for every location (bus) in the system. The congested transmission elements that require expansion are identified and ranked as part of the simulation. Each wind project in the proposed portfolio can result in benefits or costs for grid security. Policy makers can then use the method to design policies that ensure preservation of long-term system security. Developers could use the tool to identify security effects and assess their wind portfolios. Measuring network security and determining benefits of large-scale wind projects is a complex planning task that involves several aspects: temporal wind variability, spatial distribution of flows, multiple load and generation profiles, and numerous possible contingencies. All these wind project development aspects must be isolated to identify and correctly assign security costs and benefits. </para>
Research on security systems has typically focused on improving the performance and reliability of a single technique, algorithm or mechanism. There is also significant potential, however, in studying security as the product of a few key systems and then analyzing how those systems can best be integrated to achieve better overall system security. Most approaches that examine integrating one or more security mechanisms have focused on a specific implementation strategy. A systematic approach to integrated system security requires an analysis of the data relationships between all of the major mechanisms leading to a model that describes and relates all of the major elements in the domain. This paper proposes a information model for integrating access control, intrusion detection and intrusion response to enhance overall system security.
The Challenge - What is an adequate laboratory specification for emulating network attacks and experimenting with network forensics, other digital forensics techniques and social behavioral traits? Could published specifications for different scales of research assist in research development? Is the specification described here adequate for research purposes? What additional considerations are needed for simulating network attacks and validating forensic tools?
Security is often an afterthought when developing software, and is often bolted on late in development or even during deployment or maintenance, through activities such as penetration testing, add-on security software and penetrate-and-patch maintenance. We believe that security needs to be built in to the software from the beginning, and that security activities need to take place throughout the software lifecycle. Accomplishing this effectively and efficiently requires structured approach combining a detailed understanding on what causes vulnerabilities, and how to prevent them. In this paper we present a process for software security that is based on vulnerability cause graphs, a formalism we have developed for modeling the causes of software vulnerabilities. The purpose of the software security process is to evolve the software development process so that vulnerabilities are prevented. The process we present differs from most current approaches to software security in its high degree of adaptability and in its ability to evolve in step with changing threats and risks. This paper focuses on how to apply the process and the criteria that have influenced the process design.
Intrusion Detection is a critical process in network security. It is the task of detecting, preventing and possibly reacting to the attack and intrusions in a network based computer systems. This paper presents an intrusion detection system based on Self-Organizing Maps (SOM) and Back Propagation Network (BPN) for visualizing and classifying intrusion. The performance of the proposed Hybrid Neural Network approach is tested using KDD cup' 99 data available in the UCI KDD archive. The proposed approach considers all kinds of attacks under major category (Normal, DOS, Probe, U2R, and R2L) which provides an insightful visualization for network intrusion and works well in detecting different attacks in the considered system.
Directional antennas are used for Mobile Ad Hoc Networks (MANET) to improve network security, capacity and energy efficiency. In this paper, we propose an Unaided Directional Neighbor Discovery (UADND) algorithm for directional transmission and reception in MANET. With UADND, network nodes could independently find neighbors that can be reached only when both of transmission and reception are directional. Additional information provided by GPS or other methods, such as node position or synchronization information utilized by other neighbor discovery algorithms is not necessitated in UADND. The proposed algorithm uses the geometrical relationship between the communication range of DT (Directional Transmission only) mode and the communication range of DTR (Directional Transmission and Reception) mode to discover DTR neighbors. Cross layer design is used in UADND to integrate directional neighbor discovery with MANET routing, and thus making UADND a scenario-adaptive neighbor discovery algorithm. Simulations show that UADND is efficient in overhead and energy saving.

With the increase of network bandwidth, high performance protocol processing plays more and more important role in high speed network security. Recent studies show that current computer architecture advances and CPU performance improvements have limited impact on network protocol processing performance. Some studies find that in real SMT processor like Intel Xeon processor with Hyper-Threadings, the sharing resources (like cache) contention between threads can hurt the processing performance of network applications like servers or IDS. How to make protocol processing cope with the advances in computer architecture has been widely studied. In this paper, we put our focus on the processing performance of TCP automata phases, using execution based simulations to model the relationship between each phase performance and cache size, and then measuring the cache contention between threads. We find (1) The Load/Store units can be the bottleneck of protocol processing. (2) In connection establishing phase of TCP processing, cache contention between threads is more aggressive than any other phase. We also suggest a FSM decomposition based parallel processing approach to use sharing cache of SMT processors effectively.
In a computer network, network security is accomplished using elements such as firewalls, hosts, servers, routers, intrusion detection systems, and honey pots. These network elements need to know the nature or anomaly of the worm a priori to detect the attack. Modern viruses such as Code Red, Sapphire, and Nimda spread quickly. Therefore, it is impractical if not impossible for human mediated responses to these fast-spreading viruses. Several epidemic studies show that automatic tracking of resource usage and control provides an effective method to contain the damage. In this paper, we propose a novel security architecture based on the control system theory. In particular, we describe a state-space feedback control model that detects and control the spread of these viruses or worms by measuring the velocity of the number of new connections an infected host makes. The mechanism's objective is to slow down a worm's spreading velocity by controlling (delaying) the number of new connections made by an infected host. A proportional and integral (PI) controller is used for a continuous control of the feedback loop. The approach proposed here has been verified in a laboratory setup, and we were able to contain the infection so that it affected less than 5 percent of the hosts. We have also implemented a protocol for exchanging control-specific information between the network elements. The results from the simulation and experimental setup combined with the sensitivity analysis demonstrate the applicability and accuracy of the approach.
Intrusion detection is an effective approach for dealing with various problems in the area of network security. This paper presents a comparative study of using supervised probabilistic and predictive machine learning techniques for intrusion detection. Two probabilistic techniques Naive Bayes and Gaussian and two predictive techniques Decision Tree and Random Forests are employed. Different training datasets constructed from the KDD99 dataset are employed for training. The ability of each technique for detecting four attack categories (DoS,Probe,R2L and U2R) have been compared. The statistical results to show the sensitivity of each technique to the population of attacks in a dataset have also been reported. We compare the performance of the techniques and also investigate the robustness of each technique by calculating their standard deviations with respect to the detection rate of each attack category.
The proliferation of complex and fast-spreading intrusions not only requires advances in intrusion detection mechanisms but also demands development of sophisticated and automated intrusion response systems. In this paper we present a novel cost-sensitive model for intrusion response that incorporates preemptive deployment of the response actions. Specifically, our technique relies on comparing the cost of deploying a response against the cost of damage caused by an "un-attended" intrusion and decides to preemptively deploy a response with maximum benefit. Our technique further allows adaptation of responses to the changing environment through evaluation of success and failure of previously triggered responses. We demonstrate the advantages of the approach and evaluate it using a damage reduction metric.
We present novel and practical techniques to accurately detect IP prefix hijacking attacks in real time to facilitate mitigation. Attacks may hijack victim's address space to disrupt network services or perpetrate malicious activities such as spamming and DoS attacks without disclosing identity. We propose novel ways to significantly improve the detection accuracy by combining analysis of passively collected BGP routing updates with data plane ingerprints of suspicious prefixes. The key insight is to use data plane information in the form of edge network ingerprinting to disambiguate suspect IP hijacking incidences based on routing anomaly detection. Conflicts in data plane ingerprints provide much more definitive evidence of successful IP pre- fix hijacking. Utilizing multiple real-time BGP feeds, we demonstrate the ability of our system to distinguish between legitimate routing changes and actual attacks. Strong correlation with addresses that originate spam emails from a spam honeypot confirms the accuracy of our techniques.
In today's online connected world, almost all corporate networks use some form of perimeter firewalls to manage Internet connections and enforce a security policy at the corporate gateway. Although it can considerably enhance network security and protect business-critical information, a firewall with thousands of rules can become a bottleneck for network performance. The primary goal of this paper is to present a new rule order optimizer based on simulated annealing to find optimal configurations that minimize the average number of rule comparisons while preserving precedence relationships among disjoint rules. The proposed approach is evaluated and its effectiveness is compared with another approximate solution under several firewall configurations and policy profiles.
In today's large and complex network scenario vulnerability scanners play a major role from security perspective by proactively identifying the known security problems or vulnerabilities that exists across a typical organizational network. Identifying vulnerabilities before they can be exploited by malicious user often helps to test, maintain, and assess the risk of the existing network. Still there are many problems with currently available state of the art vulnerability scanners like hampering system resource. One possible solution to this problem might be reducing the number of vulnerability scans, along with the quantitative approach towards different vulnerability category in order to identify which class of vulnerability should enjoy preference in the risk mitigation procedure. This paper introduces a model that predicts vulnerabilities that will occur in near future on a Local Area Network (LAN) by using statistical measures and vulnerability history data. Two case studies have also been presented to validate the model.
With the rapid growth of computer networks and network infrastructures and increased dependency on the internet to carry out day-to-day activities, it is imperative that the components of the system are secured. In the last few years a number of Intrusion Detection Systems (IDS) have been developed as network security tools, both in commercial and academic sectors. These systems have used different approaches to detecting unauthorized activity, and have given us some insight into the problems that still have to be solved. While considerable progress has been made in the areas of string matching, header processing and detecting DoS attacks at network level, complete systems have not yet been demonstrated that provide all of the functionality necessary to perform intrusion detection at each host system there by securing the entire network. In this paper we are proposing the architecture of a Distributed Intrusion Detection System (DIDS) for use in high-speed networks. The proposed DIDS has Host IDS component at each host that combines the above-mentioned functionalities along with the capability of collecting the events at the application level to look for any signs of intrusion at the network level. DIDS consists of Central IDS component which performs sophisticated processing to detect any signs of distributed attacks on the entire network and update rules in each host system. For high speed networks it can be difficult to keep up with intrusion detection using purely software approach without affecting performance of the system intended for designed application.
Designing security softwares that evolve as quickly as threats is a truthful challenge. In addition, current software becomes increasingly more complex and difficult to handle even for security experts. Intrusion Detection Softwares (IDS) represent a solution that can alleviate these concerns. This paper proposes a framework to automatically build an effective online IDS which can check if the program's expected behavior is entirely respected during the execution. The proposed framework extracts relevant information from the program's source code to build a dedicated IDS. We use the GCC compiler to produce the structure of our behavior's model and ensure the IDS is correct. Thanks to Petri nets, our framework allows program offline monitoring and simplifies the online monitoring development.
Designing security softwares that evolve as quickly as threats is a truthful challenge. In addition, current software becomes increasingly more complex and difficult to handle even for security experts. Intrusion Detection Softwares (IDS) represent a solution that can alleviate these concerns. This paper proposes a framework to automatically build an effective online IDS which can check if the program's expected behavior is respected during the execution. The proposed framework extracts relevant information from the program's source code to build a dedicated IDS. We use the GCC compiler to produce the structure of our behavior's model and ensure the IDS is correct. Thanks to Petri nets, our framework allows program offline monitoring and simplifies the online monitoring development.
Intrusion detection system (IDS) is an important component of computer network security, while clustering analysis is a common unsupervised anomaly detection method. However, it is difficult for the single clustering algorithm to get the great effective detection, and the data of intrusion attacks is anomalistic normally. This paper presents an unsupervised anomaly detection system based on the clustering ensemble. The system is based on the multiple runs of K-means to accumulate evidence to avoid the false classification of anomalistic data; then using single-link to construct the hierarchical clustering tree to get the ultimate clustering result to solve the above problems. Finally, the KDD99 CUP test data is used to show that this system is greatly effective. It also compares with another IDS based on congeneric clustering algorithm to demonstrate the superiority of this system.
Because of the coexistence of multi-PKI in multi-domains of active networks, the implementation of authentication for such multi-domains becomes one of the most important issues in active network security. This paper proposes an authentication model and designs a suit of protocols to tackle this important issue. The proposed model supports mutual entity authentication between multi-type domains and is suitable for practical implementation of authentication in active networks.
With the development of computer networks, the spread of malicious network activities poses great risks to the operational integrity of many organizations and imposes heavy economic burdens on life and health. Therefore, risk assessment is very important in network security management and analysis. Network security situation analysis not only can describe the current state but also project the next behavior of the network. Alerts coming from IDS, Firewall, and other security tools are currently growing at a rapid pace. Large organizations are having trouble keeping on top of the current state of their networks. In this paper, we described cyberspace situational awareness from formal and visual methods. Next, to make security administrator comprehend security situation and project the next behaviors of the whole network, we present using parallel axes view to give expression clearly of security events correlations. At last, we concluded that visualization is an important research of risk evaluation and situation analysis of network.
Several network security and QoS applications require detecting multiple string matches in the packet payload by comparing it against predefined pattern set. This process of pattern matching at line speeds is a memory and computation intensive task. Hence, it requires dedicated hardware algorithms. In this paper we describe the hardware architecture of a parallel, pipelined pattern matching engine that uses trie based pattern matching algorithmic approach. The algorithm optimizes pattern matching process through two key innovations of parallel pattern matching using incoming content filter and multiple character matching using trie pruning. The hardware implementation is capable of performing at line-speeds and handle traffic rates upto OC-192, the underlying architecture allows for multiple patterns to be detected and for the system to gracefully recover from a failed partial match, the throughput of the system does not degrade with the increase in the number of patterns or the length of the patterns to be matched. The solution described outperforms most current implementations in terms of speed and memory requirement and outperforms TCAM based solutions in terms of power consumption, area, and cost while remaining competitive in terms of throughput and update times. We use the complete Snort rule set (2005 release) and VoIP RFC to validate our performance and achieve a throughput of 12Gbps with 6KBytes of content filter memory and 0.3 MBytes of total memory for Snort and 0.5KBytes of filter memory and 12KBytes of total memory for SIP.
We present a modern approach of detecting the computer network security attacks (commonly termed as "hacks") by using an artificial intelligence method known as a genetic algorithm, which is a variant of stochastic beam search, which can be applied to a Policy Based Network. For this experiment, we develop the corresponding form of genetic algorithm and the form of fitness function to detect security attacks. Then we test the algorithm by a simulation program in TC (Turbo C) to gauge its effectiveness and the side effects of its execution. Finally, we give the indication for the necessary change in policy framework to deploy the functionality.
Voice Over IP (VoIP) or telephony services over Internet announces a new revolution in the telecommunication world for its management simplicity and cost reduction. VoIP security extends the existent risk range of IP protocols and infrastructures and introduces new attacks as well. Threats identification and standardization, secure signaling and media architectures, as well as intrusion detection and prevention mechanisms are currently under debate in the research community. We propose in this article a SIP (Session Initiation Protocol) specific honeypot. We describe its design and implementation. We detail the inference mechanism which classifies the received messages. We show how the model investigates about a received call and raises an appropriate conclusion.
The implementation of network security devices such as firewalls and IDSs are constantly being improved to accommodate higher security and performance standards. Using reliable and yet practical techniques for testing the functionality of firewall devices particularly after new filtering implementation or optimization becomes necessary to assure required security. Generating random traffic to test the functionality of firewall matching is inefficient and inaccurate as it requires an exponential number of test cases for a reasonable coverage. In addition, in most cases the policies used during testing are limited and manually generated representing fixed policy profiles. In this paper, we present a framework for automatic testing of the firewall policy enforcement or implementation using efficient random traffic and policy generation techniques. Our framework is a two-stage architecture that provides a satisfying coverage of the firewall operational states. A large variety of policies are randomly generated according to custom profiles and also based on the grammar of the access control list. Testing packets are then generated intelligently and proportional to the critical regions of the generated policies to validate the firewall enforcement for such policies. We describe our implementation of the framework based on Cisco IOS, which includes the policy generation, test cases generation, capturing and analyzing firewall output, and creating detailed test reports. Our evaluation results show that the automated security testing is not only achievable but it also offers a dramatically higher degree of confidence than random or manual testing.
Researchers use random graph models to analyze complex networks that have no centralized control such as the Internet, peer-to-peer systems, and mobile <emphasis emphasistype="italic">ad hoc</emphasis> networks. These models explain phenomena like phase changes, clustering, and scaling. It is necessary to understand these phenomena when designing systems where exact node configurations cannot be known in advance. This paper presents a method for analyzing random graph models that combine discrete mathematics and probability theory. A graph connectivity matrix is constructed where each matrix element is the Bernoulli probability that an edge exists between two given nodes. We show how to construct these matrices for many graph classes, and use linear algebra to analyze the connectivity matrix. We present an application that uses this approach to analyze network cluster self-organization for sensor network security. We conclude by discussing the use of these concepts in mobile systems design. </para>
We present a trilevel optimization model of resource allocation in electric power network defense. This model identifies the most critical network components to defend against possible terrorist attacks. The goal of defense is to minimize the economic cost that the attacks may cause, subject to resource constraints. We describe a decomposition approach for finding an optimal solution to the trilevel model, which is based on iteratively solving smaller nested bilevel problems. Our testing results demonstrate the advantages of trilevel optimization over bilevel optimization in network defense. </para>
False data injection is a severe attack that compromised sensor nodes ("moles"') can launch. These moles inject large amount of bogus trafJic that can lead to application failures and exhausted network resources. Existing sensor network security proposals only passively mitigate the damage by filtering injected packets; they do not provide active means for fight back. This paper studies how to locate such moles within the framework of packet marking, when forwarding moles collude with source moles to manipulate the marks. Existing Internet traceback mechanisms do not assume compromised forwarding nodes and are easily defeated by manipulated marks. We propose a Probabilistic Nested Marking (PNM) scheme that is secure against such colluding attacks. No matter how colluding moles manipulate the marks, PNM can always locate them one by one. We prove that nested marking is both sufJicient and necessaly to resist colluding attacks. PNM also has fast-traceback: within about 50 packets, it can track down a mole up to 20 hops away from the sink. This virtually prevents any effective data injection attack: moles will be caught before they have injected any meaningful amount of bogus traffic.
This paper focuses on the detection of virtual environments and low interaction honeypots by using a feature set that is built using traditional system and network level finger printing mechanisms. Earlier work in the area has been mostly based on the system level detection. The results aim at bringing out the limitations in the current honeypot technology. This paper also describes the results concerning the robustness and generalization capabilities of kernel methods in detecting honeypots using system and network finger printing data. We use traditional support vector machines (SVM), biased support vector machine (BSVM) and leave-one-out model selection for support vector machines (looms) for model selection. We also evaluate the impact of kernel type and parameter values on the accuracy of a support vector machine (SVM) performing honeypot classification. Through a variety of comparative experiments, it is found that SVM performs the best for data sent on the same network; BSVM performs the best for data sent from a remote network.
Recently, honeynets became one of the main tools for understanding the characteristics of malicious attacks and the behavior of the attackers. However the attackers may identify the honeypots and avoid attacking them. Thus the honeynet administrators must be able to deceive the attackers and induce them to attack the honeypots. In this paper we propose a game theoretic framework for modeling deception in honeynets. The framework is based on extensive games of imperfect information. We study the equilibrium solutions of these games and show how they are used to determine the strategies of the attacker and the honeynet system.
One of the best ways to defend a computer system is to make attackers think it is not worth attacking. Deception or inconsistency during attacker reconnaissance can be an effective way to encourage this. We provide some theory of its advantages and present some data from a honeypot that suggests ways it could be fruitfully employed. We then report on experiments that manipulatedpackets of attackers of a honeypot using Snort Inline. Results show that attackers definitely responded to deceptive manipulations, although not all the responses helped defenders. We conclude with some preliminary results on analysis of "last packets" of a session which indicate more precisely what clues turn attackers away.
Security policies are a key component in protecting enterprise networks. There are many defensive options available to these policies, but current mechanically-enforced security policies are limited to traditional admission-based access control. There are defensive capabilities available that include logging, firewalls, honeypots, rollback/recovery, and intrusion detection systems, but policy enforcement is essentially limited to allow/deny semantics. Furthermore, access-control mechanisms operate independently on each service, which often leads to inconsistent or incorrect application of the intended system-wide policy. To begin to solve these problems, we propose a new system for defense-in-depth using global security policies. Under a global security policy, every policy decision is made with near-global knowledge, and re-evaluated as global knowledge changes, given an initial configuration provided by the administrator. Using a variety of actuators, we make the full array of defensive capabilities available to the global policy. We outline our proposal for enterprise-wide security policies, explore the design space, and discuss Arachne, our prototype implementation.
Network anomaly detection plays a vita role in securing network security and infrastructures. Current research focuses concentrate on how to effective reduce high false alarm rate and usually ignore the fact that the poor quality data for the modeling of normal patterns as well as the high computational cost make the current anomaly detection methods not act as well as we expect. Based on these, we first propose a novel data mining scheme for network anomaly detection in this paper. Moreover, we adopt data reduction mechanisms (including genetic algorithm (GA) based instance selection and filter based feature selection methods) to boost the detection performance, meanwhile reduce the computational cost of TCM-KNN. Experimental results on the well-known KDD Cup 1999 dataset demonstrate the proposed method can effectively detect anomalies with high detection rates, low false positives as well as with high confidence than the state-of-the-art anomaly detection methods. Furthermore, the data reduction mechanisms would greatly improve the performance of TCM-KNN and make it be a good candidate for anomaly detection in practice.
There has been excellent progress on languages for rigorously describing key exchange protocols and techniques for proving that the network security tunnels they establish preserve confidentiality and integrity. New problems arise in describing and analyzing establishment protocols and tunnels when they are used as building blocks to achieve high-level security goals for network administrative domains. We introduce a language called the tunnel calculus and associated analysis techniques that can address functional problems arising in the concurrent establishment of tunnels. In particular, we use the tunnel calculus to explain and resolve cases where interleavings of establishment messages can lead to deadlock. Deadlock can be avoided by making unwelcome security compromises, but we prove that it can be eliminated systematically without such compromises using a concept of session to relate tunnels. Our main results are noninterference and progress theorems familiar to the concurrency community, but not previously applied to tunnel establishment protocols.
This practical experience report presents the results of an experiment aimed at building a profile of attacker behavior following a remote compromise. For this experiment, we utilized four Linux honeypot computers running SSH with easily guessable passwords. During the course of our research, we also determined the most commonly attempted usernames and passwords, the average number of attempted logins per day, and the ratio of failed to successful attempts. To build a profile of attacker behavior, we looked for specific actions taken by the attacker and the order in which they occurred. These actions were: checking the configuration, changing the password, downloading a file, installing/running rogue code, and changing the system configuration.
The task of exploring and analysing large quantities of communication network security data is difficult. Visualisation of the data should help the analyses and make data exploration faster and easier. This paper describes prototype software that visualises the alerts effectively and provides a simple presentation. The needs analysis of this prototype is based on the suggested needs of network security analyst's tasks as seen in the literature. The prototype software incorporates various projections of the alert data in 3-dimensional displays. Filtering, drill-down and playback of alerts at variable speed are incorporated to strengthen the analysis. We integrate a false alert classifier using classification tree algorithm to classify alerts into false and true alerts. Real-time visual observation is also included. We describe some example analyses to prove the usefulness of our prototype.
This paper proposes a new distributed monitoring approach based on the notion of centrality of a graph and its evolution in time. We consider an activity profiling method for a distributed monitoring platform and illustrate its usage in two different target deployments. The first one concerns the monitoring of a distributed honeynet, whilst the second deployment target is the monitoring of a large network telecope. The central concept underlying our work are the intersection graphs and a centrality based locality statistics. These graphs have not been used widely in the field of network security. The advantage of this method is that analyzing aggregated activity data is possible by considering the curve of the maximum locality statistics and that important change point moments are well identified.
Peer-to-peer (p2p) networks have become a privileged communication infrastructure for file sharing applications. However, it can also provide support for cybercriminality activities performed by organized criminal groups that can voluntarily propagate strongly undesirable contents. We propose in this paper a management platform capable of tracking and identifying such cyberpredators in peer-to-peer networks. We describe the architecture consisting of a set of configurable honeypot agents and detail how a honeypot is capable of advertising strongly undesirable contents and of detecting network users that attempt to acquire them. The distributed tracking solution is experimented through an implementation prototype in realistic scenarios.
Ongoing power system automaton and open access imposed by new government deregulations aggravate cyber-vulnerability of utility computer networks. This paper proposes an open-access-compatibility (OAC) security layer, installed beneath the data-link layer of the popular utility network protocol DNP3, to enhance data transmission security for utilities with open access capabilities. The OAC security is designed as an extension for a Canadian utility integrated P&C system innovation. The OAC security increases interactions with DNP3 data-link layer to enhance utility network security that is especially important for time-data-critical transmissions of protection information. The OAC security does not alter existing DNP3 specification to maintain interoperability for devices not using OAC. The OAC security uses two independent encryptions, one for exchanging security keys and one for transmitting data, to minimize time required for security operations. The OAC security relaxes authentication requirements to reduce transmission overheads and increase efficiency.
Recent introduction of open access competition by governments in the electricity industry and increase use of network-controlled devices in the power system have resulted utilities in greater reliance on computer networks for their power system operations. This paper proposes a new network access control model that significantly increases the power system network security. The new model extends the network access control from a single security domain to multiple domains interconnected with the Internet. This paper also proposes a security policy managing method using XML that simplifies power system computer network security administrations. Unlike existing implementations, with the proposed method the authorization is independently defined and separated from policy representations and implementation mechanisms, and a digital credential is introduced to establish trust and role assignment for users in different utility domains. Case studies of the new model and its significant application for distribution system stability controls are presented.
A reliable state estimator to withstand a contingency becomes more and more important due to the increasing concerns on the network security in a deregulated power market. A novel topological approach is proposed in this paper to identify critical measurements and to examine network observability under a contingency, where the contingency can be any single branch outage or measurement loss. To advance the classical topological observability analysis, a new concept of contingency observability graph (COG) is introduced and it is proven that a power system network maintains its observability against a contingency if and only if its COG satisfies some conditions. With little extra computation load, the proposed COG based approach is a natural extension of the classical topological observability analysis for online applications. The IEEE 30-Bus system is used to demonstrate the validity and efficiency of our approach.
Power distribution system cyber-security concerns are increasing rapidly with growing demands for open accesses to the distribution systems for electricity generation and trading imposed by new government deregulations. This paper proposes a new integrated network security protocol layer, located below the data-link layer of DNP3 - a popular utility protocol, to enhance the data transmission cyber-security for power distribution systems. The security layer utilizes distribution system characteristics to provide comprehensive security while maintaining virtually no impact on the existing DNP3 specification. The proposed security layer provides end-to-end security and link security through encryption, authentication, and padding operations. `Recipe' formats, with independent cipher and authentication, are designed for the security layer operations to increase flexibility, coverage and quality of service capabilities of the security layer and to provide rapid responses for changes in cyber-security threats in the power distribution systems. This paper describes its significant applications in distribution system stability controls.
The aim of this paper is to analyze the range of crosstalk problems encountered with the major trackside equipments, the Balise and the Loops, used in Automatic Train Control for metro rails. Crosstalk impedes the efficient and safe data transmission between the tracks and the train. The installation requirements for the balises and the infill loops are elaborated in this paper. At present, coaxial leaky cables are employed for the infill loops but they do have some limitations under certain situations. A comparative study on the existing methods of crosstalk reduction is given here and an optimal solution based on the problem is proposed.
Anomaly based intrusion detection is inherently subject to false alarms. Fast and automated intrusion response based on this type of intrusion detection will cause significant usage restrictions for falsely suspected systems. To avoid these negative effects without sacrificing detection sensitivity or increasing the risk for the production network inadequately, we propose a scheme combining anomaly-based IDS with Honeynet concepts and link layer based VLANs. In addition to introducing the concept, we will describe a proof-of-concept implementation and report results from some lab tests confirming the benefits of this approach.
System and network security are significant challenges facing the Internet community today. Hi-tech crime is on the increase and is a threat to a budding online business environment. Reconnaissance technologies that enable the security community to keep abreast of new threats have been developed and deployed. These technologies monitor illegitimate Internet traffic but appropriate data analysis is required to identify new and emerging threats amongst the deluge of traffic that is illegitimate but known and understood. This paper presents an approach towards the automation of such an analysis process.
With the growing deployment of network security devices, the large volume of alerts gathered from these devices often overwhelm the administrator, and make it almost impossible to discover complicated multistage attacks in time. It is necessary to develop a real-time system to detect the ongoing attacks and predict the upcoming next step of a multistage attack in alert streams, using known attack patterns. So it is a key mission to make sure that the pattern definition is correct, complete and up to date. In this paper, a classical data mining algorithm is used to help us discover attack patterns, construct and maintain rules. It can overcome the highly dependent on knowledge of experts, time-consuming and error-prone drawbacks in previous approaches using manual analysis. Unfortunately, for a dynamic network environment where novel attack strategies appear continuously, the method shows a limited capability to detect the novel attack patterns. We can address the problem by presenting a novel approach using incremental mining algorithm to discover new attack patterns that appear recently. A series of experiments show the validity of the methods in this paper.
The Telecare project is a healthcare application aimed at minimizing the institutionalization of elderly people and patients with mobility problems and reducing the cost and burden of elderly care for the healthcare system. In this paper we investigate the utilisation of mobile ad hoc networks in the Telecare project from the view point of network security and information integrity, especially considering the fact that one main challenge in designing ad hoc networks is their vulnerability to security attacks, which is a major concern for many security sensitive applications. We have also looked at some of the threats that ad hoc network faces and the security goals to be achieved in the case of the Telecare system. Finally we have examined the main attributes of secure ad hoc networks in the light of the Telecare system, and discuss a possible solution through the use of publickey cryptography and digital signature approach.
Managing computer and network security programs has become an increasingly difficult and challenging job. Dramatic advances in computing and communications technology during the past few years have redirected the focus of data processing from the computing center to the terminals in individual offices and homes. The result is that managers must now monitor security on a more widely dispersed level. These changes are continuing to accelerate, making the security manager's job increasingly difficult. In this paper a better solution for Information Security management has been proposed by designing PrISM (Preventive Information Security Management). PrISM aims to develop and deploy an indigenous Information Security Management System (ISMS) with intrusion prevention capabilities. The objective is to develop an ISMS with appropriate security assurance controls and risk handling processes. This will provide best protection of critical assets against information warfare attacks. The task has been planned by performing reverse engineering of Open Source Security Information Management (OSSIM) system. A detailed discussion on OSSIM and commercially available software Event Horizon has also been presented.
Firewall is the core technology of today's network security and the first line of defense against external network attacks and threats. most personal firewall deals with the packets under the user model, there are a lots of limits. in order to protect our privates better some operations need to be done under the kernel model. The NDIS by which we can easy do something under the kernel model is introduced in the windows operating system that is the mostly used system in personal computer. This paper clearly describes the architecture of the NDIS, on the base of the NDIS intermediate drivers presents a personal firewall model which operations under the kernel model.
The evolution and existence of stable trust relations have been studied extensively in the context of social theory. However, reputation systems or trust schemes have only been recently used in the domain of wireless ad hoc networks. It has been shown that these schemes provide positive results as a self-policing mechanism for the routing of data in wireless ad hoc network security. This paper develops a relationship between the trust concepts in the social network theory and wireless ad hoc networks. In addition, the paper maps existing trust schemes in wireless ad hoc networks to a long-standing theory in social networks. Most importantly, a refined model of trust evaluation in social networks is constructed and mapped to a new trust scheme for ad hoc networks. The new trust scheme is analyzed and shown to outperform existing schemes using scenario and simulation analysis.
The problem of authenticating feedback in overlay multicast applications has only recently been highlighted by the network security research community. The multicast source needs to verify all individual signatures on the acknowledgements (Acks) that it receives from the intended receivers--the funnelling of traffic causes, what we call the signed-Ack implosion problem. In this paper, we propose an efficient and scalable technique to solve this problem. First, we present a novel third-order linear feedback shift register (LFSR) sequence-based, 2-party signature scheme CLFSR-S following a well-known variant of the generalized ElGamal signature scheme. Second, we construct an efficient, single round, tree-based multisignature scheme CLFSR-MS (also the first multisignature based on LFSR sequences) using CLFSR-S. The CLFSR-MS scheme has been engineered to be the most efficient and scalable, tree-based multisignature scheme (owing to the unique construction/ signature format of CLFSR-S) designed to solve the signed-Ack implosion problem in reliable, large-scale, performance sensitive multicast applications. We perform a theoretical analysis including correctness and security of CLFSR-MS and also present a performance (computation and communication costs, storage overhead) comparison of the proposed scheme with existing schemes.
The objective of context honeypot is to identify a probable privacy violator before he actually succeeds in capturing desired precious information. We believe that in the environment of robust implementation of privacy policy, privacy breaches can occur only through masquerading. Success of context honeypot depends on efficient luring (by using lure messages) of probable privacy violator. Lure messages are built by using real and fake data in such a way that the probable violator remains oblivious of being lured. In this paper, we propose a lure model and an architecture for generating lure messages.
There are at least three key decision layers in cost-effective network defense to counter immediate threats: security policies, defense strategies, and real-time defense tactics. A layered decision model (LDM) has been developed to capture the essence of this decision process. The LDM helps decision-makers gain insight into the hierarchical relationships among inter-connected entities and decision types that underlie defense goals, and supports the selection of cost-effective defense mechanisms to safeguard computer networks. To be effective as a business tool, it is necessary to validate the rationality of the model before applying it to real-world business cases. LDM rationality requires that a decision making process be consistent and free of blocked execution paths, and be able to produce cost-effective defense plans. This paper describes validation of LDM rationality.
This short research note proposes a peer-to-peer (P2P) network security pricing model that promotes the growth of P2P networks with an increased resistant to malicious code propagation. The security pricing model integrates findings from recent developments in complex network theory with incentive compatible pricing. The model links file download prices in P2P networks with a metric called the Pearson coefficient. The calculation of the Pearson coefficient indicates a structural dimension of networks called "preferential attachment". The security pricing model employs a mechanism that charges nodes (network users) higher prices for choosing "bad" links, that is links that decrease the Pearson coefficient, because these links inhibit preferential attachment in networks and ultimately degrade networks' resistance to malicious code propagation. The security pricing model provides incentives for nodes to choose the lower priced "good" links (that is, links that increase the Pearson coefficient). Continuing research will focus on formalizing and validating the P2P network security model. Simulation will be used to test the resistance of networks grown under this model to malicious code propagation. We demonstrate a different approach to P2P network security and hope that the final analysis will provide insights and provide a significant contribution to the future study of the security of distributed transient networks (DTN) including P2P, ad hoc, and other computer based networks.
Advanced hacker techniques make the effective defense at the network security perimeters impossible. Many security solutions are proposed by researchers and practitioners in recent years, most of them focus on how to enhance the functionality and capability of security modules, but few of them emphasize on the assurance assessments of security modules. Security assurance intends to provide a degree of confidence instead of a true measure of how secure the system is. Security assurance should be measured and controlled in the process of security management life cycle. In this paper, we propose a security model, Object Association Binding (OAB), to unify the access control policies and to provide an objective assessment for the confidence level of network security assurance. Based on the design principles of OAB, its prototype called Network Security Policy Assistant (NSPA) is implemented.
The inherent vulnerabilities in TCP/IP architecture give dearth of opportunities to DDoS attackers. The array of schemes proposed for detection of these attacks in real time is either targeted towards low rate attacks or high bandwidth attacks. Presence of low rate attacks leads to graceful degradation of QoS in the network thus making them further undetectable. In this paper, we propose a scheme that uses three lines of defense. The first line of defense is towards detecting the presence of low rate as well as high bandwidth attacks based on entropy variations in small time windows. The second line of defense identifies and tags attack flows in real time. The last line of defense is redirecting the attack flows to honeypot server that responds in contained manner to the attack flows, thus providing deterrence and maintaining QoS at ISP level. We validate the effectiveness of the approach with simulation in ns-2 on a Linux platform.
Many visualization systems do not get widespread adoption because they confront the user with sophisticated operations and interfaces. The author suggests augmenting visualization systems with learning capability to improve both the performance and usability of visualization systems. Several examples including volume segmentation, flow feature extraction, and network security are given illustrating how machine learning can help streamline the process of visualization, simplify the user interface and interaction, and support collaborative work.
Computer network defense (CND) requires analysts to detect both known and novel forms of attacks in massive volumes of network data. Visualization tools can potentially assist in the discovery of suspicious patterns of network activity and relationships between seemingly disparate security events, but few CND analysts are leveraging visualization technologies in their current practice. To address this, we created a new visualization framework, VIAssist, based on a comprehensive cognitive task analysis of CND analysts. We designed VIAssist to fit the work practices and operational environments of those analysts. This article describes the major visual analytic features of VIAssist that address the needs of CND analysts, including its coordinated visualizations and interactive report building capabilities. A scenario illustrates how it can be used to discover the unexpected in network flow data.
News about distributed-denial-of-service attacks on Estonian government Web sites might have represented more smoke than fire, but it also revealed a new political battlefield. Networking veterans say public officials' accusations can make ad hoc "hacktivism" seem like a state-sponsored attack. When you add porous network defenses and a credulous media, you've got the potential for a real problem.
In this paper we present a hardware architecture for string matching. Our solution based on using a Bloom filter based pre-processor and a parallelized hashing engine is capable of handling wire line speeds with zero false-positive probability. String matching modules are extensively used in the network security domain especially in network intrusion detection systems where they are required to operate at wire line speeds. Our analysis shows that our system is capable of matching 16000 strings and achieves throughput in excess of 100Gbps (i.e. capable of handling 10 OC - 192 links comfortably).
A new optical VPN configuration is proposed for implementation on a WDM PON that uses wavelength routers with multi stage AWGs. The proposed configuration offers the features of scalability and network security. The configuration is verified in terms of wavelength function by using the wavelength transfer matrix method. Network size, or scalability, is discussed by considering the optical power budget. Experiments also show the feasibility of the configuration.
Honeypots are a useful tool for discovering the distribution of malicious traffic on the Internet and how that traffic evolves over time. In addition, they allow an insight into new attacks appearing. One major problem is analysing the large amounts of data generated by such honeypots and correlating between multiple honeypots. Honey Plotter is a web-based query and visualisation tool to allow investigation into data gathered by a distributed honeypot network. It is built on top of a relational database, which allows great flexibility in the questions that can be asked and has automatic generation of visualisations based on the results of queries. The main focus is on aggregate statistics but individual attacks can also be analysed. Statistical comparison of distributions is also provided to assist with detecting anomalies in the data; helping separate out common malicious traffic from new threats and trends. Two short case studies are presented to give an example of the types of analysis that can be performed.
This paper presents a Danger Theory inspired multi-agent fusion model. Danger Theory is a relative new research subject in Artificial Immune System (AIS). It presents new viewpoint and explains some problems in traditional immune theory. This paper describes the principle of Danger Theory, analyzes the differences between it and traditional theory, and also constructs a multi-agent fusion model for network security assessment. It apply the concepts of Danger Theory to fusion model, such as danger signal, danger zone and Antigen Presenting Cells (APC), which are benefit for multi-agent fusion model. An example is presented to illustrate the operation process of the fusion model.
Current network forensics systems are static and not real-time. In order to overcome the shortages, a dynamical network forensics model based on artificial immune theory and multi-agent theory, referred to as DNF, is introduced here. Comparing with traditional computer forensics methods, the new method provides the capacity that gathering real-time evidence dynamically as soon as network intrusions take place and saving the evidence in a safe way to prepare for the collection and analysis of the original evidence. In this paper, architecture of the model and the definitions of its components inspired by the immunity theory are given out. The experiment shows that it is able to insure the authenticity, integrality and validity of the digital evidence, and it is a new method for dynamic computer forensics.
As there is a rapid rampancy of malicious intrusion, people pay more attentions to the network security. This paper presents a new method based on Hilbert-Huang Transform(HHT) to characterize intrusions that can detect many kinds of intrusion according to the number of packets. This paper demonstrates the usefulness of such a method to differentiate abnormal data from normal data. Experiments are performed using intrusion detection data sets and tested for validation. Some results are reported along with analysis and concluding remarks.
Protecting manufacturing control systems from network-based attacks is becoming more important with increasing demand on vertical integration of plant floor and corporate networks. This paper discusses the differences between corporate networks and plant floor networks from a security perspective, and outlines some requirements for the plant floor network security. The challenges for the development of security solutions for automation and control systems are highlighted. Several on-going standardization activities are introduced. A conceptual reference framework for protecting industrial automation and control systems is proposed and features for detection and prevention of network-based attacks are disclosed. This paper ends with the discussion of possible solutions for the existing vulnerability examples.
At Intelligent Methods for information audit system is hot spot in the field of network security, and application of pattern recognition and data mining in information audit system is world widely concerned and worldwide studying. This paper introduces some improvements of the Vector Space Model algorithm based on a advanced research on current algorithm. The results of our experiments prove that it promotes the precision of text categorization.
Deep Packet Inspection (DPI) is a critical function in network security applications such as Firewalls and Intrusion Detection Systems (IDS). Signature based scanners used in DPI apply multi-pattern matching algorithms to check whether the packet payload or flow content contains a specified signature in a signature set. Existing multi-pattern matching algorithms sacrifice memory space to achieve better performance. In this paper a novel fast multi-pattern matching algorithm, the Hash Boyer-Moore (HBM) Algorithm, is presented, which reduces the memory footprint of the heuristic table using a hash function and adds another heuristic table to reduce the false-positive ratio. Analyses and simulations show HBM offers higher speed and lower memory cost than some existing algorithms. The HBM algorithm was implemented on the Intel IXP 2400 Network Processor (NP) platform and experiments show suitable performance results in a Gigabit Ethernet LAN environment.
Queuing delay plays an important role in network security and Internet applications, since it carries information about traffic characteristics and congestion properties on the route. This paper presents a novel approach to measure and estimate end-to-end one-way queuing delay in the network. In contrast to existing methods that are trying to remove clock offsets and skews, the novel approach does not require any synchronization between the two measurement ends. Pairs of probe packets are sent from the source to the destination and intra-gaps between the probes are separately measured at each end. By performing an iterative Fourier-to-time reconstruction algorithm on the measured intra-gaps, distribution of the end-to-end one-way queuing delay is estimated. The iteration procedure is proved to be efficient and robust, and simulations are conducted in this paper to validate the novel approach.
Network Anomaly Detection is a critical task to ensure network security. With increasing network traffic, detecting network anomaly would require solving a large-scale pattern classification problem that often contains millions of training vectors. Each training vector may represent a particular signature of network traffic pattern and some of them may be linked to security breaching activities that need to be detected and eradicated. In this paper, a popular statistical learning algorithm known as the Support Vector Machine (SVM) was consider to solve the network anomaly detection problem. However, it is well known that SVM would require excessively long computing time and exceedingly large amount of memory when number of training vectors becomes huge. Hence, direct application of the standard SVM algorithm to solve large-scale network anomaly detection problems is impractical. In this paper, based on computational geometry theory, a new algorithm called convex-hull SVM (CH-SVM) was proposed, which can yield the same solution as original SVM while using significantly less training data, and hence less computing time. Then experiments were done on KDD'99 intrusion detection dataset to compare the performance of the proposed algorithm to a standard SVM and observed reduced training time and improved classification accuracy.
The proposal of network security situational awareness (NSSA) research means a breakthrough and an innovation to the traditional network security technologies, and it has become a new hot research topic in network security field. Combined with evolutionary strategy and neural network, a quantitative method of network security situational awareness is proposed in this paper. Evolutionary strategy is used to optimize the parameters of neural network, and then the evolutionary neural network model is established to extract the network security situational factors, so the quantification of network security situation is achieved. Finally simulated experiment is done to validate that the evolutionary neural network model can extract situational factors and the model has better generalization ability, which supports the network security technical technologies greatly.
Multi-sensor Data Fusion and Network Situation Awareness are emerging technique in the field of network security and they help administrators to be aware of the actual security situation of their networks. This paper mainly focuses on heterogeneous multi-sensor data fusion and Situation Awareness. We adopted Snort and NetFlow collector as two sensors to gather real network traffic and fused them use Multi-class Support Vector Machines that could solve a multi class problem. In order to avoid dimension disaster, we employed an effective feature reduction approach to decrease the dimension of input vector and the computation time of Support Vector Machines that improved fusion performance and real time characteristic. Our framework is proved to be feasible and effective and has better performance than Neural Network through a series of experiments that using real network traffic.
There are various security sensors deployed in the network to protect the information assets from destruction. These sensors produces a huge amount of alerts in different event granularities and semantics. Such a huge amount of alerts is hard to be comprehended; as a result, a timely response to the attacks is difficult. Thus, a better technique for alert analysis and management is imperative for promoting the network security. We have developed a two layered PA-based (primititive attack-based) correlation approach to tackle this problem. The first layer does PA construction by integrating related alerts into proper PAs. The second layer is the attack subplan-based correlation layer, which does attack scenario correlation from recognized PAs by employing attack subplan templates t guide the correlation process. This paper focuses on the second layer, by discussing how to automatically construct attack subplan templates, how to eliminate invalid attack subplan templates, how the attack subplan templates can direct the process of scenario correlation, and how a proper fusion process can reduce superfluous attack information. Our experiments show this correlation approach can effectively unearth the attack strategies fo the attackers, reduce the cognitive loading of the network manager in comprehending alert reports, and also relieve him from the time consuming and skillfull analysis task required in manually analyzing alerts to construct correlation knowledge.
The problem of building privacy-preserving accountability systems is long-standing and has been extensively studied by the network research community. We observe that blind signatures have potential to form critical building blocks of network security protocols, where an authority needs to vouch for the legitimacy of a message but there is also a need to keep the ownership of the message secret from the authority. Different forms of blind signature constructions exist in the literature and have found valuable use in areas such E-Cash technology and E-voting schemes. However, conventional blind signatures are quite heavyweight and thus, a direct application of these traditional signatures face scalability and performance challenges. In this paper, we present a novel third-order linear feedback shift register (LFSR) sequence-based, 2-party signature scheme, EGCLFSR, following a well-known variant of the generalized ElGamal signature scheme. Using EGCLFSR, and following fundamentals of a well known blind signature, originally used for E-Cash systems, we present an efficient blind signature BCLFSR (also the first blind signature based on LFSR sequences), which can serve as a protocol building block for privacy-preserving accountability systems. We perform a theoretical analysis including correctness and security of BCLFSR and also present a performance (computation and communication costs, storage overhead) comparison of the proposed scheme with well-known traditional constructions.
Nowadays, one of the big issues on the Internet is botnet. It causes various problems such as information search and theft, denial of service attack, sending SPAM e-mail, and so on. This paper describes a methodology to capture malware and track botnets. We have developed our own honeypot system for capturing malware based on honeypot technology [1,2]. Although there are some efforts for botnet monitoring [3-5], we have been improving and upgrading our monitoring system since Jan 2005. In addition, this paper mentions comparing between botnet activity and the data from fixed point Internet observation that is also unique system.
In restructured power systems, independent system operators (ISOs) or regional transmission organizations (RTOs) execute the security-constrained unit commitment (SCUC) program to plan a secure and economical hourly generation schedule for the daily/weekly-ahead market. As the size of ISO/RTO increases, the current SCUC algorithms could face critical challenges ranging from modeling accuracy to calculation complexity. This paper introduces an efficient fast SCUC (F-SCUC) for large-scale power systems. Main components used in the proposed approach include single-hour unit commitment with network security, single-hour unit commitment adjustment, unit commitment, economic dispatch, and hourly network security check. A reasonable and operational strategy for fixing and unlocking the generating units is explored. The iterative SCUC solution is controlled efficiently to accelerate the execution. Finally, the quality of F-SCUC solution is improved by executing a quick MIP-based SCUC solution according to the F-SCUC results. A 1168-bus system with 169 thermal units, a 4672-bus system with 676 thermal units, and two large systems with 1352 and 2704 thermal units are analyzed to exhibit the effectiveness of the proposed approach. </para>
As there is a rapid rampancy of malicious intrusion, people pay more attentions to the network security. This paper presents a new method based on Hilbert-Huang Transform(HHT) to characterize intrusions that can detect many kinds of intrusion according to the number of packets. This paper demonstrates the usefulness of such a method to differentiate abnormal data from normal data. Experiments are performed using intrusion detection data sets and tested for validation. Some results are reported along with analysis and concluding remarks.
This paper presents a Danger Theory inspired multi-agent fusion model. Danger Theory is a relative new research subject in Artificial Immune System (AIS). It presents new viewpoint and explains some problems in traditional immune theory. This paper describes the principle of Danger Theory, analyzes the differences between it and traditional theory, and also constructs a multi-agent fusion model for network security assessment. It apply the concepts of Danger Theory to fusion model, such as danger signal, danger zone and Antigen Presenting Cells (APC), which are benefit for multi-agent fusion model. An example is presented to illustrate the operation process of the fusion model.
Current network forensics systems are static and not real-time. In order to overcome the shortages, a dynamical network forensics model based on artificial immune theory and multi-agent theory, referred to as DNF, is introduced here. Comparing with traditional computer forensics methods, the new method provides the capacity that gathering real-time evidence dynamically as soon as network intrusions take place and saving the evidence in a safe way to prepare for the collection and analysis of the original evidence. In this paper, architecture of the model and the definitions of its components inspired by the immunity theory are given out. The experiment shows that it is able to insure the authenticity, integrality and validity of the digital evidence, and it is a new method for dynamic computer forensics.
While many protocols for sensor network security provide confidentiality for the content of messages, contextual information usually remains exposed. Such information can be critical to the mission of the sensor network, such as the location of a target object in a monitoring application, and it is often important to protect this information as well as message content. There have been several recent studies on providing location privacy in sensor networks. However, these existing approaches assume a weak adversary model where the adversary sees only local network traffic. We first argue that a strong adversary model, the global eavesdropper, is often realistic in practice and can defeat existing techniques. We then formalize the location privacy issues under this strong adversary model and show how much communication overhead is needed for achieving a given level of privacy. We also propose two techniques that prevent the leakage of location information: periodic collection and source simulation. Periodic collection provides a high level of location privacy, while source simulation provides trade-offs between privacy, communication cost, and latency. Through analysis and simulation, we demonstrate that the proposed techniques are efficient and effective in protecting location information from the attacker.
Current increase in network bandwidth raised an aggressive challenge in network security, and stateful packet inspection based security systems is playing a more and more important role. Recent advances in scheduling theory show that it is possible to reduce the expected mean response time of a queuing system, simply by changing the order in which we schedule the requests according to the job size, which is so called size-based scheduling policy. In this paper, we start by an analysis of connection sojourn time distribution of network traffic. Based on this analysis, first we design a two level session table in order to avoid session table explosion. Then we propose a connection scheduling policy in stateful packet inspection systems called LASF (Least Attained Sojourn First). We show that our policy can improve mean response time and flow throughput especially when system is overloaded. Finally we assess the costs of LASF in terms of unfairness.
"The various vendors of traditional security systems implement some portions of security functionality and security services, relying on their own, mostly non-structured, functional architecture. The key to successful network security implementation is to consider security management as an evolving integrated part of the overall network architecture. This tutorial provides an overall complete view of network security issues. The workshop covers a concise discussion on the discipline of cryptography-covering algorithms and protocols underlying network security applications, security mechanisms, digital signatures, and key exchange. Internet security vulnerabilities, gateways, firewalls and their limitations, IPSec and key management for network layer security, TLS, SSH and transport layer security, virtual private networks, and secure remote access are covered. In particular, security standards for authentication and privacy in both wire-line and wireless environment are described in detail. Implementation of IPSec as a security standard that affords the capability of securing communications across a local-area network (LAN), across public and private wide-area networks (WANs), and across the Internet is described. This course offers a framework for security architecture to implement the full range of functionality of security for enterprise networks."
The need to use quantitative methods to detect intrusion is increasing due to the high false positive and false negative rates of existing Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS). Most network security techniques employed by the IDS and IPS depend mainly on packet behavior for detection. This work applies a quantitative approach based on Maximum A Posteriori (MAP) detection rules with the hope of reducing the high false positive and false negative rates. The entire system has been represented by a mathematical model of a discrete binary communication channel having two possible input messages and two possible output symbols. The network under study is assumed to have only one entry point (sender) for now, with a number of nodes (receivers). Also, all normal operational packets are referred to as normal packets and any other packets are referred to as abnormal packets. The analysis strategy used here is anomaly detection. The developed algorithm initially calculates the a priori probabilities for the normal and abnormal packets both at the sender and entry ends. These values are further used in finding the threshold probabilities to be compared to the corresponding probabilities of future incoming packets. MATLAB was used in coding the developed algorithm. This work will be expanded by modeling the entire system as a continuous binary communication channel and also by considering multiple entry points as future works, with the intension of improving the results obtained so far.
Homes of the future are expected to be "digital" homes wherein almost all aspects of a home could be controlled using advanced computer technology. One important component of the digital home is the home security system (HSS) that helps to monitor and control various elements related to home security. HSS may be classified into different types including passive systems (PS), phone based systems (PBS), web-based systems (WBS), and hardware-based systems (HBS), where each system differs in performance, controllability, security, and cost. For the success of an HSS the most important characteristics, or non-functional requirements, are performance and security - however, it is almost impossible to have both high security and high performance at the same time. In order to achieve both these non-functional requirements at the same time we adopted the concept of perimeter security from the fields of homeland security and network security to HSS wherein different areas of the house are divided into perimeters and each perimeter uses a different type of HSS to provide security. Preliminary results indicate that both high security and high performance can be achieved using the perimeter-based approach; however, this achievement comes at the cost of high price-tag for the development and maintenance of the system, and an increase in the overall complexity of the system.
Network security systems become an essential part of many network structures in both company and university domains. These systems however require a higher semantic level of network traffic analysis like stateful filtration or TCP stream reassembling. This paper deals with an architecture of flexible FlowContext platform capable of stateful processing at multigigabit speeds. It allows to analyze and process incoming network traffic with a flow-based approach rather than packet-based one. The proposed architecture is flexible in supporting wide range of applications, allows performance scalability and state information consistency checking. The advantages and flexibility of proposed platform is demonstrated on several network security applications.
This Tutorial is devoted to the emerging topic in domain of modern e-business systems - a computer network security based on Public Key Infrastructure (PKI) systems. We consider possible vulnerabilities of the TCP/IP computer networks and possible techniques to eliminate them. We signify that only a general and multilayered security infrastructure could cope with possible attacks to the computer network systems. We evaluate security mechanisms on application, transport and network layers of ISO/OSI reference model and give examples of the today most popular security protocols applied in each of the mentioned layers. We recommend secure computer network systems that consist of combined security mechanisms on three different ISO/OSI reference model layers: application layer security based on strong user authentication, digital signature, confidentiality protection, digital certificates and hardware tokens, transport layer security based on establishment of a cryptographic tunnel between network nodes and strong node authentication procedure and network IP layer security providing bulk security mechanisms on network level between network nodes. User strong authentication procedures based on digital certificates and PKI systems are especially emphasized.
Wireless network security based on encryption is widely prevalent at this time. However, encryption techniques do not take into account wireless network characteristics such as random bit errors and fading. For example, we note that properties such as the avalanche effect that make a block cipher secure also cause them to be sensitive to bit errors. Therefore, there is a fundamental trade-off between security and throughput in encryption based wireless security. Further, if there is an adversary with a certain attack strength present in the wireless network, we see an additional twist to the security-throughput trade-off issue. In this paper, we proposed a framework called opportunistic encryption that uses channel opportunities (acceptable signal to noise ratio) to maximize the throughput subject to desired security constraints. To illustrate this framework and compare it with some current approaches this paper presents the following: (a) mathematical models to capture the secuity-throughput trade-off; (b) adversary models and their effects; (c) joint encryption and modulation (single and multi-rate) optimization; (d) the use of forward error correcting (FEC) codes to protect encrypted packets from bit errors; and (e) simulation results for Rijndael cipher. We observe that opportunistic encryption produces signficant improvement in the performance compared to traditional approaches.
Traditionally, the only standard method of testing that has consistently provided high fault coverage has been scan test due to the high controllability and high observability this technique provides. The scan chains used in scan test not only allow test engineers to control and observe a chip, but these properties also allow the scan architecture to be used as a means to breach chip security. In this paper, we propose a technique, called Lock &#x00026; Key, to neutralize the potential for scan-based side-channel attacks. It is very difficult to implement an all inclusive security strategy, but by knowing the attacker, a suitable strategy can be devised. The Lock &#x00026; Key technique provides a flexible security strategy to modern designs without significant changes to scan test practices. Using this technique, the scan chains are divided into smaller subchains. With the inclusion of a test security controller, access to subchains are randomized when being accessed by an unauthorized user. Random access reduces repeatability and predictability making reverse engineering more difficult. Without proper authorization, an attacker would need to unveil several layers of security before gaining proper access to the scan chain in order to exploit it. The proposed Lock &#x00026; Key technique is design independent while maintaining a relatively low area overhead.
Security is an intense discussed topic for industrial automation networks. This article will give an overview for security relevant considerations. The foundation of the network security is the analysis of the communication relations. Typical communication scenarios are listed with their constraints and characteristics. This paper will propose a first approach for a structured methodology to classify communications to be able to deduce security relevant use cases and proper methods.
Recent work in network security has focused on the fact that combinations of exploits are the typical means by which an attacker breaks into a network. Researchers have proposed a variety of graph-based analysis approach, and there is often a lack of logical formalism. This paper describes a new approach to represent and analyze network vulnerability. We propose logical exploitation graph, which directly illustrate logical dependencies among exploitation goals and network configure. Our logical exploitation graph generation tool builds upon LEG-NSA, a network security analyzer based on Prolog logical programming. We demonstrate how to reason all exploitation paths using bottom-up and top-down evaluation algorithms in the Prolog logic- programming engine. We show experimental evidence that our logical exploitation graph generation algorithm is very efficient.
A honeypot apparatus, as a perspective security technology has proven itself worth deploying by various malicious records made. The next step in deploying the technology can be an independent hardware device with the incorporated honeypot behaviour. Such a solution would bring an ease in deployment together with a high throughput it would be able to support to the area of network auditing and monitoring. Initial investigation and implementation steps have been conducted. A flexible base for a honeypot platform intended to be implemented on a modern Field Programmable Gate Array device, as a potential destination technology, has been developed. Correspondent results with a relevant set of details are being presented together with future perspectives and further investigation and deployment potential. No similar attempts have been documented.
Network Management is one of the most discussed topics in the networking fraternity. The efficiency of the network management suit is measured by the number of parameters/components handled by the application while making a decision. In the case of internet enabled aircrafts, along with network security, even the aircraft safety needs to be considered as a factor while designing the Network Management suit. This requires the Network Management suit to monitor/analyze the aircraft related data (avionics data, physical security parameters, etc.) while determining the proper functioning of the overall system. In this paper, the authors present a framework for Network Management suit that, along with network health, monitors inputs from Avionics, video surveillance system, weather monitoring system, and manual pilot alarms and based on the situation, reconfigures the onboard networking devices to stream appropriate flight critical data to the ground station. The proposed framework attempts to provide a comprehensive user interface for the flight health monitoring crew with all the relevant data.
The database security is usually connected with the network security. This passage will discuss and study the resolution of threat model based on STRIDE. The B/S architecture is used to illustrate that what threat this architecture will be faced with. Aimed at the threats, the responding resolutions are also given to build more safe and credible system.
The shift to network centric warfare highlights the deficiencies of existing reliable transport protocols in tactical network environments. Existing transports are designed upon a set of assumptions that are violated by noisy, wireless tactical networks where end-to-end encryption and authentication must be maintained. Protocols such as TCP that are designed for low-corruptive-loss environments provide abysmal performance when deployed in tactical network environments. TCP adaptations that address corruptive loss rates generally violate tactical-network security requirements. As part of a DARPA-funded effort to dramatically improve reliable-transport performance in tactical network environments, Architecture Technology Corporation (ATC) has analyzed the tactical network environment and has examined the behavior of various reliable transport protocols within such environments.
Spyware has become a significant threat to most Internet users as it introduces serious privacy disclosure, and potential security breach to the systems. It has not only utilized critical areas of the computer system to survive reboots, but also grown resilient against current anti-spyware tools; they are capable of self-healing themselves against deletion. Because existing anti-spyware tools are stateless in the sense that they do not remember or monitor the spyware programs that were deleted, they fail to remove self-healing spyware from the system completely. This paper proposes a stateful approach that is based on characterizing spyware invasion as a trust information flow problem, and implements STARS (stateful threat-aware removal system), which is a tool that at run time monitors critical system behaviors, and ensures that removed spyware programs do not reinstall themselves, to enforce information flow policy in the system. If a reinstallation (self-healing) is detected, STARS infers the source of such activities, and discovers additional ldquosuspiciousrdquo programs. Experimental results show that STARS is effective in removing self-healing spyware programs that resist removal by existing anti-spyware tools.
As network security gains more and more attention, network intrusion prevention systems (NIPS) gradually become one of the most important network systems used in modern Internet environment. The demand for high performance NIPS is driven by the growing bandwidth available in the last mile WAN links as well as the increasing complexity of packet inspection. In this paper, we propose an adaptive clustering scheme to scale the throughput of in-line devices. The proposed scheme aggregates the processing power of multiple in-line devices in a cluster by making incoming traffic self-dispatched in a transparent fashion, and incorporates a traffic redistribution mechanism that keeps the load of each device balanced. The cluster is also able to tolerate device failures so that devices in the cluster can be inserted or removed while the system is running. Based on the designed architecture, we deploy Snort, which is a well-known and popular NIPS, on each device of the cluster and implement all the proposed mechanisms as kernel modules over embedded Linux. According to the results of performance evaluation, we successfully build a high performance, load balancing, and fault tolerant NIPS by means of the proposed mechanisms over the designed in-line device cluster.
Provisioning of network-centric context information such as current network load, available services, etc., to hosts, network nodes and users needs to be complemented with an adequate access control in order to preserve the security and privacy requirements of context owners. The gathering, processing and distributing of context information make up the main functionality of the ambient networks information service infrastructure. In this paper we describe the adoption of a security domain framework to control authorised access to the context information collected by such infrastructure. It is anticipated that a balance between performance and guaranteed level of security can be achieved this way. Examples of context retrieval scenarios within and across administrative boundaries are provided. We discuss the relevance of our work with respect to the activities of the IEEE 802.21 standardisation group, namely illustrating how to restrict access to its information services using the security domain concept.
With the incoming of information era, internet has been developed rapidly and offered more and more services. However, intrusions, viruses and worms follow with the grown of internet, spread widely all over the world within high speed network. Although many kinds of intrusion de- tection systems (IDSs) are developed, they have some dis- advantages in that they focus on low-level attacks or ano- malies, and raise alerts independently. In this paper, we give a formal description about attack patterns, attack transition states and attack scenarios. We proposed the system architecture to generate an attack scenario database correctly and completely. We first clas- sify and extract attack patterns, then, correlate attack pat- terns with pre/post conditions matching and. Moreover, the approach, Attack Scenario Generation with Casual Rela- tionship (ASGCR), is proposed to build an attack scenario database Finally, we present the combination of our attack scenario database with security operation center (SOC) to implement the related components concerning alert inte- grations and correlations. It is shown that our method is better than CAML [4] since we can generate more attack scenarios effectively and correctly to help system managers to maintain network security. Keywords Attack scenario database, security operation center, alert correlation, attack pattern.
Intranet security is the hotspot of network security researching field nowadays. This paper discusses the load balancing based on the research and analysis of present Intranet security audit systems. It puts forward a system structure and model of the Intranet security audit systems based on load balancing. The model gathers dynamic load balancing of the cluster servers in security audit to effectively solve the bottleneck problem of the distributed audit system which has control centre. Finally, it describes the design and the implementation of the key system module. Keywords: Intranet security, audit, dynamic load balancing, distributed, cluster
To discover information endangering network security, an exploration method of network security information based on immunology is proposed. Antibody and antigen in biological immune system are used to denote keywords hid in HTML pages. Proposed method generates new antibodies to recognize unknown keywords through immune rules. By mechanisms of self-learning and evolution, antibody families form to represent distribution of different network security information. All antibodies in the same family are totalized to evaluate the information distribution degree. Simulation experiments show that the proposed method is able to find useful information threatening network and improve the intelligent degree of evaluating security of Web information.
This paper presents the global synthetical processing of network security events in network security intelligent centralized management system, which aims to build up an integrated management platform for security events management. The system framework and many processing methods are introduced in this paper. The experiment has shown that the processing process can efficiently reduce the amount of security events and improve the quality of security events.
The rapid proliferation of wireless networks and mobile computing applications has changed the landscape of network security. Anomaly detection is a pattern recognition task whose goal is to report the occurrence of abnormal or unknown behavior in a given system being monitored. This paper presents a dynamic anomaly detection scheme that can effectively identify a group of especially harmful internal attackers - masqueraders in cellular mobile networks. Our scheme uses the trace data of wireless application layer by a user as feature value. Based on this, the use pattern of a mobile's user can be captured by rough sets, and the abnormal behavior of the mobile can be also detected effectively by applying a roughness membership function with both the age of the user profile and weighted feature values. The performance of our scheme is evaluated by using a simulation.
Intrusion prevention systems (IPSs) have long been proposed as a defense against attacks that propagate too fast for any manual response to be useful. In an important class of IPSs, the host-based IPSs, honeypots are used to collect information about attacks. The collected information will then be analyzed to generate countermeasures against the observed attack. Unfortunately, these IPSs can be rendered useless by techniques that allow the honeypots in a network to be identified ([1, 9]). In particular, attacks can be designed to avoid targeting the identified honeypots. As a result, the IPSs will have no information about the attacks, and thus no countermeasure will ever be generated. The use of honeypots is also creating other practical issues which limit the usefulness/feasibility of many host-based IPSs. We propose to solve these problems by duplicating the detection and analysis capability on every protected system; i.e., turning every host into a honeypot. In this paper, we will first lay out the necessary features of any scheme for such large scale collaboration in intrusion prevention, then we will present a framework called collaborative intrusion prevention (ClP) for realizing our idea of turning every host into a honeypot.
This paper describes a reconfigurable architecture based on field-programmable gate-array (FPGA) technology for monitoring and analyzing network traffic at increasingly high network data rates. Our approach maps the performance-critical tasks of packet classification and flow monitoring into reconfigurable hardware, such that multiple flows can be processed in parallel. We explore the scalability of our system, showing that it can support flows at multi-gigabit rate; this is faster than most software-based solutions where acceptable data rates are typically no more than 100 million bits per second.
This paper compares different defense strategies against various attacks utilizing a dynamic game theoretic data fusion framework for Cyber network defense. In our game theoretic framework, alerts generated by intrusion detection sensors (IDSs) or intrusion prevention sensors (IPSs) are fed into the data refinement (Level 0) and object assessment (LI) data fusion components. High-level situation/threat assessment (L2/L3) data fusion based on Markov game model and hierarchical entity aggregation (HEA) are proposed to refine the primitive prediction generated by adaptive feature/pattern recognition and capture new unknown features. A Markov (stochastic) game method is used to estimate the belief of each possible Cyber attack pattern. Game theory captures the nature of Cyber conflicts: determination of the attacking-force strategies is tightly coupled to determination of the defense-force strategies and vice versa. A software tool is developed to demonstrate and compare the performance of different defense strategies used in game theoretic high level information fusion for Cyber network defense situations and a simulation example shows the enhanced understating of Cyber-network defense.
The idea of using power system models in problem solving and decision analysis is really not new, and is certainly not tied to the use of computers. At some point, all of us have used a modeling approach to make a decision. A typical decision tree is used to identify a strategy most likely to reach a goal. This paper describes two methodologies for finding optimal solutions to budgeting and planning processes with the use of decision trees. The first methodology describes Load Patterns for building better load profiles for generators that may help a Plant Manager to optimally budget the operating costs of a generating unit. A second methodology shows a process of screening Transmission alternatives for identifying optimal solutions based on cost and network security. Decision Trees help make good decisions, but cannot guarantee that good outcomes will always occur as a result of those decisions. However, using a structured modeling approach to decision making should produce good outcomes more frequently than making decisions in a more haphazard manner.
Both symmetric and asymmetric key algorithms are used to secure a network. Asymmetric key approach involves greater time complexity and hence prone to higher power consumption. Symmetric key cryptography, on the other hand, involves a key exchange phase. In this paper, we propose an authentication technique for new nodes to an ad-hoc network that would also extract the advantages of both symmetric and asymmetric key cryptography while avoiding the adversities. In the proposed collaborative technique, the new node is not only authenticated, but the request-reply messages have been used to generate a secret key at the two ends. The proposed algorithm estimates the distance of a node X from the power loss of a message from X. Both the sender and the receiver separately compute the same secret key based on this distance. The proposed algorithm does not use GPS and identifies location of nodes by distance and direction only.
It is necessary, though very challenging, to integrate available security technologies in a coherent manner to provide a multi-level security management in an ad hoc mobile network. This paper proposes and analyzes a policy-based network security management mechanism that consists of responsive/preemptive defensive strategies and a "Ripple Effect" security policy activating mechanism. This scheme activates different security policy modes and security levels to provide network protections efficiently.
Distributed networked-controlled-systems (NCS) are a multidisciplinary effort whose aim is to produce a network structure and components that are capable of integrating sensors, actuators, communication, and control algorithms in a manner to suit real-time applications. They have been very popular and widely applied for many years now due to the rapid advancements in data and communication wireless technologies. There are many challenges to be overcome in order to put such a heterogeneous system together. Key issues to be considered are network delay, data sensitivity and information security. This paper characterizes a wireless distributed NCS, a testbed called iSpace based on these key factors. We integrated static network security algorithms DES and 3DES with the NCS testbed iSpace - a multidisciplinary network based robot navigation system - and characterized it on the basis of bandwidth requirement, data classification and data sensitivity, network delay effect on the system performance. The paper demonstrates through results that a dynamic optimization is required between network security for reliability and time-sensitivity of the NCS. Future work in dynamic optimization in security is suggested.
This paper extends our game theoretic situation awareness and impact assessment approach for cyber network defense to consider the change of threat intent during cyber conflict. From a perspective of data fusion and adaptive control, we use a Markov (stochastic) game method to estimate the belief of each possible cyber attack pattern. With the consideration that the parameters in each game player's cost function is not accessible to other players, we design an adaptation scheme, based on the concept of Fictitious Play (FP), for the piecewise linearized Markov game model. A software tool is developed to demonstrate the performance of the adaptive game theoretic high level information fusion approach for cyber network defense and a simulation example shows the enhanced understating of cyber-network defense.
In this paper, we discuss the challenges and opportunities of using cross-layer techniques for enhancing wireless network security. Cross-layer approach has gained considerable interest in performance optimization due to their design advantages. While the architectural modification introduced by the inter-layer interactions show promising results on overall network performance, there is also a growing concern on their limitations. Here, we investigate the impact of cross-layer techniques on security and network performance. An in depth understanding of the strength and weakness of cross-layer methods is necessary in designing robust architectures. To this end, we evaluate different cross-layer architectures and analyze their efficiency in intrusion detection systems.
With the emergence and widespread use of digital technology at all levels, security of systems and the networks that they connect to has taken on paramount importance. The past decade has seen widespread development, innovation, and growth within the DoD, government, and commercial communities of public key infrastructure (PKI) to meet these security needs. PKI is a robust technology, supporting numerous applications including user and computer authentication, secured communications, data encryption, and digital signature. Concurrent with the emergence of PKI has been the explosion of network security issues related to the problem of proper user authentication and the protection of data confidentiality, integrity, and availability. With the expansion of ethernet protocol-based networking into the wireless realm via the IEEE 802.11 standards family, the problems of network security and user authentication have finally come to a head. In a wireless local-area network (WLAN), protecting the network requires protecting not only the communications medium, but also the authentication medium. This paper examines the problems inherent in securing an IEEE 802.11 WLAN, and discusses how PKI-based authentication of hosts and users can be used with the new WiFi protected access (WPA) and WPA-2 protocols to achieve a highly secure wireless network environment. It discusses issues, including digital certificate format, remote authentication dial-in user service (RADIUS) authentication, and client platform compatibility, that must be addressed as part of a successful implementation.
One of the most challenging network security concerns for network administrators is the presence of rogue access points. Rogue access points, if undetected, can be an open door to sensitive information on the network. Many data raiders have taken advantage of the undetected rogue access points in enterprises to not only get free Internet access, but also to view confidential information. Most of the current solutions to detect rouge access points are not automated and are dependent on a specific wireless technology. In this paper, we present a rogue access point detection approach. The approach is an automated solution which can be installed on any router at the edge of a network. The main premise of our approach is to distinguish authorized WLAN hosts from unauthorized WLAN hosts connected to rogue access points by analyzing traffic characteristics at the edge of a network. Simulation results verify the effectiveness of our approach in detecting rogue access points in a heterogeneous network comprised of wireless and wired subnets.
The current best practices for securing large scale SOA systems on GIG computer networks rely on layered computer network defenses based on firewalls, IDS/IPS appliances, high assurance guards, PKI certificates and VPN technologies. These are costly to procure, deploy and maintain, and require a significant amount of physical and personnel security to mitigate their shortcomings. Furthermore they do not easily facilitate the recent presidential orders to share information. This paper describes a security system based on classic reference monitor and cryptographic techniques that is cost effective, scalable, easy to use, and will allow information sharing in an MLS or MSL type of communications environment.
Distributed network-control-systems (D-NCS) are a network structure and components that are capable of integrating sensors, actuators, communication, and control algorithms to suit real-time applications. They have been gaining popularity due to their high potential in widespread applications and becoming realizable due to the rapid advancements in embedded systems, wireless communication and data transfer technologies. This paper addresses the issue of D-NCS information security as well its time-sensitive performance with respect to network security schemes. We use a wireless network based, robot navigation path tracking system called Intelligent Space (iSpace) as a D-NCS test bed in this paper. The paper classifies the delay data from every NCS module (sensors, actuators and controllers). We define performance parameters for this NCS test bed. Various system factors including network delay, system gain, affecting these performance parameters are recognized. Network security algorithms DES and 3DES are integrated with the application to secure the sensitive information flow. Standard statistical approach such as 2<sup>k</sup> factorial experiment design, analysis of variance, hypothesis testing is used to study and estimate the effect of each factor on the system performance especially security features. Thorough experimental results, tables of detailed characterization and effect estimate analysis is presented followed by the discussion on the performance comparison of NCS with and without wireless security.
Tactical mobile ad hoc wireless networks can provide flexibility, agility, and mobility for dynamic network-centric warfare. They are designed to function without fixed infrastructure support. But, this design makes network security attacks hard to detect and control. For instance, an attacker could compromise one portion of the network by inducing a worm and then move to another portion of the network and replay the attack without being detected. In this paper, we describe a policy management system (PMS) that provides ubiquitous and consistent security policy control despite adverse conditions such as node mobility, node failures, network partitions, a compromise of the routing infrastructure, a high packet loss rate, and ongoing cyber attacks. We give performance results in a 47-node emulated network. Key challenges in tactical networks include a lossy environment with bit error rates as high as 0.001 and bandwidth as low as 500Kbits/s. TCP backs off in the face of packet loss and hence behaves poorly in such networks. Tactical network are also subject to cyber attacks that have the potential even to compromise the routing infrastructure. Our PMS architecture mitigates these issues by using a transport protocol that has two components, one oriented toward normal operation when the routing infrastructure is functioning and one oriented toward limited, but robust, operation even when the routing infrastructure has been compromised. In normal operation we use a reliable UDP protocol (RUDP) and in times of compromise we use a flooding protocol.
Network-based intrusion detection systems (NIDSs) are one component of a comprehensive network security solution. The use of IPsec, which encrypts network traffic, renders network intrusion detection virtually useless unless traffic is decrypted at network gateways. Host-based intrusion detection systems (HIDSs) can provide some of the functionality of NIDSs but with limitations. HIDSs cannot perform a network-wide analysis and can be subverted if a host is compromised. We propose an approach to intrusion detection that combines HIDS, NIDS, and a version of IPsec that encrypts the header and the body of IP packets separately ("Two-Zone IPsec"). We show that all of the network events currently detectable by the Snort NIDS on unencrypted network traffic are also detectable on encrypted network traffic using this approach. The NIDS detects network-level events that HIDSs have trouble detecting and HIDSs detect application-level events that can't be detected by the NIDS.
Military Network Operations (NetOps) consist of three main components: network management, network defense (information assurance), and content staging (information dissemination management). NetOps capabilities include organizations, procedures, and technologies required to manage, defend, and utilize a network. Conventionally NetOps components are regarded as functionally orthogonal and operationally independent. However, based on our experience<sup>1</sup> with the future Army tactical network that consist of mobile ad hoc networks (MANETs), we believe these components should be integrated to enhance NetOps in MANET environments. We envision that with the creation of an over-arching architectural framework to underlie NetOps components, future Army tactical network operations will provide better support to network centric warfare. Also, the focus of NetOps should transition from maintaining network operations to providing optimal communications. In this paper, we describe issues and challenges associated with integrating NetOps components for future Army tactical networks, and discuss path forward.
High-interaction honeynets are extraordinary intrusion intelligence tools. Unfortunately, their power has come at a significant cost. Forensic analysis can be cumbersome and labor intensive, management burdens are often onerous, and compromised honeynets present a risk of being used to stage further attacks. In short, these high-interaction intelligence tools have lacked operational agility. We present a novel approach to honeypot architecture that combines advances in virtualization, low-level introspection, signature generation, and forensic analysis to construct a real-time, high-interaction intrusion intelligence and prevention tool.
The security of the Border Gateway Protocol (BGP), the standard routing protocol for inter-domain routing in the Internet, has been an increasingly intent focus of concern for many years. In the last few years, the IETF (Internet Engineering Task Force) has undertaken several efforts to specify security solutions for BGP. The IETF has chartered work to analyze BGP vulnerabilities, to investigate threat models for routing protocols in general, and to specify requirements for BGP security. Most recently, the IETF has begun work on securing the origination of prefix advertisements. Unauthorized origination of prefix advertisements has been the most frequent root cause of publicly noted routing incidents for the history of the Internet. Also, authorization of origination of prefix advertisements is the common basis to almost all suggested security solutions. Because the Global Information Grid (GIG) uses BGP for inter-domain routing and VPN provisioning just as the commercial Internet, the GIG is subject to the same BGP vulnerabilities. Security solutions to the vulnerabilities of BGP should be deployed and used in the GIG, in order to protect the GIG routing system internally and at interconnection points to the Internet. It is important that the GIG be aware of standards based security solutions and be involved in the development of new standards. This paper discusses the current status of BGP security standards, how they are evolving, and how the security of BGP relates to the GIG.
With Department of Defense (DOD) and Office of Management and Budget (OMB) mandating a migration to IPv6, Army installation Directors of Information Management (DOIMs) are beginning to feel pressured to implement IPv6 on their post networks. Unfortunately, little practical guidance exists to inform the DOIMs the procedures necessary to prepare their networks for IPv6. More is needed than simply enabling IPv6 on local area network (LAN) routers and switches. Many infrastructure components must be upgraded as well, including Domain Name Service (DNS), directory services, security, and network management. Besides the physical hardware and software components, local policies need to be defined for network security and IPv6 addressing, and steps need to be taken to provide training for administrators and registration of IPv6 pilots. This paper summarizes the steps necessary to enable an IPv6 pilot on an Army post. It attempts to address the question, "What is necessary to do today to prepare for an IPv6 application on the post network tomorrow." It will cover the procedures necessary to implement IPv6 on an Army base, including covering current status of commercial product support and Government testing of IPv6 capabilities.
In this paper, an internet based embedded network monitoring system is proposed for renewable energy systems. By using a low cost network communication module (RCM 3700) as a web server, one can achieve better network security, lower power consumption, compact size, and easier to use as compared with a PC based one. Also, Java language is chosen for designing a dynamic webpage to graphically display various real time waveforms of the controlled system for multi-user at the same time. As an illustrative example, a small scale wind power generation system equipped with an EZDSP 2812 controller is adopted for demonstration. In addition, an FPGA EC10 is implemented as a bidirectional communication interface for coordinating the asynchronous data transmission modes. Experimental results from the constructed prototype verify that the proposed monitoring system can indeed achieve the desired function.
USB Key represents one of the smallest computing platforms today. With the development of USB Key chips, more and more security features will be supported by USB Key. Network security middleware is a Java and HTTP-based development framework on USB Key. Developers can develop and on line load network security&#x02013;related programs on this framework.&#x0A0;&#x0A0;End users can use this USB Key to establish secure network connections. In our design, USB Key became an active secure device on network. Design and implementation of this middleware is given in this paper. At fist, the software architecture and security features of this middleware are discussed in detail, which includes how to implement HTTP server, CGI, security proxy etc. on USB Key. Secondly, application scenarios and security analysis of mutual authentication between Web server and client on this middleware are given. At last, advantages of this middleware and what to do further are given in conclusion and future work.
The rapid proliferation of computer networks has changed the prospect of network security. An easy accessibility condition cause computer network&#x02019;s vulnerable against several threats from hackers. Threats to networks are numerous and potentially devastating. Up to the moment, researchers have developed Intrusion Detection Systems (IDS) capable of detecting attacks in several available environments. A boundlessness of methods for misuse detection as well as anomaly detection has been applied. Many of the technologies proposed are complementary to each other, since for different kind of environments some approaches perform better than others. This paper presents a taxonomy of intrusion detection systems that is then used to survey and classify them. The taxonomy consists of the detection principle, and second of certain operational aspects of the intrusion detection system.
Security trust model is an important research topic for the P2P application. This paper firstly analyzes the existing P2P network security trust models, and then puts forward a new P2P recommendation trust model based on the network of social relationships. The model provides different types of services to show the user&#x02019;s interest in different areas, and gives recommendation based on the interested area. Feedback mechanism and punishment mechanism have been used to improve the model. The simulation shows that the model can truly evaluate entity behavior and effectively isolated from malicious nodes.
The volume of web based malware on the Internet keeps rising despite huge investments on web security. JavaScript, the dominant scripting language for web applications, is the primary channel for most of these attacks. In this paper, we describe research into the design and implementation of new web client protection system based on code instrumentation techniques. This system combines traditional static analysis techniques with a dynamic HTML, CSS and JavaScript code runtime monitoring agent to offer an efficient, easily deployable, policy driven framework for improved user protection. Rewriting and runtime monitoring are based on providing safe equivalents of JavaScript code constructs known to containin securities and hence exploitable by malicious web applications. As a demonstration of the practical capabilities of our framework, we also include a case study attack and empirical analysis of some of its various aspects across 1000 home pages belonging to the most popular web sites on the Internet.
Organizations face increasing challenges in addressing and preventing computer and network security incidents. There are financial consequences from security incidents. These include lost time and resources used during recovery, possible theft of personal and/or proprietary information, and reputational damage that may negatively impact stock prices or reduce consumer confidence in a company.&#x0A0;&#x0A0;Being able to understand and predict trends in computer and network security incidents can aid an organization with resource allocation for prevention of such incidents, as well as evaluation of mitigation strategies. We look at using time series models with a large set of security incident data. We examine appropriateness of the data for modeling and consider needed transformations. Parameter search and model selection criteria are discussed. Then, forecasts from time series models are compared to forecasts from Non-Homogeneous Poisson Process (NHPP) software reliability growth (SRG) models.
In this paper we present empirical results and speculative analysis based on observations collected over a two month period from studies with two high interaction honeynets, deployed in a corporate and an SME (Small to Medium Enterprise) environment, and a distributed honeypots deployment. All three networks contain a mixture of Windows and Linux hosts. We detail the architecture of the deployment and results of comparing the observations from the three environments. We analyze in detail the times between attacks on different hosts, operating systems, networks or geographical location. Even though results from honeynet deployments are reported often in the literature, this paper provides novel results analyzing traffic from three different types of networks and some initial exploratory models. This research aims to contribute to endeavours in the wider security research community to build methods, grounded on strong empirical work, for assessment of the robustness of computer-based systems in hostile environments.
Network malicious activity can be collected and reported by various sources using different attack detection solutions. The granularity of these solutions provides either very detailed information (Intrusion Detection Systems, honeypots) or high-level trends (CAIDA, SANS). The problem for network security operators is often to select the sources of information to better protect their network. How much information from these sources is redundant and how much is unique? The goal of this paper is to show empirically that while some global attack events can be correlated across various sensors, the majority of incomingmalicious activity has local specificities. This study presents a comparative analysis of four different attack datasets offering three different levels of granularity: 1) two high interaction honeynets deployed at two different locations (i.e., a corporate and an academic environment); 2) ATLAS which is a distributed network telescope from Arbor; and 3) Internet Protect&#x02122; which is a global alerting service from AT&#x00026;T.
This paper presents a trust-based model for secure network admission and trustworthy routing. Firstly, a dynamic routing trust model based on trust relationship and routing behaviors is brought forward. The model not only supports trust network connection, but also gives a comprehensive assessment to the network security and efficiency affected by router access according to routers' interactive behaviors. Secondly, the trust degree between any two routers can be notified to all by a dependable flooding algorithm. Finally, a new routing strategy called by "minimal trust routing entropy" is proposed, which is good at avoiding nodes with low trust while taking into account the routing efficiency. This model can be used for the next-generation connection-oriented Internet which is controllable and trustworthy.
Penetration testing is an important branch of network security evaluation, which aims at providing all-round investigation to find the vulnerabilities and security threats in systems and networks. Former penetration testing platforms lack the adaptability when applied to different types of systems or networks, and the manual tests which are prevailed in those platforms are usually long and complex processes. In this paper we focus on the improvement of penetration testing platforms and strategies, and we propose a novel penetration testing platform based on a recently launched LiveDVD system&#x02014;SolarSword. We also discuss the design and implementation details of this new platform, and use a real penetration test case study to demonstrate its advantages over former platforms. The platform provides an automatic, easy-to-deploy methodology for penetration testing process, and overcomes the obvious drawbacks of former penetration testing platforms.
Steganography, as one of alternative techniques for secure communications, has drawn more and more attentions. This paper presents a covert communication model based on least significant bits (LSB) steganography in Voice over IP (VoIP). The model aims at providing nice security of secret messages and real-time performance that is vital for VoIP. Therefore, we employ a simple encryption of secret messages before embedding them. This encryption strikes a good balance between adequate short-term protection for secret messages and low latency for VoIP. Furthermore, we design a structure of embedded messages. It can provide flexible length and avoid effectually both extraction attack and deceptive attack. We evaluate the model with ITU-T G.729a as the codec of the cover speech in StegTalk, our platform for study on covert communications theory in VoIP. In this case, the proposed model can provide two optional covert transmission speeds, i.e. 0.8kb/s and 2.6 kb/s, where the maximum payload ratio is 99.98&#x025;. The experimental results show that our method has negligible effects on speech quality and well meets the real-time requirement of VoIP.
With the rapid development of computer science, its education mode also changes a lot. Teaching and learning are not restricted by the physical distance with the help of the networking. Remote education becomes more popular because of its convenience and resource saving. Meanwhile, the network security is a hotspot in the world for the rapid and wide using of the network. In this paper, we share our experiences on a practice of information security theory curriculum based on the networking technology. We developed a remote education platform which has been used in our curriculum. The statistic results show that most students are quite satisfied with this remote education platform and show great learning effect on the information security theory curriculum.
With the improvement of software security, attacks based on RPC vulnerabilities declined, however, atta-cks based on client application software vulnerabiliti-es have increased. Such client application software includes web browsers, Email client and Office. The spread of malware using these software vulnerabiliti-es has become a severe threat to today&#x02019;s Internet. In allusion to this kind of threat, this paper designed an Internet malware collecting system based on client-side honeypot. This system can not only collect malware but also detect malicious website. It uses a unique network crawler based on client-side attack techniques to collect source of URL, and it collects URL and attachments from emails, then it creates software processes to open URL or files, and uses a device-drive monitor to detect malicious behaviors. It gives an alarm and locates the malicious file, and sends the malware coming through the Internet to the collecting server. We introduce the design and imple-ment of this system and give the results.
The attack graph, a typical model-based method, is widely used in the field of network security evaluation. The biggest disadvantage of attack graph method is its exponential growth of the state space. This paper presents an efficient algorithm based on the malefactor&#x02019;s access level vector in every host of the network to generate a reduced attack graph in polynomial compute complexity. In this algorithm, the state space is reduced to , where is the number of nodes and is the whole number of vulnerabilities in the network. We also present a standard method to generate attack templates from the vulnerabilities.
Intrusion Detection Systems (IDSs) aim at detecting malicious or unauthorized activities targeting a network and its resources. Usually engineered as self-contained applications, current IDSs are limited in protecting collaborative computing environments, like grids, whose security amplifies the concerns about intrusions and motivates advanced organizing paradigms and technical solutions for effective attack detection. We envision a new generation of IDSs defined by a set of services supporting security managers in improving the overall network security. Specifically, we show how to model the ID processes as a set of plans that a security manager may go through on a network of cooperative nodes interacting with one another in order to offer or to ask for services. Services correspond to specialized ID tasks and encapsulate problem solving and simulation capabilities. Complex ID activities are expressed by workflows, the focus being on flexibility, reuse and interoperability of ID services. Some implementation hints are suggested.
Along with the increasing wide applications of computer and network technologies, the security problems of information systems are becoming more complicated. Most computer systems have some kind of a security flaw that may allow outsiders or legitimate users to gain unauthorized access to sensitive information. Among the network exploits, Distributed Denial of Service (DDoS) attack is a large-scale, coordinated attack on the availability of services of a victim system, launched indirectly through many compromised computers on the Internet. Intrusion detection systems (IDS) are network security tools that process local audit data or monitor network traffic to search for specific patterns or certain deviations from expected behavior which indicate malicious activities against the protected network. In our study, we proposed and tested four different distributed intrusion detection methods to detect DDoS attacks in the MIT DARPA LLDOS 1.0 dataset. Currently, all of our methods use the alarms generated by Snort, a signature-based network IDS. We used mobile agents in three of the methods on the Jade (Java Agent Development Framework) platform in order to reduce network bandwidth usage by moving data analysis computations to the location of the intrusion data. Based on reliability, network load and mean detection time values of each, one of the methods is shown to be better than the others.
Distance education has witnessed a tremendous growth in the past decade. Advances in Internet technologies have made it possible to deliver not only lectures, but also hands-on labs remotely. Virtualization technology enables multiple virtual machines and their applications to run simultaneously on a single physical computer. This eliminates the need to have multiple physical machines host diverse operating systems typically deployed in remote network security labs. In preparation for a course on intrusion detection systems (IDS), the instructor creates pre-configured virtual machines and network trace files for students&#x2019; use. These virtual machines are then installed by the students on their personal computers at home and used to conduct lab exercises. The virtual lab approach is different from the centralized remote laboratory because students run the lab on their own computers and do not depend on the remote servers. Additionally, the virtual environments allow rapid changes to be made to the lab exercises or environments thus allowing instruction with up-to-date technologies. Furthermore, the burden of maintaining centralized physical labs has been lifted from the institution&#x2019;s shoulders. Our approach to decentralized virtual lab-based distance education has been well received by our students.
Exposing students to research topics enhances their learning experience. It is therefore desirable for instructors to include a research component into their courses. The courses most prone for incorporating research components are the upper level undergraduate and graduate courses. However, this becomes challenging in the case of introductory courses, where learning new programming skills is the focus. In this case, the research component needs to satisfy two key conditions: to be easy enough to implement, and to have a fast learning curve. In this paper we describe the introduction of a research component into an introductory web development course (upper undergraduate and lower graduate) for software engineering students. In this context we present a web security research application that is successful at keeping unwanted malicious attacks from happening on web services. It can be implemented using basic web development skills and fast to learn and rationale. Due to the interdisciplinary nature of the research problem, the students have been exposed to techniques and research topics in several other fields. The projects outcome including student experiences conducting user studies and human surveys are discussed in the paper.
An Intrusion Detection System (IDS) is a crucial element of a network security posture. One class of IDS, called signature-based network IDSs, monitors network traffic, looking for evidence of malicious behavior as specified in attack descriptions (referred to as signatures). Many studies have reported that IDSs can generate thousands of alarms a day, many of which are false alarms. The problem often lies in the low accuracy of IDS signatures. It is therefore important to have more accurate signatures in order to reduce the number of false alarms. One part of the false alarm problem is the inability of IDSs to verify attacks (i.e. distinguish between successful and failed attacks). If IDSs were able to accurately verify attacks, this would reduce the number of false alarms a network administrator has to investigate. In this paper, we demonstrate the feasibility of using a data mining algorithm to automatically generate IDS verification rules. We show that this automated approach is effective in reducing the number of false alarms when compared to other widely used and maintained IDSs.
The packet filter rules of firewall are established according to the needs of network security, while to manage filter rules becomes more and more complicated, and easy to make mistakes, especially in enterprise network. In order to implement correct policies, the firewall filter rules should be checked and organized carefully. This article studied the relations between firewall filter rules, defined each kind of unusual situation, and through the expert system language Visual Prolog realized the function: to inspect the accuracy of the filter rules to deal with filter rules, to inspect redundancy and so on. It has made positive effect on enhancing the intelligence of the firewall.
Firewalls are core elements in network security, the effectiveness of firewall security is dependent on configuring firewall policy correctly.Firewall policy is a lower-level policy which describes how firewall actually implements security policy. Security policy is a higher-level policy which defines the access that will be permitted or denied from the trusted network. Compare with software engineer, security policy is a design, firewall policy is a set of codes. It is useful to discover inconsistency between security policy and firewall Policy. In this paper, we present a method of verifying consistency between security policy and firewall policy, which applies the idea of model checking. First of all,two policies and the consistency are represented with logic programs. Then the verification is applied by testing whether the logic formula of the consistency is satisfied in the semantics of the logic programs. Furthermore, We prove that the method has an unique answer which can be computed in polynomial time.
Device and technology oriented policy making excessively depends on the knowledge and experiences of managers, but ignores the requirements and effects of the application environment. So the policy making is incomplete and liable to make mistakes. To solve the problem, hierarchy network security system model is designed. Policy making and representation methods are proposed based on the system modeling, which make the policy making is not limited to a single device and the only one security function. Upon the method, the policy auto making is implemented, and the policy correctness and integrity are insured, which decrease the burden of the manager and the possibilities of mistaking. According to the refinement of the policy basic attributes, multi-level policy representation described in BNF (Backus-Naur Form) form is promoted, which makes policy representation friendlier and more operable.
Considering honeypot host performs high inveiglement to worms and its data control policy--"come in easily, out strictly", this paper presents a worm propagation model based on two-factor model in the network which distributed honeynet have been deployed, then discusses worm propagation trend and the impact of honeypot on worm spread; over simulation experiment, we find that distributed honeynet is of great significance in worm warning and restraining in large-scale networks. At last, we give a correspondingly control strategy based on the mechanism of worm information-sharing and immunization and honeypot host can divides network into many parts for its data control policy under distributed honeynet, prevent worm from spreading in large-scale networks and ensure network security effectively.
The validity of the security policy has important impacts on the safety performance of network information system. For purpose of verifying it effectively, an assessment model of network security policy based on security capability is proposed. The relationship of defense methods, application targets, and information security attribute characteristics is analyzed based on the establishing of security domain and security policy, and the network security capability of security policy is evaluated. Result shows that the model can effectively reflect the protect ability of security policy. It provides a new solution and reference for assessing and adjusting the network security policy, so as to better ensure system security.
How to quantify the threat probability in network security risk assessment is an important problem to be solved. Most of the existing methods tend to consider the attacker and defender separately. However, the decision to perform the attack is a trade-off between the gain from a successful attack and the possible consequences of detection; meanwhile, the defender&#x02019;s security strategy depends mostly on the knowledge of the intentions of the attacker. Therefore, ignoring the connections between the attacker and defender&#x02019;s decisions does not correspond to reality. Game theory is the study of the ways in which strategic interactions among rational players produce outcomes with respect to the utilities of those players. In this paper, a novel Game Theoretical Attack-Defense Model (GTADM) which quantifies the probability of threats is proposed in order to construct a risk assessment framework. According to the cost-benefit analysis, we define the method of formulating the payoff matrix; the equilibrium of the model is also analyzed. In the end, a simple scenario is presented to illustrate the usage of GTADM in the risk assessment framework to show its efficiency.
This paper proposes an Achievement-based educational framework for teaching the network security course. We designed around a series of counter-activities of experiment, in which the students are divided into several groups, and their task is to attack and anti-attack each other, the winner will have a high score and a feeling of achievement. In contrast to just e-learning, the novel achievement-based teaching interventions motivates the learners&#x02019; interest and passion, enables the students to participate actively in the learning process and collaborate in groups and so achieve the expected learning objectives, which can be proved by the positive student feedback and a post-test comparison with the same academic year.
How to quantify the threat probability in network security risk assessment is an important problem to be solved. Most of the existing methods tend to consider the attacker and defender separately. However, the decision to perform the attack is a trade-off between the gain from a successful attack and the possible consequences of detection; meanwhile, the defender&#x02019;ssecurity strategy depends mostly on the knowledge of the intentions of the attacker. Therefore, ignoring the connections between the attacker and defender&#x02019;s decisions does not correspond to reality. Game theory is the study of the ways in which strategic interactions among rational players produceoutcomes with respect to the utilities of those players. In this paper, a novel Game Theoretical Attack-Defense Model (GTADM) which quantifies the probability of threats is proposed in order to construct a risk assessment framework. According to the cost-benefit analysis, we define the method of formulating the payoff matrix; the equilibrium of the model is also analyzed. In the end, a simple scenario is presented to illustrate the usage of GTADM in the risk assessment framework to show its efficiency.
In the last few years, the number and impact of security attacks over the Internet have been continuously increasing. To face this issue, the use of Intrusion Detection Systems (IDSs) has emerged as a key element in network security. In this paper we address the problem considering a novel statistical technique for detecting network anomalies. Our approach is based on the use of different families of Markovian models (namely high order and non homogeneous Markov chains) for modeling network traffic running over TCP. The performance results shown in the paper, justify the proposed method and highlight the improvements over commonly used
In order to assess the security of network information system, many graph-based approaches have been proposed. Attack Graph is the most influential one. But attack graphs grow exponentially with the size of the network. In this paper, we propose an improved access graph based model to analyze network security. As a complement to the attack graph approach, the access graph is host-centric approach, which grows polynomially with the number of hosts and so has the benefit of being computationally feasible on large networks. Compared with the related works, our approach improves in both performance and computational cost.
With the widespread application of large and complicated network, network safety has become an important issue. In this paper, a security operation center (SOC) concept based on multi-sensor data fusion technology is presented from the viewpoint of the network security. A structure of a SOC system based on radial basis function neural (RBFN) network is proposed, and the detailed method of data fusion in SOC is discussed. A prototype of SOC system is developed according to this structure of the SOC, Experimental results indicate that the SOC system based on RBFN network can increase greatly the correctness of detection intrusion and decrease the rate of false positive.
Nowadays the network information security is an important research direction on data communications. In order to solve a number of shortcomings in information security password system, in this paper, a new criterion based on image segmentation on face recognition is applied to network security. Firstly, in proposed approach, the original images are divided into modular images, which are also called sub-images. Then, the well-known Fisher linear discriminant analysis is directly employed to the sub-images obtained from the previous step. Finally the recognition results are obtained by the general minimum distance classifier. The ORL face image database is made use of to simulate, and when the training sample is only one, the recognition rate of 90.83 percent is achieved.
High end network security applications demand high speed operation and large rule set support. Packet classification is the core functionality that demands high throughput in such applications. This paper proposes a packet classification architecture to meet such high throughput. We have implemented a Firewall with this architecture in reconfigurable hardware. We propose an extension to Distributed Crossproducting of Field Labels (DCFL) technique to achieve scalable and high performance architecture. The implemented Firewall takes advantage of inherent structure and redundancy of rule set by using our DCFL Extended (DCFLE)algorithm. The use of DCFLE algorithm results in both speed and area improvement when it is implemented in hardware. Although we restrict ourselves to standard 5-tuple matching, the architecture supports additional fields. High throughput classification invariably uses Ternary Content Addressable Memory (TCAM) for prefix matching, though TCAM fares poorly in terms of area and power efficiency. Use of TCAM for port range matching is expensive, as the range to prefix conversion results in large number of prefixes leading to storage inefficiency. Extended TCAM (ETCAM) is fast and the most storage efficient solution for range matching. We present for the first time a reconfigurable hardware implementation of ETCAM. We have implemented our Firewall as an embedded system on Virtex-II Pro FPGA based platform, running Linux with the packet classification in hardware. The Firewall was tested in real time with 1 Gbps Ethernet link and 128 sample rules. The packet classification hardware uses aquarter of logic resources and slightly over one third of memoryresources of XC2VP30 FPGA. It achieves a maximum classification throughput of 50 million packets corresponding to16 Gbps link rate for the worst case packet size. The Firewall rule update involves only memory re-initialization in software without any hardware change.
Under the application background of network security evaluation, a mechanism for situation element extraction based on Particle Swarm Optimization (PSO) and Fuzzy Neural Network (FNN) is proposed. Firstly, the input dataset of historical situation element is pre-fuzzed and then transformed into fuzzy logic rule which can be mapped between neural network layers. Meanwhile, PSO is used to achieve global optimization of BP network&#x2019;s weight value and threshold, and then an extraction model based on FNN and PSO (PSO-FNN) is built up. Experiment results prove that this extraction mechanism is effective in situation element extraction, and can be applied in the area of situation extraction for network security situation awareness.
Due to various network intrusions, network security has always been a main concern of the network administrator. However, nowadays traditional security tools like IDSs, firewalls etc cannot play the roles of effective defense mechanisms. Instead, they only generate elementary alerts to form alert flooding and they often have high false alerts rates. Moreover due to their weak collaboration-awareness, they cannot detect large distributed attacks such as a DDoS attack. In this paper, we present an efficient and effective model for collaborative alerts analyzing. Our system enhances the alert verification using assets&#x02019; contextual information. By applying alert fusion and using a precisely defined knowledge base in the correlation phase, it also provides a method to get general and synthetic alerts from the large volume of elementary alerts. Moreover, this system is able to reconstruct the attack scenarios for multi-step attacks. Experiments show the system can effectively distinguish false positives, detect and predicate large-scale attacks in their early stage.
Along with the extensive application of the network, network security has received increasing attention recently.This paper researches on the network security risk evaluation and analyze the traditional risk evaluation methods,then proposes a new network security risk evaluation method based on Support Vector Machine (SVM) and Binary tree. Unlike the traditional risk evaluation methods, SVM is a novel type of learning machine technique which developed on structural risk minimization principle.SVM has many advantages in solving small sample size, nonlinear and high dimensional pattern recognition problem.The principles of SVM and binary tree are introduced in detail and apply it into network security risk assessment,it divided risk rate of network security into 4 different rates and more .Compare to ANN about the Classification precision,Generalization Performance,learning and testing time,it indicates that SVM has higher Classification precision,better generalization Performance and less learning and testing time,especially get a better assessment performance under small samples. It indicates that SVM has absolute superiority on network security risk evaluation, the validity and superiority of this method is approved through the experiment.
NetGAP is used to physically isolate the internal and external networks. Its session strides over both sides of the network. Based on the characteristics of agent, this paper designs several kinds of agent components with file accumulation, content inspection, file transmittance, protocol stream control and so on. Through these agent components it can easily establish a new connection, dealt with protocol events and close a connection process in a session. According to practice it indicates the session mechanism basing on agent can effectively increase the security of the internal network and keep the adaptability of network security system.
Classification of intrusion attacks and normal network traffic is a challenging and critical problem in network security. Many classification methods for intrusion detection have been proposed, but there are few algorithms that are capable of distinguishing among the various attacks and normal connections effectively. This paper presents an effective intrusion detection algorithm based on Conscientious Rival Penalized Competitive Learning (CRPCL), which improves RPCL to set a conscientious threshold to restrict a winner that won too many times and to make every neural unit win the competition at near ideal probability. To assess the classification performance of the algorithm, it is compared with some well-known classifiers. The experiments with KDD CUP 99 data indicate that this method has good performance and can improve the detection quality effectively.
Intrusion detection has become the important component of the network security. Many intelligent intrusion detection models are proposed, but the performance and efficiency are not satisfied to real computer network system. This paper extends these works by applying a new high efficient technique, named Twin Support Vector Machines ( TWSVM), to intrusion detection. Using the KDD'99 data set collected at MIT&#x02019;s Lincoln Labs evaluates the performance and efficiency of the proposed intrusion detection models. The experimental results indicate that the proposed models based on TWSVM is more efficient and has higher detection rate than conventional SVM based model and other models.
Intrusion detection plays an important part in network security today. BP algorithm which is an algorithm in artificial intelligences can also be used in IDS. This paper make use of improved BP algorithm compared to the traditional one, the new algorithm brings us higher speed in constringency and more precise in detection .The new algorithm solves the difficulties of real-time system.
A risk situation evaluation model for network security is proposed based on analyzing security threat status. This method first perceived and identified the characteristics of critical risk factors, such as asset, vulnerability and threat, from multi-security-sensors. Then a quantitative evaluation algorithm was presented to estimate current threats and potential threats. By analyzing their threat degree, the real-time status and dynamic evolvement trend of security risk was revealed. Sequentially, the security administrator can comprehend the situation about both the single entity and the overall network then timely and effectively make security decisions. Finally this method was illustrated and validated in an emulated network environment.
Secure interaction between trusted-domains is a major problem on network security. Combining with the advantages of Role-based Access Control (RBAC) and the existing authentication technique on crossing the trusted-domain, this paper proposes a privilege management model on crossing the trusted-domains (PMCT) which is suitable for large scale distributed network. Role recommending policy and unilateral role mapping policy are proposed to back the safe access between two trusted-domains. By using set theory and the logic of predication, we give formal description of the PMCT Model systematically, then put forward a suit of rules on privilege and safety, and analyze the security characteristic of PMCT Model. Finally, the safe application of this model is demonstrated by showing how it can be used in an access control system.
Firewalls are core elements in network security, the effectiveness of firewall security is dependent on configuring firewall policy correctly. Firewall policy describes the access that will be permitted or denied from the trusted network. In corporate network, several firewalls are set up and administrated by different teams. The consistency between those firewall policies is crucial to corporate network security. In this paper, we present a method of discover the inconsistencies between two given firewall policies. Firstly, Firewall policies are represented with logic formulas, the consistency regarded as a property of firewall policies is represented with logic formulas. The inconsistencies are discovered from the semantics of the union of the logic formulas. Furthermore, we prove that the method has the unique answer which can be computed in polynomial time. Firewall policies often need to be updated, as networks evolve. Many firewall policy errors are caused by the side effects of policy updates. Our method can be used to verify the updates of firewall policy by comparing the policy before changes and the policy after changes.
The threat of Denial of Service flooding attacks in the Internet is rapidly increasing. Especially the use of techniques that allow attackers to hide their attack traffic raises concerns: attack distribution and rotation in botnets to obfuscate senders, low-rate bandwidth attacks, and attacks that mimic realistic patterns such as flash crowds. The defense against such attacks is limited due to a deadlock: the attacks must be stopped inside the network, but the network is unable to distinguish legitimate and unsolicited traffic. In contrast, end systems may distinguish legitimate users from bots, but are unable to stop the attacks inside the network. This paper advocates for a joint end system-network defense to address such attacks in the future. Edge-based Capabilities (EC) is a novel framework that combines end-to-end authentication with network-based control. Applications authenticate legitimate senders and issue capabilities to tag their packets, and the network filters out untagged packets. This paper describes the mechanisms that make EC a secure, efficient, and scalable solution. Moreover, we argue that EC is an attractive solution because it can be incrementally deployed and because it provides the right incentives to users, servers, and ISPs.
With the development of VoIP (Voice over IP) service, new security threats are expected to be appeared. However, existing IP network security solutions can not detect new VoIP specified network threats because they can not reflect characteristics of VoIP. In this paper, we propose a novel system that can monitor VoIP service and detect VoIP network threats practically. The proposed system collects attributes of VoIP traffic based on NetFlow, and executes monitoring and detecting based on statistic and behavior.
Network security risk assessment depends on the prediction of attacker&#x02019;s behavioral decision. In computer network attack and defense area, this kind of decision is the optimal judgment for attackers and defenders themselves in consideration of the opponents&#x02019; strategy spaces. Thus, The attack and defend behavior can be seen as a game process. In this paper, we studied how to bring Game Theory into the research area of network security risk assessment. First, we analyze the concept and the process of risk assessment to find the combining point where game theory can be used in network security risk assessment. Then we present a risk assessment framework based on game theory, and set up a risk assessment system using this framework. We emphatically introduce GTADM (Game Theoretical Attack-Defense Model) and HRCM (Hierarchical Risk Computing Model) in the system, and provide detailed analysis and specification by a scenario.
Today, honeypot operators are strongly relying on network analysis tools to examine network traces collected intheir honeynet environment. The accuracy of such analysis depends on the ability of the tools to properly reassemble streams especially TCP sessions. Network forensics analysis quality is tight to those tools and we evaluated widely used network analysis tools. We pinpoint TCP reassembly errors with their causes and propose algorithms and analytical techniques to measure them in order to improve network forensic analysis.
In order to remedy defects of the current network security evaluation systems, a novel evaluation algorithm, called quantitative network security situation evaluation based on the classification of attacks, is presented. Using this method, the traditional risk assessment is combined with network environment factors such as the network running status, asset security characteristic etc., and several quantitative indexes are extracted based on the analysis of factors which can affect the LAN's security situation. Then, evaluations are done based on the classification of attacks. Experiment results show that the novel method can provide situation information which is more objective and detailed so the security administrator can understand the LAN's security situation more clearly.
This article introduced the present situation of network information security and the present domestic network face existence main threat, as well as the prevente security threat main safety work classification and detailed explanation; Based on this elaboration this article main analysis each kind of invasion examinational technology characteristic and function superiority; In view of the current world network application scope expanding and popularization, network users increase in the number of technological capability and users of the network security question also get attention. Especially in the network attack, have many network researchers actively looking for research and solve the network intrusion detection technology of vulnerability; Finally the basis discusses the invasion examinational technology which analyzes to the showing off correspondence evaluating indicator as well as the present network invades the limitation which the examinational technology exists, and the summarizes manifests the network security invasion examination engineering research the necessity.
Mitigating misbehavior in mobile ad hoc networks (MANET) requires effective intrusion response systems. In this work, we present an intrusion response scheme that is tailored to support the infrastructure less nature of MANETs. We propose a geographic solution towards excluding misbehaving nodes which is robust against address spoofing from the attacker. In particular, we investigate how an adaptive transmission power can be used to physically keep communication away from misbehaving nodes. We present different strategies for adapting transmission power taking into account effects of asymmetric links, and we provide a detailed performance evaluation based on a series of simulation studies. Our results show that the proposed solution significantly reduces the artificial packet loss that is introduced by geographic intrusion response strategies. Yet, we further observe side-effects of an adaptive transmission power on standard (non power-aware) MANET routing protocols.
Nowadays, one of the reasons for the lack of legal sanctions taken against attackers is that the collection and analysis of forensic evidence is very troublesome and time-consuming. There are many research results about events correlation but not directly suitable for network forensics. The work presented in this paper is based on an idea to collect the evidences from multiple network sensors and analyze them to improve the quality of forensic evidence automatically. This paper discusses the issues of log evidence first. The framework of IEAAS (Automated Analysis System of Intrusion Evidences) is illustrated with LCA (Log Collection Agent) in network sensors and multiple modules in IEAAS. Analysis mechanism is discussed, particularly the improved aggregation algorithm and evidence preservation method are described. Then a series of experiments are performed to validate our method on actual attack network environments of CERNET. The results of experiments show that our approach is practical and effective for dynamic forensics to augment the computer crime investigators efforts.
The diversification and intelligent of hackers attacks makes it hard for single intrusion detection measure to attain favorable detection result. Therefore, it becomes one of the new hot spots in current research that how to combine multiple security measures to provide the network system more effective protection. A new computer information security protection system based on data fusion theory is proposed in this paper. Multiple detection measures are fused in this system, so that it has lower false negatives rate and false positive rate as well as better scalabilities and robust.
The diversification and intelligent of hackers attacks makes it hard for single intrusion detection measure to attain favorable detection result. Therefore, it becomes one of the new hot spots in current research that how to combine multiple security measures to provide the network system more effective protection. A new computer information security protection system based on data fusion theory is proposed in this paper. Multiple detection measures are fused in this system, so that it has lower false negatives rate and false positive rate as well as better scalabilities and robust.
The Intrusion Detection Systems (IDSs) play an important role in monitoring networks, but they lack abilities in automated intrusion response; the Intrusion Prevention Systems (IPSs) can guard networks in entrance, but they have no abilities in response inner-intranet attacks; many proposals focus on solutions in automated intrusion response, but they still have various problems existed, the main challenge is the accurate measurement of those related factors. This paper presents a virtual inline technique which is based on the technique of the Man in the Middle attack (MITM), it combines the NIDS and NIPS together in providing all-wave protection to networks. This technique integrates the advantages of both IDSs and IPSs, and avoids their shortages; it also avoids those problems baffle our researchers in this field. Empirical experiments show this technique is practicable.
Network Intrusion Detection System (IDS), as the main security defending technique, is second guard for a network after firewall. Since it can discern and respond to the hostile behavior of the computer and network resource, it is a hot area for research network security nowadays. Data mining technology is applied to the network intrusion detection, and Precision of the detection will be improved by the superiority of data mining. In this paper, there is an example running to contract two algorithms. The result is that the fuzzy rule mining algorithm is more convenient than Apriori algorithm to mine mass network log database.
We provide a framework for implementing IPSec security services in a well-structured functional architecture based on a layered functional architecture for network security management [1]. The proposed architecture is modular, and supports open standards and interfaces, and implements the security services of IPSec as an integrated solution under a unified security management system.
Client honeypots are security devices designed to find servers that attack clients. High-interaction client honeypots (HICHPs) classify potentially malicious web pages by driving a dedicated vulnerable web browser to retrieve and classify these pages. Considering the size of the Internet, the ability to identify many malicious web pages is a crucial task. HICHPs, however, present challenges: They are slow and tend to miss attacks. For researchers to address these shortcomings, they need methods for evaluating HICHPs. This paper (1) presents an evaluation method called the True Positive Cost Curve (TPCC), which makes it possible to evaluate and compare HICHPs in an operating environment, but also allows an operator to tune HICHPs within a specific operating environment; (2) presents improvements on the way HICHPs visit web pages and evaluates them with the TPCC method; and (3) discusses the impact of time bombs on the performance of HICHPs in an operating environment and the ability to tune an HICHP for optimal performance with the help of the TPCC.
Threat analysis gives how potential adversaries exploit system weakness to achieve their goals. It identifies threats and defines a risk mitigation policy for a specific architecture, functionality and configuration. In a threat analysis security metrics are a challenging requirement in order to determine the status of network security performance and to further enhance it by minimizing exposure to considerable threats and vulnerabilities. In this paper the authors propose a generic methodology for threat analysis and security metrics in order to prioritize threats and vulnerabilities and proceed with security enhancement planning in Personal Networks (PNs).
Node behavior profiling is a promising tool for many aspects in network security. In our research, our goal is to couple node behavior profiles with statistical tests with a focus on enterprise security. Limited work has been done in the literature. In this paper, we first propose a correlation based node behavior profiling approach to study node behaviors in enterprise network environments. We then propose formal statistical test on the most common behavior profiles which is able to detect worm propagation. In our initial studies, we evaluate our profiling and detection schemes using real enterprise data (LBNL traces). The results show that the correlation based node behavior profiling approach can capture normal behaviors of different types. Consequently, the behavior profiles are promising for anomaly detection when coupled with statistical methods.
One of the most important issues in traditional and modern networks architecture is security. Data integrity and authenticity are the most critical points that a network security model should protect and ensure. Authentication, Authorization and Accounting model (AAA Protocol) is one of the most portable security concepts. Authentication acts providing proof of authenticity for stored data and verifying proof of authenticity for received. Authorization acts providing privileges to those clients that present specific credentials. Accounting acts collecting accounting metrics for two reasons; to forward them to the billing server for billing results, and to keep them saved locally for the procedure of trend analysis. In this paper, the generic AAA architecture is introduced, personalized and practically designed for usage in modern networks. The most efficient way, using supported protocols and cryptographic algorithms, for administrating AAA in practice, is proposed for mobile networks or administrative domains. In this work, a web based application scenario using the AAA protocol is proposed, with the server-side developed in PHP, SQL and Java, implementation platform.
This paper designs an active Voice over Internet Protocol (VoIP) security defense model, aiming to cope with various attack problems that the network is confronting. Based on the model, a dynamic self-adaptive diffluence algorithm that can be applied to the VoIP Gateway was proposed. It combines advantages of UBS selection strategy and weighted Bayesian classification algorithm, the algorithm can accurately, quickly complete the task of intrusion detection through using few high-quality training sample for learning. Experimental results demonstrate the algorithm is more accurate under the smaller training set.
We discuss the need for adaptive load control in network security environments in order to cope with the increasing bandwidth requirements. In earlier work, we developed a simple model to study the behavior of feedback loops for self-configuring security environments. We primarily considered the traffic between the monitoring probes, the IDS systems, and associated firewalls. We now enhanced the model to study fully self-organized network security environments in a simulation model. First simulation results outline both the feasibility of the general approach and the possibilities of the simulation model.
This paper enlarges previous works of the authors related to the security of a high-interaction honeypot. The challenge is to have a Security Property Language (SPL) for defining the required properties for controlling the activities between processes and resources. That language must authorize the definition of security properties related to confidentiality, integrity and availability. Moreover, that SPL must be able to enforce the security of target Operating Systems. It is an open problem not only regarding the security of Operating Systems but also regarding the security of high-interaction honeypots. That paper shows that existing approaches really fail to manage a large range of security properties. The first reason is that a SPL is really missing to express and enforce a large set of security properties. The second reason is that protection and detection approaches fail to manage a large set of security properties. Our paper proposes PIGA-Protect a new approach to control the system calls in order to guarantee the requested security properties.
In the IEEE Transactions on Industrial Electronics, vol. 55, no. 6, Juang  proposed a password-authenticated key agreement scheme using smart cards. Although the scheme of Juang  has many benefits, we find that it suffers from three weaknesses: 1) inability of the password-changing operation; 2) the session-key problem; and 3) inefficiency of the double secret keys. Therefore, we propose an improved scheme to overcome the weaknesses and maintain the benefits of the original scheme. In addition, our improved scheme reduces the storage and computation costs on the smart card compared with the scheme of Juang  We believe that our improved scheme is more suitable for real-life applications than that of Juang
According to mathematical analysis, using primary components and factor analysis (PCFA) and its advantages, the main function indexes of intrusion detection system in network security are analyzed and assessed. Based on PCFA model and the scores by the experts, the authors computed some results that can reflect some aspects about main function indexes of intrusion detection system in network security. Through case study, it can be seen that some indexes play very important role in analyzing and assessing main function indexes of intrusion detection system in network security and all the reasons should be considered because network security depends on the interaction of all the reasons. Through analysis and deduction, it is concluded that, under the premise of minimizing the information loss of data, the application of PCFA in assessing intrusion detection system in network security approved to be practicable.
Knowing an attackers intentions can significantly improve the effectiveness of a decision-making system. However, recognition such intentions and the attackers intended plans for achieving them is not an easy task because there are too many uncertain and dynamic factors in network environment. In this paper, intrusive intention recognition using dynamic Bayesian network is proposed to cope with uncertainty and dynamics in network security awareness. Furthermore, attack actions forecast based on goal recognition is given and discussed. Finally, feasibility and validity of this method are proved from the experiments.
The hidden security troubles of 3G network based on SIP is analyzed. In view of the security demand of 3G and the security bug of AKA protocol adopted in 3G network, an enhanced secure access protocol E-AKA is designed with SIP messages. E-AKA protocol not only effectively avoids the security threat, but also realizes the two-way authentication between the user equipment (UE) and home environment (HE) or serving network (SN). Through agreeing the key, the cipher key (CK) and the integrity key (IK) are generated, and the encryption method of one-session-one-key is realized. The analytic results show that E-AKA protocol can satisfy the need of 3G session protocol developing toward SIP; at the same time, it can ensure the access security of 3G network.
As traditional network security cannot meet the security requirements, the international research shows that network security is on the way to Trustworthy Internet and that the trustworthy issue becomes a hot topic in the future Internet. Trust evaluation of users behavior is an important part of Trustworthy Internet and a rational trust model plays a key role in the evaluation. This paper concludes many principles of the trust model by analyzing existing trust models and proposes a novel trust model including both the direct and indirect trust. In our model, the direct trust is more trustwor-thy and is weighted more with the more and more interactions and the indirect evaluation becomes less and less important. The subjective factor is considered to make the direct trust obey the principle of rise-lowly, decline-quickly. Furthermore, recommenders credibility is taken into account in the indirect trust evaluation, which can avoid the denigration and make indirect trust more objective. We also consider the process of referral of the credibility to gain more information to evaluate the indirect trust effectively. Credibility evaluation is the basis of the filtering of low credibility and the improvement of trust evaluation. Finally, several experiments are shown to illustrate the properties of the model.
This work provides an information-theoretic view to better understand the relationships between aggregated vulnerability information viewed by attackers and a class of randomized epidemic scanning algorithms. In particular, this work investigates three aspects: 1) a network vulnerability as the nonuniform vulnerable-host distribution, 2) threats, i.e., intelligent malwares that exploit such a vulnerability, and 3) defense, i.e., challenges for fighting the threats. We first study five large data sets and observe consistent clustered vulnerable-host distributions. We then present a new metric, referred to as the nonuniformity factor, that quantifies the unevenness of a vulnerable-host distribution. This metric is essentially the Renyi information entropy that unifies the nonuniformity of a vulnerable-host distribution with different malware-scanning methods. Next, we draw a relationship between Renyi entropies and randomized epidemic scanning algorithms. We find that the infection rates of malware-scanning methods are characterized by the Renyi entropies that relate to the information bits in a nonunform vulnerable-host distribution extracted by a randomized scanning algorithm. Meanwhile, we show that a representative network-aware malware can increase the spreading speed by exactly or nearly a nonuniformity factor when compared to a random-scanning malware at an early stage of malware propagation. This quantifies that how much more rapidly the Internet can be infected at the early stage when a malware exploits an uneven vulnerable-host distribution as a network-wide vulnerability. Furthermore, we analyze the effectiveness of defense strategies on the spread of network-aware malwares. Our results demonstrate that counteracting network-aware malwares is a significant challenge for the strateg-
To design and build a dynamic web security and defense mechanism, we use Device Driver Kit (DDK) Pass thru routine as the framework and adopt a method of capture based on Network Driver Interface Specification (NDIS) intermediate driver to realize non-bypassing monitoring of all packets; study, design and test detection rules against SQL injection attack and Cross-Site Scripting (CSS) attack; build a pervasive dynamic security system integrating Intrusion Detection System (IDS) and Firewall.Once an intrusion is detected, this Interaction Agent starts to work immediately followed by modification of firewall access control polices, thus complete security control is achieved.
We consider in this paper a single-channel wireless sensor network (WSN) where communication among sensor nodes are subject to jamming by an attacker. In particular, we address the detection of a jamming event, and investigate the optimal network defense strategy to mitigate jamming effects. A multiple-monitor distributed detection structure is considered. The optimal detection scheme is developed under the Bayesian criterion, with the goal of minimizing the error probability of detection at a fusion center. For the optimal network defense strategy, we formulate and solve the design problem using a constrained maximization problem by jointly considering Quality-of-Service and resource constraints of WSNs such as communication throughput, energy, and delay.
Intrusion detection technique has become increasingly important in the area of network security research. It is innovative that various soft computing approaches have been applied to the intrusion detection field. This paper presents an intelligent intrusion detection system which incorporates several soft computing techniques to implement either misuse or anomaly detection. Genetic algorithm is used to optimize the structure of the system. In the proposed system principal component analysis neural network is used to reduce the dimensions of the feature space. An enhanced Fuzzy C-Means clustering algorithm is used to cluster the preprocessed data to obtain fuzzy rules. And a Hierarchical Neuro-Fuzzy classifier is developmented. The experiments and evaluations of the proposed method were performed with the KDD Cup 99 intrusion detection dataset. Results indicate the high detection accuracy for intrusion attacks and low false alarm rate of the reliable system.
The validity of the network security policy has important impacts on the safety performance of network information system. For purpose of verifying the network security policy repository effectively, a verification model of security policy repository based on EHLPN is proposed. The HLPN is expanded, and based on the establishing of EHLPN model directed graph, the relationship of place and transition about the policy knowledge expression is analyzed, and the verification algorithm of security policy repository is established. Result shows that the model can effectively find the structural errors and provide a new solution and reference for verifying and adjusting the security policy repository, so as to better ensure network system security.
A computer network intrusion detection and prevention system consists of collecting network traffic data, discovering user behavior patterns as intrusion detection rules, and applying these rules to prevent malicious and misuse. Many commercial off-the-shelf (COTS) products have been developed to perform each of these tasks. In this paper, the component-based software engineering approach is exploited to integrate these COTS products as components into a computerized system to automatically detect intrusion rules from network traffic data and prevent future potential attacks. The component-based software architecture of this kind of system is designed, COTS components are analyzed, adaptor components to compose COTS products are developed, and the system implementation is illustrated.
OpenID authentication is a method to provide a single sign-on (SSO) service among Internet service sites. OpenID has been widely adopted by blog sites because of its usability and ease of implementation. However, the assurance of the ID in OpenID authentication is a concern because currently anyone can hold accounts on an OpenID provider (OP) simply by sending a registration mail and OPs usually do not check to confirm the real identity of their applicants. In contrast, a telephone company checks the identity of their mobile service users before a contract is completed by referring to such credentials as a drivers license or passport. Therefore, on a cellular phone, the ID, such as subscriber ID, is assured by the contract process and telephone companies can trace the users identity through the ID. In this paper, we propose a federation authentication scheme between Open ID and a cellular phone in order to assure the ID of the OpenID. In addition, by using the cellular phone at user authentication for each service use, secure authentication is also provided.
The underpinning of situational awareness in computer networks is to identify adversaries, estimate impact of attacks, evaluate risks, understand situations and make sound decisions on how to protect valued assets swiftly and accurately. SA also underscores situation assessment in order to make accurate forecast in dynamic and complex environments. In this paper, situational awareness in computer network security is investigated. Functional attributes of situational awareness in computer network security are discussed: dynamism and complexity, automation, realtime processing, multisource data fusion, heterogeneity, security visualisation, decision control, risk assessment, resolution, forecasting and prediction.
In the past several years, extensive research has been performed in various honeypot technologies, including honeynets, honeywalls, and honeytokens, primarily to gather information about external threats. Little to no research has been performed on how honeytokens, pieces of digital information designed to attract and trace illicit uses of data, can be implemented to catch one of the most dangerous threats, the trusted insider. The goal of this work is to detect, identify, and confirm insider threats, specifically threats that are after personally identifiable information (PII) data.
The paper presents a method of automatic detection of identity theft related event. The method is based on the social network analysis. The protection of electronic identity in contemporary information society is one of the greatest challenge in security research. The main idea about method of identity theft detection described in the paper is that identity theft events are related to change of the subjects behavior as the node of the social network (or several different networks). In consequence identity theft may be detected similarly as the other types of malicious activities by analyzing the change of observed behavior patterns.
Wireless sensor networks (WSNs) have been regarded as an incarnation of Ad Hoc Networks for a specific application. Since a WSN consists of potentially hundreds of low cost, small size and battery powered sensor nodes, it has more potentials than a MANET to be depolyed in many emerging areas. But they also raised many new challenges and these challenges include the design of embedded sensors and wireless networking technology, ie. routing protocols and network security.Many ad hoc routing protocols such as AODV, DSR, DSDR, TORA and OLSR, which have been developed particularly for the mobile wireless ad hoc networks(MANETs), performed satisfactorily on MANETs. In this paper, we investigate how well these ad-hoc routing protocols work on wireless sensor networks (WSNs). We focus on their performances in terms of average end-to-end delay, packet delivery ratio and routing overheads.
Detection of intrusion attacks is an important issue in network security, this paper introduces a new anomaly detection scheme based on multi-attribute decisional framework. the system calls are used to characterize the processs behavior, The data classification is performed by k-Nearest Neighbors(kNN) method and Support Vector Machines(SVM) model . the experiments with KDD Cup 1999 data demonstrate that our proposed method achieves 97.26%in hit rate with the false alarm rate 6.03%and outperforms the RIPPER method, and the time complexity is linear with the size of dataset and the number of attributes. since there is no need to build a profile for each program and check every sequence during the new program execution, the amount of calculation involved is largely reduced.
Honeypots are flexible security tools for gathering artefacts associated with a variety of Internet attack activities. While existing work on honeypot traffic analysis focuses mainly on identifying existing attacks, this paper describes a technique for detecting new attacks based on principal component analysis. The proposed technique requires no prior knowledge of attack types and has low computational requirements that makes it suitable for online detection systems. Our method of detecting new attacks is based on measuring changes in the residual space using square prediction error (SPE) statistics. When attack vectors are projected onto the residual space, attacks that are not presented by the main hyperspace will create new directions with high SPE values. We demonstrate the usefulness of our technique by using real traffic data from the Leurr.com project, a world-wide deployment of low-interaction honeypots, where several examples of new traffic detected by the system are illustrated.
One of the most important threats to personal and corporate Internet security is the proliferation of Zombie PCs operating as an organized network. Zombie detection is currently performed at the host level and/or network level, but these options have some important drawbacks: antivirus, anti-spyware and personal firewalls are ineffective in the detection of hosts that are compromised via new or target-specific malicious software, while network firewalls and Intrusion Detection Systems were developed to protect the network from external attacks but they were not designed to detect and protect against vulnerabilities that are already present inside the local area network. This paper presents a new approach, based on neural networks, that is able to detect Zombie PCs based on the historical traffic profiles presented by "licit" and "illicit" network applications. The evaluation of the proposed methodology relies on traffic traces obtained in a controlled environment and composed by licit traffic measured from normal activity of network applications and malicious traffic synthetically generated using the SubSeven backdoor. The results obtained show that the proposed methodology is able to achieve good identification results, being at the same time computationally efficient and easy to deploy in real network scenarios.
This study is focus on SIP of VoIP, and simulated and analysed by NS2. In addition, because of the security issues are the main factor to affect its popularization, and the DDOS attack has become one of the mose serious issues in VoIP network security domain. Therefore this research carries on the simulation analysis to various types attack by NS2, and further discusses under the DDoS attack between each kind of Queue regarding the attack comparison of bearing capacity.
Fault tolerance in the form of diverse redundancy is well known to improve the detection rates for both malicious and non-malicious failures. What is of interest to designers of security protection systems are the actual gains in detection rates that they may give. In this paper we provide exploratory analysis of the potential gains in detection capability from using diverse AntiVirus products for the detection of self-propagating malware. The analysis is based on 1599 malware samples collected by the operation of a distributed honeypot deployment over a period of 178 days. We sent these samples to the signature engines of 32 different AntiVirus products taking advantage of the VirusTotal service. The resulting dataset allowed us to perform analysis of the effects of diversity on the detection capability of these components as well as how their detection capability evolves in time.
This Special Issue is dedicated to the topic of network intrusion detection. The wide use of many varieties of intrusion detection systems reveals a certain degree of acceptance that, despite our best intentions for building secure systems, there is always enough opportunity for attacks, possibly exploiting weaknesses of the very systems we thought to be secure. Moreover, even though attacks usually follow patterns that can be subsequently used to detect them, they also happen to be the product of intelligent adversaries ? humans. The ability to orchestrate ever more elaborate attacks, exhibiting patterns not seen before, means that we expect an IDS to be ready to discern new patterns found to be conceivably detrimental to the security requirements of a system. Intrusion detection systems are sometimes criticized as doing too little too late. However, there are cases where even a late response is better than no response at all.
Standard pattern-matching methods used for deep packet inspection and network security can be evaded by means of TCP and IP fragmentation. To detect such attacks, intrusion detection systems must reassemble packets before applying matching algorithms, thus requiring a large amount of memory and time to respond to the threat. In the literature, only a few efforts proposed a method to detect evasion attacks at high speed without reassembly. The aim of this article is to introduce an efficient system for anti-evasion that can be implemented in real devices. It is based on counting bloom filters and exploits their capabilities to quickly update the string set and deal with partial signatures. In this way, the detection of attacks and almost all of the traffic processing is performed in the fast data path, thus improving the scalability of intrusion detection systems.
All-Optical Networks provide ultra-fast data rates, but present a new set of challenges for network security. However optical performance monitoring (OPM) and optical network management (ONM) are essential in building a reliable, high-capacity, and service-differentiation enabled all-optical network. One of the serious problems with transparency is the fact that optical crosstalk is additive, and thus the aggregate effect of crosstalk over a whole AON may be more nefarious than a single point of crosstalk. Attacks can spread rapidly through the network, causing additional awkward failures and triggering multiple undesirable alarms, they must be detected and identified at any point in the network where they may occur. This results in the continuous monitoring and identification of the impairments becoming challenging in the event of transmission failures. However, a simple and reliable signal quality monitoring method does not exist at present. In this paper we present a novel method for attack identification and localization in networks with a minimum of complexity and cost. This method can participate in some tasks for fault management in optical network.
Intrusion tolerance is the rising third generation technology of network security. The self-cleansing intrusion tolerant system (SCITS) is a typical application of this technology. Through quantifying the redundancy of SCITS, a conclusion that a specific SCITS has an optimal redundancy which can maximize its intrusion tolerant performance is obtained in this paper. The method of computing the optimal redundancy is given, and the effect of SCITSs parameters on intrusion tolerant performance and optimal redundancy is analyzed through simulation. Lastly some notes about the design of SCITS are given based on the simulation.
In view of the current limitations of Internet security,it designed a system of Encryption and Authentication basedon internet system.. It detail realizes network security featuresof data confidentiality, data integrity, data originauthentication and access control and so on. Combine theactual application, it given specific implementation of theencryption and authentication.
The major technical objectives of the RC-NSPES are to provide a framework for the concurrent operation of reactive and pro-active security functions to deliver efficient and optimised intrusion detection schemes as well as enhanced and highly correlated rule sets for more effective alerts management and root-cause analysis. The design and implementation of the RC-NSPES solution includes a number of innovative features in terms of real-time programmable embedded hardware (FPGA) deployment as well as in the integrated management station. These have been devised so as to deliver enhanced detection of attacks and contextualised alerts against threats that can arise from both the network layer and the application layer protocols. The resulting architecture represents an efficient and effective framework for the future deployment of network security systems.
Traditionally, backup and archiving have been performed on tapes. With the rapid advances in disk storage technology witnessed in recent years, it becomes practical to use disks other than tape libraries as backend storage device for a backup system. For such a disk-based system, storage space efficiency is essential. Since traditional backup method cannot eliminate redundancies during backup, a new data deduplication backup technique should be developed to provide more efficient data storage at the system.This paper describes the design and performance evaluation of a data de-duplication disk-based network backup system,called 3DNBS. 3DNBS breaks files into variable sized chunks using content-defined chunking (CDC) for the purpose of duplication detection. Chunks are indexed and addressed by hashing their content, which leads to intrinsically single instance storage. Experimental results show that in comparison with traditional backup method such as Bacula, 3DNBSpresents dramatic reduction in required storage space on various workloads. By eliminating duplicated data, 3DNBS also reduces the size of data to be transmitted, hence reducing time to perform backup in a bandwidth constraint environment.
We introduce an Internet traffic anomaly detection mechanism based on large deviations results for empirical measures. Using past traffic traces we characterize network traffic during various time-of-day intervals, assuming that it is anomaly-free. We present two different approaches to characterize traffic: (i) a model-free approach based on the method of types and Sanov's theorem, and (ii) a model-based approach modeling traffic using a Markov modulated process. Using these characterizations as a reference we continuously monitor traffic and employ large deviations and decision theory results to compare the empirical measure of the monitored traffic with the corresponding reference characterization, thus, identifying traffic anomalies in real-time. Our experimental results show that applying our methodology (even short-lived) anomalies are identified within a small number of observations. Throughout, we compare the two approaches presenting their advantages and disadvantages to identify and classify temporal network anomalies. We also demonstrate how our framework can be used to monitor traffic from multiple network elements in order to identify both spatial and temporal anomalies. We validate our techniques by analyzing real traffic traces with time-stamped anomalies.
SYN flooding has been a serious security threat to Internet. For a host server, it is necessary to take some kind of admission control in defense against SYN flooding attacks. In this paper, a probabilistic drop scheme is presented for implementation in a host server to mitigate SYN flooding attacks. An analytical model is proposed for this scheme, and a general principle for evaluating the probability of successful connection establishment during a SYN flooding attack is presented. Performance analysis results show (i) retransmission behavior of an application has positive influence on the successful establishment of its connection requests; (ii) a higher probability of connection establishment can be obtained by the probabilistic drop scheme than that by the random drop scheme when a SYN flooding attack occurs.
Getting a better grasp of computer network security is of great significance to protect the normal operation of network system. Based on rough set (RS), clustering model, security features reduction and clustering algorithm are presented, which provides a basis of network security strategies. Further research is to mine and process the dynamic risks and management of network security. Using the reduction methods, the simplified network security assessment data set is established. The extraction by the decision-making rules is proposed and verified. Through the results, it is concluded that the method could be in line with the actual situation of decision-making rules.
With the multiplication of attacks on computer networks, system administrators need to monitor carefully the networks. But all the techniques or tools that they use still heavily rely on human detection. In this paper a visual interactive network connection system called NetViewer is designed in 3D view for representing traffic activities that reside in network flows and their patterns. The experiments show that NetViewer can not only monitor all the activities in the network, but also detect DDoS attacks, network scans and port scans etc.
The compact attack graphs implicitly reveal the threat of sophisticated multi-step attacks by enumerating possible sequences of exploits leading to the compromising given critical resources in enterprise networks with thousands of hosts. For security analysts, the challenge is how to analyze the complex attack graphs with possible ten thousands of nodes for defending the security of network. In the paper, we will essentially discuss two issues about it. The first is to compute non-loop attack paths with the distance less than the given number that the real attacker may take practically in realistic attack scenarios. The second is to find the solution to removing vulnerabilities in such a way that given critical resources cannot be compromised while the cost for such removal incurs the least cost. We propose two scalable approaches to solve the above two issues respectively. These two approaches are proved to have a polynomial time complexity and can scale to the attack graphs with ten thousands of nodes corresponding large enterprise networks.
Exactly assessing the security risk of a network is the key to improving the security level of the network. The Hidden Markov Model based real time network security risk quantification method can get the risk value dynamically and in real-time, whose input is Intrusion Detection System alerts. The method is better than the traditional static assessment method. The paper resolves main fault of this method, which improves its accuracy and simplifies the configuration by automatically working out matrixes in HMM. In an experimental study we demonstrate the usefulness of our techniques.
Through mathematical analysis, the primary components analysis (PCA) is used to analyze the effect of influential factors on campus network security (CNS). Based on PCA model and, the results are computed. Through case study, it can be seen that some factors and characteristics play very important role in influencing CNS and all factors should be considered because the effect depends on the factors interaction. Through analysis and discussion, it is concluded that, under the premise of minimizing the information loss, the application of PCA in analyzing effect on CNS is approved to be practicable and applicable.
Phishing is a constantly changing network deception. It has been a huge threat to the ecommerce industry. Not only does it shatter the confidence of customers towards e-commerce, but also causes information service providers a bulk of economic loss. In this paper, we analyzed the phishing attack behaviors and eight anti-phishing solutions were presented by utilizing TRIZ. Our research also proved TRIZ could be used to address network security problems.
Now, network attack-defense is a hotspot of network security fields. Computer network war will be the most important type of future information war. A relative general and extensible simulation platform can offer training support for building the troops battle effectiveness. So a method of building Network Attack-Defense Simulation Training Platform (NADSTP) based on HLA and network simulation is put forward. With this method, the platform is designed composing of several different training federation members and hierarchy framework is adopted. Key technologies involve attack-defense theory and skills, network simulation and simulation driving are also detailed. Finally, platform is realized based on plug-in framework and simulation examples are presented. It can provide beneficial reference for constructing similar large-scale simulation training system.
Securing group communications in resource constrained, infrastructure-less environments such as Mobile Ad Hoc Networks (MANETs) has become one of the most challenging research directions in the areas of wireless network security. MANETs are emerging as the desired environment for an increasing number of commercial and military applications, addressing also an increasing number of users. Security on the other hand, is an indispensable requirement of modern life for such applications. The inherent limitations of MANETs impose major difficulties in establishing a suitable secure group communications framework. This is even more so for the operation of Key Agreement (KA), under which all parties contribute equally to the group key. The logical design of efficient KA protocols has been the main focus of the related research. Such a consideration however, gives only a partial account on the actual performance of a KA protocol in a multi-hop network as protocols have been evaluated only in terms of the key related messaging in isolation from network functions that interact with the logical scheme (i.e. routing). In recent work, we contributed towards the latter by efficiently extending a number of Diffie-Hellman group KA protocols in wireless multi hop ad hoc networks. In this work, we are extending a scheme that was left out in our previous work: Hypercube. Through analysis and simulations we demonstrate the superiority of the new, enriched H-Cube that takes into account the underlying routing by the use of a topologically aware communications schedule.
This paper discusses an application of a neural network in wireless sensor network security. It presents a multilayer perceptron (MLP) based media access control protocol (MAC) to secure a CSMA-based wireless sensor network against the denial-of-service attacks launched by adversaries. The MLP enhances the security of a WSN by constantly monitoring the parameters that exhibit unusual variations in case of an attack. The MLP shuts down the MAC layer and the physical layer of the sensor node when the suspicion factor, the output of the MLP, exceeds a preset threshold level. Backpropagation and particle swarm optimization algorithms are used for training the MLP. The MLP-guarded secure WSN is implemented using the Vanderbilt Prowler simulator. Simulation results show that the MLP helps in extending the lifetime of the WSN.
Collecting information about user activity in peer-to-peer systems is a key but challenging task. We describe here a distributed platform for doing so on the eDonkey network, relying on a group of honeypot peers which claim to have certain files and log queries they receive for these files. We then conduct some measurements with typical scenarios and use the obtained data to analyze the impact of key parameters like measurement duration, number of honeypots involved, and number of advertised files. This illustrates both the possible uses of our measurement system, and the kind of data one may collect using it.
With the development of intrusion technologies, dynamic forensics is becoming more and more important. Dynamic forensics using IDS or honeypot are all based on a common hypothesis that the system is still in a reliable working situation and collected evidences are believable even if the system is suffered from intrusion. In fact, the system has already transferred into an insecurity and unreliable state, it is uncertain that whether the intrusion detectors and investigators could run as normal and whether the obtained evidences are credible. Although intrusion tolerance has been applied in many areas of security for years, few researches are referred to network forensics. The work presented in this paper is based on an idea to integrate Intrusion tolerance into dynamic forensics to make the system under control, ensure the reliability of evidences and aim to gather more useful evidences for investigation. A mechanism of dynamic forensics based on intrusion forensics is proposed. This paper introduces the architecture of the model which uses IDS as tolerance and forensics trigger and honeypot as shadow server, the finite state machine model is described to specify the mechanism, and then two cases are analyzed to illuminate the mechanism.
The concepts of self, nonself, antibody, antigen and vaccine in in-Depth-Defense system for Network Security was presented in this paper, the architecture of in-Depth Defense for network intrusion and detection based on immune principle is proposed. The intrusion information gotten from current monitored network is encapsulated and sent to the neighbor network as bacterin; therefore the neighbor network can make use of the bacterin and predict the danger of network. We can use communicate agent cooperated with response agent to achieve active defense formwork. The experimental results show that the new model not only actualizes an active prevention method but also improves the ability of intrusion detection and prevention than that of the traditional passive intrusion prevention systems
Firewalls are crucial elements in network security, and have been widely deployed in most businesses and institutions for securing private networks. The function of a firewall is to examine each incoming and outgoing packet and decide whether to accept or to discard the packet based on its policy. Due to the lack of tools for analyzing firewall policies, most firewalls on the Internet have been plagued with policy errors. A firewall policy error either creates security holes that will allow malicious traffic to sneak into a private network or blocks legitimate traffic and disrupts normal business processes, which in turn could lead to irreparable, if not tragic, consequences. Because a firewall may have a large number of rules and the rules often conflict, understanding and analyzing the function of a firewall has been known to be notoriously difficult. An effective way to assist firewall administrators to understand and analyze the function of their firewalls is by issuing queries. An example of a firewall query is "Which computers in the private network can receive packets from a known malicious host in the outside Internet? Two problems need to be solved in order to make firewall queries practically useful: how to describe a firewall query and how to process a firewall query. In this paper, we first introduce a simple and effective SQL-like query language, called the Structured Firewall Query Language (SFQL), for describing firewall queries. Second, we give a theorem, called the Firewall Query Theorem, as the foundation for developing firewall query processing algorithms. Third, we present an efficient firewall query processing algorithm, which uses decision diagrams as its core data structure. Fourth, we propose methods for optimizing firewall query results. Finally, we present methods for performing the union, intersect, and minus operations on firewall query results. Our experimental results show that our firewall query processing algorithm is very e-
Security has become one of the major issues for data communication over wired and wireless networks. Different from the past work on the designs of cryptography algorithms and system infrastructures, we aim at the proposing of a dynamic routing algorithm that could randomize delivery paths for data transmission. The algorithm is easy to implement and compatible with popular routing protocols, such as routing information protocol in wired networks and destination-sequenced distance vector protocol in wireless networks, without introducing extra control messages. An analytic study on the proposed algorithm is presented, and a series of simulation experiments are conducted to verify the analytic results and to show the capability of the proposed algorithm.
The paper considers an integrated proactive framework for defense against spreading network worms in the Internet. The framework is intended for network worm detection (by recognizing the actions on scanning of network hosts) and containment of worm spreading (by limiting and blocking the packets transmitted by infected hosts). The framework is based on application of different heuristic detection and response mechanisms, their combination and automatic dynamic adaptation according to current network conditions. The paper describes the software system for simulation and evaluation of defense mechanisms investigated against spreading network worms and the results of experiments on detection and containment of network worms.
Distributed generation (DG) can offer an alternative planning approach to utilities to satisfy demand growth and distribution network security, planning and management issues. However, an appropriate framework is required to foster the integration of DG within grid network planning, thus avoiding potential inefficiencies in electricity supply infrastructure. In this work, in order to capture the effects of network investment deferral on DG expansion, different regulations for distribution network operators (DNOs) ownership of DG and how they influence the optimal connection of new generation within existing networks are examined. Using a multiyear multiperiod optimal power flow, DNOs preference for the siting and sizing of DG installation are analyzed.
This paper presents a stochastic coordination of generation and transmission expansion planning model in a competitive electricity market. The Monte Carlo simulation method is applied to consider random outages of generating units and transmission lines as well as inaccuracies in the long-term load forecasting. The scenario reduction technique is introduced for reducing the computational burden of a large number of planning scenarios. The proposed model assumes a capacity payment mechanism and a joint energy and transmission market for investors' costs recovery. The proposed approach simulates the decision making behavior of individual market participants and the ISO. It is an iterative process for simulating the interactions among GENCOs, TRANSCOs and ISO. The iterative process might be terminated by the ISO based on a pre-specified stopping criterion. The case studies illustrate the applications of proposed stochastic method in a coordinated generation and transmission planning problem when considering uncertainties.
Jinsha river has very abundant hydro power reservation, ranking first in all the hydro power bases of China. To deliver the 38-GW hydro power from Jinsha power stations by HVDC transmission lines to load centers 1000-2000 km away, it is necessary to determine the right voltage level and the capacity of each bipole. This paper proposes three preliminary power delivery scheme and presents an optimization method for transmission capacity, dc voltage and conductor size. Based on a comprehensive comparison of cost-effectiveness, technical feasibility and network security and stability, it was recommended that the 3 bipolar plusmn 800-kV ultra HVDC (UHVDC) links with rated capacity of 6.4 GW for each link be adopted as transmission scheme of phase I hydro power station on Jinsha River.
This article outlines some recently emerging research in network-based malicious software detection. The author discusses differences between traditional network intrusion detection and these new techniques, and highlights a new freely available tool called BotHunter.
Cyber attacks are deliberate actions to alter, disrupt, deceive, degrade, or destroy computer systems or networks or the information and/or programs resident in or transiting these systems or networks. The use of cyber attack as an instrument of US policy is rarely discussed but is an important topic to the nation.
Proper configuration management is vital for host and network security. We outline the problems, especially for large-scale environments, and discuss the security aspects of a number of different configuration scenarios, including security appliances (e.g., firewalls), desktop and server computers, and PDAs. We conclude by discussing research challenges.
Enterprise network security management is a complex task of balancing security and usability, with trade-offs often necessary between the two. Past work has provided ways to identify intricate attack paths due to misconfiguration and vulnerabilities in an enterprise system, but little has been done to address how to correct the security problems within the context of various other requirements such as usability, ease of access, and cost of countermeasures. This paper presents an approach based on Boolean Satisfiability Solving (SAT Solving) that can reason about attacks, usability requirements, cost of actions, etc. in a unified, logical framework. Preliminary results show that the approach is both effective and efficient.
Computers face an ever increasing number of threats from hackers, viruses and other malware; effective Network Intrusion Detection (NID) before a threat affects end-user machines is critical for both financial and national security. As the number of threats and network speeds increase (over 1 gigabit/sec), users of conventional software based NID methods must choose between protection or higher data rates.
Defenders of today's critical cyber-infrastructure (e.g., the Internet) are equipped with a wide array of security techniques including network-based intrusion detection systems (IDS), host-based anti-virus systems (AV), and decoy or reconnaissance systems such as host-based honeypots or network-based telescopes. While effective at detecting and mitigating some of the threats posed to critical infrastructure, the ubiquitous nature of malicious activity (e.g., phishing, spam, DDoS) on the Internet indicates that the current deployments of these tools do not fully live up to their promise. Over the past 10 years our research group has investigated ways of detecting and stopping cyber-attacks by using the context available in the network, host, and the environment. In this paper, we explain what exactly we mean by context, why it is difficult to measure, and what one can do with context when it is available. We illustrate these points by examining several studies in which context was used to enable or enhance new security techniques. We conclude with some ideas about the future of context-aware security.
This paper first points out the ineffectiveness of current security assessment methods that only focus on one aspect of security at a time. Organizations typically conduct a cyber security assessment that only looks at the IT-related assets, or may hire a physical security firm to review the physical security aspects of their facility. Seldom is an approach used that reviews all critical components of an effective security program at the same time. Core information and technical systems that power critical infrastructure rely on more than just cyber security for safe and reliable operations. The paper then describes several key aspects of an infrastructure protection program, and provides a methodology for performing an all-hazards approach for analyzing the readiness of the system to withstand multiple types of threats, both internal and external.
The Cyber Defense Laboratory at Western Kentucky University has established a multidisciplinary research team in order to gain a better understanding of the motives and methods of hackers. This involves the collaboration of computer scientists, network analysts, sociologists, and anthropologists. Most research along these lines to date has been performed by technical experts and has failed to consider social research and methods. On the other hand, social theorists tend to group network attacks with other forms of terrorism. We believe that network intrusions are sufficiently different from other forms of terrorism to warrant their own study. This research is innovative in both the way it defines network attacks as well as in the fusion of data from diverse fields of research. By bringing the strengths of social and technical research together, a clearer picture of hackers will emerge. This has implications for government and law enforcement as well as network security. A literature review of related research in the social sciences has been completed. Social scientists with experience in the identified sub-fields were brought onto the team and their results will then be integrated into the Cyber Defense Lab's network security testbed. Using proven social research methods, team members will gain an understanding of the motives of hackers, the methods of attack, communication of ideas, learning resources and strategies, and any other aspects that may be revealed. This paper will discuss the results to date of this ongoing project.
Many network security applications rely on string matching to detect intrusions, viruses, spam, and so on. Since software implementation may not keep pace with the high-speed demand, turning to hardware-based solutions becomes promising. This work presents an innovative architecture to realize string matching in sub-linear time based on algorithmic heuristics, which come from parallel queries to a set of space-efficient Bloom filters. The algorithm allows skipping characters not in a match in the text, and in turn simultaneously inspect multiple characters in effect. The techniques to reduce the impact of certain bad situations on performance are also proposed: the bad-block heuristic, a linear worst-case time method and a non-blocking interface to hand over the verification job to a verification module. This architecture is simulated with both behavior simulation in C and timing simulation in HDL for antivirus applications. The simulation shows that the throughput of scanning Windows executable files for more than 10$thinspace$ 000 virus signatures can achieve 5.64 Gb/s, while the worst-case performance is 1.2 Gb/s if the signatures are properly specified.
Intrusion detection is a important network security research direction. SVM(Support Vector Machine) is considered as a good substitute for traditional learning classification approach, and has a good generalization performance especially in small samples in non-linear case. LLE(Local Linear Embedding) is a good nonlinear dimensionality reduction method, which is good for the data that lies on the nonlinear manifold. This paper proposes an approach using SVM and LLE in intrusion detection system. In the Matlab simulation experiment, we can achieve higher classification accuracy rate, lower false positive rare and false negative rate using the method, compared to PCA(Principal Component Analysis) and ICA(Independent Component Analysis) approach.
Because of the growing complexity of networks and the difficult task of security policy enforcement, system administrators need simple and powerful security management tools. This paper presents a network security management tool that allows policy specification and administration of network security components such as firewall. The tool consists of four main modules. First module is considered the network repository of our toolkit. Through the second module the security policy is introduced and the necessary validation and verification is done by a policy engine. The third module is responsible for the translation of the high level security policy into an intermediate model level. Finally, the intermediate level is translated automatically into a vendor-specific security mechanism through the vendor specific compiler.
Previous research on sensor network security mainly considers homogeneous sensor networks, where all sensor nodes have the same capabilities. Research has shown that homogeneous ad hoc networks have poor performance and scalability. The many-to-one traffic pattern dominates in sensor networks, and hence a sensor may only communicate with a small portion of its neighbors. Key management is a fundamental security operation. Most existing key management schemes try to establish shared keys for all pairs of neighbor sensors, no matter whether these nodes communicate with each other or not, and this causes large overhead. In this paper, we adopt a Heterogeneous Sensor Network (HSN) model for better performance and security. We propose a novel routing-driven key management scheme, which only establishes shared keys for neighbor sensors that communicate with each other. We utilize Elliptic Curve Cryptography in the design of an efficient key management scheme for sensor nodes. The performance evaluation and security analysis show that our key management scheme can provide better security with significant reductions on communication overhead, storage space and energy consumption than other key management schemes.
With rapid development of computer networks, users need a new solution for network security management, aiming at integration. This paper focuses on context-aware alert analysis, which is one of its key functionalities. A practical and efficient approach to guarantee unified representation of context information, background knowledge and attack knowledge for security alerts is still lacking these days. This paper applies security ontology by means of OWL+SWRL+OWL-S based on CIM Schema to describe context information and security knowledge in a unified manner. We argue that, our proposed approach improves existing alert analysis techniques by providing formal representations with the use of security ontology, which may possibly be an important stage for implementation of unified network security management.
Over the past few years, anomaly detection has been an increasing concern with the rapid growth of the network security. Hidden Markov model (HMM) has been applied in various methods in intrusion detection and proved to be a good tool to model normal behaviors of privileged processes, however, one major problem with this approach is that it demands excessive computing resources and costs a long model training time, which makes it inefficient for practical intrusion detection. This paper presents a new method of bringing rough set reduction into HMM to overcome the shortcoming. The proposed approach classifies and simplifies the long observation sequence by virtue of rough set reduction, and the decision conditions obtained in rough set reduction phase could be used in further detection. The experimental results indicate that this method can promote the model training efficiency. Further-more, it is suitable for anomaly detection with high detect rate and low false alarm rate.
This paper analyzed the specialty of LM system based on GPRS/CDMA. The shortage of security scheme used in LM system in safety zone I/II was discussed. The reduced possibility of the safety demand is presented also. A new safety scheme in safety zone III is designed. The new scheme was more simple and it improved the safety capability of the control zone I/II more effectively.
This paper presents a new approach of congestion management with network based measures, market based measures and generation management in wind farms for transmission system with large scale integration of wind energy. The congestion management problem is solved using a genetic algorithm based approach, that enables to combine network based measures, market based measures and generation management in order to avoid congestions. In doing so, the power system requirements are assessed with network security algorithms like the ACload flow calculations for the base case, the (n-1)-security calculations or the short circuit calculations. Additionally to the mentioned possibilities of congestion management, the paper presents an approach of energy management using energy storage systems to enhance the collaboration between wind farm operators and TSO in order avoid congestions on the grid.
This paper discusses the network security scanning and some scanning methods which contain port scanning, vulnerability scanning and remote operating system detection are studied. In order to reduce the complexity and get high performance, the architecture of a common network security scanning system based on Libnet and Libpcap is provided and the every module of system is designed and implemented. The key modules include packet constructing, packet sending, packet capture, packet analyzing module and information log analysis module. The methods of packet injection based on Libnet and the packet capture based on Libpcap are presented and it can improve scanning performance and enhance the scalability and expansibility of system.
In deployments using ATM aggregation-network, access loop identification is facilitated by the typical one-to-one mapping between an access loop and ATM PVC between the Access Node and the Broadband Network Gateway. The User Identification technology is very important to improve the network security. This paper introduces some methods of user identification technology and discussed the implementation of scheme.
Intrusion detection plays more important role in network security today. This paper introduces a method, particle swarm optimization and support vector machine, to intrusion detection system, and presents a new design of ID Based on Particle Swarm Optimization and Support Vector Machine. This paper presents an optimal selection approach of the SVM parameters(regulation parameter C and the radial basis function width parameter  ) based on particle swarm optimization algorithm. The experiments show that the optimal parameter selection approach based on PSO is available and the Research of Intrusion Detection Based on Particle Swarm Optimization and Support Vector Machine is effective in reducing the number of alerts, false positive, false negative better.
We propose the use of optical processing for enhancing network security. Optical steganography, anti-jamming, and optical encryption is experimentally demonstrated. Service availability is also improved during physical infrastructure attacks using optical CDMA for backup channels.
Computer network defense (CND) capabilities are designed for business efficiency in a static, stable network environment, where network assets are known and the mission of the network rarely varies. Such capabilities do not consider mission needs when identifying and compensating for attacks on assets critical to the current mission. Security awareness remains the same whether the network is carrying redundant inventory data or supporting critical operations. Advanced attackers who may already know the network details, its systems and software, and any mission objectives therefore already have an advantage. We propose a three-step process to enable migration from the current CND strategy toward one of mission network defense. This transition would lead to better, more adaptable security capabilities by encouraging exchange of mission-related event information. By adopting and extending the Common Event Expression standard to support mission-relevant data, CND systems can begin to monitor their mission environment. Once standardized mission event information becomes available, we can concentrate on developing more adaptive CND capabilities that are better suited to the various mission environments.
Two existing network security problems are ensuring anonymous communications and preventing data exfiltration through network covert channels. We present a new concept in network addressing: one-time encrypted network addresses. We describe a particular instantiation: one-time CPP addresses. We then show how one-time encrypted addresses can prevent intersection and other traffic analysis attacks that can undermine low-latency anonymous communications. We show how one-time encrypted addresses can also be used, with certain assumptions, to greatly reduce network covert channels. Thus one-time encrypted network addresses, when combined with other network security countermeasures, are able to provide a back-up defense against malicious software on hosts that attempt to ldquophone homerdquo in order to leak confidential information, including location information. We describe how these techniques can be used to protect confidential data in a MANET.
The transformational satellite communications system (TSAT) is the DoDpsilas next generation geosynchronous satellite communications constellation. The TSAT system features packet and circuit interfaces that support a wide variety of military missions by providing communication services to fixed ground, mobile ground, marine, and airborne terminals. These terminals access a variety of link rates and communications services and allow the TSAT system to interoperate with many different peer networks. The TSAT system features several types of coverage, steerable beams, and adaptive waveforms that can be configured dynamically in real-time. The TSAT system responds to user requests for services ranging from long-term fixed service, to on-demand short-term communications, to critical services under emergency conditions. The TSAT system must also provide network security services at physical, link, and network layers. Finally, the TSAT system must provide highly available services and be able to operate autonomously. In order to achieve these goals of providing a wide range of services in an automated and responsive manner, TSAT employs a sophisticated TSAT mission operations system (TMOS). TMOS provides highly capable, agile and adaptive network planning and management services. This paper discusses the TMOS approach for planning and network management. The first goal of the paper is to explain the approach for reliable configuration of the satellite and terminal network elements. The second goal is to explain the approach for fault and performance management in the TSAT network.
To support research in wireless mobile networks and mobile ad-hoc network security, the U.S. army research laboratory (ARL) has developed a ldquoWireless emulation laboratoryrdquo (WEL). A key component of the WEL is a Mobile Ad-hoc network (MANET) emulation testbed on which algorithms and applications can be subjected to emulated wireless network conditions. The testbed is based on the MANE (mobile ad-hoc network emulator) software originally developed by the naval research laboratory (NRL). It has since been improved through the incorporation of advanced modeling methods and computing technologies. Important additional features include (1) the integration of the terrain integrated rough earth model (TIREM) propagation model, (2) the use of virtual machine technologies to scale the size of the network, and (3) the inclusion of custom-designed mobility patterns to create a specific dynamic topology of a MANET under test. Currently the WEL testbed can emulate a 101-node MANET and, through the use of virtualization technologies, will scale well beyond that number. This paper discusses the current capabilities of ARLpsilas WEL for conducting empirical evaluation and demonstration of MANET technologies and concludes with planned future enhancements.
This paper discusses issues surrounding the design of cognitive radio (CR) networks that must operate in the presence of jamming and hostile Electronic Countermeasure (ECM) environments. Much of the research to date in the CR community has focused on spectrum-sensing techniques and multiple-access methods. Until recently, little work had been done in the general area of wireless network security and specifically the area of performance in the presence of jamming. Discussions on this topic have been inhibited in the greater CR community by a poorly defined lexicon, which makes addressing issues such as security and performance in jamming difficult. This paper attempts to provide a basic lexicon that can be used to clearly discuss the issues surrounding CR network security and performance in the presence of jamming. This paper then goes on to discuss the various jammer threats that might be encountered by a CR network and the ramifications of these threat types on CR network design.
Attack graphs are important tools for analyzing network security vulnerabilities. Recently, the generation method of attack graphs is a hot topic to the security researchers. As previous works encounter the scalability problem and inaccurate input information problem, we propose a novel method to automatic construction of attack graphs based on probability. After introducing prior-probability, match-probability,and transition-probability into attack graphs generation process, we develop a new attack model and relevant generation algorithms. Our method uses threshold and key states to control the scale of result attack graphs with important attack paths reserved. The following experiments show our approach could get meaningful results with less time and space, especially when one wants to get a few shortest attack paths quickly.
In recent years, the substantial increase in the volume of e-commerce transactions, consumers have been gradually changing consumption habits and consumption patterns to online shopping, but due to the problems of the website credibility, network security and etc., many internet users are outside of e-commerce. Under this condition, this article, from the perspective of consumer satisfaction, study B to C e-commerce website evaluation. Using evaluation method of AHP and grey evaluation, we evaluate e-commerce websites and try to make the evaluation practically, so that consumers can easily find a good website from a large number of shopping sites, and access to high quality products and good services.
With the development of Internet and Intranet, Web and distributed databases have been used more and more widely. It is important to properly handle network and Web database security issues including authentication, denial of service, and fine-grained access control. When database access control and the network security are addressed separately, the security systems are not optimized sufficiently as a whole. This paper propose a method of integrating network security with criterion based access control to handle network security and the fine-grained Web database access control simultaneously. To improve efficiency, the model adopts two step access controls. The first preliminary access control is combined with the firewall function, and the second fine-grained access decisions are determined by the user's digital credentials as well as other factors such as his/her IP address.
Bilingual Teaching is one of the important measures to bring China's higher education in line with international practices. However, there are many issues to be studied and discussed in this new teaching approach. Based on the practice and experience of bilingual approach in teaching the course of Network Security and Management, this paper explored the necessity and significance of offering bilingual teaching to undergraduates majoring in computer science, and the specific ways to implement bilingual teaching. In the meanwhile, problems that exist in the bilingual teaching initiatives are analyzed and summarized.
Timeout mechanisms are a useful feature for web applications. However, these mechanisms need to be used with care because, if used as-is, they are vulnerable to timing attacks. This paper focuses on internal timing attacks, a particularly dangerous class of timing attacks, where the attacker needs no access to a clock. In the context of client-side web application security, we present JavaScript-based exploits against the timeout mechanism of the DOM (document object model), supported by the modern browsers. Our experimental findings reveal rather liberal choices for the timeout semantics by different browsers and motivate the need for a general security solution. We propose a foundation for such a solution in the form of a runtime monitor. We illustrate for a simple language that, while being more permissive than a typical static analysis, the monitor enforces termination-insensitive noninterference.
As networked systems grow in complexity, they are increasingly vulnerable to denial-of-service (DoS) attacks involving resource exhaustion. A single malicious "input of coma" can trigger high-complexity behavior such as deep recursion in a carelessly implemented server, exhausting CPU time or stack space and making the server unavailable to legitimate clients. These DoS attacks exploit the semantics of the target application, are rarely associated with network traffic anomalies, and are thus extremely difficult to detect using conventional methods.We present SAFER, a static analysis tool for identifying potential DoS vulnerabilities and the root causes of resource-exhaustion attacks before the software is deployed. Our tool combines taint analysis with control dependency analysis to detect high-complexity control structures whose execution can be triggered by untrusted network inputs.When evaluated on real-world networked applications, SAFER discovered previously unknown DoS vulnerabilities in the Expat XML parser and the SQLite library, as well as a new attack on a previously patched version of the wu-ftpd server. This demonstrates the importance of understanding and repairing the root causes of DoS vulnerabilities rather than simply blocking known malicious inputs.
This paper proposes a network security dynamic situation forecasting method (Unbiased Gray Markov Forecasting Method: UGM_HM), which based on the Unbiased Grey system theory and Markov Forecasting theory. UGM_HM combines advantages of Unbiased Grey system theory and Markov Forecasting theory. UGM_HM takes the complex network environment as a Grey system and takes the dynamic risk value of network as a Grey value. The long-term network security situation is reflected by the Unbiased GM (1, 1) and the state transition probabilities are identified by Markov chain theory. The above mentioned dynamic risk value of network, which based on the artificial immune can reflect the network real-time state. The conclusions of experiment prove that UGM_HM compares with Unbiased GM (1, 1) has high precision in the forecasting of the network security dynamic situation.
The traditional ideas and solutions focusing on network security problems are unsuitable for wireless sensor networks in which storage space and computing power are limited. In order to reduce cost in processor of sensor node, this paper proposes a new embedded encryption algorithm (AEEA) based on linear feedback shift register (LFSR). Because the algorithm is only composed by some basic operations such as the XOR and displacement, FPGA coprocessor with high performance and price ratio is adopted, through which our algorithm can achieve low cost and high speed in FPGA implementation. The experimental results show, compared with other known encryption algorithms for wireless sensor networks, our algorithm has better performance on the whole, in particular the safety performance ratio reaches 0.757, greatly outperforming other algorithms.
With the development of network and communication technology, people's demands on wireless network are getting higher and higher. Security problem is becoming the focus that wireless networks pay attention to. This article discusses the security structures of ZigBee wireless network, analyzes the security of MAC. NWK and APL layer firstly. Moreover, it analyzes the authentication and encryption in ZigBee technology completely, and proposes opinions for network security protection, solves the confidentiality, integrity and access control problem in network communication.
Risk detection about network security can discover anomaly traffic in time and decrease the harm and losing caused by network attacks. By means of NetFlow, packets were collected from Internet. Analyzing these data, four addresses (source IP/port and destination IP/port) were abstracted to form information entropy, which was extended from thermodynamic. Based on information entropy, risk coefficient was defined to describe the whole condition of network security. To forecast the change of risk coefficient, a model was set up according to grey theory. Using a few data, the model can forecast risk coefficient accurately. Simulation and experiments were completed by MATLAB and in LAN. The experiments in LAN prove that information entropy can reflect anomaly traffic caused by DDoS attacks and Internet worms in time and effectively. The simulation by MATLAB shows that by GM(1,1) information entropy and risk coefficient can be forecasted accurately. Now the system has been applied for two years and provides good performance.
With the applying of artificial immune technology in the field of network security situation awareness, this article puts forward a new immune network security situation awareness model to enable self-learning and self-adapting of network system, and increase its immunity and viability. When network is under attack, it can find out the current network security situation and future trend in an all-around way, provide grounds for reasonable and accurate response to guarantee the usability of system.
One of the big challenges of E-government adoption among citizens is security. Numerous security approaches exist in E-commerce, and all of them are applicable to E-government. Usually government networks can communicate to each other better than business networks, because, most of them are connected for transferring information, but businesses are competitors and they don't disclose their sensitive information. Utilizing "honeypots" is a good solution for tracing hackers and revealing their tools. In this paper, "connectedness" of E-government networks and honeypots are employed to propose an approach for securing a fault tolerance E-government network. This framework solves the problem of low interaction and high interaction honeypots. It creates an environment for hackers that they can use many of the available resources; at the same time, it prevents them from misusing those resources for future attacks.
In the development of 3G devices, all element of multimedia (text image audio and video) are used. To use this information, a channel of high bandwidth and more secured system is required. In this era, network security has become an issue of importance, on which lot of research is going on. We have proposed image encryption method using elliptic curve cryptography (ECC). RSA is too slow compared to ECC because ECC required smaller key size. In this method, every pixel of the original image is transformed into the elliptic curve point (Xm,Ym), these elliptic curve point convert into cipher image pixel. The resulting system gives comparatively small block size, high speed and high security.
With the rapid development of World Wide Web, the Web malicious attackers have taken the initiative jamming in Chinese to transform the form of the key words to be avoided being mined by the software existed. So how to filter the unhealthy webpage quickly and effectively has become the main content of the Web security. Because of the limitation of traditional rigid strings matching on the key words mining in Chinese, people have turned to research flexible string matching. Based on the Zhou-Zhang algorithm and the technique of key words mining in Chinese, this paper designed and realized a key words mining system about malicious jamming in English. The details in the design of the program and the realization of the process are introduced. The experiments prove that more than 98%of the key words about malicious jamming in English in the document could be mined and then be judged whether it is healthy by this system accurately.
Web services are often deployed with critical software bugs that can be maliciously exploited. Web vulnerability scanners are regarded as an easy way to test web applications against security vulnerabilities. However, previous research shows that the effectiveness of these tools in web services environments is very poor. In fact, the high number of false-positives and the low coverage observed in practice highlight the strong limitations of these tools. The goal of this paper is to demonstrate that it is possible to develop a vulnerability scanner for web services that performs much better than the commercial ones currently available. Thus, we propose an approach to detect SQL Injection vulnerabilities, one of the most common and most critical types of vulnerabilities in web environments. Experimental evaluation shows that our approach performs much better than well-known commercial tools, achieving very high detection coverage while maintaining the false positives rate quite low.
In the wireless sensor network, network security communication has gradually attracted the attention of network architecture designers and researchers and key distribution protocol plays a fundamental role in the secure transmission of wireless sensor network. This paper mainly describes the key pre-distribution algorithm of binary t-order polynomial, its principle is similar to Blom key pre-distribution protocol and the key pair generation model is finished according to a binary t-order polynomial f(x,y) defined in finite field F(q). Through the analysis of TinyOS integrated file, we obtain the bottom operation mechanism of TinyOS operating system and write the correlative codes in the system to realize the random generation of key and a low power network security communication process.
With rapidly growing the network bandwidth to multi-gigabits, the need for high-performance and wire-speed network security tools, such as network intrusion detection systems (NIDS), is vital. Instead of using proprietary custom network adapters, there are some flexible software solutions, such as nCap or DMA ring, which can be used with low-cost commercial network adapter products. In this paper we introduce a new solution, called DashCap, for high-speed packet reception and transmission. This solution proposes multi-core aware features, such as load-balancing incoming traffic among multiple processes or threads, which are not offered in the existing solutions. Using the proposed solution, it is possible to design and implement high-performance multi-threaded NIDSs or application-layer firewalls completely in user space and with better utilization of computational resources of multi-processor/multi-core systems.
The access loop identification is very important to improve the network security. Tthe methods to implement access loop- identification and traceability are very important to the broadband network. Some methods of access loop identification need to get user information. This paper introduces access loop identification methods in broadband network.some methods
With the increase of the web applications in information society, web application software security become more and more important. Recent investigations show that web application vulnerabilities have become the largest security threat. Websense security report shows that in the first half of year 2008 above 75%of the most popular web site have utilized by the hackers to run malicious code. Detecting and solving vulnerability is the effective way to enhance web security. In this paper we focus on the regression test in web vulnerability detection, and present a strong-association rule based algorithm to make the detection more efficient. In the first step we traverse the whole web site to get the web page collection. And then, in the regression test, we make the association between the pages and expand the pages to a collection set. The set will used in the following iterate traverse. And we define the relational grade to describe the association. Finally, we do the experiment on our target web site which contains the known vulnerabilities such as XSS and SQL injection, and the result shows that the algorithm can detect almost all the pages that may contains vulnerabilities in the target web site.
In recent years, IP over WDM has become a key technology of choice for meeting the tremendous bandwidth demand in the optical infrastructure. Survivability is an emerging discipline that blends optical network security with business risk management for the purpose of protecting highly distributed information services and assets. No system is totally immune to attacks, accidents, or failures. Therefore, the focus of this new discipline is not only to that suddenly accident, but also to ensure that mission-critical functions are sustained and a essential set of services is delivered, despite the presence of accident. Improving survivability is very important for optical network.
At present, the focus of network security research mainly centers on the increase of detection ability of a single detection tool. For example, improve the accuracy and detection efficiency of firewall and intrusion detection system but despise the defense ability of the whole network system. This paper puts forward the concept of network active defense system and emphatically analyzes its architecture, working principle, software platform, hardware platform, and development platform and data packet acquisition mode.
In this paper, on the basis of the research on Honeypot technology, in view of the many problems in current traditional security resource applications, the Honeypot technology is used in network security defense, and a Honeypot-based distributed intrusion prevention model is presented. The experimental results show that the program can successfully remedy the deficiencies of existing monitoring system and improve the performance of the safety defense systems.
The paper analyzes plug-in technology ways to be used in vulnerability scanners at home and abroad. So far plug-in development in the vulnerability scanners has not arrived the unified standard. If the public standard is designed for plug-in in vulnerability scanners, it will add flexibility of write plug-in (not confined to special language). Plug-in is designed by system development staff. The terminal users on-line upgrade plug-in library similar to upgrade virus library. Combining with COM standard widespread used in Windows platform, the paper puts forth the designing idea of scanning master routine and the COM plug-in in the vunerability scanners, and verifies ideas.
Network security situation is a hot research realm in the area of network security, which helps security analysts to solve the challenges they encounter. This paper presents the evaluation index, and the characteristic attributes to describe the state of network situation, discusses the grey model includes grey correlation model and grey forecast algorithm to get the evaluation index and predication of network situation. The network situation based on the grey model using evaluation index .is proposed and the experimental result is shown in this paper.
In an environment where computer compromises are no longer anomalies, but are frequent occurrences, the field of computer forensics has increasingly gained importance. The development of this forensic field is matched by a growth in anti-forensic techniques. To overcome potential difficulties with external applications, operating systems should contain methods for storing and protecting meaningful information. The Linux Ext3 journal is one source of information that should be fully utilized for its intended purpose and forensics as well. However, due to its limited size and circular nature, this source of information has restrictions that can be addressed by the operating system. For example, when collecting and examining Ext3 journal data, it can be difficult to determine the filename that an inode number is associated with. In this paper, the design of a method for honeypots is presented which takes advantage of the Virtual File System Layer in Linux to address this difficulty. This technique allows the translation of inode numbers to filenames in a historical context thereby providing a forensic analyst with a better picture of what has transpired.
In recent years, cost-sensitive intrusion response has gained significant interest, mainly due to its emphasis on the balance between potential damage incurred by the intrusion and cost of the response. However, one of the challenges in applying this approach is defining a consistent and adaptable measurement of these cost factors on the basis of system requirements and policy. In this paper,we present a host-based framework for the cost-sensitive assessment and selection of intrusion response. Specifically,we introduce a set of measurements that characterize the potential costs associated with the intrusion handling process, and propose an intrusion response evaluation method with respect to the risk of potential intrusion damage, the effectiveness of the response action and the response cost for a system. We provide an implementation of the proposed solution as an IDS-independent plugin tool and demonstrate its advantages on the several attack examples.
In this paper we show that it is possible to attack and gain control over PROFINET IO nodes and also that this can be done without any of the communicating peers detecting the attack. Analysis of attacks in both shared and packet switched networks show that the attacker can control the process data and thus the state of the machines connected to the I/O modules. As the security risks are increasing in automation with the level of vertical and horizontal integration, the concept of security modules is proposed towards a method to retrofit security in PROFINET IO. The concept of security modules can be applied without changing anything in the underlying transmission system and is extendable if and when new security threats are identified.
In many wireless sensor network applications have a demand for high reliability transmission, in view of this need to design and achieve a reliable multi-path routing (RMR) protocol is a suitable application for wireless sensor network. In the trunk path failure through the path of transmission of data backup methods, the routing protocol improves reliability of data transmissions by the agreement establishing backup path. Finally performed simulation tests, results show that the routing protocol can effectively reduce packet loss caused by the path of fracture, and improve overall network reliability of data transmissions in the trunk path failure.
The SIP-based VoIP system is build on the IP network, so it is affected by the IP network security problem. SIP-based VoIP system has many security problems because of the security mechanism of VoIP system and other external factors. These effects relate to the following three aspects: confidentiality, integrity and availability. This paper analyzed the mechanism of VoIP system and the components of SIP: SIP UA and SIP network server. The attacks on the SIP system, such as the registration hijacking, impersonating a proxy, denial of service and spam are discussed in detail. At last this paper points out the insufficiency of the SIP security mechanism, including certification attack, DoS attack and spam attack. SIP-based VoIP system involves so many security aspects that relational technology is used to assure the security of SIP-based VoIP system.
The growing concern about global climate change has led the European Union and the Portuguese Government to set targets for the percentage of electricity to be produced from renewable sources. In order to achieve the defined targets, Distributed Generation (DG) is expected to be increasingly integrated into networks. However, the intermittency of some of those DGs (such as wind energy) may enhance network operating costs or decrease network security. Thus, Network Operators started to concern about these effects and in order to avoid them, new wind parks were required to provide ancillary services to the network. These ancillary services include the ride-through-fault capability. Although some wind parks can already supply ride-through-fault capability to the distribution network (i.e. wind parks with Double Fed Induction Generators (DFIG)), most of them are still largely unable to do so due to the current DG protection scheme. This work concentrates on the development of new settings for the DG protection scheme which aims at allowing DG to provide ride-through fault capability to the distribution network. A DFIG with ride-through-fault capability was modeled on PSCAD/EMTDC and tested under the Portuguese Distributed Generation Protection Scheme Regulation Code. New relay settings for the DG protection scheme are advanced and simulated on PSCAD/EMTDC software in order to permit DGs providing ride-through fault capability to the distribution network. Conclusions of the new relay settings performance are withdrawn and commented on.
The document describes the approach by ENEL Distribuzione in integrating remote control systems in the TSO system architecture, in order to improved network security by new techniques of load shedding.
In recent years power system companies are trying to save money and improve their capabilities or convenience by using consumer network technology in their ICT networks for maintaining and monitoring power systems.
User's mobility is increasing with the height level of mobility that the mobile networks offer. The information transmitted from A to B in a network has to be secured from any eavesdropping, so the main solution used is the cryptography.
The dynamic partial reconfiguration technology enables an embedded system to adapt its hardware functionalities at run-time to changing environment conditions. However, reconfigurable hardware functions are still managed as conventional hardware devices, and the enhancement of system performance using the partial reconfiguration technology is thus still limited. To further raise the utilization of reconfigurable hardware designs, we propose a virtual hardware mechanism, including the logic virtualization and the hardware device virtualization, for dynamically partially reconfigurable systems. Using the logic virtualization technique, a hardware function that has been configured in the field-programmable gate array (FPGA) can be virtualized to support more than one software application at run-time. Using the hardware device virtualization, a software application can access two or more different hardware functions through the same device node. In a network security reconfigurable system for multimedia applications, our experimental results also demonstrate that the utilization of reconfigurable hardware functions can be further raised using the virtual hardware mechanism. Furthermore, the virtual hardware mechanism can also reduce up to 26%of the time required by using the conventional hardware reuse.
This article investigates network policies and mechanisms to enhance security in SCADA networks using a mix of TCP and UDP transport protocols over IP. It recommends creating a trust system that can be added in strategic locations to protect existing legacy architectures and to accommodate a transition to IP through the introduction of equipment based on modern standards such as IEC 61850. The trust system is based on a best-of-breed application of standard information technology (IT) network security mechanisms and IP protocols. The trust system provides seamless, automated command and control for the suppression of network attacks and other suspicious events. It also supplies access control, format validation, event analysis, alerting, blocking, and event logging at any network-level and can do so on behalf of any system that does not have the resources to perform these functions itself. Latency calculations are used to estimate limits of applicability within a company and between geographically separated company and area control centers, scalable to hierarchical regional implementations.
This paper is dealing with the important task of protection coordination maintaining network security in nationwide power systems under changing network conditions. Existing protection systems have to be analyzed and checked under higher loading and contingency conditions. Adapted protection concepts should be evaluated applied to the network. Nowadays numerical simulation tools like PSSSINCAL provide the possibility of a holistic simulation of network and protection behavior. This paper is presenting the results of a protection coordination study based on the proposed method. The collection of protection and network data has been done on the basis of data plausibility algorithms firstly. Numerous different network contingencies were investigated. The software assistance and the visualisation of the enormous amount of output data are described. Generic network and protection configurations leading to maloperation can be clearly found. Further on the possibility of the development of systematic solutions covering the identified practical constraints under the changing network conditions is provided.
The paper describes typical developments of blackouts from simple cause to cascading outages. Possible measures to increase the system observability, combined with power electronic equipments as controlling elements, can lead to a future closed loop control of the power supply. The possible increase of the overall network security by protection security assessment (PSA) and dynamic network security assessment (DSA) will be shown. The described network and protection simulations, which are available today for the transmission sector, have to be adopted to future distribution systems with dispersed generation.
Electric utilities require secure network and control system communications that conform to the recommendations of the NERC (North America Electric Reliability Corporation) CIP (Critical Infrastructure Protection) cyber security framework for identification and protection of critical cyber assets to support reliable operation of the bulk electric system. Major technological requirements of secure control networks are summarized in this paper. Implications of the newly developed IEC 62351 security standard for SCADA data and communication protocols are included. The paper reaches beyond the current standards and recommendations, in that it introduces a new reference secure control network architecture that is a sound foundation for conformance to the regulatory security requirements and industry standards related to control network operation, like NERC CIP, IEC and NIST.
Broadband networks have been well developed and increasedly applied in Chinese industries' and organizations networks. The security problems in broadband networks are discussed widely and frequently. There exit various network security protection tools, such as firewall, IDS (Intrusion Detection System), leak scanning and so on. All these tools deal with packet heads, not with detailed packet content. But some attacks, for example, guessing password, can only be examined in packet content. This paper presents a broadband network monitoring system (BBNMS) based on Libnids, which is an implementation of an E-component of Network Intrusion Detection System. In the system not only packet heads but packet contents can be analyzed. The paper has explored three main topics: packet content analysis, packet real-time analyzing with optimized programming technologies and enhancing Libnids function to detect PPPoE protocol packets of Broadband network. Empirical results illustrate that the intrusion detection model can detect intrusion more accurately.
Network security problems emerge in an endless stream and cause the inestimable damage. To solve network security problems efficiently, it is not enough to make good protection at nodes or protect networks from outside attacks. Many network security problems should be solved efficiently in collaborative approaches which can integrate various resources over internet to defense network security. In this paper, we have designed and implemented a collaborative network security platform based on P2P system. The nodes participated in the P2P system can publish their designed defensible services against network security problems. Based on the published services, collaborative network applications can be developed easily to solve the network security problems on demand. An experiment against TCP SYN flooding attack is demonstrated by the designed defensible services including packets sniffing, forwarding, filtering, and logging services, which can trace the attack origins and filter malicious traffic efficiently.
Arc-flash detection sensors provide a cost-effective way to reduce arc-flash energy by minimizing detection times. High-speed light detection and tripping can compromise protection security by misoperating during changing light conditions. Trip circuits using arc-flash light detection should be supervised using overcurrent protection with similar fast detection speeds. Combining arc-flash detection and high-speed overcurrent from a protective relay provides fast tripping and security, using both instantaneous overcurrent and light from the arc flash. The combination of relay and arc sensor provides independent fault detection with two separate technologies, thereby eliminating false trips from lighting and providing the fastest detection and tripping possible. Time coordination delays are eliminated when the arc is detected concurrently with an overcurrent. This paper presents the advantages of fast overcurrent detection combined with arc-flash measurement to produce a sensitive, fast, and secure tripping scheme.
In this paper, robust adaptive tracking control is proposed for ocean surface vessels based on neural network. In the tracking control design, parametric uncertainties, unknown disturbances and input saturation are explicitly considered. Using neural network (NN) approximation and backstepping control techniques, full state feedback control and output feedback control are investigated to tackle system uncertainties and control input saturation. An auxiliary design system is presented to analyze the effect of input saturation, and states of auxiliary design system are utilized to develop the tracking control. Under the both of developed tracking control approaches, semi-global uniform boundedness of all closed-loop signals are guaranteed via Lyapunov analysis. Simulation studies are given to illustrate the effectiveness of the proposed tracking control.
Authentication, access Control, encryption and auditing make up the essential elements of network security. Researchers have dedicated a large amount of efforts to implement security features that fully incorporate the use of all these elements. Currently, data networks mainly provide authentication and confidentiality services. Confidentiality alone is not able to protect the system, thus, suitable security measures must be taken. However, this security is itself an overhead which must be accounted for. A trade-off must exist between performance and security. This trade-off must be carefully managed so as not to deteriorate the systems being secured. This calls for the true cost of security to be accurately measured. This research work measures the overheads involved in implementation of security using IPSec in different wired/wireless data network protocols.
Security is amongst one of the major issues in Broadband Wireless Access (BWA) Networks. After the launch of the IEEE 802.16 standard (WiMAX), a number of security issues were reported in several articles. Ever since the beginning, work has been in progress for the neutralization of these identified threats. In this paper, the analysis of the authentication protocols implemented in WiMAX has been presented along with the description of the threats posed to them. The paper also describes security sub-layer and limitations of the existing architecture. An approach has also been presented for the prevention of these threats like the avoidance of replay; suppress replay and man-in-the-middle attacks. The proposed approach enhances the network security and strengthens it by resynchronization of the Subscriber Station with the Base Station.
To effectively evaluate and predict network security situation, a quantitative model for network security situation awareness based on artificial immune system and grey theory is proposed. In this model, the formal definitions of self, non-self, antigen and detector is given. According to the relationship between the antibody-concentration of memory detector and the attack intensity of network, network security situation evaluation sub-model based on artificial immune system is given. And to forecast the attack intensity that the current network faces in the next step, network situation predication sub-model based on grey theory is given. Experimental results exhibit that the proposed model provides a novel approach for network security situation awareness, and holds better characters of self-adaptability and real-time processing.
The mass of network security data is automatically and intelligently related and analyzed, which reveals the logic relations and the attack intention hidden behind the events. Sorting the attacks on various severities, a variety of fast and intuitive analysis report is generated. It can comprehensively monitor network status, effectively guide network security management, effective prevention, block or reduce security threats, which is a very valuable and necessary means to network security. Through grey relation, the multidimensional reference space data rows, the comparing rows of main characteristic indexes and the factor indexes have been set up. The relation coefficients and degree are computed and the quantity of dynamic trend of network security events is analyzed. Through the expression of correlation coefficient and case study, it can be concluded that the grey relation is effective to discuss and analyze the network security events predominantly.
An important problem in network security risk assessment is to uncover network threats due to current software vulnerabilities and misconfigurations. This paper proposes a logic-programming approach to conduct this risk assessment automatically. We use Datalog to specify network security property states and attack rules. The threat analysis could be conducted by a logic-programming engine that can evaluate Datalog efficiently (such as XSB [1]). We analyze trace proofs produced by the reasoning engine, and get threat information of evaluated network system. After identifying the threats, we apply game theory to compute threat risks. A simple network attack has been simulated to illuminate the appliance of the new approach. Results on how the approach has been able to help the system administrator understand the threat risks of attacks and take countermeasures accordingly are also analyzed.
Arc-flash detection sensors provide a cost-effective way to reduce arc-flash energy by minimizing detection times. High-speed light detection and tripping can compromise protection security by misoperating during changing light conditions. Trip circuits using arc-flash light detection should be supervised using overcurrent protection with similar fast detection speeds. Combining arc-flash detection and high-speed overcurrent from a protective relay provides fast tripping and security, using both instantaneous overcurrent and light from the arc flash. The combination of relay and arc sensor provides independent fault detection with two separate technologies, thereby eliminating false trips from lighting and providing the fastest detection and tripping possible. Time coordination delays are eliminated when the arc is detected concurrently with an overcurrent. This paper presents the advantages of fast overcurrent detection combined with arc-flash measurement to produce a sensitive, fast, and secure tripping scheme.
In this paper, robust adaptive tracking control is proposed for ocean surface vessels based on neural network. In the tracking control design, parametric uncertainties, unknown disturbances and input saturation are explicitly considered. Using neural network (NN) approximation and backstepping control techniques, full state feedback control and output feedback control are investigated to tackle system uncertainties and control input saturation. An auxiliary design system is presented to analyze the effect of input saturation, and states of auxiliary design system are utilized to develop the tracking control. Under the both of developed tracking control approaches, semi-global uniform boundedness of all closed-loop signals are guaranteed via Lyapunov analysis. Simulation studies are given to illustrate the effectiveness of the proposed tracking control.
Authentication, access Control, encryption and auditing make up the essential elements of network security. Researchers have dedicated a large amount of efforts to implement security features that fully incorporate the use of all these elements. Currently, data networks mainly provide authentication and confidentiality services. Confidentiality alone is not able to protect the system, thus, suitable security measures must be taken. However, this security is itself an overhead which must be accounted for. A trade-off must exist between performance and security. This trade-off must be carefully managed so as not to deteriorate the systems being secured. This calls for the true cost of security to be accurately measured. This research work measures the overheads involved in implementation of security using IPSec in different wired/wireless data network protocols.
Security is amongst one of the major issues in Broadband Wireless Access (BWA) Networks. After the launch of the IEEE 802.16 standard (WiMAX), a number of security issues were reported in several articles. Ever since the beginning, work has been in progress for the neutralization of these identified threats. In this paper, the analysis of the authentication protocols implemented in WiMAX has been presented along with the description of the threats posed to them. The paper also describes security sub-layer and limitations of the existing architecture. An approach has also been presented for the prevention of these threats like the avoidance of replay; suppress replay and man-in-the-middle attacks. The proposed approach enhances the network security and strengthens it by resynchronization of the Subscriber Station with the Base Station.
To effectively evaluate and predict network security situation, a quantitative model for network security situation awareness based on artificial immune system and grey theory is proposed. In this model, the formal definitions of self, non-self, antigen and detector is given. According to the relationship between the antibody-concentration of memory detector and the attack intensity of network, network security situation evaluation sub-model based on artificial immune system is given. And to forecast the attack intensity that the current network faces in the next step, network situation predication sub-model based on grey theory is given. Experimental results exhibit that the proposed model provides a novel approach for network security situation awareness, and holds better characters of self-adaptability and real-time processing.
The mass of network security data is automatically and intelligently related and analyzed, which reveals the logic relations and the attack intention hidden behind the events. Sorting the attacks on various severities, a variety of fast and intuitive analysis report is generated. It can comprehensively monitor network status, effectively guide network security management, effective prevention, block or reduce security threats, which is a very valuable and necessary means to network security. Through grey relation, the multidimensional reference space data rows, the comparing rows of main characteristic indexes and the factor indexes have been set up. The relation coefficients and degree are computed and the quantity of dynamic trend of network security events is analyzed. Through the expression of correlation coefficient and case study, it can be concluded that the grey relation is effective to discuss and analyze the network security events predominantly.
An important problem in network security risk assessment is to uncover network threats due to current software vulnerabilities and misconfigurations. This paper proposes a logic-programming approach to conduct this risk assessment automatically. We use Datalog to specify network security property states and attack rules. The threat analysis could be conducted by a logic-programming engine that can evaluate Datalog efficiently (such as XSB [1]). We analyze trace proofs produced by the reasoning engine, and get threat information of evaluated network system. After identifying the threats, we apply game theory to compute threat risks. A simple network attack has been simulated to illuminate the appliance of the new approach. Results on how the approach has been able to help the system administrator understand the threat risks of attacks and take countermeasures accordingly are also analyzed.
Although web services are becoming business-critical components, they are often deployed with critical software bugs that can be maliciously explored. Web vulnerability scanners allow detecting security vulnerabilities in web services by stressing the service from the point of view of an attacker. However, research and practice show that different scanners have different performance on vulnerabilities detection. In this paper we present an experimental evaluation of security vulnerabilities in 300 publicly available web services. Four well known vulnerability scanners have been used to identify security flaws in web services implementations. A large number of vulnerabilities has been observed, which confirms that many services are deployed without proper security testing. Additionally, the differences in the vulnerabilities detected and the high number of false-positives (35%and 40%in two cases) and low coverage (less than 20%for two of the scanners) observed highlight the limitations of web vulnerability scanners on detecting security vulnerabilities in web services.
This article discusses the concept of cyberspace as it applies to government. Government employees must be "cyberwarriors" and accountable for their actions while using IT, and CIOs and CISOs must be aligned to effectively balance security and access.
The role of communications networks in military operations continues to grow in importance, with mission areas such as covert special operations, time-critical targeting, command and control, and logistics all relying heavily on networks and network applications. While networks carry the promise of increased flexibility and efficiency for defense organizations, they also present a myriad of new challenge areas arising from unique operating conditions and environments and the need for high levels of network security. The rapidly increasing dependence on networks makes it especially important that these challenges are clearly understood and addressed, and infrastructures made robust and secure from current and future cyber threats.
There is a growing threat of illegal computer access to web-based application and security measures are now being implemented for applications that incorporate security logic in addition to business logic. This paper introduces a framework for web security, which protects against illegal computer access to resources by granting different layers of privileges to different user roles. The role-based access control (RBAC) features are analyzed and implemented in the most recent versions of Java Application server and PointBase DB Server. RBAC features are categorized under three broad areas: user role assignment, supporting tolr relationships and credentials, and assignable privileges. These features provide a sound basis for implementing the basic features of RBAC.
In this paper, it is focused on the method about the distributional database backup system. The system consists of MySQL database backup system in the data gathering station and a client application procedure for MySQL database in monitor center. Under the condition of not increasing the equipment cost, the system can resolve the problem of the lost gathering data in the long time running gathering system, whose data backup always uses the single database system. According to the industrial machine hardware configuration and the sensor data characteristic, the distributional database essential entity model is established. The effective index and the backup strategy are designed by the actual demand and the practice. And the dual cycle buffer and multithread database interface solves the hard disk I/O speed bottleneck. The problem of the network transmission pressure that is caught by the great data capacity is relaxed by the data compression algorithm.
Network-based attacks are so devastating that they have become major threats to network security. Early yet accurate warning of these attacks is critical for both operators and end users. However, neither speed nor accuracy is easy to achieve because both require effective extraction and interpretation of anomalous patterns from overwhelmingly massive, noisy network traffic. The intrusion detection system presented here is designed to assist in diagnosing and identifying network attacks. This IDS is based on the notion of packet dynamics, rather than packet content, as a way to cope with the increasing complexity of attacks. We employ a concept of entropy to measure time-variant packet dynamics and, further, to extrapolate this entropy to detect network attacks. The entropy of network traffic should vary abruptly once the distinct patterns of packet dynamics embedded in attacks appear. The proposed classifier is evaluated by comparing independent statistics derived from five well-known attacks. Our classifier detects those five attacks with high accuracy and does so in a timely manner.
Based on this essay, we introduce the current network security technologies and problems. Also, we analyze the excellence and weakness of the firewall technologies, the intrusion detection and prevention technology in order to integrate the firewall technology and intrusion prevention technology, which can help to improve the real-time check, accuracy and recovery ability of the network security system.
In this paper, it is focused on the method about the distributional database backup system. The system consists of MySQL database backup system in the data gathering station and a client application procedure for MySQL database in monitor center. Under the condition of not increasing the equipment cost, the system can resolve the problem of the lost gathering data in the long time running gathering system, whose data backup always uses the single database system. According to the industrial machine hardware configuration and the sensor data characteristic, the distributional database essential entity model is established. The effective index and the backup strategy are designed by the actual demand and the practice. And the dual cycle buffer and multithread database interface solves the hard disk I/O speed bottleneck. The problem of the network transmission pressure that is caught by the great data capacity is relaxed by the data compression algorithm.
Network-based attacks are so devastating that they have become major threats to network security. Early yet accurate warning of these attacks is critical for both operators and end users. However, neither speed nor accuracy is easy to achieve because both require effective extraction and interpretation of anomalous patterns from overwhelmingly massive, noisy network traffic. The intrusion detection system presented here is designed to assist in diagnosing and identifying network attacks. This IDS is based on the notion of packet dynamics, rather than packet content, as a way to cope with the increasing complexity of attacks. We employ a concept of entropy to measure time-variant packet dynamics and, further, to extrapolate this entropy to detect network attacks. The entropy of network traffic should vary abruptly once the distinct patterns of packet dynamics embedded in attacks appear. The proposed classifier is evaluated by comparing independent statistics derived from five well-known attacks. Our classifier detects those five attacks with high accuracy and does so in a timely manner.
Based on this essay, we introduce the current network security technologies and problems. Also, we analyze the excellence and weakness of the firewall technologies, the intrusion detection and prevention technology in order to integrate the firewall technology and intrusion prevention technology, which can help to improve the real-time check, accuracy and recovery ability of the network security system.
Recently, major advances have been made in the area of honeypot technologies. These include the development of very accurate and reliable detection methods for unknown attacks targeted at memory corruption vulnerabilities and the design of efficient network architectures. These architectures allow to monitor a large network of IP addresses applying advanced detection methods for zero-day exploits and new Internet worms. Such an advanced architecture and detection method was developed by the NoAH research project funded by the Sixth EUs Framework Programme for Research and Technological Development. A pilot testbed was set up to demonstrate its effectiveness to detect well-known as well as new attacks on the Internet. While the technical components are well-understood, the interpretation and analysis of the resulting information is to the best of our knowledge still not fully explored by research projects. For the NoAH pilot testbed, a critical test to demonstrate its effectiveness arose with the appearance of the W32.Conficker worm in November 2008. In this paper we present the experimental results of this testbed focusing on the detection and analysis of the W32.Conficker worm which is still widely spread and an ongoing threat to the Internet. In detail, we introduce the detection process starting with the first suspicion of a new Internet worm towards its analysis and capture of malware.
This paper addresses the adaptive neural network tracking control problem for a class of strict-feedback systems with unknown non-linearly parameterised and time-varying disturbed function of known periods. Radial basis function neural network and Fourier series expansion are combined into a new function approximator to model each suitable disturbed function in systems. Dynamic surface control approach is used to solve the problem of `explosion of complexity? in backstepping design procedure. The uniform boundedness of all closed-loop signals is guaranteed. The tracking error is proved to converge to a small residual set around the origin. A simulation example is provided to illustrate the effectiveness of the control scheme designed.
We propose an efficient and decentralized method for detecting change-points in high-dimensional data. This issue is of growing concern to the network security community since, in this context, network anomalies such as denial of service (DoS) attacks are likely to lead to statistical changes in Internet traffic. Our method proposes a way of distributing a centralized approach called TopRank, which consists of a data reduction stage based on record filtering, followed by a nonparametric change-point detection test based on U-statistics. The key point is to aggregate censored time series built locally and to perform a nonparametric test for doubly censored time series resulting from this aggregation. With this new approach, called distributed TopRank in the following, we can address massive data streams and perform network anomaly detection and localization on the fly while limiting the quantity of data exchanged within the network.
The following topics are dealt with: wireless and mobile communications; wireless simulation; wireless network security; mobile network management and protocol.
The study presents the major factors for Internet banking adoption and compares the levels of adoption across countries, in order to identify more easily what factors to consider most while providing banking services over the Internet. Based on prior studies, web security, Internet usage, economy status, high branch intensity, competition, government prioritization regulations, and literacy level were identified as the major factors affecting Internet banking adoption. This study uses fuzzy inference systems (FIS) to define the adoption rate. Our experimental results show that security is the most important factor because no matter how high government prioritization, literacy level, Internet users, and competition among Internet service providers are, as long as there are low security levels, the adoption rate will be at the lowest level. We conclude that, overall, the banks-specific factors are the main drivers for Internet banking adoption.
This article introduced the existing third-party mobile payment model, as well as third-party network security payment model and analysis their pros and cons and then concluded a mobile payment model of a combination of the two models - mobile payment model based on third-party security .Then systematically introduced the model structure, made the detailed design of their payment flow and pointed out its market advantages and characteristics of the application.
This paper deals with one of the issues consequent on the recent restructuring of the electricity industry: the lack of coordination between generation and transmission system development. Indeed, in this new environment the planning of these two systems is driven by different objectives, resulting in a possible conflict between economics and security. The nodal index called Weighted Transmission Loading Relief (WTLR), recently proposed in literature, seems to be able to supply clear information about the impact of generation and transmission system investments on network security. Therefore, the authors of this paper have developed an automatic procedure to calculate and represent graphically these indices. The procedure has been applied to a detailed model of the Italian EHV network with reference to a summer peak load condition at the projection horizon of the year 2013, in order to highlight the various kinds of information which the above mentioned indices can supply.
Protection security assessment is an important task in nationwide power systems maintaining network security under changing network conditions. It can be reached preferably by a software assisted method of analysis which is presented in this paper. Existing protection systems have to be analyzed and checked under higher loading and contingency conditions. Adapted protection concepts should be evaluated applied to the network. Nowadays numerical simulation tools like PSS?SINCAL provide the possibility of a holistic simulation of network and protection behavior. This paper is presenting the proposed method of analysis and gives some results applying this method on a real transmission system. The collection of protection and network data has been done on the basis of data plausibility algorithms firstly. Numerous different network contingencies were investigated. The software assistance and the visualisation of the enormous amount of output data are described. Generic network and protection configurations leading to maloperation can be clearly detected.
Pricing for the use of the networks is essential in the way that it should be able to reflect the costs/benefits imposed on a network when connecting a new generator or demand and to provide forward-looking message to influence the site and size of future network customers. Studies have been extensively carried out over the years to achieve this pricing goal. Few methodologies can directly link nodal generation/demand increment to network long-run marginal/incremental costs. Even fewer consider network security in their pricing methodologies, considering it is one of the most important cost drivers. All networks are designed to be able to withstand credible contingencies, but this comes at a significant cost to network development. This paper proposes a new approach that can establish the direct link between nodal generation/demand increment and changes in investment cost while ensuring network security. The investment cost is reflected by the change in the spare capacity of a network asset from a nodal injection, which is in turn translated into an investment horizon, leading to the change in the present value of a future investment cost. The security is reflected in the pricing through a full  ${rm N}-1$ contingency analysis to define the maximum allowed power flow along each circuit, from which the time horizon of future investment is determined. This paper illustrates the implementation of the proposed pricing model for a system whose demand grows either at a uniform rate or at variable growth rates. The benefits of introducing security into the long-run pricing model are demonstrated on the IEEE 14-busbar system and a practical 87-busbar distribution network.
Abstract--Dealing with network security requires knowledge of the attacker. The question of attacker motivations is complex. This paper is a literature review examining whether attacker motivations are homogenous or heterogeneous. This is part of an ongoing research effort to characterize system attackers with the goal of helping to mold policy decisions.
In this paper we propose a new reciprocity index for quantifying social relationships based on mobile phone call detail records and Twitter blogs. We use this reciprocity index to measure the level of reciprocity between users. This work is useful for detecting unwanted calls (e.g., spam) and product marketing. For validation of our results, we used actual call logs of 100 users collected at MIT by the Reality Mining Project group for a period of 8 months and Twitter blogs of 460 users collected by the Network Security team at UNT for a period of 12 months. The experimental results show that our model achieves results with high accuracy.
In this paper we propose a new reciprocity index for quantifying social relationships based on mobile phone call detail records and Twitter blogs. We use this reciprocity index to measure the level of reciprocity between users. This work is useful for detecting unwanted calls (e.g., spam) and product marketing. For validation of our results, we used actual call logs of 100 users collected at MIT by the Reality Mining Project group for a period of 8 months and Twitter blogs of 460 users collected by the Network Security team at UNT for a period of 12 months. The experimental results show that our model achieves results with high accuracy.
The following topics are dealt with: information assurance; security coding; cryptanalysis; model retrieval; data hiding; Internet security; electronic commerce security; network security; system design; image processing; sensor network; identity validation; Internet protocol; image clustering; feature analysis; e-payment security; image filtering; watermarking; steganography; and visual cryptography.
Intention recognition is the ability to predict an opposing forcepsilas high level goals. Knowing an attackerpsilas intention can support the decision-making of the network security administrators. Furthermore intent analysis plays an import role in the calculation of the inherent threat value. So how to recognize attack intention has become a research hot in network security domain recently.In this paper taxonomy of attack intention characterized by consequences of attack and targets of attack is introduced at first. Then a graphical model based on network security state is presented and used to recognize attack intention. D-S evidence theory is also introduced to deal with the uncertainty in the process of intent inference. Next algorithms of intention recognition and threat assessment are given in detail in order to offer a way to assess the network security situation. Finally several experiments are done in a local network. The results of the experiments prove the feasibility and validity of this method.
From the network birth to the vigorous development, as well as many concomitant security problems, all symbolized the network developing process approaches the ecological organization evolution process. But todays tradition network security defense architecture is exposing the mechanical and stiff malpractice gradually, which is unable to deal with a variety of the security threats that are latent and multiply unceasingly in the network world. A novel eco-defending architecture for network security similar to an artificial ecosystem is proposed, which builds an artificial ecosystem simulating and implementing network security self-organization and evolution. We give its related definitions, design and critical functional components that emulate different lymphocytes and fleeting contacts between them, and finally it shows many unique properties in ecological organization evolution process, such as high degree of autonomy, distribution, evolution, and scalability and so on.
Network security is attracting more and more attention. Simulation is a better choice to research the problems of network security because of their high complexity. Based on the purpose and actuality of simulation of network security, this paper puts forward a simulation method of network security using system dynamics. After giving the steps of system dynamics simulation of network security, this paper has simulated the attack of worm using system dynamics. The simulation results indicate system dynamics can describe the processes of worm attack well. The research of system dynamics of network security will extend the methods of simulation of network security.
Network security situation awareness (NSSA) is a hotspot in the network security research field, based on the security situation values, decision makers can be aware of the actual security situation of their networks and then make rational decision to make their networks safer. In this paper, we build a multi-level quantization model for NSSA firstly; this model is comprised of three levels, namely, special oriented level, essential oriented level and holistic level. We can not only perform a certain kind of situation awareness, but also an overall one using this model. Different from the previous methods which compute network security situation of whole network just by summing up the values of each assets network security situation, we propose a novel algorithm based on exponential and logarithmic analysis, this novel method is more appropriate to obtain rational results. Our model and algorithm are proved to be feasible and effective through a series of experiments.
Predicting or discovering the possible propagation direction of spreading network worms can efficiently benefit the enforcement of network security countermeasures like blocking them in real-time way. Most worms exhaust all of the network bandwidth maliciously in very short time. This paper proposed a model on predicting the propagation direction between areas based on two key indexes including area-infected-time (AIT) and area-infected-probability (AIP), and calculates alert level for each area by fuzzy reasoning. The higher alert level is, the more likely that the corresponding area is infected by worm in short time, and this area is the propagation direction of worm at the moment. Simulation experimental results show that the early warning model proposed in this paper can deduce area-alert-level (AAL) correctly and predict the propagation direction of network worm dynamically.
Recently, an interest has arisen for network worms that propagate using Domain Name Servers (DNS) in order to discover victim hosts.These worms generate random strings, as possible network domain names, and then query Domain Name Servers in order to discover the corresponding IP addresses. In this paper we present models for the dynamics of the co-evolution of worm agents in the presence of anti-worm agents that move in the network in order to stop worm propagation. The proposed models consider anti-worm agents who know the network and anti-worm agents that do not know it and need to issue queries in order to discover valid IP addresses. We,further, introduce "honeypot'' domain name servers that attempt to lure worms, introducing only a delay and providing no answer.We show that by simply delaying the response to DNS queries issued by the worm has little positive effect on the worms propagation.
Source address authentication is very important to change current serious network security situation. In this article, we propose a new source address validation scheme based on IBS algorithm and study its security using SVO logic. We find that our scheme can successfully guarantees authenticity of IP packetpsilas information source.
The recent era has witnessed tremendous increase in the usage of computer network applications. Users of any type and requirement are compelled to be on a network. Today, the computer has become a network machine rather than a standalone system. This has generated challenges to the network security devices in terms of accuracy and reliability.Intrusion Detection Systems (IDS) are designed for the security needs of networks. Existing Network Intrusion Detection Systems (NIDS) are found to be limited in performance and utility especially once subjected to heavy traffic conditions. It has been observed that NIDS become less effective even when presented with a bandwidth of a few hundred megabits per second. In this work, we have endeavored to identify the causes which lead to unsatisfactory performance of NIDSs. In this regard, we have conducted an extensive performance evaluation of an open source intrusion detection system (Snort). This has been done on a highly sophisticated test-bench with different traffic conditions. We have also used different hardware and software platforms to determine the efficacy of the NIDS under test. Finally, in our results/ analysis, we have identified the factors responsible for the limited performance of Snort. We have also recommended few solutions for improving the performance of Snort.
The increasing use of experimental platforms for networking research is due to their ability to support experimentation with complex systems, like the Internet, that simplistic simulators and small scale testbeds fail to reproduce. Therefore many projects and research initiatives have spawned - mainly in the field of Future Internet architectures. Although numerous publications can be found, most of them refer to prototypes and work in progress rather than to publicly available software that is ready to be widely used for the creation of testbeds. The first contribution is the development of a framework for comparing the available software based on their features. The second contribution is a literature review of state-of-the-art tools and their comparison under common criteria. This systematic analysis allows other researchers to make informed decisions about the usability of already available tools and decrease the initial cost of developing a new testbed, leading to an even wider use of such platforms. Our work provides the reader with a useful reference list of readily available software to choose from while designing or upgrading a research infrastructure, laboratory or experimentation facility.
Due to the increasing demands for network security, distributed intrusion detection has become a hot research topic in computer science. However, the design and maintenance of the intrusion detection system (IDS) is still a challenging task due to its dynamic, scalability, and privacy properties. In this paper, we propose a distributed IDS framework which consists of the individual and global models. Specifically, the individual model for the local unit derives from Gaussian Mixture Model based on online Adaboost algorithm, while the global model is constructed through the PSO-SVM fusion algorithm. Experimental results demonstrate that our approach can achieve a good detection performance while being trained online and consuming little traffic to communicate between local units.
With the rapid prevalence of botnet, Internet is facing growing threats. Botnet detection has recently become very important in the field of network security. Aiming at the weakness of the existing botnet detection architectures, we propose a hierarchical collaborative model, which shares information and cooperates in the three levels of information, feature, and decision-making. On the basis of the proposed model, we design a collaboration-based botnet detection architecture in this paper. With a novel feature extraction module, the architecture is able to extract the essential features of botnet from a variety of data. We believe that the architecture could improve detection accuracy and enhance detection capability.
ZigBee is a wireless sensor network standard that defines network and application layers on top of IEEE 802.15.4's physical and medium access control layers. In the latest version of ZigBee, enhancements are prescribed for the security sublayer but we show in this paper that problems persist. In particular we show that the End-to-End Application Key Establishment Protocol is flawed and we propose a secure protocol instead. We do so by using formal verification techniques based on static program analysis and process algebras. We present a way of using formal methods in wireless network security, and propose a secure key establishment protocol for ZigBee networks.
Integrity is an important aspect of network security. Lack of integrity could result in many problems. In this article, the threats and requirements of the integrity are summarized. Some possible solutions to the integrity are discussed.
Network-based intrusion detection and prevention systems -NIDS /NIPS- have been widely implemented in order to build layered information security infrastructures. Many intrusion detection and prevention algorithms have been developed and its operation modes imply benefits and drawbacks that are not taken into account when choosing a network IDS/IPS. This paper assesses the network security provided by five NIDS/NIPS software solutions, by analyzing the protection against malicious traffic. Security platforms against network intrusion such as Snort Inline, Stonegate IPS, Strataguard, Intrupro and Packetalarm are tested and analyzed.
Recent years have witnessed a huge increase in the number and variety of Internet applications, as well as on the number and diversity of security attacks to network users and systems. Consequently, the need for an accurate mapping of traffic to its corresponding applications has also raised in order to allow ISPs to provide better Quality-of-Service (QoS) standards, implement traffic engineering methodologies and deploy efficient security strategies. Several approaches have been proposed to identify Internet applications, starting from port-based identification and going into the detailed analysis of the packet's payload content or to the statistical analysis of the generated traffic flows. However, even the most efficient methodologies present some constraints that limit their applicability, namely some confidentiality constraints or difficulties to classify traffic with unknown behavior. This paper presents a new methodology for traffic classification that relies on the multiscale analysis of the sampled traffic by estimating the multifractal coefficients of the different traffic flows and grouping them, using clustering techniques, according to their multifractal behavior over different time scales. Besides applying this approach to classify traffic from three of the most important Internet protocols, the methodology's efficiency was also tested by identifying two of the most frequent network security attacks.
This paper proposes a new index to better determine the vulnerability of a power system. The procedure for calculating this index combines stochastic and probabilistic methods to determine the likelihood of a line overloading in a power system. The methods used include a basic load flow analysis using network security factors and Monte Carlo simulation that incorporates the likelihood of component failure within the system. A case study has been performed to analyze the ability of such an index in accurately measuring one aspect of power system vulnerability. Three different variations of this indexing method are explored in an attempt to define an accurate and descriptive measure for this index.
Quality of service (QoS) is critical for delivering real-time media services (RTMSs). The authors identify a new class of denial-of-service attacks against RTMSs  indirect contention-in-hosts (ICiH). Here, attackers attempt to indirectly degrade RTMS QoS by directing packets at other concurrent services, thereby inducing resource contention between RTMS packets and attack packets in the protocol stack. To analyze such attacks, the authors' operation-trace analysis method formalizes the notion of contention among concurrent services and develops several metrics to quantify ICiH's effects.
Multi-core processors represent a major evolution in computing hardware technology. Multi-core provides a network security application with more processing power from the hardware perspective. However, there are still significant software design challenges that must be overcome. In this paper, we present new architecture for multi-core supported Intrusion Detection System, which aims at providing network security processing without causing performance penalty to normal network operations. While hardware-based parallelisms have shown their advantage on throughput performance, parallelisms based multi-core provides more flexible, high performance, comprehensive, intelligent, and scalable solutions to network security applications. The Intrusion Detection System that we presented in this paper also protect the multi core systems from Real Time attacks and Packet Filtrations with high performance without any penalty.
As the rapid increase of network security events, network security monitoring and management on network behavior become more and more focused in the fields of computer science. This paper develops a kind of fuzzy constraint correlation algorithm based on prerequisites and consequences of security event. The introduction of fuzzy constraint correlation can resolve the issues of false negatives and false positives of security events in some extent.
As an important part of computer forensics, network forensics particularly places emphasis on dynamic network information collection and proactive defense. Most forensics systems based on intrusion detection or honeypot rarely emphasize the availability of actual servers. In addition, few of them discussed the occasion of dynamic forensics particularly. The work presented in this paper is based on an idea to assist dynamic forensics with intrusion tolerance and deception technology to enhance the availability of server system and gather more useful evidences on a proper occasion. A mechanism of dynamic forensics based on intrusion forensics is proposed and is modeled with finite state machine. The workflow is described. A semi Markov process based on the embedded Markov chain of the states transition model is built and described. Finally, the forensics capability and server availability are analysis. According to the numerical analysis result, the security performance and forensics capability of the forensics system are enhanced to a certain degree.
Border access control, network security mechanismand system access control are three separately access controlprocedures when user accessing the resource on network.Network access control mechanism, which adding theauthentication of endpoints platform besides the classicalidentity-based border access control, enhances the level ofprotection for network resources to some extent. In theexisting network access control mechanisms, TNC has beentrying to integrate the network access control mechanism andnetwork security mechanism to add dynamic characteristics,but still can not cover the whole resource access controlprocess. Once a user passes the authentication, he can stillmake damage to the system. This paper proposes a UnifiedNetwork Access Control (UNAC) architecture whicheffectively integrates the network access control, networksecurity mechanism and system access control through thetrust degree, and it can achieve dynamically user controlduring the resource accessing period.
A practical pairwise key distribution scheme is necessary for wireless sensor networks since sensor nodes are susceptible to physical capture and constrained in their resources. In this paper, we investigate a simple and practical scheme that achieves higher connectivities and perfect resilience with less resources, even in case of deployment errors.
Security is a fundamental prerequisite for the survivability and reliability of wireless networks. In a network where limited wireless resources have to be shared, selfish nodes can manipulate relevant network parameters to gain more access to the resources, and hence obtain a higher performance than their fair share, while the performance of well-behaved nodes will be significantly degraded. This paper considers the environment of an IEEE802.11 WLAN, and proposes a solution from the prospective of a principal-agent system. Our solution uses an incentive and a constraint mechanism to encourage the selfish agent to perform normally. Our method does not modify the IEEE 802.11 protocol, but requires an additional principal node only. Simulation results show that our method can overcome the influence of selfish nodes improve the network fairness performance while maintaining the throughput performance.
Peer-to-peer computing overcomes communication bottleneck problems associated with centralized game servers and provides an alternative mechanism to support massively multi-player online gaming (MMOG) applications. A potential security problem associated with this approach relates to player identity. In an MMOG, users from different parts of the network interact with each other in a virtual world. In a peer-to-peer model, these interactions happen directly between the peers and this leads to the IP addresses of peers being available in the network packets. Malicious users can extract the IP addresses of their opponents and the information can be used to gain unfair advantage by compromising their opponents' computers. The well known approach to this problem is the use of anonymizing networks (onion routing and mixing), that anonymize the sender and the destination IP addresses from the peers along a path. However this approach introduces significant delays which are not desirable in MMOG applications. This paper proposes the use of a secret shared key to reduce computational delay and also provides a theoretical framework for trading off the strength of anonymity for reduced delay with respect to classes of interactions in the MMOG. The results suggest that appropriately low delays can be achieved with small reductions in anonymity strength.
The threat of Distributed Denial of Service (DDoS) has become a major issue in network security and is difficult to detect because all DDoS traffics have normal packet characteristics. Various detection and defense algorithms have been studied. One of them is an entropy-based intrusion detection approach that is a powerful and simple way to identify abnormal conditions from network channels. However, the burden of computing information entropy values from heavy flow still exists. To reduce the computing time, we have developed a DDoS detection scheme using a compression entropy method. It allows us to significantly reduce the computation time for calculating information entropy. However, our experiment suggests that the compression entropy approach tends to be too sensitive to verify real network attacks and produces many false negatives. In this paper, we propose a fast entropy scheme that can overcome the issue of false negatives and will not increase the computational time. Our simulation shows that the fast entropy computing method not only reduced computational time by more than 90%compared to conventional entropy, but also increased the detection accuracy compared to conventional and compression entropy approaches.
In communication equipments, pattern matching techniques are used to handle various requirements, such as network security, QoS and so on. This paper presents a pattern matching technique to process multiple characters at a single clock cycle. And this paper presents the way to find a particular pattern from communication packets using TCAMs.
Increasingly advances in file carving, memory analysis and network forensics requires the ability to identify the underlying type of a file given only a file fragment. Work to date on this problem has relied on identification of specific byte sequences in file headers and footers, and the use of statistical analysis and machine learning algorithms taken from the middle of the file. We argue that these approaches are fundamentally flawed because they fail to consider the inherent internal structure in widely used file types such as PDF, DOC, and ZIP. We support our argument with a bottom-up examination of some popular formats and an analysis of TK PDF files. Based on our analysis, we argue that specialized methods targeted to each specific file type will be necessary to make progress in this area.
The Distributed Denial of Service attacks (DDoS) is one of the major threats to network security that exhausts network bandwidth and resources. The current detection schemes are sensitive to the number of attackers and may lead to a high false positive probability especially for largescale networks with huge number of attackers. It is notable, however, that in the current DDoS attacks, the flooding rate is usually distributed among many flooding sources to make the detection more difficult. In this paper we propose a more efficient detection scheme for Web Service DDoS attackers. The proposed scheme is based on the number of incoming requests to the server with the consideration of the clients activity (Active and Non-Active clients during the detection time). To make our scheme scalable to large-Scale networks, the non-adaptive group testing theory is applied to detect attackers using low state overhead. Extensive tracedriven simulation has been conducted on real Web trace to demonstrate the efficiency of the proposed scheme in terms of its false positive, false negative probabilities and also detection time.
Currently, several researches have concentrated on the new or vital security requirements of mobile commerce. However, some recent studies asserted a particular serious attack, called wormhole attack, that mainly targeting in a wireless-based system may also occur in the mobile commerce systems. Although there are many endeavors to confront wormhole attacks in the field of wireless communications, the solutions still have respective problems to be overcome and are not yet tailor-made for mobile commerce environment. This paper is the first one to unveil the emergence of wormhole attacks in mobile commerce and also proposes the precautionary method to prevent them. On the other hand, considering that the mobile device with limited resource, we utilize the secure filter to efficiently manage group key. Moreover, the proposed secure routing scheme is not only more efficient and robust than that in Ariadne but also resolve the new problem of Ariadne (insider attack) that is demonstrated by A?cs et al.
Firewalls are among the most important components in network security. Traditionally, the rules of the firewall are kept private under the assumption that privacy of the ruleset makes attacks on the network more difficult. We posit that this assumption is no longer valid in the Internet of today due to two factors: the emergence of botnets reducing probing difficulty and second, the emergence of distributed applications where private rules increase the difficulty of troubleshooting. We argue that the enforcement of the policy is the key, not the secrecy of the policy itself. In this paper, we demonstrate through the application of game theory that public firewall rules when coupled with false information (lying) are not only viable but actually better.
As a powerful anti-phishing tool, honeypots have been widely used by security service providers and financial institutes to collect phishing mails, so that new phishing sites can be earlier detected and quickly shut down. Another popular use of honeypots is to collect useful information about phishers' activities, which is used to make various kinds of statistics for the purposes of research and forensics. Recently, it has also been proposed to actively feed phishers with honeytokens.
In this article there have been given results of researches and realization of honeypot introspection analysis mechanism, which provides the exposure- and blocking-resistant monitoring of malefactor's activity on Linux operating system. Difficulties of realization, and features of architectural realization, are considered in detail. On the basis of the realized software complete and effective control of events becomes possible in virtualized OS.
Network intrusion detection systems (NIDS) have become vital components in securing today?s computer networks. To be highly effective, NIDS must perform packet inspection of incoming traffic at or near wire speed. Failing to do so will allow malicious packets to sneak through the network undetected, and thus jeopardising network security. Snort is one of the most popular IDS and intrusion prevention system (IPS) applications. Snort is a publicly available open-source NIDS application that typically runs on Linux. In this study, the authors present and discuss the essential software components of Snort and its underlying Linux support architecture. The authors characterise Snort execution and present an analytical queuing model to give insight into understanding the kernel and Snort behaviour as well as to identify key-dominating factors that strongly influence and impact Snort performance. The authors demonstrate that the current default configurations of the packet reception mechanism of the Linux networking subsystem (a.k.a. NAPI) are not suitable for Snort performance and show that the performance of Snort can be improved significantly by tuning certain configuration parameters, specifically by having a small NAPI budge value of 2. The performance is measured in terms of throughput and packet loss. The authors also measure the packet loss encountered at the kernel level as well as the interrupt rate of incoming traffic. Performance was measured when subjecting a PC host running Snort to both normal and malicious traffic, and with different traffic load conditions.
A novel digital forensics tool is developed by combining wavelet invariant with spatial moments. A forensic printed circuit board image matching system is presented that is capable of probing a large database of digital images of circuit boards and compare them for similarity to provide investigation leads for electronic crimes digital forensic science investigations. The developed system has been implemented, and proved to be very efficient in detection similarities between a target image and a large image database even when the target image is noisy, scaled or mirrored.
This paper presents the theoretical account for designing and developing an algorithm for network security inspired by the behavior of Beavers. The algorithm uses the beaver behavioral patterns in constructing dams and water tunnels to create analogous secure tunnels and information lakes. The approach is a user-centric and the paper demonstrates the use of the algorithm in security and route optimization with the assumption that the beaver agent is deployed on a mobile device (e.g. smart mobile phone). An algorithmic approach to design the beaver agent and swarm is followed here. The set of algorithms presented is complemented by critical review outlining the further work needed.
In this paper we show that it is possible to retrofit a security layer on top of PROFINET IO without changing the underlying transmission system or standards. By introducing security modules, end-to-end network security can be achieved and ensure authentication, integrity and confidentiality for real-time communication. The concept of security modules is a flexible framework and countermeasures can be changed, as security threats and exploits are changing over time. A proof-of-concept implementation shows that it is possible to implement security modules on existing products and secure them against, for example, man-in-the-middle attacks.
The deep study of anomaly feature based on the particular server was made in this paper. By continuously monitoring on the honeypot deployed in Internet Data Center for more than two months, the experimental results were summarized and some initial exploratory models were built. The models show that the number of attackers for the main attack types and ports can be described by normal distribution; meanwhile, the average packet number that each attacker generates per day can be described by log-normal distribution. This research aims to contribute to endeavor in the wider security research community to build methods and obtain some statistical models, grounded on strong empirical work, for assessment of the robustness of systems in hostile environments, and the anomaly traffic sampling, detection and classification on the backbone.
Universal Plug and Play, or UPnP in short, is a &#x201C;plug and play&#x201D; methodology used in network environment. It's designed to share resources, connect network devices and offer control over services. It implements zero-configuration. Developer can save the cost of network setup and concentrate on the offering services only and do not need to concern the under layer protocol. As the wide spreading of UPnP protocol, building a secure UPnP system is becoming more important. The paper introduces a secure UPnP architecture based on the UPnP network and implements the protocols and algorithms of the secure architecture. It is different from create a secure communication channel point to point based on the UPnP devices. We adopted the decentralized mechanisms for key management, analysed the limits of our protocol and proposed the corresponding measures. Finally, we compared with two important protocols using decentralized key management mechanisms, stated their limits and solutions of our scheme.
Currently, Internet, adopting the TCP/IP suits, is an end-to-end architecture. TCP/IP is an open architecture, and its design principle only focuses on the efficiency of information transmission, while it does not consider the security issues. IPsec VPN has been proposed to solve the core network security issue. However, it is believed that the current use of IP addresses to denote both the location and the identity of a host is seen as the source of many Internet problems. Address-split-mapping network cleanly separates location from identity of host, and divides Internet into the backbone network and access network. This mechanism helps to solve the Internet problems such as security, mobility, multihoming, etc. Based our previous research work on Address-Split-Mapping mechanism, this paper describes the design, implementation and experiment results of IPsec-VPN based on Address-split-mapping.
Since the virtual network traffic is invisible outside the hypervisor, it is impossible for traditional network-base security devices to harness the attacks happened in virtual computing environment. Industry and academies adopt the network security enabled hypervisor (NSE-H) to protect virtual machines (VM) residing in the virtual network. In this paper, we identified the insufficiency of the existing live migration implementation, which prevents itself from providing transparent VM relocation between NSE-Hs. This occurs because the contemporary migration implementation only takes VM encapsulated states into account, but ignores VM related security context(SC) needed by NSE-H embedded security engines (SE). We presented a comprehensive live migration framework for the NSE-H, considering both the execution context encapsulated in VM instance and the VM related security context within the SEs. We built a prototype system of the framework based on stateful firewall enabled Xen hypervisor. Our experiment was performed with realistic applications and the results demonstrate that the solution complements the insufficiency without introducing significant performance downgrade. Even in the worst case, the downtime that occurs during migration increases no more than 15%, comparing to existing implementation.
Deep packet inspection at high speed has become extremely important due to its application in a wide range of network applications, such as network security and network monitoring. Network intrusion detection system (NIDS) uses a collection of signatures of known security threats and viruses to scan the payload of each packet. Signatures are often specified in the form of regular expressions (regex), called patterns, which are traditionally implemented as finite automata. Deterministic Finite Automata (DFA) is fast, but requires prohibitive amounts of memory which limits their practical use. Instead of matching an incoming packet with each individual regex in a ruleset, we match the packet with a fixed substring, called fingerprint, of a regex first. Fixed string matching is faster and consumes less energy than regex matching. The fact is that if a packet does not match with the fingerprint of a regex, it will not match the regex itself. So fingerprints can be used in a prefilter engine to filter out those packets and do not match any of the fingerprints of the regex in a rule set, which represents normal non-malicious traffic. This actually reduces the number of regex rules being matched, which results in increased throughput of the NIDS. We present a weight scheme to extract a good fingerprint from a regex. A good fingerprint is the one that not only indicates the regex uniquely, but also occurs as less as possible in the matching procedure. We demonstrate how to use fingerprints for efficient prefiltering by means of Bloom filters in practice.
The clustered topology is an important mechanism for WSN (wireless sensor networks) to balance the energy consumption, extend the network lifetime, enhance the management efficiency and improve the network scalability. It adapts to use in large scale deployment. CPK (Combined Public Key) algorithm produces public/private keys offline, and the trusted third party is not necessary for the authentication of identity. It provides high security, with low data traffic and memory requirement. A reliable clustering scheme based on CPK is proposed for WSN, here CPK algorithm is introduced to the clustering process to effectively strengthen the network security and drastically reduce the unnecessary energy consumption. The new scheme is of great value for the application of WSN.
Intrusion detection can no longer satisfy security needs of an organization solely. Recently, the attention of security community turned to automatic intrusion response and prevention, as the techniques, to protect network resources as well as to reduce the attack damages. Knowing attack scenarios enables the system administrator to respond to the threats swiftly by either blocking the attacks or preventing them from escalating. Alert correlation is a technique to extract attack scenarios by investigating the correlation of intrusion detection systems alerts. In this paper, we propose a new learning-based method for alert correlation that employs supervised and transductive learning techniques. Using this method, we are able to extract attack scenarios automatically.
In a high performance network security co-processor, the low power masking technique is used to promote the power attack resistant level of the AES crypto engine. Based on the original AES module which shares one S-box when ciphering and decoding, in order to achieve higher security, the novel circuit design of masking is achieved by two ways respectively, one utilized SRAM, the other replicated some modules. Over 1000 different power curves are recorded and compared between the two masked engines and the original one respectively, and over 10000 curves are recorded to show the strength of the masking architecture. The design is verified to be feasible by FPGA<sup>1</sup>.
The integrated core network architecture and various mobile subscriber behaviors can result in a significant increase of signaling loads inside the evolved packet core network proposed by 3GPP in Release 8. Consequently, an authentication signaling analysis can provide insights into reducing the authentication signaling loads and latency, satisfying the quality-of-experience. In this paper, we evaluate the signaling loads in the EPS architecture via analytical modeling based on the renewal process theory. The renewal process theory works well, irrespective of a specific random process (i.e. Poisson). This paper considers various subscribers patterns in terms of call arrival rate, mobility, subscribers' preference and operational policy. Numerical results are illustrated to show the interactions between the parameters and the performance metrics. The sensitivity of vertical handover performance and the effects of heavy-tail process are also discussed.
We describe a novel architecture for network defense designed for scaling to very high data rates (100 Gb/s) and very large user populations. Scaling requires both efficient attack detection algorithms as well as appropriate an execution environment. Our architecture considers the time budget of traffic data extraction and algorithmic processing, provides a suite of detection algorithms&#x201D;each designed to present different and complementary views of the data&#x2014;that generate many &#x201C;traffic events,&#x201D; and reduces false positives by correlating these traffic events into benign or malicious hypotheses.
SQL injection attacks continue to be a major problem for web applications. We investigate design considerations for an application layer honeypot to attract and learn about SQL injection attacks. The honeypot responds with indications of vulnerability leading attackers ultimately to disinformation that could be useful to track them. The honeypot restricts attackers from escalating the attack to the operating system or launching attacks on other systems. The honeypot could emulate the appearance of common defenses against SQL injection in order to seem more genuine. Finally, we describe considerations to implement an experimental honeypot with honeyd.
Network Protocol Reverse Engineering (NPRE) has played an increasing role in honeypot operations. It allows to automatically generate Statemodels and scripts being able to act as realistic counterpart for capturing unknown malware. This work proposes a novel approach in the field of NPRE. By passively listening to network traces, our system automatically derives the protocol state machines of the peers involved allowing the analyst to understand its intrinsic logic. We present a new methodology to extract the relevant fields from arbitrary binary protocols to construct a statemodel. We prove our methodology by deriving the statemachine of documented protocols ARP, DHCP and TCP. We then apply it to Kademlia, the results show the usefulness to support binary reverse engineering processes and detect a new undocumented feature.
As the traditional network security assessment methods have subjective factors when the wights a assessment indexes are identified, it is difficult to make accurate and objective assessment. However, the Rough set theory has the advantages of not needing apriori knowledge when dealing with uncertain problems. Therefore, the application of the Rough set theory in network security assessment is quite necessary. This paper is identifies the principle of assessment indexes system of network security and establishes the indexes system of network security assessment, establishes security assessment model and common steps of network security assessment which are both based on the Rough set theory, and finally analyzes and validates this model by an example. The network security assessment based on the Rough set theory effectively overcomes the subjectivity of determining the weights of indexes by traditional methods, gives more objective results, and enhances the veracity and validity of network security assessment.
Authentication is one of the most important properties in network security, it is usually ensured by designing authentication protocols which use cryptographic techniques. Unfortunately, practice proved that the analysis and design of a protocol is not easy even it is very simple. The paper describes an automatic generation of authentication security protocols based on the evolutionary algorithm and SVO logic and illustrates the approach can automatic generate large-scale authentication protocols, such as three-party key agreement etc.
Risk assessment is one of the most important issues in the field of e-Government network security. Inspired by the principles of biological immune system, a new risk assessment model for e-Government network security, referred to as ACENS, is proposed in this paper. In ACENS, the artificial lymphocytes are used for assessing the risk of e-Government network security, and the lymphocytes' concentration changes with the trend of the real attack intensity. Theoretical analysis and experimental results show that ACENS is valid, and it has the features of real time and self-adaptive. Thus, it provides a good solution to risk assessment for e-Government network security.
With more and more applications of XML in logistics data exchange, data security will become increasingly important. In the paper, according to network security related technologies and XML security component technology, data security control method based on XML in logistics data exchange is discussed. More ever, how to strengthen the security of internet commerce is discussed by instance, and it useful for user, who only needs access to part content of XML document, effectively control XML data access
Starting from the popular application of JNLP protocol in the network flow monitor products, through the analysis of security issues, this article presents reflection of current network security for industry to explore and improve.
Enterprise Network Information System is not only the platform for information sharing and information exchanging, but also the platform for Enterprise Production Automation System and Enterprise Management System working together. As a result, the security defense of Enterprise Network Information System does not only include information system network security and data security, but also include the security of network business running on information system network, which is the confidentiality, integrity, continuity and real-time of network business. According to the security defense of Enterprise Network Information System, this paper proposes the "network business security" concept. In this paper, the object of information security is defined in three parts - - data security, network system security and network business security, and the network business security model is described. The proposal of the concept "network business security" provides theoretical basis for security defense of enterprise automatic production system and enterprise management information system.
Fiber to the home (FTTH) network security has become a very essential and important topic for optical network. Fault isolation became important issues in order to provide an efficient FTTH network and simultaneously will provide continuous services to the end user without being interrupted by any failure in fiber line. Protection scheme against the failure in distribution fiber line is carried out using our protection and restoration scheme. This paper focused on the implementation of automatic device switching to perform in restoration scheme located at distribution fiber section. This protection device is potentially will improve the system efficiency in order to provide the continuous data flow until it reach to the end user. We implement for immediate split structure in Fiber to the Home (FTTH-EPON) network when the signal is totally split when it pass through the optical splitter. Automatic Protection Unit (APU) is an automatic switching device designed as a package that offered for the ease of customer to perform security and self restoration at the end user side. While in this paper also includes the power penalty analysis for optical switch cascaded in our network protection design according to the APU device.
Constructing a security metric architecture in Next Generation Network is a critical part for network security assessment. In this paper, a three-dimensional security metric architecture model is given. In this model, four comprehensive metrics, which are vulnerability, threat, stability and survivability, are first presented. Then based on these four metrics, a hierarchical security metric architecture is devised, and instanced in a prototype system of NGN. Compared with qualitative metrics, the presented metrics are not only accurate, but also measurable and easy to collect based on the quantitative parameters. Since there is limited research result of the security metric architecture for NGN at present, this model can be referenced to a certain extent.
This paper presents a field study on web security vulnerabilities from the programming language type system perspective. Security patches reported for a set of 11 widely used web applications written in strongly typed languages (Java, C#, VB.NET) were analyzed in order to understand the fault types that are responsible for the vulnerabilities observed (SQL injection and XSS). The results are analyzed and compared with a similar work on web applications written using a weakly typed language (PHP). This comparison points out that some of the types of defects that lead to vulnerabilities are programming language independent, while others are strongly related to the language used. Strongly typed languages do reduce the frequency of vulnerabilities, as expected, but there still is a considerable number of vulnerabilities observed in the field. The characterization of those vulnerabilities shows that they are caused by a small number of fault types. This result is relevant to train programmers and code inspectors in the manual detection of such faults, and to improve static code analyzers to automatically detect the most frequent vulnerable program structures found in the field.
This paper proposes an improved immunological surveillance for network danger evaluation model, focusing on intrusion detection and countermeasures with respect to widely-used networks. An improved intrusion detection mechanism based on self-tolerance, clone selection, and immune surveillance is established. A new network security evaluation method using antibody concentration to quantitatively analyze the degree of intrusion danger level is presented. Additionally, this new hierarchical management framework of the proposed model adopt to improve the detection efficiency and to overcome the shortcoming of the local optimum. The experimental results show that the proposed model is a good solution for network security evaluation.
Trusted Network Connect (TNC), whose goal is to improve network security from source, now has become hot topic in security domain. We proposed a new access model for the terminals without meeting the requirements in TNC specifications. Also we discussed the work flow and communication process of presented model. Simulations show that the feasibility of the presented model used in the scenario where terminals fall short of network demand can pass the access authentication of network with TNC specifications.
Currently, the network security appliance is one of the most important research topics in IT security. The use of log files for further security analysis was proven their importance in the development of a three DNS query traffic based detection model system for a proactive detection of security threat in the university campus network. In the current detection strategy we can detect some suspicious infected candidates that need further analysis to prove their infection level. In order to perform this detailed analysis we need to collect the traffic from the suspicious candidate. We decide to use a hardware based system which is going to collect the traffic directly from the suspicious candidate. The traffic collected will be useful as a proof of the infection of the system. This information will provide us the possibility to create a personalized and portable device, which can be located in any network to analyze the same traffic without decreasing the network efficiency.
In resent years, secure communication is becoming one of the researching hotspots in information safety. Some problems of chaotic synchronization and its application in communication are studied on the bases of the theory of nonlinear dynamics. It is a mode of correspondence, which transmits the desired information in the channel by means of encrypting, and then decrypts them at the receiving end. Chaos encryption is to encrypt the plaintexts and utilize random-like characteristics of chaos signal to secure the real-time communication. This paper bases on nonlinear dynamics theory and Starts from stability theory of dynamic system , In addition, it adopts Logistic mapping of discrete chaotic system and uses modular operation to design the synchronization and secure communication. This design is easily realization and has stronger anti-interference ability, and then the secure capacity of secure communication system is nice.
With the rapid development of network, some novel applications like remote database access, electric commerce, etc. bring forth many new requirements for network loads, network communication efficiency and network security. This paper adopts mobile agent technology to achieve remote access to databases. Compared to the traditional C/S computing model, this method can significantly improve the communication efficiency and system performance when a large number of data located at remote sites must be handled. In addition, the security of data can be enhanced to some extent by using message digest, encryption and decryption techniques on the results carried by mobile agents. The experimental result shows that this access method for remote databases based on mobile agents is security and feasible. Especially to a large number of remote data processing, this technology gets more advantages over the C/S computing model in the aspect of performance.
Recently in-depth analysis of network security vulnerability must consider attacker exploits not just in isolation, but also in combination. The general approach to this problem is to compute attack graphs using a variety of graph-based algorithms. However, such methods generally suffer the exponential state space problem. Therefore, this paper brings forward two conceptions of vulnerability correlation matrix and vulnerability correlation graph (VCG). An algorithm based on vulnerability correlation matrix was proposed to generate VCGs. An example was given to illustrate the application and effect of the algorithm in network security analysis. Deep analysis proves that VCGs have polynomial complexity of the number of network vulnerabilities and scale well for large networks. Moreover, the example shows that VCGs are a good help to and convenient for network security management.
Network Security Situational Awareness(NSSA) has been a hot research spot in the network security domain. In this paper, a quantification method for NSSA based on conditional random fields(CRFs) was proposed. The data of network attacks from Intrusion Detection System (IDS), the hosts&#x02019; vulnerabilities and the hosts&#x02019; states were firstly combined as the network security factors. And then the network security threat degree was defined to quantify the risk of the whole network and classify the attacks. A diverse set of effective features were incorporated in CRFs Model. Finally the experiments on the DARPA 2000 data set generate the explicit network security situational graph. It proves that the method introduced in this paper can represent network risk more accurate and offer a good quantification for the network security situation.
in recent years, intrusion detection has emerged as an important technique for network security. Machine learning techniques have been applied to the field of intrusion detection. They can learn normal and anomalous patterns from training data and via Feature selection improving classification by searching for the subset of features which best classifies the training data to detect attacks on computer system. The quality of features directly affects the performance of classification. Many feature selection methods introduced to remove redundant and irrelevant features, because raw features may reduce accuracy or robustness of classification. In this paper we compared three methods for feature selection based on Decision trees (DT), Flexible Neural Tree (FNT) and Particle Swarm Optimization (PSO). The results based on comparison of three methods on DARPA KDD99 benchmark dataset indicate that DT has almost better accuracy.
In this paper, we proposed a file backup method. Files in devices of computer system can be destructed under abnormal condition. We divide file or database into ranges. Each range contains records and fields. We backup ranges to different devices. When files process the operation of creation, insertion, update or deletion, we only backup the corresponding ranges. When the original file is destructed, we create it from these backup files. For security, we have method to encrypt files before stored. These operations of backup method make more secure and better performance.
The target of worm attacking is the whole internet; usually, the defense of worm intrusion based on single computer cannot work well when facing large scale of worms&#x02019; attacking, so it cannot assure the network security all the time. Inspired by the collaboration of the immune cells, we apply the artificial immunology theory to the design of the active worm defense model AI-WDM, in which the computer network constitute an immune network and each computer in the network working as a detection node, they corporate with each other to defense the intrusion of worms while running detection processes independently.
The DoS attack is the most popular attack in the network security with the development of network and internet. In this paper, the DoS attack principle is discussed and some DoS attack methods are deeply analyzed. The DoS attack detection technologies which include network traffic detection and packet content detection are presented. The DDoS based on DoS is introduced and some DDoS tools are described and the important TCP flood DoS attack theory is discussed. The DoS attack program and a DoS attack detection program based on Winpcap for experiment are designed and the network packet generation and capture are implemented. The experiment expressed the key progress of DoS attack and detection in detail.
Detecting all kinds of intrusions efficiently is significant to network security. Radial basis function (RBF) neural network is a kind of feed forward neural network, which is widely employed as a real-time pattern classification. In RBF neural network, the center of radial basis function, the variance of radial basis of function and the weight have to be chosen. If they are not appropriately chosen, the RBF neural network may degrade validity and accuracy of modeling. Particle swarm optimization algorithm (PSO) is a member of the wide category of swarm intelligence methods to solve non-linear programming problems. PSO has proved to be competitive with genetic algorithm (GA) in parameter optimization. So PSO is used to optimize the RBF neural network parameters in this work. Therefore, the novel combination method based on RBF neural network and PSO (PSO-RBFNN) is adapted to network intrusion detection. The experimental results show that the proposed model is superior to the conventional RBF neural network.
Risk assessment is one of the most important issues in network security research. Inspired by the principles of artificial immune system, a novel risk assessment approach for network security, referred to as IRAA, is proposed in this paper. After defining the concepts and formal definitions, the architecture and principles of IRAA are described. And then, the lifecycle models of mature and memory lymphocytes are built. Theoretical analysis and experimental results show that IRAA is valid, and it has the features of self-adaptive, distributed, and real time. Therefore, it provides a good solution to risk assessment for network security.
As a reaction to the threat of network security, the appliance of Virtual Private Network (VPN) has become more and more prevalent. In order to support IPSec VPN on ForTER, the paper proposes an implementation model by IPSec Security Policy LFB, which is the critical step to achieve IPSec VPN on ForTER. Experiment results show the feasibility and effectiveness of the model.
Detection of intrusion attacks is an important issue in network security, now fuzzy set theory has been applied to many fields, therefore, research into fuzzy clustering method for knowledge is significant not only to theory, but also to application. the Fuzzy Possibility C-Means Algorithm for intrusion detection is adopted in this paper, the experiments with KDD Cup 1999 data demonstrate that our proposed method achieves 91.00&#x025; average detection rate, and the false positive rate ranges from 0.50&#x025; to 1.80&#x025;, the total performance evaluation is outperforms the RIPPER method.
Since attack graphs provide practical attack context and relationships among vulnerabilities, researchers have been trying to evaluate network security based on attack graphs. However, previous works focus their attention on specific evaluations they concerned, and each does things in his own way. There is no explicit way telling network administrators how to measure network security in a general way. In this paper, we propose a new metric framework, whose main goal is to guide people to perform evaluations based on attack graphs. The main components of proposed metric framework include Security Index, Target of Evaluation, Elementary Attribute, Composition Algorithm, and Arithmetic operators. Relative definitions and analysis of these five components are also given. The following examples show the applications of our metric framework, and validate it.
The lack of trust is one of the most important problems that affect supply chain cooperation. Selecting trustworthy members is an effectual way to solve this problem. Based on this, in this paper, we introduce the idea of trust management in network security and propose a searching model of trustworthy supply chain called TSFM aiming to choose trustworthy supplier in the setting up of supply chain. The model not only gives selection process of suppliers, but also gives the store and update methods of suppliers&#x02019; information. A calculation method of the expected trust value is also put forward in it. Finally, the simulation experiment results show that this model is feasible and effective.
The research proposed an approach that based on situational awareness to make decisions about the protection and defense against cyber attacks. The statistics of the situational awareness system outputs were formalized and described. Strategy sets of each player were set according to the system states. Cost-benefit factors were considered comprehensively to calculate the payoffs of each player. The best strategy is determined based on Bayesian Nash equilibrium. On one hand, the best strategy was based on the working status of network system and critical equipments; on the other hand the payoffs of the players were calculated comprehensively. The model deals well with the attack's intents and the alternation of strategies, and takes account of incentives of system and attacker across-the-board. The experimental results show that the model can effectively improve the accuracy and effectiveness of network defense.
Intention recognition is the ability to predict an opposing force's high level goals. Knowing an attacker's intention can support the decision-making of the network security administrators. Threat assessment based on intention analysis is an important part of network security situation awareness. So how to recognize attack intention and assess threat has become a research hot in network security domain recently. In this paper attack path graph generation algorithms at a different granularity is presented at first. Then the methods of intrusive intention recognition and threat assessment based on attack path analysis are proposed. Next in order to block an attacker's intention to be achieved, the way to provide protective measures at minimum cost based on minimum vertex cut theory is addressed. Finally several experiments are done in a local network, and the results of the experiments prove the feasibility and validity of this method.
Unauthorized network address translation (NAT) devices may be a significant security problem. They provide unrestricted access to any number of hosts connecting to them. Some attackers may use computers hidden behind NAT devices to conduct malicious activities such as denial of service. An algorithm is proposed in this work to detect hosts hidden behind NAT. Different from previous researches, the algorithm does not depend on any special field in any packet header. It is based on analyzing traffic features with directed acyclic graph support vector machine (DAGSVM). Firstly, traffic models of hosts are selected from training samples with DAGSVM. Then the models and classifier are used for predicting host number of unknown traces. What revealed by the experiment includes that the proposed algorithm is effective, even when there are more hosts in the test set than it is in the training set, and the accuracy will fall when there are more unknown hosts in the test traces.
IPSec provides authentication and encryption mechanisms for network security. How to enhance efficiency and flexibility is one of important issues of IPSec. This article introduces the traditional solution of IPSec security, analyses the designs to deal with data packets by IPSec, and provides a solution which integrates the IPSec processing and the core processing of IP layer and, as a result, enhance the security of IC payment. Through the implementation of digital sports projects, it verifies the usability and feasibility of the IPSec design.
IPSec is a policy-driven security mechanism. How to react on the diversity of network security and quickly generate corresponding security policy is one of the core issues of IPSec. This article introduces the traditional IPSec security policy and demonstrates an improved mechanism of implementing the IPSec security policy. Secondly, it constructs the security policy model based on ID3 algorithm by adding the policy engine components for enhancing efficiency and flexibility of security policy management for IPSec. Finally, through a case analysis, it describes the dynamic generation process of IPSec security policy.
The resource cost of intrusion tolerance is analyzed quantitatively. The concepts of steady cost and transient cost of intrusion tolerant system are proposed basing on the classification of intrusion tolerant measures. Reference to the computational method of the information entropy in information theory, the computational formula of steady cost is given. The function of the transient cost and the intrusion time is obtained according to the distributive property between the intrusion time and the intrusion probability, and the transient cost is described intuitively through computing its mathematical expectation. Lastly, some characteristics of the transient cost are analyzed through simulation, and some methods and notes of reducing the transient cost are given basing on the analysis result.
Evaluation for computer network information security is helpful for taking corresponding preventive measures. In order to obtain a comprehensive assessment of network security, analytic hierarchy process (AHP) model is proposed to assess the computer network information security. As the criteria and the relevant factors are decomposed hierarchically corresponding to evaluation and judgment of the problem, all kinds of factors of influencing network security are researched and the evaluation indexes for computer network information security are constructed, analytic hierarchy process (AHP) evaluation model for computer network information security is constructed on the basis of the evaluation indexes. The experimental results indicate that the evaluation of computer network information security by analytic hierarchy process is effective.
With the fast development of WLAN, wireless intrusion prevention systems have recently become the research hotspot. In this paper, we first analyze the drawbacks of WLAN and indicate the primary 802.11-specific threats, then present the framework of the wireless IPS with an intelligent plan recognition and pre-decision engine using honeypot technology, especially illustrate the need for plan recognition in WLAN. By importing supporting degree of intrusion plan, this engine can predict the future attacks and directly respond to these actions. We design and implement an improved model for conducting plan recognition and making pre-decision. Experimental results showed that the plan recognition and pre-decision engine can not only improve detection and prevention performance but also reduce false positives evidently.
Some media faults occur at random. To lessen the overhead of backup processing, the operation of a incremental backup with small overhead is adopted between the operations of a full backup. The mean time to full backup and the expected cost of these schemes are derived, using the theory of stocastic processes. The total expected cost of backup operation is obtained and an optimal full backup interval is easily determined.
Epidemic attack such as worms and viruses spreading in complex networks like the Internet is a serious problem faced by network security researchers. Designing effective network immunization strategies is important to defend against them. Targeted immunization towards the most highly connected nodes in the network like HDF (High Degree First) strategy has been regarded to be the most effective strategy so far. Recently, two new immunization strategies are proposed that are shown to be more effective than HDF. One is called EGP (Equal Graph Partitioning), and the other is called &#x201C;max &#x2212; &#x03B4;&#x201D;. A natural question is which of the two is better? We do extensive simulations using NLDS (nonlinear dynamical systems) approach which accurately models SIS (Susceptible-Infected-Susceptible) type epidemic propagation, on BA model networks, AS-level Internet model networks and real AS-level Internet networks. We find that there exists an immunization dose threshold depending on the network topology below which &#x201C;max &#x2212; &#x03B4;&#x201D; is better than EGP while EGP is better than &#x201C;max &#x2212; &#x03B4;&#x201D; when the immunization doses used exceed the threshold. We point out that one should choose the better immunization strategy according to the network topology and the immunization doses at hand.
User testing is an integral component of user-centered design, but has only rarely been applied to visualization for cyber security applications. This paper describes a comparative evaluation of a visualization application and a traditional interface for analyzing network packet captures, that was conducted as part of the user-centered design process. Structured, well-defined tasks and exploratory, open-ended tasks were completed with both tools. Accuracy and efficiency were measured for the well-defined tasks, number of insights was measured for exploratory tasks and user perceptions were recorded for each tool. The results of this evaluation demonstrated that users performed significantly more accurately in the well-defined tasks, discovered a higher number of insights and demonstrated a clear preference for the visualization tool. The study presented here may be useful for future visualization for network security visualization evaluation designers. Some of the challenges and lessons learned are described.
Internet grows day to day and so on the complexity of its security. Different types of people all around the world use Internet in their daily routine tasks. Internet and network security challenges make use of more efficient and complicated defense tools such as Intrusion Detection Systems (IDSs) vital. Nowadays attempts to solve IDS problems are under consideration. One of the deficiencies of current commercial IDSs is huge number of alerts. Most of generated IDS alerts are related to benign events which overwhelm the analyst. In this paper we try to reduce number of IDS false alerts and filter out those with high scores to decrease analyst workload. Our approach is evaluated on DARPA 2000 dataset and its efficiency has been shown.
With active node&#x02019; s opening to active application, Security has become a crucial aspect in active network. This paper proposed an extended RBAC model for active network to protect active nodes. By enforcing dynamic authorization as well as fine-grained management of resource, the access control policy provides the required flexibility of active network security management for customized services.
The modularization immunity neural network model is an intelligent solution to network security, but the function relationships among the neural network, immune algorithm and genetic algorithm in the model. By following biological mechanism, this paper builds up interaction functions and function systems among all the parts in the model, which leads to the organic combination among the neural network, immune algorithm and genetic algorithm and makes them effectively be used in network security. This provides visiting control, computer virus disposal and computer security environment with theoretical foundation, ensuring the sound operation of network security program .
It introduced virtual private network (VPN) technology and its classification in network security protection system. The advantages of VPN technology in establishing security protection system were analyzed. Four kinds of key technology of VPN including tunneling, encryption &#x00026; decryption, key management and authentication were presented. Taking electric power data network and load management system for example, it proposed network security solutions of electric power enterprise based on VPN technology. The technology can provide safe data transmission channels and make electric power enterprise share information resources. It can also make electric power enterprise reduce costs and improve economic benefits.
The security and trustworthiness of enterprise networks have been a major concern in the research and practice of Intranet security. The security of endpoints and their network access are inevitably two important factors regarding enterprise network security. In this paper we present a novel architecture to enforce controls on endpoint application execution and network access, in which the Policy Decision Point (PDP) and Policy Enforcement Point (PEP) are introduced. A hybrid mechanism is proposed such that the control of application and network access of endpoints are integrated. Security analysis and performance evaluation prove that the proposed architecture maintains a balance between security and flexibility of enterprise network control.
Current practice for real-time security risk assessment typically takes Intrusion Detection Systems alerts as the only source of risk factor. Their assessment results are more likely to suffer from the impact of false positive alerts in the increasingly complex and severe network security environment. This paper proposes a novel online fusion model for dynamical network risk assessment by using multiple risk factors. The model is composed by three fusion levels. First, an online alert fusion algorithm is proposed and the redundancy of the raw alerts is dramatically reduced. Then, the model employs Dempster-Shafer theory to handle uncertainties and ignorance existed in the multiple risk factors. Threats in different kinds of severity levels are identified. Finally, the whole network risk distribution is dynamically calculated and reported by using HMM approach. Experiments show the effectiveness and validity of our method.
Intrusion detection is still a crucial issue for network security. Support vector machine (SVM) has been successfully applied in intrusion detection systems. However, for further improvement in performance, data dimension reduction should have drawn special attention. This paper proposes a scheme using popular non-linear dimension reduction tool Isomap and one-class support vector machine to detect U2R (user to root) and R2L (remote to local) intrusions. Experiment results on KDDCUP 99 datasets show that our scheme achieves high detection rate for R2L or U2R intrusions and significantly low false positive rate compared with one class SVM alone. It is justified that data dimension reduction is a worthwhile preprocessing stage for achieving high performance in the intrusion detection system.
Aiming at the rapid increase of network flow, especially the threat caused by abnormal flow to the network security and control, this paper puts forward a network flow evaluation benchmark model. Based on wavelet transformation and auto regressive moving average model, the purpose of the model is to offer reliable reference benchmark to the evaluation of network flow and help people establish the pre-warning mechanism of network flow. Experiments show that the flow the model fits has a good conformity with the actual network flow, which can reflect accurately the development trend and change degree of normal network flow, and can be used as the reference standard of flow evaluation.
"Malware" is an umbrella term that describes a variety of Internet-borne threats, including viruses, spyware,Trojan horses, spam, bots and more. This sophisticated and evolving security threat puts all businesses at risk, no matter how big or small. Malware creates unique challenges to Higher Learning Institutions (HLI) in protecting their information assets. This is largely due to the fact that HLI Information Technology (IT) departments must balance between enabling a highly collaborative, non-restrictive environment and ensuring the confidentiality, integrity, and availability of data and computing resources. HLIs can proactively defend the network to reduce the risks associated with this threat by assessing the vulnerabilities and threats present in their networks and implementing appropriate multilayer security. A multilayer approach involves applying countermeasures at every layer of the computer network, from the perimeter routers and firewalls to users' personal computers in order to increase network security. The goal of this study is to propose a framework for network security protection against malware to increase the level of protection using non technical countermeasures.
By accurately measuring risk for enterprise networks, attack graphs allow network defenders to understand the most critical threats and select the most effective countermeasures. This paper describes substantial enhancements to the NetSPA attack graph system required to model additional present-day threats (zero-day exploits and client-side attacks) and countermeasures (intrusion prevention systems, proxy firewalls, personal firewalls, and host-based vulnerability scans). Point-to-point reachability algorithms and structures were extensively redesigned to support "reverse" reachability computations and personal firewalls. Host-based vulnerability scans are imported and analyzed. Analysis of an operational network with 84 hosts demonstrates that client-side attacks pose a serious threat. Experiments on larger simulated networks demonstrated that NetSPA's previous excellent scaling is maintained. Less than two minutes are required to completely analyze a four-enclave simulated network with more than 40,000 hosts protected by personal firewalls.
Attack graphs play important roles in analyzing network security vulnerabilities, and previous works have provided meaningful conclusions on the generation and security measurement of attack graphs. However, it is still hard for us to understand attack graphs in a large network, and few suggestions have been proposed to prevent inside malicious attackers from attacking networks. To address these problems, we propose a novel approach to generate and describe attack graphs. Firstly, we construct a two-layer attack graph, where the upper layer is a hosts access graph and the lower layer is composed of some host-pair attack graphs. Compared with previous works, our attack graph has simpler structures, and reaches the best upper bound of computation cost in O(N2). Furthermore, we introduce the adjacency matrix to efficiently evaluate network security, with overall evaluation results presented by gray scale images vividly. Thirdly, by applying prospective damage and important weight factors on key hosts with crucial resources, we can create prioritized lists of potential threatening hosts and stepping stones, both of which can help network administrators to harden network security. Analysis on computation cost shows that the upper bound computation cost of our measurement methodology is O(N3 ), which could also be completed in real time. Finally, we give some examples to show how to put our methods in practice.
With the emergence of multiple cloud providers of Infrastructure-as-a-Service, it becomes possible to envision a near-future when high-performance computing users could combine services from different clouds to access huge numbers of resources. However, as more administrative privileges are exposed to end users, providers are required to deploy network security measures that present challenges to the network virtualization technologies that are needed to enable inter-cloud communication. This paper studies these challenges and proposes techniques to enable unmodified applications on resources across distinct clouds. The techniques are implemented in TinyViNe, an extension to ViNe, a virtual networking technology for distributed resources in different administrative domains. The results of evaluating TinyViNe on a WAN-based testbed across three sites are reported for a bioinformatics application (BLAST) and MPI benchmarks. The results confirm that TinyViNe enables cross-cloud computing while having little impact on application performance. TinyViNe also has auto-configuration and &#x0201C;download-and-run&#x0201D; capabilities for easy deployment by users who are not knowledgeable about networking.
Research on Protocol Reverse Engineering is of great significance in network security applications. This paper firstly describes the existing Protocol Reverse Engineering technologies, and then detailedly analyses their advantages and disadvantages. Finally, a new approach of unknown Protocol reverse extraction based on DynamoRIO is proposed, adopting both dynamic binary analysis and dynamic taint analysis techniques to extract protocol format from the data flow information revealed by the protocol application while processing the protocol data.
Importance of web security cannot be overemphasized in the era of web-based economy. Although anomaly detection has long been considered a promising alternative to signature-based misuse detection technique, most studies to date used either small scale or artificially generated attack data. In this paper, based on security analysis applied on anonymous www.microsoft.com log of about 250GB,we propose Anomaly Feature Matrix (AFM) as an effective framework to characterize anomalies. Feature selection of AFM is based on the characteristics of well-known (e.g., DDoS) attacks as well as patterns of anomalous logs found in the Microsoft data. Independent security analysis performed on the same data by Microsoft security engineers concluded that 1) We did not miss any major attacks; and 2) AFM is a general enough framework to characterize likely web attacks. In order to assist AFM-based anomaly analysis in large organizations, we implemented an interactive and visual analysis tool named ADAM (Anomaly Detection Assistant based on feature Matrix). Integrated with mapping software such as Virtual Earth, ADAM enables efficient and focused security analysis on web logs.
This article mainly describes a way of designing an efficient pseudo-random number generator. Combining that the cellular automaton has many characteristics, such as simple rules of the component units, the local connectivity of units, the high degree of parallelism in information processing, and the complicated global characteristics, we use the rule 30 of one-dimensional cellular automaton to drive the state change. Finally, the pseudo-random numbers produced from the generator are tested the performance. And the results show that this design can be better applied to cryptography to ensure information technology and network security[1].
Anomaly intrusion detection is an important issue in computer network security. As a step of data preprocessing, attribute normalization is essential to detection performance. However, many anomaly detection methods do not normalize attributes before training and detection. Few methods consider to normalize the attributes but the question of which normalization method is more effective still remains. In this paper, we introduce four different schemes of attribute normalization to preprocess the data for anomaly intrusion detection. Three methods, k-NN, PCA as well as SVM, are then employed on the normalized data for comparison of the detection results. KDD Cup 1999 data are used to evaluate the normalization schemes and the detection methods. The systematical evaluation results show that the process of attribute normalization improves a lot the detection performance. The statistical normalization scheme is the best choice for detection if the data set is large.
In a cognitive radio network, cooperative spectrum sensing is conducted among the secondary users so as to improve the sensing performance. However, it's not necessary to cooperate all users to perform spectrum sensing while achieve a given error bound. In this paper, we investigate the problem of optimal number of secondary users in cooperative spectrum sensing. Moreover, an efficient and robust cooperative spectrum sensing algorithm is presented. A few secondary users are selected to cooperative based on reputation, while guarantee an error bound. Simulation results show the improvement of the sensing performance and network security as opposed to the random choosing.
This talk gives a brief overview of how to use basic concepts of quantum mechanics to achieve unconditional network security.
This paper will introduce simple modifications to the database of the widely deployed Kerberos authentication protocol. The principle's long-term secret key will be independent of the user password with the aim to overcome the weak passwords chosen by the network principal that are susceptible to password guessing attacks, the main drawback of the Kerberos protocol. Instead, the Kerberos Distribution Center will save a profile for every instance in the realm that it mange and the secret key will be generated based on that profile. This profile will be hashed and then, the output digest will be encrypted to generate the secret key. Besides, the lifetime of the secret key will be controlled using the system lifetime. We will use Triple-Des as an encryption algorithm, SHA-256 as a hashing algorithm, and Blum Blum Shub as a random number generator algorithm.
Firewalls are essential components in network security solutions. In order to implement correct security policy, the anomalies in firewall rules should be analyzed carefully, especially in enterprise network. In this paper, we present a new formal framework for analysis and resolution of anomalies in firewall rules. First of all, a formal model based on propositional logic is presented to specify rules. Then we specify all anomalies that identified in the latest researches based on our model. Current studies for analysis of anomalies are based on one to one rule anomalies, but we identify total version of anomalies based on one to many relationship of rules. Furthermore we have designed and implemented a tool based on theorem proving for verification of the specified anomalies. In addition, we present two algorithms for resolving anomalies in a rule database based on our formal model. These algorithms minimize the number of rules without changing the policy. Experimental results indicate that our algorithms for discovery single and total anomalies run in 2&#x2013;3 seconds for a very large firewall with thousands of rules.
With increasing network security threats, the network vulnerability must consider exploits in the context of multistage, multi-host attack scenarios. The general approach to this problem is to construct an attack graph for a given network configuration. An attack graph consists of a number of attack paths which are essentially series of exploits which an attacker employs to reach the destination. Each attack path depicts an attack scenario. As the number of attack scenarios increases, the overall security of the network reduces. Thus there is need for quantification of security level of a given network. In this paper, two security metrics, namely probabilistic security metric and attack resistance metric, have been employed to evaluate the relative security levels of various network configurations. A case study has been presented to demonstrate the applicability of the proposed approach.
With the development of computer technology, network security has become an important issue of concern. In view of the growing number of network security threats and the current intrusion detection system development, this paper gives a new model of anomaly intrusion detection based on clustering algorithm. Because of the k-means algorithm&#x02019;s shortcomings about dependence and complexity, the paper puts forward an improved clustering algorithm through studying on the traditional means clustering algorithm. The new algorithm learns the strong points from the k-medoids and improved relations trilateral triangle theorem. The experiments proved that the new algorithm could improve accuracy of data classification and detection efficiency significantly. The results show that this algorithm achieves the desired objectives with a high detection rate and high efficiency.
Packet classification has been widely used in network security protection. A number of the major techniques for network security protection, such as Virtual Private Networks (VPN), firewalls, and Network Intrusion Detection Systems (NIDS), are all dependent on the speed and ability of packet classification. The complexity of multi-dimensional packet classification will result in large scale rule-sets, which makes it prohibitive for software implementation. Algorithms based on Ternary Content Addressable Memory (TCAM) can solve this problem but cause entries expansion during the range matching. Our paper introduces a new region encoding mechanism of range mapping which can eliminate the expansion. By adding a new regional code, which is used to encode the ranges spanning two regions or more, the improved mechanism is able to express all the ranges by only one entry. Simulation and characteristics analysis of real rule-sets verify this encoding mechanism's feasibility and efficiency in actual applications.
This paper presents set-valued estimation algorithms for a mobile sensor network. A decentralized controller is also discussed; the controller utilizes the mobility of each sensor to improve the overall performance of the estimator. Simulations of the performance of the different set-valued estimation algorithms compare the results of a multi-agent system to a system using only local estimates, along with the performance of the sensor network tracking an evasive target.
Nowadays, most of the countries have come to this conclusion that their plan for counter terrorism should be changed from &#x201C;passive&#x201D; to &#x201C;active&#x201D;. Consistently, this should be reflected on the Internet as an important channel of communication and doing business in many countries. However, there are different barriers to achieve an effective online counter terrorism such as lack of &#x201C;cooperation&#x201D; and &#x201C;universal legislation&#x201D;. Accordingly, the first step of moving toward an effective online counter terrorism is to relieve these barriers. In this paper, a framework is proposed that aims at relieving these problems, using &#x201C;honeypots&#x201D; and &#x201C;web mining&#x201D; techniques.
Evaluation of forensics evidence is an essential step in proving the malicious intents of an attacker or adversary and the severity of the damages caused to any network. This paper takes a step forward showing how security metrics can be used to sustain a sense of credibility to network evidence gathered as an elaboration and extension to an embedded feature of Network Forensic Readiness (NFR) &#x2014; Redress that is defined as holding intruders responsible. We propose a procedure of evidence acquisition in network forensics where we then analyse sample of packet data in order to extract useful information as evidence through a formalised intuitive model, based on capturing adversarial behaviour and layer analysis. We then apply the Common Vulnerability Scoring System (CVSS) metrics to show that a forensics metrics system could assess the severity of network attacks committed, thus giving a degree of credibility to the evidence gathered. This way, hard evidence could be objectively collected to lend support to the resource-intensive process of investigation and litigation, leading to successful conviction, while reducing effort expended on the process.
The model and design of a generic security provider provides a comprehensive set of security services, mechanisms, encapsulation methods, and security protocols for Java applications. The model is structured in four layers; each layer provides services to the upper layer and the top layer provide services to applications. The services reflect security requirements derived from a wide range of applications; from small desktop applications to large distributed enterprise environments. Based on the abstract model, this paper describes design and implementation of an instance of the provider comprising various generic security modules: symmetric key cryptography, asymmetric key cryptography, hashing, encapsulation, certificates management, creation and verification of signatures, and various network security protocols. This paper also describes the properties extensibility, flexibility, abstraction, and compatibility of the Java Security Provider.
The rapid burst of Internet usage and the corresponding growth of security risks and online attacks for the everyday user or the enterprise employee have emerged the terms Awareness Creation and Information Security Culture. Nevertheless, security education widely has remained an academic issue. Teaching system or network security on the basis of practical experience inherits a great challenge for the teaching environment, which is traditionally solved using a computer laboratory at a university campus. The Tele-Lab project offers a system for hands-on IT security training within a remote virtual lab environment &#x2014; over the web, accessible by everyone. Such a system is inherently exposed to various security threats, since it has to provide full access to virtual machines running attack tools for potentially malicious users. The paper at hand introduces usage, management and operation of Tele-Lab as well as its architecture. Furthermore, this work focuses on possible attacks, the challenges when securing such a system, and shows how to set up an infrastructure that ensures the main security objectives identified as authentication, authorisation and availability.
Network intrusion prevention system (NIPS) becomes more complex due to the rapid growth of network bandwidth and requirement of network security. However existing solutions, either hardware-based or software-based cannot obtain a good tradeoff between performance and flexibility. In this paper, we propose a parallel NIPS architecture using emerging network services processor. To resolve the problems and bottlenecks of high-speed processing, we investigate the main design aspects which have dramatic impacts on most parallel network security system implementations: efficient and flexible pipeline and parallel processing, flow-level packet-order preserving, and latency hiding of deep packet inspection. To these key points, we address several optimizations and modifications with an architecture-aware design principle to guarantee high performance and flexibility of the NIPS on a network services processor implementation. Performance evaluation shows that, our prototype NIPS on Cavium OCTEON3860 processor can reach line-rate stateful inspection and multi-Gbps deep inspection performance.
Intrusion Detection System (IDS) is a crucial part of network security area and is widely employed. Signature-based matching mechanisms require a completed analysis of attack patterns and the availability of knowledge detection beforehand. To cope with new attacks, IDS tools require to be continuously updated with the signature rules. In this paper, we present anomaly detection technique by using Complex Gaussian Coefficient to calculate the threshold for detecting unknown flooding attacks. The Network traffics are generated for three types of situations in the normal light traffic period, during the attacking period and in the heavy traffic period. The numbers of packets in time domain are transformed to complex Gaussian coefficient. The variances of the complex wavelet magnitude in each derivative level significantly describe network situation. This technique can be applied to detect unknown DDoS flooding patterns.
Combined with the characteristics of network security audit system, paper introduced the idea of dynamic programming to pattern-matching algorithm, and gave the best dynamic matching algorithm, that is, by solving for matching pairs of local sub-problem, and for the number of different rules, (deleted) to find the best combination for the pattern-matching algorithms, then to achieve the strategy of optimal solution overall. Experimental results show that, the algorithm will greatly improve the efficiency of pattern matching in the network security auditing system.
Security in Wireless Sensor Networks (WSNs) is especially challenging and quite different from traditional network security mechanisms. There are two major reasons. Firstly, there are severe constraints on these devices namely their minimal energy, computational and communicational capabilities. Secondly, there is an additional risk of physical attacks such as node capture and tampering. Moreover, cryptography based techniques alone are insufficient to secure WSNs [1]. Hence, intrusion detection techniques must be designed to detect the attacks. Further, these techniques should be lightweight because of resource-constrained nature of WSNs [2]. In this paper, we present a new approach of robust and lightweight solution for detecting the Sinkhole attack and the Selective Forwarding attack based on Received Signal Strength Indicator (RSSI) readings of messages. The proposed solution needs collaboration of some Extra Monitor (EM) node apart from the ordinary nodes. We use RSSI value from four EM nodes to determine the position of all sensor nodes which the Base Station (BS) is origin position (0,0). Later, we use this information as weight from the BS. Another functions of EM nodes are eavesdropper and monitor all traffics, in order to detect the Selective Forwarding attack in the network. Our solution is lightweight in the sense that monitor nodes were not loaded any ordinary nodes or BS and not cause a communication overhead.
ISO 14001 environmental management system assists organizations in creation of structured mechanisms for continuous improvement in environmental performance, while the OHSAS 18001 occupational health and safety management system helps organizations in industrial operations that involve various occupational risks. ISO14001 shares many common traits with OHSAS 18001 and this has paved the road for the idea of an integrated management system. The aim of this study was to determine the extent of involvement in best environment and safety practices in the recycling industry towards development of environment and safety culture. The paper evaluates how the management systems aids companies in operating continuous environment protection programs to minimize significant environmental impacts, accidents and diseases by exploring some critical elements for success including task-based risk assessment and emergency preparedness and response. This analyze is carried out supported by secondary data obtained from environmental and safety reporting. This paper discusses the experiences of the management system in the recycling industry. Effective integration between environmental and safety awareness will create companies and employees who are responsible towards environment conservation and implementation of safe work practices.
The traditional network simulator don't have the ability of simulation for the process of large-scale network security incidents outbreak, we designed a grid-based Internet worm behavior simulator which is consisted of foreground applications and background analog subsystems in parallel. It makes the most of the benefit of the grid computing capability, resource and task management. The experimental result shows that the grid based Internet worm behavior simulator provides a very effective analysis of data for the early detection of unknown worms' research and that is the effective means to solve large-scale worm behavioral analysis, study the unknown detection methods, and assess the situation and a variety of worm containment strategies.
This paper analyzes the computer network security features and the main threat, synthesis the firewall technology of current domestic and international, on the basis of various firewalls' principles, advantages and shortcomings. Through the synthesis and compare of various techniques, in-depth study of the main factors affection firewall performance, combined with the network status quo of Heilongjiang Provincial Center, this paper researches the firewall technology of the actual application in the computer network security, then refers the new technique called tight coupling firewall, finally the article leads to a lack of firewall technology and the direction of its development.
This paper first introduces network security and the conventional firewall technology and points out the problems that the conventional firewalls faced. Then lucubrate the structure, key technologies and advantages of the distributed firewalls. Presents a secure model of campus network based on a distributed firewall that deals with the features of campus network directly, and set forth the advantage of it. At last, the thesis analyzes the faults and development of Distribute firewall in the future.
Evaluating the enterprise network security becomes more and more significant and important. The enterprise network security and its characteristics are introduced and analyzed. The quality evaluation system framework of enterprise network security index is established. Fuzzy evaluation index system is put forward to consider various factors. Through case study, it is concluded that comprehensive fuzzy evaluation can evaluate and solve the unclear boundary of some enterprise network security indexes and inaccurate fuzziness. As to the initial fuzzy evaluation and calculation of network security, the vector forms of results reflect the fuzzy distribution of the evaluation conclusions, which is more objective and scientific.
Due to the randomicity of network suffered from security attacks and the uncertainty of network security situation, it is difficult to precisely predict network security situation in single predication model. Therefore, an Immune-based Combination Predication Model for network security situation (ICPM) is proposed by using GM(1,1) and Artificial Immune Predication Model (AIPM). In ICPM, the trend component of a time series of network security situation are predicted by GM(1,1) model, and the random component of that are predicated by AIPM model, and then the slide window mechanism is introduced to be used for dynamically predicting network security situation. Experimental results show that ICPM model can forecast the future network security situation real-timely and correctly, and simultaneity its results are more precise than that of GM(1,1) model and AIPM model.
To conquer the problems existing in access control mechanisms available nowadays, such as how to manage the access authorities of the users and the protected resources, and how to keep the confidential data from visiting and modifying illegally, a new access control mechanism called Multi-Device TBPM-RBAC (MD-TBPM-RBAC) is proposed in this paper. According to the demand for unified users management in the network management system (NMS), MD-TBPM-RBAC uses Role-Based Access Control (RBAC) for the center, and expands the TBPM-RBAC. In MD-RBPM-RBAC, the users, resources and permissions are stored in the remote server, when a user lands, the device will communicate with the server to authenticate and authorize. As the MD-TBPM-RBAC implements the users' unified authentication and authorization, it manages the users centralized, protects the resources effectively, prevents the important resources from visiting illegally, and protects the critical processes from stopping illegally.
In this technology mediated world, e-Commerce is becoming more and more powerful medium for doing business, globally. Existing security technologies are proven to protect online transaction and fund transfer. However, while transacting with global e-merchants, trustworthiness of secure fund transfer and delivery of products/services are affected, and they could not leverage the potential benefit of e-Commerce. In this paper, the authors firstly overview the current state-of-threat by surveying was people's perception about trust in e-Commerce in developing countries like Bangladesh and argue that the security requirements of e-Commerce service generally go beyond the more traditional requirements of network security. The result analysis shows that, challenges in building trust for Business-to-Consumer (B2C) e-Commerce venture (local/international) is the major concern. The authors propose an e-Commerce enabled model for secure electronic fund transfer, and discuss ways to mitigate challenges in building trust in B2C.
Network forensics is an extension of the network security model which traditionally emphasizes prevention and detection of network attacks. It addresses the need for dedicated investigative capabilities in the current model to allow investigating malicious behavior in networks. It helps organizations in investigating outside and inside network attacks. It is also important for law enforcement investigations. In this paper, various aspects of network forensics are reviewed as well as related technologies and their limitations. Also, challenges in deploying a network forensics infrastructure are highlighted.
Awerbuch and Scheideler have shown that peer-to-peer overlays networks can survive Byzantine attacks only if malicious nodes are not able to predict what will be the topology of the network for a given sequence of join and leave operations. In this paper we investigate adversarial strategies by following specific protocols. Our analysis demonstrates first that an adversary can very quickly subvert DHT-based overlays by simply never triggering leave operations. We then show that when all nodes (honest and malicious ones) are imposed on a limited lifetime, the system eventually reaches a stationary regime where the ratio of polluted clusters is bounded, independently from the initial amount of corruption in the system.
A remote lab for electrical engineering experiments has been designed and implemented. This has been intended as a platform that supports existing traditional labs by offering lab experiments that serve some of the student learning outcomes of the program. This offers fully interactive environment to the remote user. A dedicated architecture that meets network security and equipment safety requirements has been implemented. The remote control of the experiment is ensured through the powerful and flexible shared variables engine of Lab VIEW. The remote lab has been successfully tested with two sample experiments. The infrastructure may accommodate many more concurrently-running experiments. Full details of the proposed platform are given.
A remote lab for electrical engineering experiments has been designed and implemented. This has been intended as a platform that supports existing traditional labs by offering lab experiments that serve some of the student learning outcomes of the program. This offers fully interactive environment to the remote user. A dedicated architecture that meets network security and equipment safety requirements has been implemented. The remote control of the experiment is ensured through the powerful and flexible shared variables engine of LabVIEW. The remote lab has been successfully tested with two sample experiments. The infrastructure may accommodate many more concurrently-running experiments. Full details of the proposed platform are given.
In this paper, the network security and traditional firewall technology were introduced, pointing out that the problems faced by traditional firewall. Then the structure of the distributed firewall, key technology and advantages of an in-depth study. The various parts of the text on the design and implementation of a detailed introduction. Then, the analysis of the control of several now distributed firewall technology at home and abroad, chose KeyNote distributed firewall model to achieve specific functions, and KeyNote model describes the various modules and functions to make..
In the context of complex networks,such as Internet, security testing determines whether an implementation under test conforms to its specification. Testing was carried out in a centralized and sequential way. This is no longer applicable or feasible as systems to be tested have become increasingly complex and can be widely distributed. We present a new solution to evaluate how a distributed approach can help in testing complex network security such as a network's components efficiently. The paper proposes two solutions to generate automatically distributed tests. By applying these solutions to Internet, it showed that the solutions ease development of distributed tests and distributed tests obtained are well suited for testing complex systems.
IPv6 is an attractive technology for innovative services such as health care monitoring, alarm systems, peer to peer applications, virtual machine systems and so on. The generalization of end to end paradigm, possible due to the length of IPv6 addresses, eases the deployment of such services. Nevertheless end to end connection can be a threat since application can be easily accessible from outside and thus a compromised application may endanger others. In this paper, we study some of the advantages of using the IPv6 protocol in home networks but most particularly how to improve the security of home networks. We present an architecture allowing the definition of a partition between groups of applications and where communication between these groups is not permitted if there is no explicit delegation. We overview the key points of the current implementation and some initial results of our approach.
The following topics are dealt with: optimization; mobile ad hoc networks; adaptive control; autonomous systems; information processing; healthcare; middleware; multimedia processing; radio frequency identification; sensor network security; structural health monitoring; and wireless sensor networks.
As new concepts for eSciene like Grid computing and Cloud computing tend to leave the research phase and develop towards production quality, the security eventually moves into focus. Up to now research in the security area concentrates on authentication and authorization on the resources themselves, but to enhance network security more generally, access control must be pushed back to the entry point of the resource providers' network. In this paper TCP-AuthN is presented, an approach for dynamic firewall operation, which uses the TCP three-way handshake to transport users' authentication information for dynamic firewall operation. The authentication information enables firewalls to authorize each connection establishment individually, based on the user's proven identity. To prevent man-in-the-middle attacks and replay attacks, a challenge-response procedure must be accomplished before the connection is finally allowed. To distinguish the authentication information from application level data, a new TCP option tcpauthn was designed. The presented approach is intended to withdraw the initial authorization decision from the resources and therefore from the internal network and move this decision to firewalls, which are employed to protect networks and services.
This paper presents a study made on network security in an organization's local area network (LAN). Intrusion detection test in a campus network was performed using penetration test methods and the results analyzed. The objectives were to identify different form of network attacks and methods used to capture the hacking. During the study, the risks and attacks caused by hackers to the network were evaluated. The results obtained are seen as a good indicator of the security state of the network. Hence, an organization network that responds well to penetration test can be given a certificate. Such certificate will provide a positive sign and confidence to the network users. The study was conducted in a dynamic situation by doing experiments during different periods of time. The case study was a campus LAN, The network administrator permitted network information like internet protocol (IP) address to be gathered and analyzed and to performed the penetration test that enabled , hackers and attackers methods to be dentified. It was realized that 90%of network the users has no fear of the network security risk inspite of the finding that network security rating of the case study is at 50 percent.
In recent years, many network users has been the serious impact by the intrusion of the Internet, there are many attack events occur at the present time. On the Internet, the intruders usually launch attacks from the stepping stone that they previously compromised. In this way, the attackers can reduce their risk of directly detected. Even if the network managers detect the invasion that still can't identify the true attack intruders IP location through the observation records. And there are many network attacks using a large amount of stepping stone to launch a joint flooding attack at the same time. The victim computer or Web site will not bear this kind large flood traffic that make the servers cannot work properly to provide normal Internet services. The problem of stepping stone has being caused serious impact on network security. In this study, we use the association rules mining technique to establish an automatically stepping stone detection module. By collect the connecting records in the local network to gather the information of source and destination IP addresses in the same period. The traffic data analyse by association rule algorithms trying to figure out the transmission characteristics of the stepping stone. By identifying the suspicious IP addresses of stepping stone will be helpful for the network security administrators to improve the security of network.
This paper proposes an efficient and secure three-party smart card-based authentication (3PSA) scheme based on one-way hash function without verification table to minimize the complexity of hash operation among all users and fit three-party communication. The proposed 3PSA scheme is based on the random nonce, uses one-way hash function, and does not need to store any verification table in the server. In addition, the proposed 3PSA scheme can withstand seven well known network security attacks.
False positives are critical problems of network intrusion detection systems that use pattern matching algorithm to detect network intrusions. The algorithm is unable to eliminate false packets with short lifespan. Secondly, the algorithm lacks the capability to manage the trade-offs between false and true positives. Consequently, system administrators are frequently swamped with massive false alerts from intrusive packets that cannot achieve their objectives and unfortunately, such alerts are often mixed with few true positives. However, how to substantiate these two generic groups of alerts without incurring additional overheads are classical research issues. Therefore, we present clustering-based adaptive P-filter model to investigate false positives. Alerts from Snort were the input to the P-filter model and they were clustered with some sequential filtering criteria. Extensive evaluations that we performed have demonstrated high efficacy of our approach to collaborate with pattern matching algorithm in achieving significant reduction of false positives during intrusion detections.
Network intrusion detection systems are used in network forensics and network auditing to log suspicious activities that potentially signify security violations on the networks as alerts. However, the efficacies of intrusion aggregations to succinctly process audit logs that are gaining wider acceptability in computer security are flawed because the methods frequently require high level of expertise to validate each alert and the methods only focus on interesting events. Thus, deceptive attacks that are intentionally launched to be uninteresting events frequently elude detections. Consequently, aggregated alerts are not seriously considered for litigation and incident handling exercises. Therefore, this paper presents extensive investigations of these problems. We deployed Snort to sniff offline datasets in intrusion detection mode and we clustered the alerts of each dataset with several filtering criteria. Furthermore, the results obtained have established how to detect various kinds of interesting and uninteresting attacks that frequently elude detections.
Internet is facilitating numerous services while being the most commonly attacked environment. Hackers attack the vulnerabilities in the protocols used and there is a serious need to prevent, detect, mitigate and identify the source of the attacks. Network forensics involves monitoring network traffic and determining if the anomaly in the traffic indicates an attack. The network forensic techniques enable investigators to trace and prosecute the attackers. This paper proposes a simple architecture for network forensics to overcome the problem of handling large volumes of network data and the resource intensive processing required for analysis. It uses open source network security tools to collect and store the data. The system is tested against various port scanning attacks and the results obtained illustrate the effectiveness in its storage and processing capabilities. The model can be extended to add detection and investigation of various attacks.
Network information monitoring technology (NIMT) is a key technology to the next generation of network defense system. The existed NIMTs mostly do not have a dynamic warning and controlling function, neither support IPv6. This paper provides a scalable network dynamic monitoring and warning system. It forms an associated dynamic security defense system based on traffic analyse, intrusion detection and statistics table. It can be connected in parallel or series with high flexibility. Proved by experimental comparisons, it can significantly increase the efficiency of data capture and network monitoring through the association of various modules.
In the regulated electricity market the difference between the demand forecast and the actual demand is balanced by additional generation from the cheapest available source after network security is taken into consideration. The electricity price to supply this difference in demand forecast error is already pre-fixed through the bulk supply tariff. In deregulated markets many of the current market structures rely on bilateral contracts for trading. The financial settlement for this imbalance energy is different from the normal contract settlement. In the ideal case the imbalance should be zero but this is almost impossible as the contracted values are usually based on demand forecasts and which contain errors. The imbalance settlement could be viewed as an ex post mechanism. The imbalance settlement thus depends very much on the electricity prices of the real time balancing market, and on the markets structure. This structure can differ from one country to another and could even be different for different markets within the same country. This paper reviewed two different imbalance settlement systems and a comparison will be given at the end.
The increasing interconnectivity of SCADA (Supervisory Control and Data Acquisition) networks has exposed them to a wide range of network security problems. Also in that case WSN (Wireless Sensor Networks, which is a new computing paradigm that emerged from the fusion of the SCADA systems and Ad hoc networks technologies, have gained the advantage over SCADA due to its simplicity and the ad-hoc nature of the network. This paper provides an overview of all the issues that are involved in strengthening the interconnectivity of SCADA networks and how the WSN has gained the advantage as a solution for SCADA. The paper describes the general architecture of WSN and SCADA networks and the properties of some of the commonly used SCADA communication protocols. This paper presents an overview of challenges in the design and implementation of WSNs. It summarizes the potential challenges that influence the WSNs design. Also this paper proposes an example solution to interconnect such environments using low cost and customizable sensor nodes which each has the computational power built in.
Network security is a complex and challenging problem. The area of network defense mechanism design is receiving immense attention from the research community for more than two decades. However, the network security problem is far from completely solved. Researchers have been exploring the applicability of game theoretic approaches to address the network security issues and some of these approaches look promising. This paper surveys the existing game theoretic solutions which are designed to enhance network security and presents a taxonomy for classifying the proposed solutions. This taxonomy should provide the reader with a better understanding of game theoretic solutions to a variety of cyber security problems.
One of the challenges facing system e-government security professionals is the laborious task of sifting through numerous log files in an attempt to identify malicious traffic and conduct a forensics analysis to determine an appropriate course of action. This process is complicated significantly by the volume of traffic that can be associated with a production system environment. A honeynet can provide a mechanism to identify much of the forensically interesting traffic by creating a representative system to collect traffic data. However, it is challenging to maintain an accurate representation of a dynamic system in order to consistently collect the appropriate data of interest. This research effort addresses a current challenge identified by researchers at the Honeynet Project by describing a methodology for automatically creating and dynamically updating a honeynet in order to facilitate IDS support.
Applying channel information for user authentication is gaining attention in the area of wireless network security. Numerous studies have demonstrated reconfigurable antennas with multiple operating modes that have the ability to choose the best channel in a given environment. We investigate the performance gains that can be achieved by using multiple uncorrelated channel realizations for user authentication at the physical layer by employing a reconfigurable antenna. Ray tracing simulations using a reconfigurable circular patch antenna were carried out and the intruder detection rate and false alarm rates were studied as a function of the number of available antenna modes and the SNR at the receiver.
This paper describes a new approach for network security based on the combination of biological intrusion prevention and self-healing concepts. The presented system integrates an artificial immune intrusion prevention system for network security inspired by immunological theory known as danger theory. The approach is based upon data inspired by the human immune system (HIS). The IPS analyzes the malicious activities and their effect to trigger the self-healing system. That is detection of the damage caused by malicious activities is used to start the self - healing (SH) mechanism. This system is automated and enhances the fault repair and system recovery.
A more and more enterprises using Web services to accelerate its own development. At this point, how to network in an open application environment, guard the confidentiality of their data, resources are more and more concern for people, network security has become the network an integral part of the building. This paper introduces the architecture of network security, and further information on the types of security threats, network security so that readers with some basic understanding of the types of security services in this article in accordance with established safety precautions, and finally the introduction of security rules can be achieved.
Various technologies have emerged since the Internet protocols were first introduced. Many of these technologies were compellingly complex to implement that it was thought that the existing Internet would have to be redesigned in order to accommodate them. While these proved to be false, security, however, remains to be a major challenge that current and future network designs should accommodate to achieve survivable and secure networks. This paper reaffirms that future network security can be effectively addressed on the lower-level by introducing an identity packet for network protocols. In this paper, we will focus on the newly developed network protocols, such as UDT, a considered next generation protocol. This original work is aimed at providing another way of securing high speed network protocols such as UDT.
McAfee SecurityCenter Firewall is one of the most popular security software installed on millions of Internet connected computers worldwide. &#x0201C;McAfee claims that if you have installed McAfee SecurityCentre with anti-virus and anti-spyware and Firewall version 9.3 then you always have the most current security to combat the ever-evolving threats on the Internet for the duration of the subscription&#x0201D;. In this paper, we present our findings regarding the effectiveness of McAfee SecurityCentre software against one of the most popular Distributed Denial Of Service (DDoS) attack, namely Ping-flood attack on the computer which has McAfee SecurityCentre installed. The McAfee SecurityCentre software has an in built firewall which can be activated to control and filter the Inbound/Outbound traffic. It can also block the Ping Requests in order to stop or subside the Ping based DDoS Attacks. To test the McAfee Security Centre software, we created the Ping traffic in the controlled lab environment of Networking Research Lab here at The University of Texas-Pan American. It was found that the McAfee Firewall software itself was incurring DoS (Denial of Service) by completely exhausting the available memory resources during its operation of stopping the external Ping Attack.
Owing to characteristics of open medium, dynamic topology and distribution, the security of Mobile Ad hoc Network (MANET) is more rigorous than that of traditional network. Referring to the I-ADD process of security analysis, the paper analyzed features, insecurity factors and security threats of MANET. Aiming at the security requirements, the design requirements for network security defense of MANET was brought out. Based on this and referring to OSI hierarchy model, the security architecture was designed. The analysis on relationship between each layer of the architecture and that of OSI was also provided, which offers framework for planning and designing secure and reliable MANET.
Network security study has been developing methods for discovering anomalies and for recognizing malignant patterns for intrusion detection. On the other hand, network traffic study has been investigating methods for traffic classification mainly for traffic engineering. Intuitively, combining these two fields of study is expected to achieve an innovative solution for various issues in intrusion detection as well as in bots attack mitigation recently proliferating. To this end, we conduct a survey on the existing methods for applying traffic measurement to resolving network security issues and attempt to identify a list of challenges in classifying malicious traffic.
This paper introduces some known non-optimum to the networks security, categorizes the non-optimum, and analyses protection mechanisms and techniques for countering the non-optimum. The non-optimum have been classified more so as definitions and then followed by the classifications of these non-optimum. Also mentioned are the protection mechanisms. The paper establishes the syndrome and empirical analysis based on the non-optimum category of the network system. At the same time, it also puts forward the non-optimum measurement of the networks system along with non-optimum tracing and self-learning of the networks systems. Besides, the various characteristics and functions of the network security can be measured from the non-optimum attributes. By summing the practice, this paper has also come at non-optimum analysis principle of the networks, established the conception of non-optimum thresholds and put forward three theorems about non-optimum parameters. Through the concept of extensionality networks function, it analyzes the actual significance of networks security based on non-optimum analysis. Based on the analysis from non-optimum to sub-optimum, it puts out the academic idea of extension networks optimal. Meanwhile, it discusses about the general framework of extension optimum. Finally, according to the previous practice of optimization, kind of method has been developed to self-organization approach the sub-optimum from non-optimum network.
Certified reputation that can be saved by the owner simplifies reputation propagation, it moves the burden of obtaining and maintaining trust information from the trust evaluator to the peer being evaluated who is incentive to do so. But the reputation owner can discard some disadvantageous certified reputation without being detected. In this paper, a mediator-based certified reputation scheme (MCR) is proposed through appending a serial number to certified reputation.
The network dynamic evaluation is in the proven prescription legal science has truthfully given the trusted degree of the network from the real network to the simulation experiment evaluation concrete model and the realization process. The network modelling process uses the extraction real network data, establishes the simulation environment, under the simulation environment destructive collapse test method in the dynamic test process. Using the result data to carries on the sampling test in the real network, to confirmation model establishment accuracy and the evaluation result credibility.
An attack graph increasingly plays an important role in network security. It shows possible paths of actions consisting of the network vulnerability exploits that can lead to security breaches. Because most attack graphs are very large and complex, much research has focused on how these graphs can be automatically and efficiently generated. However, little has been done on attack graph analysis, namely how we can use attack graphs to better protect the network. This paper addresses the latter issue. We present a suit of systematic approaches to statically analyzing attack graphs by means of reasoning mechanisms based on logical expressions and conditional preference networks. The proposed approaches are general and theoretically grounded. The paper describes the approaches in details. We show how the resulting analysis can help derive many useful decisions. For example, it can assist a security administrator in selecting most cost-effective countermeasures, based on his preference criteria, to improve the security flaws found in the attack graph. For understandability, we illustrate our approach by presenting a study of a simple and small but realistic case scenario.
Today, there are difficulties finding all malicious programs due to juridical restrictions and deficits concerning the anti-malicious programs. Also, a "grey-zone" of questionable programs exists, hard for different protection programs to handle and almost impossible for a single user to judge. A software reputation system consisting of expert, average and novice users are proposed as a complement to let anti-malware programs or dedicated human experts decide about questionable programs. A simulation of the factors involved is accomplished by varying the user groups involved, modifying each user's individual trust factor, specifying an upper trust factor limit and accounting for previous rating influence. As a proposed result, a balanced, well-informed rating of judged programs appears, i.e. a balance between quickly reaching a well-informed decision and not giving a single voter too much power.
This paper studies the analysis on the Cyber Clean Center (CCC) Data Set 2009, consisting of raw packets captured more than 90 independent honeypots, in order for detecting behavior of downloads and the port-scans. The analyses show that some new features of the coordinated attacks performed by Botnet, e.g., some particular strings contained in packets in downloading malwares, and the common patterns in downloading malwares from distributed servers. Based on the analysis, the paper proposes the heuristic techniques for detection of malwares made by Botnet coordinated attack and reports the accuracy of the proposed heuristics. The detection process is automated in the proposed decision tree consisting of statistics, such as, a number of total inbound packets, and an average rate of downloading malwares.
Although highly promising to meet the challenges of pervasive network security, self-managed protection has been little addressed in this setting. This paper adopts a policy-based management approach to the problem, and presents a policy-driven security framework called ASPF. Enforced authorization policies in a device are adapted according to the security context, both at the network and device levels. ASPF describes how an autonomic security manager may control OS-level authorization mechanisms supporting multiple classes of policies. Evaluationof an ASPF implementation shows that the design is applicable for effective and yet flexible self-protection of pervasive systems.
It is very important in theory and application to evaluate credibility of trust model. Although many application models are carefully analyzed, designed and implemented, there exist some faults yet. In this paper we studied trust relationship of agents, proposed a credibility calculation model among entities and gave their complete calculation method. It considered dynamic changes of trust relationship and focused on dynamic updating strategy of credibility. In course of calculation cross chain of trust was simplified reasonably by defining and processing associated chain of trust. When carrying on updating calculation of recommendation trust, it considered not only present evaluation result but also historical evaluation records and made evaluation system more rational. A dynamic access control algorithm based on credibility was also present. It enhanced flexibility of mobile process and availability of network security facility. In end of this paper, an experiment on direct trust relationship and recommendation trust relationship among agents was carried on. Experiment results showed that after some circle calculations trust relationships among agents reached a stable state and the credibility calculation model was feasible.
This paper proposes effective countermeasures to hardware vulnerability of the keyboard controller. Through the vulnerability, some possible attacker is able to snoop the password string from the keyboard even when keyboard protection software is running. However, it will be impossible for attackers to gather the exact password if the proposed policy is applied though they can sniff the keyboard hardware protocol.
In response to the growing number of network security threats, this paper gives a new design based on the combination of initiative and passive network intrusion detection system. The system uses initiative expert system for efficient intrusion detection, and integrates honeypot technology to extract and update the attack knowledge base. It possesses a certain autonomous learning and self-adaptation.
<para> Large-scale bandwidth-based distributed denial-of-service (DDoS) attacks can quickly knock out substantial parts of a network before reactive defenses can respond. Even traffic that is not under direct attack can suffer significant <emphasis emphasistype="boldital">collateral damage</emphasis> if the traffic passes through links that are common to attack routes. This paper presents a <emphasis emphasistype="boldital">Proactive Surge Protection</emphasis> (PSP) mechanism that aims to provide a broad first line of defense against DDoS attacks. The approach aims to minimize collateral damage by providing bandwidth isolation between traffic flows. The proposed solution is readily deployable using existing router mechanisms and does not rely on any unauthenticated packet header information. Our extensive evaluation across two large commercial backbone networks, using both distributed and targeted attacks, shows that up to 95.5%of the network could suffer collateral damage, but our solution was able to significantly reduce the amount of collateral damage by up to 97.58%in terms of the number of packets dropped and 90.36%in terms of the number of flows with packet loss. Further, we show that PSP can maintain low packet loss rates even when the intensity of attacks is increased significantly. </para>
Detecting reconnaissance provides a key warning of an adversary's impending attack or intelligence-gathering effort against a network. Yet many current network defense tools provide little capability to detect network reconnaissance.
<para> As the Internet's <emphasis emphasistype="italic">de facto</emphasis> interdomain routing protocol, the Border Gateway Protocol (BGP) is the glue that holds the disparate parts of the Internet together. A major limitation of BGP is its failure to adequately address security. Recent high-profile outages and security analyses clearly indicate that the Internet routing infrastructure is highly vulnerable. Moreover, the design of BGP and the ubiquity of its deployment have frustrated past efforts at securing interdomain routing. This paper considers the current vulnerabilities of the interdomain routing system and surveys both research and standardization efforts relating to BGP security. We explore the limitations and advantages of proposed security extensions to BGP, and explain why no solution has yet struck an adequate balance between comprehensive security and deployment cost. </para>
<para> This paper discusses the use of a communications network security device, called a trust system, to enhance supervisory control and data-acquisition (SCADA) security. The major goal of the trust system is to increase security with minimal impact on existing utility communication systems. A previous paper focused on the technical operation of the trust system by augmenting routers to protect User Datagram Protocol (UDP)-based traffic. This paper concentrates on placing the trust system into a broader context, creates new trust system implementations to increase its flexibility, and demonstrates the trust system using TCP traffic. Specifically, the article expands on previous work in the following ways: 1) the article summarizes major threats against SCADA systems; 2) it discusses new trust system implementations, which allow the trust system to be used with a wider array of network-enabled equipment; 3) it discusses key SCADA security issues in the literature and shows how the trust system responds to such issues; 4) the paper shows the impact of the trust system when widely prevalent TCP/IP network communication is used; and 5) finally, the paper discusses a new hypothetical scenario to illustrate the protection that a trust system provides against insider threats. </para>
The Dartmouth College Cyber Security Initiative (CSI) is a collaboration between faculty, staff, and students that focuses on projects to improve the security of Dartmouth's information systems. The CSI gives students experience in real-world, hands-on problem solving and shows that they can be trusted to perform such tasks.
A wide variety of security software competes for control of desktops, servers, and handhelds. Competition for control over a system's security posture can leave systems mired in a performance tar pit and subvert the very security they were meant to provide. Although the use of defense in-depth is widely recommended, it isn't nearly as automated as it could be, particularly when it comes to composing policy in addition to functionality. We suggest a paradigm in which security programmers intentionally design their code to cooperate with similar software by negotiating over security-critical resources, system measurement points, event types, and trusted information flow paths.
To achieve their full potential, networks must be secure as well as functional. With this in mind, the author identifies metrics designed to mitigate vulnerabilities to cyberattacks in networks that are key to the critical infrastructure of the US. He discusses both growth metrics &#x02014; based on data obtained from the US National Institute of Standards and Technology and Department of Homeland Security vulnerability database &#x02014; and metrics designed to mitigate the risk of security vulnerabilities in networks. If used together, these two types of metrics can help make networks more secure.
Identifying network anomalies is essential in enterprise and provider networks for diagnosing events, like attacks or failures, that severely impact performance, security, and Service Level Agreements (SLAs). Feature-based anomaly detection models (ab)normal network traffic behavior by analyzing different packet header features, like IP addresses and port numbers. In this work, we describe a new approach to feature-based anomaly detection that constructs histograms of different traffic features, models histogram patterns, and identifies deviations from the created models. We assess the strengths and weaknesses of many design options, like the utility of different features, the construction of feature histograms, the modeling and clustering algorithms, and the detection of deviations. Compared to previous feature-based anomaly detection approaches, our work differs by constructing detailed histogram models, rather than using coarse entropy-based distribution approximations. We evaluate histogram-based anomaly detection and compare it to previous approaches using collected network traffic traces. Our results demonstrate the effectiveness of our technique in identifying a wide range of anomalies. The assessed technical details are generic and, therefore, we expect that the derived insights will be useful for similar future research efforts.
This article describes an innovative approach to modeling network designs in order to quantify their ability to mitigate the impact of denial of service attacks on end-user services. The methodology has been developed and implemented into a tool that calculates end-user downtime and failure rate from DoS frequency data and design attributes such as security feature coverage. The application of security vulnerability modeling during design enables designers to evaluate design options and quantify the outage risks for different design strategies. The optimum design can be selected, and verification and operational action plans can be determined.
This paper presents a novel method for determining the capacity of a network to accommodate new generation under network security constraints. The assessment is performed by maximizing the total generation capacity in an optimal power flow model; this is solved by gradually adding limited numbers of line outage contingencies, until a solution to the complete problem is obtained. The limit on the number of contingencies added is key to the method's efficiency, as it reduces the size of the optimization problems encountered. Moreover, varying this limit on contingencies added provides a simple and highly efficient means of searching for multiple local optima of the nonlinear optimization problem. The method has been tested on a modified version of the highly meshed IEEE Reliability Test System with <i>N</i>-1 security, where a significant reduction in the system's capacity for new generation is seen when security constraints are imposed. The method is generic and may be applied at any voltage level, for other security models and for other similarly structured problems such as the analysis of multiple resource availability scenarios.
In this letter, an enhanced version of Address Resolution Protocol (ARP) is proposed to prevent ARP poisoning-based Man-in-the-Middle (MITM) attacks. The proposed mechanism is based on the following concept. When a node knows the correct Media Access Control (MAC) address for a given IP address, if it retains the IP/MAC address mapping while that machine is alive, then MITM attack is impossible for that IP address. In order to prevent MITM attacks even for a new IP address, a voting-based resolution mechanism is proposed. The proposed scheme is backward compatible with existing ARP and incrementally deployable.
This brief addresses the problem of designing adaptive neural network tracking control for a class of strict-feedback systems with unknown time-varying disturbances of known periods which nonlinearly appear in unknown functions. Multilayer neural network (MNN) and Fourier series expansion (FSE) are combined into a novel approximator to model each uncertainty in systems. Dynamic surface control (DSC) approach and integral-type Lyapunov function (ILF) technique are combined to design the control algorithm. The ultimate uniform boundedness of all closed-loop signals is guaranteed. The tracking error is proved to converge to a small residual set around the origin. Two simulation examples are provided to illustrate the feasibility of control scheme proposed in this brief.
Although the theory of distributed firewall was proposed shortly, but because of its advantages relative to traditional firewall, business user oriented characteristics, it could meet customer demand for higher security. This paper proposed a firewall program suitable for network security in accordance with small and medium enterprise network security situation, systematically analyzed the distributed firewall policy management and policy actuator functions, features and related technologies, to discuss the overall design and structure of the log server module.
This paper presents a DDoS (Distributed Denial of Service) two stage defense model based on roaming honeypot. The model can accurately identify the early features of DDoS attacks, and can automatically select effective detection features according to different types of DDoS attacks to accurately determine the legitimacy of IP flows with a smaller consumption, so that to a smaller consumption roam and avoid being attacked.
This paper studies the solution to a kind of data acquisition model, introduces its design principle and architecture, makes a detailed study of its key technology and sums up a kind of method for realizing multi-source data synthesis acquisition, thereby laying a good foundation for the upper data analysis of unified network security management system.
<para> This paper explores the impact of <formula formulatype="inline"> <tex Notation="TeX">${rm CO}_{2}$</tex></formula> emission trading on capacity planning of electric power transmission systems. Two different models for annual emission costs are assumed. The <formula formulatype="inline"><tex Notation="TeX">${rm CO}_{2}$</tex></formula> emission price is modeled as a probability density function in the transmission network planning problem. The Monte Carlo technique is deployed to simulate the <formula formulatype="inline"> <tex Notation="TeX">${rm CO}_{2}$</tex></formula> emission price volatility. The transmission network planning problem is formulated as a mixed-integer optimization whose objective is to minimize the sum of annual generator operating costs and annuitized transmission investment costs over different demand levels subject to N-1 network security constraints as well as operating limits on system components. The overall problem is formulated within the framework of a linear dc optimal power flow incorporating binary decision variables to model the lumpy nature of transmission investment. A linear model of losses is also proposed and included in the dc power flow model. The proposed approach can be used to determine the most probable optimal transmission capacity. The methodology is demonstrated through case studies simulated on the IEEE 24-bus network. </para>
This paper explores the impact of CO<sub>2</sub> emission trading on capacity planning of electric power transmission systems. Two different models for annual emission costs are assumed. The CO<sub>2</sub> emission price is modeled as a probability density function in the transmission network planning problem. The Monte Carlo technique is deployed to simulate the CO<sub>2</sub> emission price volatility. The transmission network planning problem is formulated as a mixed-integer optimization whose objective is to minimize the sum of annual generator operating costs and annuitized transmission investment costs over different demand levels subject to N-1 network security constraints as well as operating limits on system components. The overall problem is formulated within the framework of a linear dc optimal power flow incorporating binary decision variables to model the lumpy nature of transmission investment. A linear model of losses is also proposed and included in the dc power flow model. The proposed approach can be used to determine the most probable optimal transmission capacity. The methodology is demonstrated through case studies simulated on the IEEE 24-bus network.
<para> IP security (IPsec) protocols are widely used to protect sensitive data over the Internet. For equipment linked by high-bandwidth optical fibers, the throughput requirement usually results in the adoption of high-performance network security processors. In this paper, we propose a parallel mesh-structured IPsec (MIPsec) processor, which executes the IPsec protocols for Internet security gateway applications. We have developed several area-efficient cryptographic IPs embedded in MIPsec to lower silicon cost. Thanks to structural regularity, the simple deterministic programming of MIPsec guarantees high utilization of the hardware. Also, both handshake and contention issues are solved in the scheme, such that performance can be scaled up. Specifically, the 6.23-million-gate MIPsec achieves 10-Gb/s wire speed for each routing direction. The proposed MIPsec is suitable for transport mode or other crypto mix as well. </para>
To overcome the challenges of recovery polices generation in the presence of inaccurate failure detection, a failure recovery model for microrebootable distributed systems based on discounted Partially Observable Markov Decision Processes is presented in this paper. Thus the reasonable recovery policies are generated by solving the POMDP model. To tackle the problem of computational complexity of exact solution, a value function approximate solution called fast informed bound solution is used for the near-optimal policies. Simulation-based experimental results on a realistic network security situation prediction system demonstrate that the proposed model can be solved effectively, and the resulting policies convincingly outperform others.
With the rapid development of Internet, it is an important task to ensure that college students accessing the Internet in a healthy way. This paper discusses the monitoring of user behavior by means of SNORT software in order to establish a campus network security monitoring system.
Network risk assessment has become a hot topic in the area of network security research in recent years. Risk Calculation in the risk assessment is a very important part. Some methods have been presented to resolve this problem. These methods have strengths and drawbacks. In order to achieve more actual risk assessment result of network system, this paper proposed a new network risk calculation method based on Source and Destination of attack events, the two risks to determine whether the event is a alert from two different aspects in order to reflect the security of the entire network. At the end, through the experimental results with DARPA Data sets 2000 from Lincoln laboratory, it demonstrates the potential of the proposed method.
Wireless networks play critical roles in present work, home, and public places, so the needs of protecting of such networks are increased. Encryption algorithms play vital roles in information systems security. Those algorithms consume a significant amount of computing resources such as CPU time, memory, and battery power. CPU and memory usability are increasing with a suitable rates, but battery technology is increasing at slower rate. The problem of the slower increasing battery technology forms &#x201C;battery gap&#x201D;. The design of efficient secure protocols for wireless devices from the view of battery consumption needs to understand how encryption techniques affect the consumption of battery power with and without data transmission. This paper studies the effects of six of the most common symmetric encryption algorithms on power consumption for wireless devices. at different settings for each algorithm. These setting include different sizes of data blocks, different data types (text, images, and video files), battery power consumption, different key size, different cases of transmission of the data, effect of varying signal to noise ratio and finally encryption/decryption speed. The experimental results show the superiority of two encryption algorithm over other algorithms in terms of the power consumption, processing time, and throughput. These results can aid in new design of security protocol where energy efficiency is the main focus. Some suggestions for design of secure communications systems to handle the varying wireless environment have been provided to reduce the energy consumption of security protocols. So that current wireless network security protocols has no clothes according to power consumption
Arc-flash detection sensors provide a cost-effective way to reduce arc-flash energy by minimizing detection times. High-speed light detection and tripping can compromise protection security by misoperating during changing light conditions. Trip circuits using arc-flash light detection should be supervised using overcurrent protection with similar fast detection speeds. Combining arc-flash detection and high-speed overcurrent from a protective relay provides fast tripping and security, using both instantaneous overcurrent and light from the arc flash. The combination of relay and arc sensor provides independent fault detection with two separate technologies, thereby eliminating false trips from lighting and providing the fastest detection and tripping possible. Time coordination delays are eliminated when the arc is detected concurrently with an overcurrent. This paper presents the advantages of fast overcurrent detection combined with arc-flash measurement to produce a sensitive, fast, and secure tripping scheme.
The current Internet era has begun change from "Available" to "Trusted." Building a trusted, secure and stabile Internet, can effectively protect the network information security and interests of users. The main purpose of trusted network is to improve network security and integrity. It is technically improving network security, the right remedy, the cure of the Internet security issues. Trusted network is an important way for network development in the future, it has broad prospects and will open a new Internet era.
This article firstly introduces current situation of network security. Then a brief history and process of network attack have been stated. Methods to launch network attack and principles against network attack have been brought to the notice in detail. Through this paper, people can learn how to build safer network environment.
The course titled Network Security Protocols is critical for graduate students interesting in research on information security, with two education goals focusing on protocol design and analysis. Due to the absence of proper textbooks on network security protocols, it is a little hard to propose efficient education schemes. In this paper, a compositive education purpose solution is proposed for knowledge of security protocol design and analysis. It covers the technology of formal theoretical analysis, engineering design and implementation, and performance analysis on security protocols. It is a systematical education framework with multilevel specialty techniques supported, which can make students deeply understand both theory and practice on network security protocols.
Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks are posing major threat to today&#x02019;s essential Internet service. The need to protect servers and connected systems is an important aspect in network security. Hence this research work proposes a novel approach called Hop Count based Packet processing to counter DDoS attacks. DDoS attacks are difficult to identify at the source since the attackers use spoofed IP addresses. But it is not possible for the attackers to spoof the Hop Count value in the IPV6 header. This research work utilizes this idea to counter the attacks and it is assumed that all the systems in the current Internet architecture are located within a maximum hop count value of 255. In this approach the packets from the systems at the same hop count and traversing through the same router are marked with the same identification number. This number is derived by the concatenation of the 32 bits of the IP address of the router path and the encrypted value of the hop count. At the receiving side of the router interface the hop count value of the incoming packet is checked with the already stored value. This technique provides an advantage of immediately filtering the traffic after receiving just one attack packet and it does not require any change in the existing protocols. Thus this technique has a significant potential in reducing the threats caused by the DDoS attacks.
This paper proposes an implementation of an intrusion detection and prevention system in a Linux environment, with an extensive database of attack signatures, but also with a flexible interface for defining new signatures. The project relies on the analysis of current requirements and challenges in network security, leading to an evaluation of existing detection and network attack mitigation techniques. Attack evaluation and mitigation framework has a modular design, with multiple configuration options and availability during configuration. For increased accessibility, the solution provides a graphical interface available through a web browser, and a command line interface for the network administrator.
The objective of this work is to create usable security architecture that will minimize network risk while considering usability and budget. We propose and formulate a novel framework for automatic creation of network security architecture including configuration rules and device placements in order to minimize risk while satisfying the business requirements, service usability and budget constraints. Our framework also automates the creation of external and internal Demilitarized Zones &#40;DMZ&#41; to improve security by increasing isolation. We formalize this as an optimization problem and show that it is NP-hard. We then provide heuristic approximation algorithms. The implemented systems, called SecBuilder, were evaluated under different network sizes, topologies and security requirements. Our evaluation study shows that the results obtained by SecBuilder are close to the theoretical lower bound and the performance is scalable with the network size.
A firewall is the most important tool of network security defense. Its proper functioning is critical to the network it protects. Therefore a firewall should be tested rigorously with respect to its implemented network protocols and security policy specification. We propose a combined approach for test case generation to uncover errors both in firewall software and in its configuration. In the proposed approach, abstract test cases are generated by mutating event sequence graph model of chosen network protocol and filled with values from policy specification by using equivalence partitioning and boundary value analysis. A case study is presented to validate the presented approach.
Multi-string matching is a key technique for network security applications like Network Intrusion Detection Systems &#40;NIDS&#41; and anti-virus scanners. &#37;, where every packet is inspected against thousands of predefined signatures in real time. Existing DFA-based approaches always tradeoff between memory and throughput, no known approach has the best of both worlds. Hence, they fail to be used in the embedded systems like high speed routers where only limited on-chip resources are available. This post proposes a multi-step string matching acceleration scheme named step Finite Automata &#40;Step-FA&#41;. Different from classical string matching approaches, we suppose to match an additional structural characteristic named as the distance of certain characters in Step-FA rather than the pure explicit characters themselves.&#37; in classical DFA. As Step-FA does not follow each-byte-one-memory-access manner in the classical DFA, a high speedup can be achieved meanwhile the memory requirement decreased sharply. The Step-DFA gives the trade-off between accurate and approximate matching. For the purpose of guaranteeing he equivalence between the Step-FA and DFA, a verification module is introduced to fast check the already matched results. Experimental evaluations with ClamAV show that a 6 times of speedup can be practically achieved by a single Step-FA Matching System with a 70&#37; reduced memory comparing to the up-to-date DFA-based approaches.
This paper presents a complete novel smart mobile terminal antenna system (SMTA) for optimising wireless network and security performance. This SMTA system uses a switched radiation beam approach in an attempt to achieve optimal wireless network performance while increasing network security for indoor and outdoor applications, and signal tracking for marine application. Test results are presented that demonstrate the ability of the SMTA system to maintain consistently high wireless signal levels in a dynamic and noisy environment.
Gary McGraw interviews Steven Kent, Chief Scientist for Information Security at BBN Technology, a division of Raytheon, where he's been engaged in network security research and development for several decades. Hear the full podcast of the interview at www.computer.org/security/podcasts/ or www.cigital.com/silverbullet/. See the full text of this interview at www.computer.org/cms/Computer.org/dl/mags/sp/2010/03/extras/msp2010030005s.pdf.
Since emerged in 2004, Unified Threat Management (UTM) has been used widely to enhance network security protection. Typical UTM device integrates multiple security technologies, therefore its control and management involves various interfaces, message formats, communication protocols, and security policies and so on. Therefore, it is a big challenge to design and implement the configuration and management of security technologies in UTM. To address this issue, this paper proposes a practical UTM control mechanism that features ease-to-use, scalability, interoperability, high-efficiency and reliability. The solution, called UTM-Configuration and Management (UTM-CM), has been implemented and its performance was evaluated.
With the growing number of botnet attacks, the botnet detection is becoming increasingly important for the network security. To enhance the existing botnet detection systems which are short of efficient information collection functions, this paper presents a collaborative information collection model with a new 5-tuple structural mode. In the model, we introduce the static and dynamic roles to meet the requirements of information collection and collaboration respectively. Moreover, we give an efficient design for the collection agent and its communication mechanism, which are the core components in the model. Finally, a representative example is given to show that our design for the collection agent can effectively collect the information about the widespread botnet activities, which can help to improve the detection performance and accuracy for a botnet detection system.
Intrusion detection system is of most importance to network security. Support Vector Machine &#40;SVM&#41; is algorithm about how to solve machine learning problems under circumstance of small sample. The paper respectively applies SVM based on least square and least-square SVM improved by greedy algorithm to intrusion detection, and does simulation experiment on intrusions detection data. Experiment result shows that least-square SVM based on greedy algorithm is more suitable in intrusion detection system in circumstance that the prior knowledge is less.
Intrusion detection is defined as a computer network system to collect information on a number of key points, and analyze this information from the security audit, monitoring, attack recognition and response, etc. to see whether there are violations of network security policy behavior and signs of attack. The classification of the data is the key to intrusion detection. It is faced how to extract the suspect data from the magnanimous data. This article applies cloud model in intrusion detection, and we can obtain the invasion data sets effectively.
Increasingly widespread application of mobile agent, mobile agent system's security is a prominent problem to be solved, mobile agent's security question is critical. In this paper, based on cryptography, computer network security, mobile agent security architecture at the same time gives the practical methods and suggested ways can be used in other new security measures. The core problem is how to use measures to ensure the security of mobile agent communication and mobile agent execution environment of security, at the same time ensure that mobile agent can be applied more widely.
this paper combines the characteristics of network security audit system and improves the original BM algorithm and makes it suitable for the use of network security audit system. New BMLA algorithm combines some of the strengths of BM Algorithm. In the matching process, it Not only generate greater jumping distance , but also Increases the probability of producing the largest mobile distance .Thus it accelerate the matching speed and Excellent improves the efficiency of matching.
With high speed development of the internet, network security is increasingly outstanding. In the large-scale high-traffic network environment, the traditional technologies for network-based intrusion detection system can not satisfy the needs for real-time processing for the growing network traffic. This paper presented a NetFlow based anomaly intrusion detection system. In addition, guidelines to properly configure and setup network device to minimize the possibilities that network attacks come from inside are also proposed. We propose an inexpensive and easy to implement way to perform the anomaly type intrusion detection based on the NetFlow information exported from the routers or other network probes. Our system can detect several types of network attack from inside or outside and perform counter maneuver accordingly.
Intrusion detection plays a very important role in network security system. It is proved to analyze the payload of network protocol and to model a payload-based anomaly detector (PAYL) can successfully detect outliers of network servers. This paper extends these works by applying a new noise-reduced fuzzy support vector machine (fSVM) to improve the detection rate. The new method named PAYL-fSVM employs reconstruction error based fuzzy membership function to reduce the noisy of the data and to solve the sharp boundary problem. Experimental results based on DARPA data set demonstrated that the proposed scheme can achieve higher detection rate at very low false positive rate than the original PAYL method.
Web-based systems commonly face unique set of vulnerabilities and security threats due to their high exposure, access by browsers, and integration with databases. In this paper we present empirical analysis of attackers activities based on data collected by two high-interaction honeypots. The contributions of our work include: (1) Classification of the malicious traffic to port scans, vulnerability scans, and attacks; (2) Conducting experiments which, in addition to attackers activities aimed at individual components, allowed us to observe and study vulnerability scans and attacks that span multiple system components; and (3) Statistical characterization of the malicious traffic.
Propagation of Peer-to-Peer (P2P) worms in the Internet is posing a serious challenge to network security because of P2P worms&#x02019; increasing complexity and sophistication, significant damages their propagation can cause, and recent popularity of P2P networks with increasing number of users. No existing work has solved the problem of propagation modeling of P2P worms due to its complexity. This paper presents a study on propagation modeling of P2P worms. It also presents our applications of the proposed approach in worm propagation research. Our major contributions in this paper are firstly, we propose a novel logic matrix approach to modeling the propagation of P2P worms, and demonstrate the approach&#x02019;s ease of employment through the simulation experiments; and secondly, we find the impacts of the network-related characteristics on a P2P worm&#x02019;s attack performance in structured P2P networks, and a critical value of vulnerability rate. The proposed discrete-time deterministic propagation model of a P2P worm is written in a difference equation of logic matrix. To the best of our knowledge, we are the first using logic matrix in network security research in general and worm propagation research in particular. The proposed approach&#x02019;s ease of employment makes it an attractive instrument to conduct worm propagation research.
This paper deals with an approach to security analysis of TCP/IP-based computer networks. The method developed stems from a formal model of network topology with changing link states, and deploys bounded model checking of network security properties supported by SAT-based decision procedure. Its implementation should consist of a set of tools that can provide automatic analysis of router configurations, network topologies, and states with respect to checked properties. While this project aims at supporting a real practice, it stems from the previous, more theoretical research designing the method in detail including its formal background.
The multiple pattern matching has always been a significant principle for applying to a network security system. This principle is adapted to accommodate the target patterns to be detected in a pre-processing phase, and the objective text and the suspect patterns are simultaneously detected by comparing with the existing structure. This research article introduces a new algorithm of multi-string pattern matching by means of a new data structure called inverted lists. The inverted lists structure, inherited from the inverted index, is represented by the positions of characters which appear in the patterns. The new algorithm, which is more efficient time than the traditional algorithms, scans the given text in a single pass. More importantly, the structure of patterns is able to update the patterns over time.
A crucial aspect in the development of a fault management system is the selection of backup paths. The main issue in this selection is a tradeoff between the resource consumption and the minimization of failure recovery time, link failure, and packet loss. In this paper, a general QoS backup protection optimization policy applicable to a wide range of network situations and traffic types is proposed. A comparison between the proposed policy and the conventional protection backup methods is performed. The numerical results show a significant improvement of the QoS protection over conventional protection backup methods at variable protection parameters, network constraints and traffic types.
Fast Internet growth and increase in number of users make network security essential in recent decades. Lately one of the most hot research topics in network security is intrusion detection systems (IDSs) which try to keep security at the highest level. This paper addresses a IDS using a 2-layered feed-forward neural network. In training phase, &#x0201C;early stopping&#x0201D; strategy is used to overcome the &#x0201C;over-fitting&#x0201D; problem in neural networks. The proposed system is evaluated by DARPA dataset. The connections selected from DARPA is preprocessed and feature range is converted into [-1, 1]. These modifications affect final detection results notably. Experimental results show that the system, with simplicity in comparison with similar cases, has suitable performance with high precision.
Within immunology, the danger theory is changing the traditional thinking ways of self/non-self discrimination. In this paper, an immune danger theory inspired model for network security threat awareness, referred to as NSTAidt, is proposed. After introducing the main idea of immune danger theory, the model architecture and the formal definitions are described in detail. Following that, with the network intrusion detection and danger signal computation method illustrated, the principle of network security threat awareness of NSTAidt is given. Theoretical analysis shows that NSTAidt is feasible. Thus, it provides a good solution to computer network situation awareness.
The wireless local area network commonly uses WEP (Wired Equivalent Privacy) encryption technology. Although it has encryption mechanism, but the WEP encryption mechanism has serious structural design deficiencies. In this article, we focus on the IEEE 802.11, hacker attack methods and WEP wireless networks to carry out instructions. Furthermore, the DoS / DDoS attacks also have become the most serious network security problems. Therefore, we use NS2 simulation to analyze the DoS / DDoS attacks and actual test common hacker using a wireless network to hacking, cracking WEP in actual operation.
With the popularity of logistics commerce and the development of enterprise information management technology, the application of Managements Information System based on network becomes more and more extensive. On this open condition of logistics commerce, the need of security of management information system accordingly is higher than before. The paper firstly introduces the actuality and preventing measure of the network security briefly, then presents the establishment of this network security system based on the logistics information platform, which includes visit control, transmit control and server security control etc.
Today Internet is very prosperous. Network firewalls have become basic network security equipments. Not only enterprises will set up firewall system, most of SOHOs and individual users also set up a firewall as a standard device. Because public IP is not sufficient in the IPv4-based environment, so most of networks are using NAT (Network Address Translation) mechanism to translation their private IP. Thus NAT also increases the complexity of VoIP (Voice over IP) through a firewall environment.[1] In order to allow VoIP network environment in normal operation in a firewall environment, IETF and the VoIP industry propose a lot of solution, This article is going to present some common frameworks for analysis VoIP operation and looking forward to provide VoIP normally working in a firewall network environment.
After analyzing and quantifying the network security situation awareness, a quantification method for NSSA based on conditional random fields (CRFs) was proposed. Based on the positive and negative ideal comparative standards, the evaluation index elements are processed in a non-dimensional grey way, and a qualitative-quantitative evaluation method with the multilayer linear weight for the network security situation awareness is put forward. Finally, the feasibility and validity of the method are verified by analyzing some practical examples.
Intrusion Detection Systems are essential in a network security solution. However, with the significant development of network technologies, the current IDS architecture does not support high-speed communications. Therefore, improving the performance of IDS is a major concern for researchers. In this paper, we present a model of intrusion detection based on the classification of network connections. Our approach is based on the principle of an intelligent loss. We propose a classification model based on the principle that a connection is either malicious or not. In the first case, the connection must be handled by the IDS; otherwise we can ignore it. This method reduces significantly the network flow sent to the IDS with a tolerance of an error threshold. This threshold can be adjusted by the updating process of the classification model.
Elliptic curve cryptography (ECC) is having good potential for wireless sensor network security due to its smaller key size and its high strength of security. But there is a room to reduce key calculation time to meet the potential applications in particular for wireless sensor networks. Scalar multiplication is the operation in elliptical curve cryptography which takes 80 &#x025; of key calculation time on wireless sensor network motes. This research proposes algorithm based on 1's complement subtraction to represent scalar in scalar multiplication which offer less Hamming weight and will remarkably improve the computational efficiency of scalar multiplication.
Wireless network technology has been well developed and used for various applications in the world. Particularly in resident area, most people use Wi-Fi system such as IEEE802.11a. b. g. n system as access lines to Internet. However, since most of the residents leak their electromagnetic waves to neighbor without any security keys, the network security problems have become serious in resident areas. On the other hand, large scale disasters in metropolitan areas as well as country side area in Japan are anticipated. In those cases of natural disasters information infrastructure must be preserved to keep the safe life for residents. In this paper, we propose a disaster communication network method based on leaked wave from resident wireless LANs system. The leaked waves outdoor from each resident can be used as urgent information infrastructure on which not only residents but public officers can also use to transmit the disaster information, resident safety information, evacuation information each other through Internet. We investigated practical feasibility of the disaster communication network by the leaked wave by evaluating the throughput, round trio time in actual resident area in Japan. Throughput this performance evaluation, we could verify the feasibility of our suggested disaster network method.
As network components are often highly interdependent and interconnected,an adversary outside can take advantage of multiple vulnerabilities in unexpected ways, incrementally penetrate a network and compromise critical systems. Attack graph is commonly used for analyzing network security level for its capability in reflecting all network vulnerabilities and their inter relationships. However, attack graph assumes an over pessimistic situation by giving the attacker unlimited power of exploiting each chain of vulnerabilities in the network, leading the complexity of analyzing to grow exponentially with the size of network. Therefore, the weakest paths suggested by such analysis could be inaccurate for adversary with limited computation power. In this paper, we investigate how attackers are planning to exploit vulnerabilities towards their targets and present the idea of a goal-oriented analysis of attack graph to address this problem. We give algorithms for analyzing network vulnerabilities, predicting attackers's potential target, and giving suggestions on patching the weakest nodes based on attackers' targets.
AMI is the totality of systems and networks used to measure, collect, store, analyze, and use energy usage data. The industry and technology surrounding AMI has been evolving at a very fast pace for the past several years. In this paper, we propose new key establishment and security algorithm based on public key cryptography to solve AMI network security problems. We evaluate the performance of the proposed key establishment procedure compared with existing algorithm, and we find that the performance of proposed algorithm.
Network security is a complex and systematic project. The intrusion detection system is the first line of defense against network security. Snort is a famous intrusion detection system in the field of open source software. It is widely used in the intrusion prevention and detection domain in the world. In this paper, we explain how Snort implements the intrusion detection, which includes building the compiling environment and analysizing the work-flow and rule tree. This paper will provide a valuable reference for the study of Snort.
An MPLS VPN network structure was proposed to be used in the highway monitoring network to solve the bottleneck drawbacks of the existing network,to improve the network security,and to meet the needs of the highway monitoring network. The labeling process of the network edge MPLS is similar to the classification process of the Diff-Serv model,on the basis of which the model was introduced into the network. Thus,the labeling process was distinguished in accordance with the requirements of different services. QOS was improved then. The simulation results showed that this technology not only met the basic needs of building special road monitoring networks,but improved effectively the performance-price ratio of the routing technology of the network layer,increased the network expansion,and satisfied requirements of the real-time detection and security of the road monitoring network.
This article gives examples to explain application and implementation schemes of digital signature in network security. and points out the existing problems and countermeasures of digital signature technology application. This article also explains technical significance and technical assurance of digital signature which plays important role in network security technology
Inspired by the principles of immune danger theory, a novel model for network security evaluation, referred to as DTIMNSE, is proposed in this paper. With the improved con-cepts and formal definitions of antigen, antibody, danger signal, and detection lymphocyte presented, the distributed architecture of the proposed model is described. Following that, the principle of network intrusion detection is expounded. And then, the method of network security evaluation is given. Theoretical analysis and simulation results show that the proposed model can evaluate the network attack threats in real time. Thus, it provides an effective risk evaluation solution to network security.
Traditional methods for evaluating network security neglect the correlation of network vulnerabilities. To solve this problem, a method based on Colored Petri Net (CPN) modeling is presented. Potential attack sequences are built according to the correlation of network vulnerabilities. The weakness and key paths can be found through analyzing the attack sequences. Simulation results show that the proposed method can help network managers to find the hidden danger actively and make security strategies effectively.
This paper proposes a wireless network security evaluation method based on Fuzzy-AHP (Analytic Hierarchy Process) with variable weight. In this paper, we first design the evaluation index system based on characteristics of wireless networks, and then analyze each uncertainty measured by the Fuzzy-AHP method, and introduce variable weight theory into Fuzzy-AHP method to address the difficulty of weight decision, overcome the subjectivity of weight decision to some extent and improve the accuracy of evaluation. It is shown in the case study that the method is effective and practical to evaluate security for wireless networks.
Network security situation perception is to predict the probability of attacks, may occur in the future, by a variety of predicting methods, by recent network attacking data obtained from IDS (Intrusion Detection System). Neural Network model has many features, high degree of fault tolerance, associability, self-organizing and self-learning ability, and strong nonlinear mapping and generalization for a complex system, for example. Therefore, Neural Network was applied to the field of network security situation prediction. Adaptive Learning of neuron was introduced. It will be more flexibility to meet changing security environment of such a complex system requirements. The design and achievement of the adaptive learning neuron was stated in detail.
Privacy protection is one of the important topics in network security. Although a lot of work has been done, there are still some open issues and challenges that need to be addressed. Now, open and dynamic computing environments offer flexible and convenient sharing of information and services for users. Consequently, there exist a lot of different policies for privacy protection to deal with dynamic changes in each application scenario. This raises the issue of privacy policy specification. Although trust can help protect user privacy, the use of unified trust values or a simple trust rank cannot satisfy the requirement for privacy protection because different privacy may need its own trust evidence for the evaluation of trust. To solve this problem, we propose a novel approach for privacy protection based-on ontology in this paper. Our main contributions in this paper include (1) specifying privacy policies in a semantic way and (2) abstracting the policies as trust attributes for privacy ontology. In addition, our method can be used to protect sensitive policies at the same time.
The great advance in attacks against network has led to outgrowth of interest in more aggressive forms of defense to supplement the existing security approaches. One of these techniques involves the use of the deception to collect information about attacks. A honeypot is a security deception resource whose value lies in being probed, attacked or compromised. In this paper we present an overview of techniques used by honeypots to deceive the attacker and attacking process, and provide a comparison for persons who are interested in study and developing this new technology. We examine various types of honeypots, and deception techniques they use to counter attacks.
In recent years, client user has become the main target for attacks, as the adversary believe that the end user is the weakest link in the security chain. Traditional honeypots and security tools are not effective against these new attacks. Therefore, client honeypot has appeared as new technology to supplement the existing protection tools. Client honeypot is a honeypot actively searches for malicious sites on the web. In this paper, we will show and analyze the main approaches used by client honeypots to detect client-side attacks. We will also address how attacker can evade and hide from client honeypots. Moreover, we discuss and analyze various issues relates to client honeypot: detection problems, invisibility of honeypots, and integrity issues. By analyzing characteristics of client honeypots, we will introduce factors to define and measure client honeypots effectiveness.
The network security nowadays has become a major priority for both network design and implementation to protect the valuable applications, sensitive data, and network resources from unauthorized access. The firewall is one of the network security devices which effective in defending known intrusions. This paper provides an overview of network firewalls, its importance, and different types of network firewalls, studying the effect of implementing the firewall on the network performance and how using parallel firewalls. We note that by using parallel firewalls the network performance is improved in order to limit the network delay and average response time.
The goal of this research is to increase honeypot security through data analysis with Artificial Neural Network (ANN). Thus, first we present an approach to detection presence of computer malcode in the honeypot based on ANN while using the computer's behavioral measures. Then, we identify significant features, which describe the activity of a malcode within a honeypot, by acquiring these from security experts. We suggest employing fisher's score, one of the feature selection techniques, for the dimensionality reduction and identification of the most prominent features to capture efficiently the computer behavior in content of malcode activity. Later on, we preprocess the dataset according to this technique and train the ANN model with preprocessed data. Finally, we evaluate the ability of the model to detect the presence of a malcode in the honeypot when honeypot is at risk.
Malware and botnets pose a steady and growing threat to network security. Therefore, packet analysis systems examine network traffic to detect active botnets and spreading worms. However, with the advent of multi-gigabit link speeds, capturing and analysing header and payload of every packet requires enormous amounts of computational resources and is therefore not feasible in many situations. We address this problem by presenting an efficient packet sampling algorithm that picks a small number of packets from the beginning of every TCP connection. Bloom filters are used to store the required connection state information with constant amount of memory. Our analysis of worm and botnet traffic shows that the large majority of attack signatures is actually found in these packets. Thus, our sampling algorithm can be deployed in front of a detection system to reduce the amount of inspected packets without degrading the detection results significantly.
In this note, we first give an overview of the economic and traffic conditions of residential broadband Internet access in the United states, with a focus on Comcast's multiple-priority approach to congestion control for its cable modem termination system (CMTS). We compare the Comcast framework to alternative proposals, including those based on usage based pricing and quotas. The security overhead and potential network security benefits of usage priced systems are also explored
Client honeypots visit and interact with suspect web sites in order to detect and collect information about malware. This paper will show the benefits of client honeypots as well as the use of an automated state machine in conjunction with a client honeypot. The state machine provides a powerful framework to organize monitoring of malware activity and record the results. We are developing a platform for integrating client honeypots applied on state machines in a Windows environment.
Phishing is continually shown to be a problem in network security. Phishing is trickery towards the user believing that he or she is communicating with a trusted source. Most phishing attacks are focused on the financial infrastructure such as financial institutions like banks and services like Paypal. We proposed a different interface than the traditional browsers which prove through past research are prone to fail in complete security. A thin client instead will be created to allow a secure connection between a client and the institution. We believe that this is a better way to prevent people from losing their private information due to phishing.
Abstract- The security of complex networks with multiple elements is very difficult to evaluate and characterize by numbers. The interaction between the network elements, the different layer topologies and the numerous features makes the security quantification almost impossible. On the other side, the lack of security benchmarking is very problematic for the budget and invests allocation by companies. Numerical economical indexes for the costs and potential benefits are used to set the budgets. The security is not be quantified and it cannot be mapped to these economical indexes, thus the budget is not set objectively. This paper suggests a novel framework for quantification of network security, thus security benchmarking. The relative vector expresses the different layers, physical connections, operation risk, and human resources. The benchmark is relative and not absolute value, which is an indirect indication for the security. The relative security vector maps to economical values and helps the management to take the decisions. The suggested framework extends the common standards like ISO 27000, BSI, ITIL, which characterize single network elements or processes in corporations. This framework is the missing link between the security standards, subjective expert analysis and the monetary instruments. The benchmarking is not saying if a system is secured, then it gives a relative indirect comparison between systems.
Networked control systems (NCSs) have been one of the main research focuses in academia as well as in industry for many decades and have become a multidisciplinary area. With these growing research trends, it is important to consolidate the latest knowledge and information to keep up with the research needs. In this paper, the NCS and its different forms are introduced and discussed. The beginning of this paper discusses the history and evolution of NCSs. The next part of this paper focuses on different fields and research arenas such as networking technology, network delay, network resource allocation, scheduling, network security in real-time NCSs, integration of components on a network, fault tolerance, etc. A brief literature survey and possible future direction concerning each topic is included.
An Internet worm is a typical Internet attack that can rapidly pervade a computer without user intervention. In the frequent episodes mining, data is regarded as a sequence of events, where each event has an associated time of occurrence, thus, it has significant effect on the discovery of sophisticated Internet attacks. The method proposed in this paper can be used to detect abnormal Internet episodes from the log files of a honeypot system in order to discover known or unknown attack episodes. The experiment successfully identified sophisticated Internet attack episodes, which were caused by Internet worms, such as Sasser, Shelp, Korgo, etc.
This paper discusses a curriculum approach that will give emphasis on practical sessions of teaching network security subjects in information and communication technology courses. As we are well aware, the need to use a practice and application oriented approach in education is paramount [1]. Research on active learning and cooperative groups showed that students grasps and have more tendency towards obtaining and realizing soft skills like leadership, communication and team work as opposed to learning using the traditional theory and exam based method. While this teaching and learning paradigm is relatively new in Malaysia, it has been practiced widely in the West. This paper examines a particular approach whereby students learning wireless security are divided into small manageable groups consisting of black hat and white hat team. The former will try to find and expose vulnerabilities in a wireless network while the latter will try to prevent such attacks on their wireless networks using hardware, software, design and enforcement of security policy and etc. This paper will try to demonstrate whether this approach will result in a more fruitful outcome in terms of students concept and theory understandings and motivation to learn.
Packet classification algorithms are widely used in network security devices. As network speeds are increasing, the demand for hardware acceleration of packet classification in FPGAs or ASICs is growing. Nowadays hardware architectures can achieve multigigabit speeds only at the cost of large data structures, which can not fit into the on-chip memory. We propose novel method how to reduce data structure size for the family of decomposition architectures at the cost of additional pipelined processing with only small amount of logic resources. The reduction significantly decreases overhead given by the Cartesian product nature of classification rules. Therefore the data structure can be compressed to 10%on average. As high compression ratio is achieved, fast on-chip memory can be used to store data structures and hardware architectures can process network traffic at significantly higher speed.
At present, the economy, politics, military and many other fields of the various countries are increasingly dependent on the Internet, and the future Internet is undoubtedly part of IPv6, so now began to study how to make IPv6 networks more secure is significant. Based on the summary of the existing DOS/DDOS attacks and network security models, this paper proposes an improved network security model and analyzes the advantages of it.
Attack graphs can be applied to analyze network security, but there are two issues, namely scalability and loop. To solve these problems, we propose a new kind of attack graph named GP-AG, which is generated based on greedy policy. The construction process of GP-AG falls into two phases, the first phase generates the main attack graph with complete greedy policy, and the second phase generates all the sub attack graphs with incomplete greedy policy, the entire attack graph is composed of the main attack graph and all the sub attack graphs. The experiment results indicate that GP-AG provide a novel solution to the problem of efficient attack graph representation and analysis with less nodes and edges and without loops generated, and can help network administrators to find the critical vulnerabilities and attack paths effectively.
This In this paper, the community is now growing network security issue, which is based on principles of active immune intrusion defense system. First, design improvements based on immune mechanisms of the existing intrusion prevention system, detection engine, and use mathematical methods to be proved. Then proposed a model of active defense system, in which the model was introduced honey net system as a network module lured them with the traditional intrusion prevention system combines creative to join attack on the module, thereby achieving intrusion prevention real-time and initiative.
based on the introduction of the common network security technology and network security management platform module partition, this paper provides a network security management platform system design, and in detail designs its subsystem. Also in the system development process, the various features of the system are carried out corresponding testing, and the test results show that the system has reached the expected requirements.
Network security situation awareness provides the unique high level security view based upon the security alert events. But the complexities and diversities of security alert data on modern networks make such analysis extremely difficult. In this paper, we analyze the existing problems of network security situation awareness system and propose a framework for network security situation awareness based on knowledge discovery. The framework consists of the modeling of network security situation and the generation of network security situation. The purpose of modeling is to construct the formal model of network security situation measurement based upon the D-S evidence theory, and support the general process of fusing and analyzing security alert events collected from security situation sensors. The generation of network security situation is to extract the frequent patterns and sequential patterns from the dataset of network security situation based upon knowledge discovery method and transform these patterns to the correlation rules of network security situation, and finally to automatically generate the network security situation graph. Application of the integrated Network Security Situation Awareness system (Net-SSA) shows that the proposed framework supports for the accurate modeling and effective generation of network security situation.
On the Internet, high-rate flows that do not obey the TCP flow control mechanism can consume a large share of the link bandwidth and seriously affect other flows. Therefore, identifying high-rate flows is important for active queue management, traffic measurement and network security. Explicit measurement of high-rate flows is difficult because tracking the possible millions of flows needs correspondingly large high-speed memories. To reduce the measurement overhead, the deterministic 1-out-of-k sampling technique is adopted. Since the sampled packets are only a part of the whole traffic transmitted, it is critically important to identify high-rate flows correctly. However, there are no methods which are able to specify the identification accuracy. We develop a Bayesian single sampling method (BSS) which is able to identify high-rate flows with user-specified false positive rate (FPR) and false negative rate(FNR). The experimental results show that BSS can successfully identify high-rate flows with satisfied accuracy constraint.
For the shortcoming of traditional intrusion detection system (IDS) in complex and unknown attack detection. A distributed intrusion detection system based on honeypot was proposed. We make use of honeypot to collect the invasion characteristics on the network, and use the method of unsupervised clustering (UC) and genetic clustering to extract the data for analysis. In addition, in order to improve the detection performance of the IDS, it combined protocol analysis with signature detection modules. Experiments result show that this system can better detect intrusion and improve the overall safety performance of large-scale networks.
As the mine coal industry information and networking development, the majority of our coal mining companies are carrying out the actual process of production of production, safety monitoring and control systems, a variety of automatic control system construction, integration of these systems through the network together, set up a variety of overall mine digital information network. But with this set of integrated application networking, network security issues has cropped up that if he let hackers or other purposeful people who use the Internet invaded coal mines integrated network control center, which consequences could be very serious. In this paper, focus on intrusion prevention system in this area was the role of a proposed method of active defense can be a very good network security to protect mine.
With the rapid development of Internet and the expanding application fields of network applications, it is not enough to maintain the network security only depend on those professional network technical administrators. In this paper a novel visualization tool based on treemap, which is called NetVis, is introduced to bind the network security technique and general network management together in an integrated visualization. NetVis is designed in 2D view. The experiments show that NetVis can not only detect the abnormal activities in the network, but also make the network management more intuitive and efficient.
The latest survey data about China network security were shown in this article. They revealed that the network security problems were becoming more serious in China. Computers affected and damaged by virus were very serious. Hackers had become one of the most dangerous threat. The security of the infrastructure of information service was challenged. Most of the Chinese Internet users did not believe its security. The main constraints of China's Internet security were put forward based on the analysis of the investigation result. And also the author talked about the measures should be taken to strengthen the network security of China.
Steganography is an important issue in the filed of information safety. This paper concretely designs and carries out a steganography of the text secret information. The introduction of chaos theory conveniences to the test of steganography characteristics and enhance the safe of steganography. Adopting an improvement type BPCS steganography, this scheme can resist the analysis of the steganalysis. Results show that the design of the paper has certain of application value.
The hierarchy model of computer network security assessment was constructed in the light of the characteristics of computer network and the factors influencing the network security. The fuzzy analytic hierarchy process based on triangular fuzzy numbers was applied to assess the network security quantitatively. Final results of the example indicate that the fuzzy analytic hierarchy process is an exploratory method for network security assessment.
The evaluation of network security is a hot issue in present research of network security. After acquiring each host's confidentiality, integrity and availability evaluation index by attacking the network, we can directly evaluate the network security. But, for the network with Read-Write management privilege setting to the root directory, it is necessary to execute the state transition of the security attribute value by reason of the existence of root directory Read-Write relationship. Firstly, the Read-Write transition model and correlative concepts are presented for the root directory with Read-Write privilege setting; then the model transition function is formalized by the state transition arithmetic operator &#x2299;. Based on the transition model, we have designed the network security state transition machine and the network security evaluation model. Finally, the method's feasibility and validity are verified by practical example analysis.
Applications to evaluate Internet quality-of-service and increase network security are essential to maintaining reliability and high performance in computer networks. These applications typically use very accurate, but high cost, hardware measurement systems. Alternate, less expensive software based systems are often impractical for use with analysis applications because they reduce the number and accuracy of measurements using a technique called interrupt coalescence, which can be viewed as a form of sampling. The goal of this paper is to optimize the way interrupt coalescence groups packets into measurements so as to retain as much of the packet timing information as possible. Our optimized solution produces estimates of timing distributions much closer to those obtained using hardware based systems. Further we show that for a real Internet analysis application, periodic signal detection, using measurements generated with our method improved detection times by at least 36%.
It is an important factor for the honeypot decoy system to accurately analyze on intrusion information and precisely locate them. As there are some conflict and cooperation relationship among the collected data by honeypot system, it needs to fuse the honey information so that normal decision can be made for intrusions. The paper regarded the fusion process of honeypot information as game process among information, so as to integrate theory game theory and information fusion, then the optimal security decision can be arrived. The research uses a new approach to fuse the multi-source information, which plays certain role on early warning of network.
Key management is considered as the fundamental part of any secure communication. In previous work, we propose RKPH scheme that was improved based on connectivity and resiliency compared to some other schemes. In this paper, we propose ARKPH, a modified scheme of RKPH that decrease impact of node compromise on sensor network security. In this scheme that we name it as alternative shared key replacement, impact of existence of multiple shared key between two nodes is studied. In RKPH, with compromising a secure link, link expire and secure connection between two neighboring node terminate. Expiring secure connections cause that connectivity in network decrease. For decreasing effect of compromising a node on expiring secure connections, we discover another shared key between two nodes that have not compromised yet. This work is done directly or indirectly by cluster head. Simulation results show that expiring secure connections in ARKPH scheme is decreased.
Digital forensics techniques are still seen as specialized tools for cyber police. Likewise, until a few decades ago, computer and network security had a perception of defence utility for military establishments. But now computer and network security has become a commodity of every corporate system and home PC. Today&#x02019;s businesses are feeling the need of efficient monitoring mechanisms to protect them from emerging commercial threats such as competitor analysis and steganalysis. Enterprises in the United States have widely embraced the digital forensics technology; however, European enterprises especially small and medium enterprises (SMEs) have yet to tap the potential of this technology. The European Commission initiative of Future Internet Enterprise Systems (FInES) presents a promising new era for enterprise innovation. However, existing enterprise security solutions and practices generally concentrate on the pre-incidence measures i.e. attack preventions with virtually no considerable approach for the post-accident scenarios. This article presents a framework of digital forensics for SMEs and evaluates the scope of various methodologies and tools for these enterprises. It also presents a detailed analysis of the risks to the competitive-edge of the companies that will not employ the forensics solutions to protect their business interests. This work also provides a high-level roadmap for the adaption of digital forensics in the emerging core business technologies such as cloud computing and virtualization infrastructures.
The Greek Atomic Energy Commission is the competent authority responsible for designing, implementing and supervising the radiation protection programme in Greece. According to its statutory law one of its main responsibilities is the provision of education and training to people involved in the national emergency response plan against nuclear and radiological threats. Due to the high requirements demanded for the safe conduct of the Athens 2004 Olympic Games, a nuclear security programme was established and the nuclear security infrastructure of the country was upgraded. Under this framework, GAEC provided training on radiation protection, prevention, detection, emergency preparedness and response to the personnel involved in the emergency plan. Since that time, the GAEC continues to organize seminars frequently addressed to the organizations involved in the emergency plan, in order to establish the sustainability of national operational capability on preparedness and response.
In Mobile Ad hoc NETworks (MANETs), certification systems play an important role in maintaining network security because attackers can freely move and repeatedly launch attacks against different nodes. By adopting certification systems, it becomes possible to exclude identified attackers from the network permanently by revoking the certifications of the attackers. A simple way to identify attackers is to collect information on attackers from nodes in the network. However, in this approach, it is difficult to differentiate valid accusations made by legitimate nodes from false accusations made by malicious nodes. In addition, the amount of traffic in order to exchange information on attackers and the necessary time to gather the information increases as the network size becomes larger. In this paper, we propose a certificate revocation scheme which can revoke the certification of attackers in a short time with a small amount of operating traffic. By clustering nodes and introducing multi-level node reliability, the proposed scheme can mitigate the improper certificate revocation due to false accusations by malicious users.
In order to overcome the deficiencies of the traditional Artificial Immune Systems (AIS) in the security field of computer information system, an immune danger theory inspired model for network security monitoring is proposed in this paper. After introducing the back ground to the danger theory, a novel model for network security monitoring is presented. With the formal definitions of antigen, antibody, and detector improved, the computation method of affinity and danger signal are described. And then, the network at-tack monitoring algorithm is given. Theoretical analysis results show that the proposed model is feasible. Thus, it pro-vides a good solution to the security assurance of computer information systems.
Network security situation evaluation and prediction is a new technology to monitor network security, and it is one of hot research domains in information security. The research situation of network security situation evaluation and prediction all over the world is analyzed. A network security situation evaluation and prediction model based on grey theory is presented. The model is divided into two stages: current network security situation evaluation modeling and future network security situation prediction modeling. The model of current network security situation evaluation using simple additive weight method is established by the threat of various services attacked. The model of future network security situation prediction adopting grey theory is built by past and current network security situation.
Aiming to solve the problem of network security evaluation index, the data extracted from net flow are analyzed. And the data are aggregated by several relational dimensions. Using the method of grey relation advantage analysis, the grey relation values of network security events and the characters of the data are calculated. The pivot characters representing the events were decided. Then a set of network security evaluation indices are proposed. The experiment results show that the indices can reflect the network situation efficiently.
We present techniques for synchronizing nodes that periodically broadcast content and presence updates to colocated nodes over an ad hoc network, where nodes may exhibit Byzantine malicious behavior. Instead of aligning duty cycles, our algorithms synchronize the periodic transmissions of nodes. This allows nodes to save battery power by switching off their network cards without missing updates from their neighbors. We propose several novel attack classes and show that they are able to disrupt synchronization even when launched by a single attacker. Finally, we devise a rating based algorithm (RBA) that rates neighbors based on the consistency of their behavior. By favoring well-behaved nodes in the synchronization process, we show that RBA quickly stabilizes the synchronization process and reduces the number of lost updates by 85 percent. Our evaluation also shows that all our algorithms are computationally efficient and, for the setup considered, extend the device lifetime by 30 percent over an always-on Wi-Fi scenario.
P2P is a network model that is developing rapidly in the recent years. Compared with the traditional Client/Server model, P2P has a lot of advantages, such as the higher utilization of network resources, the elimination of bottleneck caused by central servers, and so on. So P2P has a great potential value on commerce and technology. This paper first analyzes the network encryption, and combines the characteristics of P2P network model and Web Services transmission structure. Also a P2P file sharing system is designed and realized based on .NET framework. The feasibility of the proposed P2P security transmission model is certified and the experimental results and analyses are given.
There is a growing demand for network devices capable of examining the content of data packets in order to improve network security and provide application-specific services. Most high performance systems perform content inspection in the use of regular expression-based pattern matching, since regular expressions offer superior expressive power and flexibility. The regular expression searching algorithms that use filtration are in general much faster searching. The effectiveness of this method depends basically on two factors, l<inf>min</inf> and Pref (RE). l<inf>min</inf> is the shortest length from the initial state to final states of the NFA and Pref (RE) is the prefixes of length l<inf>min</inf> for all the strings in the language of the regular expression. The search speed is faster for longer l<inf>min</inf> and less prefixes. In this paper a regular expression searching method based on bloom filter is proposed. It makes the searching speed notable fast and immune from the size of Pref (RE). Especially, if multiple bloom filter engines are used, the shift distance may be larger than l<inf>min</inf>. Experiments showed that the method can achieve much higher speed than currently known algorithms and the acceleration is especially remarkable for set of regular expression matching
To protect our networks against malicious intru- sions, we need to evaluate these networks security. Previous works on attack graphs have provided meaningful conclusions on security measurement. However, large attack graphs are still hard to be understood vividly, and few suggestions have been proposed to prevent inside malicious attackers from attacking networks. To address these problems, we propose a novel approach to evaluate network security based on adjacency matrixes, which are constructed from existing attack graphs. With our model, we use gray scale images to show overall security vividly, and get quantitative evaluation scores. Moreover, we create a prioritized list of potential threatening hosts, which can help network administrators to harden network step by step. Analysis on computation cost shows that the upper bound computation cost of our measurement methodology is O&#40;N3&#41;, which could be completed in real time. We also give an example to show how to put our methods in practice.
Interactive network traffic replay plays an important role in testing and evaluating in-line network security devices such as Firewalls, IPSs, etc. In this paper we present a balance-based method for improving the performance of interactive TCP traffic replay. The new method is based on the inherent feature of the TCP protocol, that is, two communicating peers keep synchronized with each other using data acknowledgment. This feature is converted as a balance mechanism in interactive TCP traffic replay and incorporated into the current state-based method. In this way, the cost of state-checking can be significantly reduced and the replay performance is thus enhanced. To validate the effectiveness of the method we implement it by building an interactive replay system. The experimental results indicate that: 1) balance-checking reduces the overhead of state-checking by 40&#37;; 2) the balance-based method enhances the overall replay performance by an average of 5&#37; when the actual TCP traffic traces are replayed.
There is an increasing demand for network devices to perform deep packet inspection (DPI) to enhance network security. In DPI the packet payload is compared against a set of predefined patterns which can be specified using regular expressions (regexes). It is well-known that mapping regexes to deterministic finite automata (DFA) will suffer from the state explosion problem. Through observation, we attribute DFA explosion to the necessity of remembering matching history. In this paper, we investigate how to record the matching history efficiently and propose an extended DFA approach for regex matching called fcq-FA, which can make a memory size reduction of about 1000 times with a fully automated approach. In fcq-FA, we use pipeline queues and counters to help recording the matching history. Hence, state explosion caused by Kleene closure and repetitions can be definitely avoided. Further, it achieves a fully automated signature compilation with polynomial running time and space.
Beyond Quality of Service and billing, one of the most important applications of traffic identification is in the field of network security. Despite their simplicity, current approaches based on port numbers are highly unreliable. This paper proposes an identification approach, based on a cascade of decision trees. The approach uses the sign pattern and payload size of the first four packets in each flow, thus remaining applicable to encrypted traffic too. The effectiveness of the proposed approach is evaluated on five real traffic traces collected in different time periods and over four different networks. The obtained overall accuracy gives us grounds to consider the adoption of this approach as stand-alone in on-line platforms for network traffic identification or in combination with classical firewall architectures.
Multi-pattern matching is a key technique for implementing network security applications such as Network Intrusion Detection/Protection Systems (NIDS/NIPSes) where every packet is inspected against predefined attack signatures written in regular expressions (regexes). To this end, Deterministic Finite Automaton (DFA) is widely used for multi-regex matching, but existing DFAbased researches have claimed high throughput at an expenses of extremely high memory cost. In this paper, we propose a parallel architecture of DFA called Parallel DFA (PDFA), using multiple flow aggregations to increase the throughput with nearly no extra memory cost. The basic idea is to selectively store the DFA in multiple memory modules which can be accessed in parallel and to explore the potential parallelism. The memory cost of our system in both the average cases and the worst cases is analyzed, optimized and evaluated by numerical results. The evaluation shows that we obtain an average speedup of about 0.5k to 0.7k where k is the number of parallel memory modules under our synthetic trace and compressed real trace in a statistical average case, compared with the traditional DFA-based matching approaches.
To detect viruses, worms and, malware in the multi- gigabit environment, it is crucial for modern content-aware network security appliances to have a fast virus scanning scheme.Signature based multi- pattern matching algorithm is the core technology to enable fast virus scanning accurately and quickly. This paper proposes a multi-pattern matching algorithm with a simple shift/hash technique and a novel heuristic by inspecting overlaps between pairs of patterns to ensure both average and worst-case performance. Experimental results show that our algorithm performs 600Mbps to 1.4Gbps faster than the ClamAV AC and BM-based algorithms and achieves a maximum of 3.8Gbps throughput in inline virus scanning while the memory consumption is nearly the same.
Online Social Networks &#40;OSNs&#41; have become a mainstream cultural phenomenon in the past years, where million of people connect to each other and share memories, digital media and business relations. Many users also publish personal information about their activities, relationships, locations and interests on these sites, seemingly unaware of how these data can be used by other parties. Sites typically attempt to restrict data-sharing to members of a user''s social network, but this is only effective if these social networks cannot be exploited by malicious users. In this paper we perform an experiment in order to assess the vulnerability and privacy awareness of users when engaging in online relations with random unknown users, or those pretending to be a famous character. We find that usually users do not accept random friendship requests, but some aggressively search for celebrities, making a perfect case for spammers to form honeypots using such fake profiles. We present a set of suggestions for enhancing privacy on social networks which could reduce the threats of identity theft in such environments.
Black-box web application vulnerability scanners are automated tools that probe web applications for security vulnerabilities. In order to assess the current state of the art, we obtained access to eight leading tools and carried out a study of: (i) the class of vulnerabilities tested by these scanners, (ii) their effectiveness against target vulnerabilities, and (iii) the relevance of the target vulnerabilities to vulnerabilities found in the wild. To conduct our study we used a custom web application vulnerable to known and projected vulnerabilities, and previous versions of widely used web applications containing known vulnerabilities. Our results show the promise and effectiveness of automated tools, as a group, and also some limitations. In particular, "stored" forms of Cross Site Scripting (XSS) and SQL Injection (SQLI) vulnerabilities are not currently found by many tools. Because our goal is to assess the potential of future research, not to evaluate specific vendors, we do not report comparative data or make any recommendations about purchase of specific tools.
Sensor networks have been proposed for military and scientific applications such as border security and environment monitoring. They are usually deployed in unattended and hostile environments, so security is a major concern. A fundamental requirement in wireless network security is the ability to establish keys between pairs of sensors. In this paper, we propose a new location-based key management protocol in which polynomial-based and random key pre-distribution are both used for key establishment between sensor pairs. Key establishment between near sensors is provided by the polynomials, while key establishment between far sensors is accomplished by random key pre- distribution. Using these two approaches simultaneously reduces the overhead required for key establishment. Analysis is presented which shows that the proposed scheme has good performance compared with other approaches.
Organization has come to realize that network security technology has become very important in protecting its information. With tremendous growth of internet, attack cases are increasing each day along with the modern attack method. One of the solutions to this problem is by using Intrusion Detection System (IDS). Machine Learning is one of the methods used in the IDS. In recent years, Machine Learning Intrusion Detection system has been giving high accuracy and good detection on novel attacks. In this paper the performance of a Machine Learning algorithm called Decision Tree (J48) is evaluated and compared with two other Machine Learning algorithms namely Neural Network and Support Vector Machines which has been conducted by A. Osareh [1] for detecting intrusion. The algorithms were tested based on accuracy, detection rate, false alarm rate and accuracy of four categories of attacks. From the experiments conducted, it was found that the Decision tree (J48) algorithm outperformed the other two algorithms.
In recent years, automated intrusion response has become a promising research problem in network security. Several approaches have been proposed to perform an effective automated response policy. However, these approaches have some limitations, i.e., heavily depending on attack alerts and not taking in account uncertainty of system runtime state. In this paper, we present a comprehensive sequential decision-making based automated intrusion response approach. We utilize different decision approaches and models to respectively represent and reason about attack activities and system runtime state in view of their different dynamic nature. We perform some experiments to validate proposed approach and the results show that our approach has good performance in response accuracy to different attack scenarios and robustness against false alerts.
The development of computer networks is followed by a rapid evolution of threats to the security of their use. The phenomenon, though observed for years, has increased in the last period of time causing a growing problem to sustain an adequate level of security. New Internet threats utilize the latest techniques and Internet technologies. Computer crimes constitute a true challenge for software engineers and hardware producers as well as for users and all institutions responsible for the security of networks, including state institutions. Numerous world organizations supervise the issue of security. In the last period of time, the governments of many countries, including the European Parliament, have been working intensively on the laws concerning the network security.
UTM (Unified Threat Management) is rapidly becoming a most important network security device in many enterprises, particularly in small-sized and mid-sized offices. In this paper, the author introduces the concept, background, central function and the typical technologies of UTM; then the UTM system model based on NP structure is achieved.
Today VoIP and video conferencing applications are very common in internet. On the other side, network firewalls have become basic network security equipments. Not only enterprises will set up firewall system, most of SOHOs and individual users also set up a firewall as a standard device. In order to allow Video Conference network environment in normal operation in a firewall environment, IETF and the Video Conference industry propose a lot of solution. [1] This article is going to present some common frameworks for analysis Video Conference operation and looking forward to provide Video Conference normally working in a firewall network environment.
Stochastic game theory is proposed to apply in the research on network security situational awareness (NSSA), which is a research focus in network security field at present. A novel dynamic awareness method of network security situation (NSS) based on analyses of network service states is proposed in this paper. Realizing situation awareness is a dynamic process, and the diverse states of network services are just direct mirrors of the whole network security situation. Network security situation reflects what is happening in the network, including both the offense and defense behaviors in it. Stochastic game model of network security system is constructed in this paper, and network security situation is quantified by the game mathematical formulation, costs or rewards of attackers and defenders are established, and finally non-linear programming is used to compute the Nash equilibrium points, at which point both of the two sides get a balance between their benefits. Network security situation can then be dynamically achieved by visualizing the diverse metrics information of network services at Nash equilibrium during the operating of network system.
Online social networks such as Facebook, Myspace, and Twitter have experienced exponential growth in recent years. These OSNs offer attractive means of online social interactions and communications, but also raise privacy and security concerns. In this article we discuss the design issues for the security and privacy of OSNs. We find there are inherent design conflicts between these and the traditional design goals of OSNs such as usability and sociability. We present the unique security and privacy design challenges brought by the core functionalities of OSNs and highlight some opportunities of utilizing social network theory to mitigate these design conflicts.
In this paper, we study a network security configuration problem. More specifically, we consider distributed intrusion detection systems in a network subject to possible simultaneous attacks launched by a number of attackers. We formulate an N + M-person nonzero-sum stochastic game to capture the interactions among detection systems in the network as well as their interactions against exogenous intruders. We show the existence of stationary Nash equilibrium of the game and a value iteration method to attain an &#x2208;&#x2014;Nash equilibrium. Mimicking the concept of Shannon's capacity in information theory, we propose the notion of security capacity as the largest achievable payoff to an agent at an equilibrium to yield performance limits on the network security. Furthermore, we discuss a mathematical programming approach to characterize the equilibrium as well as the feasibility of a given security target.
Here's a sobering thought for all managers responsible for Web applications: Without proactive consideration for an application's security, attackers can bypass nearly all lower-layer security controls simply by using the application in a way its developers didn't envision. Learn how to address vulnerabilities proactively and early on to avoid the devastating consequences of a successful attack.
We analyzed data from the National Vulnerability Database (NVD). Designed and operated by the National Institute of Standards and Technology (NIST) with support from the Department of Homeland Security, the NVD provides fine-grained search capabilities of all publicly reported software vulnerabilities since 1997-a total of 41,810 vulnerabilities for more than 20,000 products. Frequently, a single vulnerability can affect a large number of products-for example, when the fault occurs in a library function.
The increase of electronic data processing and transmission via computer networks has laid more and more emphasis on the security of these networks. The people attempting to breach network security may belong to a wide range of categories, committing more or less serious offences. Conventional means do not offer adequate protection for internet transmitted threats, and there is no method or technology that can ensure by itself an improvement of this situation. By contrast, a multilayer, complete set of techniques might &#x2014; if not eliminate &#x2014; at least diminish the negative effect of these threats. Therefore, a new approach appears to be necessary to ensure the informatics security. By identification of the profile of the cybercriminal, the psychology of cyber criminology directs its attention towards the application of the physical, psychological, social and mental characteristics, as well as towards the evidence of the cybercrimes. The present paper presents a system used to identify the profile of the cybercriminal through the involvement of the feedforward neural networks using genetic algorithms. This system is based upon the environmental offences and variables that reflect the evidence of the cybercrime, as well as its circumstances, and its outcome is the description of the characteristics of the psychological and behavioural profile.
Network damage evaluation is a hot research topic in network security area. This paper reviews previous works in the area of network damage evaluation. These studies focus on building an index system and computing the damage by the entropy before and after attacks without taking the correlation of index into consideration, which could results in damage increased, and it cannot express the variable of attack damage. This paper measures the damage degree on network availability in the sudden traffic under attack, and then uses dynamic relevant coefficient adjustment method, which combine with real-time network entropy calculation method, to obtain a more accurate reflection during the dynamic process of attack damage. The simulation experimental results show that the method can reflect the dynamic process of network availability damage accurately and integrally.
With the growing number of botnet attacks, the botnet detection is becoming increasingly important for the network security. Towards the deficiencies of integrating and analyzing the heterogeneous multi-sensor information in existing botnet detection techniques, a novel information fusion model is proposed. The model is designed to carry out information integration of temporal and spatial dimensions based on the idea of information fusion. Furthermore, a representative case is provided in the paper to illustrate that the information fusion model can effectively fuse the complicated information from various sensors, thus to improve detection accuracy and enhance detection.
Modern security problems focus on sensibly allocating resources to decrease the magnitude of potential hazards, decrease the chances of adversary success given an attempt, or minimize loss following a successful attack. However, current risk assessment methodologies focus on manual risk analysis of network during design or through periodic reviews. Techniques for real-time risk assessment are scarce. In this paper, we propose a novel real-time risk assessment method using fuzzy logic and Petri Nets. The proposed method enables decision analysts to better understand the complete evaluation process of network security risk assessment, Furthermore, this approach can predict the potential network risk and provide credible confidence scores of risk assessment. The experimental results show that the proposed method is very useful in network security risk assessment.
This article proposes and evaluates the deployment of an automated virtual honeypot. Given that an unskilled attacker uses predefined exploit code, honeypots can be built to identify hackers and prevent them from attacking the production network. Building the virtual honeypot relies on mechanisms from multiple fields such as virtualization, scripting, penetration testing and system administration. The proposed solution also introduces automated scripts for virtual machine management, attack mitigation and recovery of compromised resources once an attack is detected. The complete solution will pose as an appealing multi-platform vulnerable network, presenting certain chosen weak points that once exploited trigger self recovery and denial of future attacks from the discovered source.
In the context of a growing concern for data security, network access control has become an important part of every network security system. The purpose of this paper is to provide a comparison between different EAP Methods that can be used with IEEE 802.1x, Port Based Network Access Control, standard as a means of protecting a computer network against unauthorized access. Three of the most common methods, EAP-MD5, EAP-TLS and PEAP, are compared with regard to time and packet performance both between each other and against a default situation, with no access control in place. The factors considered, authentication and reauthentication time, packet loss during reconnection and throughput are measured in a specially designed test environment using test equipment capable of accurate measurement of time in the range of milliseconds and constant high rate traffic generation.
This paper introduces a light-weighted extension of anonymous communications in IPv6 network. This method integrates the IP generation and updating with onion routing based mechanisms. It first uses DHCPv6 to get the &#x201C;seed IP address&#x201D; and generates changeable addresses in communications, and then imports this mechanism into the traditional onion routing based anonymous communication systems such as WindTalker. Experiment and simulation prove that this method can enhance the overall anonymity of the host. Users with this method can configure the simple mechanism easily and need not modify much network design of IPv6.
Nowadays Botnets have been identified as one of the most serious threats to network security, especially the peer-to-peer (P2P)based Botnets. Security experts destroy the botnets by target-attacking the weaknesses of high degree nodes or high betweens edges of the botnets. To make this work, the security experts need to know the features of the topology of botnets. But, topics such as how botnets' topology is formed and what the features of botnets' topology are are rarely studied up to now. Inspired by the method of complex network, we propose the growing model of botnets to try to resolve these questions. As far as we know, the growing model of Botnets has not been studied. Here, we focuse on the network metrics of Botnets, present a growing model of Botnets and discusse the performance of Botnets when their topology construction parameters change.
With the fast development of computer network, the problem of network security becomes more and more serious. The single firewall can not solve the problem entirely and an intrusion detection system is helpful. This paper presents the design and development of a distributed intrusion detection system to protect the network security. Experiments shows that it works well.
This paper proposes a design and realization of a network security system based on unidirectional network data control technique and configurable Rijndael AES algorithm. The unidirectional control technique does not process the data downloaded from the server to the client side but checks the data that is uploaded to the server side according to certain security rules, which promises the client side can receive complete and real-time data flow from the server side and prevents key information in private network from being disclosed. Moreover, using the improved AES data encryption standard, messages within the private network are encrypted, which promises the information could be transmitted in security even it is eavesdropped.
As most assessment indices are subjective and uncertainly in wireless network security evaluation, this paper proposes a model that BP neural network is used in wireless network security assessment system. After building a neural network model, we could obtain the values of each evaluation index and the whole risk situation of the wireless network by inputting impact factors into neural network. At the same time, we also could know the training error of neural network under the help of MATLAB toolbox.
This paper analyses and compares the existing network security situation evaluation methods, defines various network security evaluation concepts, and proposes a method for network security situation evaluation using virtual Honeynet. With the relationships among Honeynet active, host active of computer networks and IP active when network intrusion occurs, a binary linear regression prediction model is proposed. In the end, a prototype system is designed for data collection and regression fitting, and the validity of the regression prediction model is proved.
The security evaluation for an information network system is an important management tool to insure its normal operation. We must realize the comprehensive network security risks and take effective security measures. A network evaluation model and the corresponding fuzzy algorithm are presented and adapt the hierarchical method to characterize the security risk situation. The model combined with the importance of the security measure, environment and the key nodes. The evaluation method based on RST is used to evaluate the key nodes and the fuzzy mathematics is used to analyze the whole network security situation. Compared with others, the method can automatically create a rule-based security evaluation model to evaluate the security threat from the individual security elements and the combination of security elements, and then evaluation the network situation. It is shown by experimental results that this system provides a valuable model and algorithms to help to find the security rules, adjust the security measure, improve the security performance and design the appropriate security risk evaluation and management tools.
A concerted fight against botnets is needed in order to avoid them from becoming a serious threat to global security in the forthcoming years. Zombie detection is currently performed at the host and/or network levels, but these options have important drawbacks: antivirus, firewalls and anti-spyware are not effective against this threat because they are not able to detect hosts that are compromised via new or target specific malicious software and were not designed to protect the network from external attacks or vulnerabilities that are already present inside the local area network. To overcome these limitations, we propose a new botnet detection approach based on the identification of traffic patterns: since each network application, whether it is licit or illicit, has a characteristic traffic pattern that can uniquely identify it, the detection framework will rely on an Artificial Neural Network to identify the licit and illicit patterns. After the identification phase, the system will generate alarms to the system administrator, that can trigger the most appropriate security actions, like blocking the corresponding IP addresses, putting them under a deeper surveillance or acting over some suspicious network segment. A general detection framework was developed in order to incorporate the detection methodology itself, as well as the data collection and storage modules and all the necessary management functions. Some performance tests were already carried out on the proposed system and the results obtained show that the system is stable and fast and the detection approach is efficient, since it provides high detection rates with low computational overhead.
Capturing the uncertain aspects in cyber security is important for security analysis in enterprise networks. However, there has been insufficient effort in studying what modeling approaches correctly capture such uncertainty, and how to construct the models to make them useful in practice. In this paper, we present our work on justifying uncertainty modeling for cyber security, and initial evidence indicating that it is a useful approach. Our work is centered around near real-time security analysis such as intrusion response. We need to know what is really happening, the scope and severity level, possible consequences, and potential countermeasures. We report our current efforts on identifying the important types of uncertainty and on using Bayesian networks to capture them for enhanced security analysis. We build an example Bayesian network based on a current security graph model, justify our modeling approach through attack semantics and experimental study, and show that the resulting Bayesian network is not sensitive to parameter perturbation.
Broad network bandwidth and deep inspection impose great challenge for the capability of 10Gpbs network security monitoring. Proper scheduling policies can improve system capability without requiring additional resources. LAS, a size-based scheduling policy which can achieve optimal mean response time by giving preferential analysis to short flows, is widely used in various aspects of network field. Due to the high variability property of Internet traffic, LAS favors short flows without penalizing large flows very much. Unfortunately, the inspection of large flows can not be guaranteed in those network intrusion detection systems on 10Gbps links, which are usually heavily loaded, or even overloaded. Although tiny in percentage, large flows comprise more than 50%of the total load, and therefore can not be ignored, especially when specified by users as critical. How to avoid starving large flows while still giving higher priority to short flows is a dilemma we have to face in practice. In this paper, we propose a QoS-supported three-level scheduling policy (QTL), which can remedy LAS' defect. The experimental results show that our QTL scheduling policy has approximately the same performance as LAS for short flows, and meanwhile exhibits greatly enhanced processing capability for large flows.
Vulnerabilities in web applications expose computer networks to security threats. In fact, a large number of websites are used by attackers as hopping sites for attacking other websites and user terminals. These incidents prevent service providers from constructing secure networking environments. To protect websites from attacks based on vulnerabilities of web applications, security vendors and service providers collect attack information using web honeypots, which masquerade as vulnerable systems. To gain full access and to launch further network attacks by executing malware, such as a downloader, vendors and providers use high-interaction web honeypots, which are composed of real vulnerable systems and surveillance functions. However, conventional high-interactive web honeypots can collect only limited information and malware from attacks, whose path to the destination URL does not match the path structure of the web honeypot, due to the fact that these attacks are failures. To solve this problem, we propose scheme in which the destination URLs of these attacks are corrected by determining the correct path from the path structure of the web honeypot. Our Internet investigation reveals that 97 percent of attacks are failures. However, we confirmed that about 50 percent of these attacks will succeed with our proposed scheme. With our proposed scheme, we can use much more information to protect websites than with conventional high-interaction web honeypots because we can collect complete information and malware from these attacks.
The concept of response factor and its significances are briefly introduced in this paper. The existing response decision-making models and their related response factors are presented. According to the practical meaning of these response factors, their names are unified for the convenience of discussion. The statistics of response factors in typical response decision-making models are made, meanwhile these response factors are classified according to the proposed standards including related feature, subjective and objective feature, and original feature. In order to choose proper factors in response time decision-making and response measure decision-making processes respectively, a taxonomy of response factors is given. In addition, the problems of the improper response factor used in existing response decision-making models are indicated in the paper. The architecture, response decision-making process and experiments of the intrusion detection alert management &amp; intrusion response system (IDAM&amp;IRS) are shown. Especially, response factors used in IDAM&amp;IRS are discussed in detail. The role and function of response factors are summarized at last.
Accurate traffic classification is critical in network security monitoring and traffic engineering. To overcome the deficiencies of traditional traffic classification methods with port mapping and signature matching, several machine learning techniques were proposed. However, there are two main challenges for classifying network traffic using machine learning method. Firstly, labeled samples are scarce and difficult to obtain. Secondly, not all types of applications are known a priori, and new ones may appear over time. To address the above-mentioned problems, This paper proposed a semi-supervised classification method that allows classifier to be designed from training dataset consisting of a few labeled and many unlabeled samples. This method consist two steps: Particle Swarm Optimization (PSO) clustering algorithm was employed to partition a training dataset that mixed few labeled samples with abundant unlabeled samples. Then, available labeled samples were used to map the clusters to the application classes. Two host features: IP Address Discreteness and Success Rate of Connections had been proposed and used in this paper. Experimental results using traffic from campus backbone show that high classification accuracy can be achieved with a few labeled samples.
Nowadays, internet-based applications are widely deployed by enterprises; network security problems affect enterprises greatly. The high cost of computing and network resources brought by existing mainstream vulnerability assessment tools affects the running of application systems seriously. To solve problems mentioned above, the algorithm, framework and prototype system of evolution-based vulnerability assessment is proposed here. Components of the system are organized to comply with the subscriber/publisher architecture; the subscriber is responsible for system characteristics collecting while the publisher is responsible for vulnerability assessment. Experimental results demonstrate that compared with other vulnerability assessment tools, the system is more efficiency, lower computing and network resources required, less affection on applications and applicable to large-scale computing network.
Today Internet is very prosperous. Network firewalls have become basic network security equipments. Not only enterprises will set up firewall system, most of SOHOs and individual users also set up a firewall as a standard device. Because public IP is not sufficient in the IPv4-based environment, so most of networks are using NAT (Network Address Translation) mechanism to translation their private IP. Thus NAT also increases the complexity of Video Conference (Voice over IP) through a firewall environment.[1] In order to allow Video Conference network environment in normal operation in a firewall environment, IETF and the Video Conference industry propose a lot of solution, This article is going to present some common frameworks for analysis Video Conference operation and looking forward to provide Video Conference normally working in a firewall network environment.
With the risk factor of network security continuously improving, firewalls, which once as a means of the most important safety precautions, can no longer satisfy people's demand for network security. As a complement of the firewall, Distributed Intrusion Detection System can effectively improve the security. This article describes the framework, the structure, the Working principle, the functional structure and the development status quo, the development trend and the algorithm design of Distributed Intrusion Detection System, analyzes the data collection module operation of Distributed Intrusion Detection System. As Distributed Intrusion Detection System can prevent internal attacks, external attacks and disoperation, it plays a crucial role in the network security protection.
Intrusion detection is an important area of research. Traditionally, the approach taken to find attacks is to inspect the contents of every packet. However, packet inspection cannot easily be performed at high-speeds. Therefore, researchers and operators started investigating alternative approaches, such as flow-based intrusion detection. In that approach the flow of data through the network is analyzed, instead of the contents of each individual packet. The goal of this paper is to provide a survey of current research in the area of flow-based intrusion detection. The survey starts with a motivation why flow-based intrusion detection is needed. The concept of flows is explained, and relevant standards are identified. The paper provides a classification of attacks and defense techniques and shows how flow-based techniques can be used to detect scans, worms, Botnets and (DoS) attacks.
Data security is an essential requirement, especially when sending information over a network. Network security has three goals called confidentiality, integrity and availability (or Access). Encryption is the most common technique used to achieve this goal. However, the computer society has not yet agreed on a standard method to measure data security. The ultimate goal of this study is to define security metrics based on different aspects of network security, and then demonstrate how these metrics could be used in Quality of Service (QoS) routing to find the most secure path connecting two distant nodes (source and destination) across an internetwork. Three security metrics are proposed in this document, these metrics have been derived from three important issues of network security, namely: authentication, encryption and traffic filtration techniques (firewalls and intrusion detection systems). The metrics follow different composition rules in that the first is binary, the second is either concave or additive and the last is multiplicative. Routing algorithms that make use of such metrics have been implemented in the C# programming language to test the viability of the proposed solution. Computational effort and blocking probability are the most commonly used performance measures were used to assess the behavior and the performance of these routing algorithms. Results obtained show that the algorithms were able to find feasible paths between communicating parties and helped in making reasonable savings in the computational effort needed to find an acceptable path. Consequently, higher blocking probabilities were encountered, which is thus the price to be paid for the savings.
Control Protocols based on IP have attracted attention in recent years. While many protocols apply IP to Information networks and apply independent protocols to the field networks, there are protocols applying IP to field networks like FF-HSE. Such protocols enable seamless end-to-end communication between devices. Meanwhile, it becomes necessary to introduce network security architecture. It has been proposed security architecture which is suitable for field network. However, it hasn't been considered to apply it to the actual protocol. So if it would be applied to the actual protocol directly, it might cause function duplication. This paper proposes the optimal design of the security architecture when it would be applied it to FF-HSE.
through the research and analysis of intrusion detection systems and distributed network intrusion detection systems, this paper designs a campus network security system based on distributed network intrusion detection technology, and then implements it. This system can not only lift the user's worries, but also provide a larger extent of security protection for the computer network system, to achieve good security effect.
This paper introduces virtual private network technology from network security. It selects the implementation technique with strong safety through analysis and comparison to commence the exploration and study on the support for specific implementation of virtual private network technology in the new generation of IP protocol as well as some prospects in the relative research results.
The typical security solution can only ensure the security of the network boundary, but not involve the internal security. According to different types of applications and secrets that it provides, the network can be divided into a number of logical security domains. Furthermore, the access control of the network could be realized by applying dynamical VLAN technology, and the filtration and audit of the information exchange between security domains is realized by mandatory access control policies, and the unified identity authentication and access control is realized by applying SSL VPN technology. The practical application shows that the solution can not only enforce the access control and secure audit, but also reduce the risk of revealing the secret information.
There are many problems in traditional network security and PC security protection systems, such as insecurity, inflexibility and so on, whether they are implemented by software or hardware. To solve the problems, this paper presents a reconfigurable security protection system on the basis of NetFPGA platform and embedded soft-core technology. This system consists of two subsystems, one is implemented on the NetFPGA for the subnet protection and the other is implemented on DE2 board to protect the terminal, in which NetFPGA is used to achieve packet filtering in hardware, immunity from ARP attacks in hardware, flow monitoring and transmitting with hardware acceleration, and DE2 board is used to realize AES/DES encryption modules in hardware. Moreover, the USB virus can be isolated effectively by the subsystem designed on DE2. To further enhance performance, security and flexibility of our system, including both the subnet protection subsystem and the terminal protection subsystem, we introduce two types of remote reconfigurable design method, by which administrator is able to reconfigure the two subsystems for both the software and the hardware logical circuits via any authorized devices. Extensive experiments show that all the functions of the blocks of the designed system are valid and the designed system is feasible.
Aiming at the security problems of network management,the paper describes the basic principle of PPPOE agreement based on the RouterOS. The paper shows that PPPOE services are realized by using RouterOS. At last combined with the application of the campus network of Huangshi Institute of Technology, gives out a PPPOE access program based on campus network. The conclusion demonstrate: the PPPOE using RouterOS could significantly improve the security of network well.
Today VoIP (Voice over IP) and video conference are very common in internet. But the other side, network security is more complicated than before, our network infrastructure set firewalls become a basic network security equipments. Not only enterprises will set up firewall system, most of SOHOs and individual users also set at least one firewall in their network. In order to allow video conference environment normally operates in a firewall environment, IETF and the video conference industry have already propose a lot of solution.[1] In this article, we use Skype video video call to measure and compare the image transmission in two difference firewall environment. We will use some common frameworks to measure and analysis video conference normally working under the DDoS attack.
We propose a formal model of web security based on an abstraction of the web platform and use this model to analyze the security of several sample web mechanisms and applications. We identify three distinct threat models that can be used to analyze web applications, ranging from a web attacker who controls malicious web sites and clients, to stronger attackers who can control the network and/or leverage sites designed to display user-supplied content. We propose two broadly applicable security goals and study five security mechanisms. In our case studies, which include HTML5 forms, Referer validation, and a single sign-on solution, we use a SAT-based model-checking tool to find two previously known vulnerabilities and three new vulnerabilities. Our case study of a Kerberos-based single sign-on system illustrates the differences between a secure network protocol using custom client software and a similar but vulnerable web protocol that uses cookies, redirects, and embedded links instead.
It is obvious that satellite networks are of usefulness in the area of information technology, and can be utilized to the extent of extending the application of information technologies further. Because of the high variability and complexity of satellite network, the problems of network security in satellite network must be solved. The unique attributes of satellite network that is different from those of the ground network is introduced first, then the authentication and access control models selected based on the attributes of satellite network are discussed. The analysis of satellite network security is performed by means of simulation, and it comes to the conclusion based on the simulation results.
Trust model is the basis of the entire network security system. It determines the form of trust used in the network and the risk caused by the form. Furthermore, it provides the framework for establishing and managing the trust relations. It can clearly be seen that the study of trust model in the network security is considerably significant. This paper mainly analyses the trust mechanism, the certification process and their advantages and disadvantages about the various types of trust models based on multi-CA. It also points out the main reasons why trust models influence the network information security. By summing up and comparison, this paper points out the future research strategies and development trend of the trust models based on multi-CA.
The impact of WPA2 security-bandwidth trade-off for IPv4 and IPv6 on wireless 802.11n network implementing Windows 7-Windows Server 2008 is investigated. The highest point of difference between open system and WPA2 for UDP was noticed at packet size 1408 bytes where IPv4 provided 31.48 Mbps and IPv6 provided 24.33 Mbps higher throughput in the open environment. The performance of IPv4 and IPv6 are also compared.
The present work is dedicated to study attacks and contremusure in MANET. After a short introduction to what MANETs are and network security we present a survey of various attacks in MANETs pertaining to fail routing protocols. We also present the different tools used by these attacks and the mechanisms used by the secured routing protocols to counter them. Our work ends with a proposal analytical modeling to model some of these attacks like cooperative Blackhole, Blackmail, Overflow, Selfish and a simulation of these attacks by using a simulator network named ns2.
The current paper presents a symmetric key based network security system where Cellular Automata domain functions have been used for key management purpose. A special class of Cellular Automata referred to as Max Length Cellular Automata has been reported as a potential candidate for such key management. A CA codebook based key generation, encryption and decryption algorithms are presented. The complexity analysis of the session specific security system has been discussed in support of the fact that simple, modular hardware oriented design of CA based security is ideally suited for heavy traffic closed network.
Spread spectrum signaling technique is used for security purpose in wireless network because it becomes difficult to access the wireless network by unwanted users. We are developing the algorithm to retrieve the PN sequence &amp; pattern of spectrum in spread spectrum system. This algorithm will increase the wireless network security &amp; solve the problem of accessing the wireless network in secured way. Because, when only sender end knows the PN sequence &amp; pattern of spread spectrum communication system the network security will be enhanced. Spread spectrum is a means of transmission in which data sequence occupies a much more bandwidth than minimum required bandwidth necessary to send it. The spectrum spreading at transmitter &amp; de-spreading at receiver is obtained by PN-sequence which is independent of data sequence. There are two types of the spread spectrum techniques direct sequence spread spectrum &amp; frequency hopped spread spectrum. Direct sequence spread spectrum is technique in which data sequence directly modulates the pseudo noise sequence known to only transmitter &amp; receiver. Frequency hopping means to transmit data in different frequency slots.
Firewalls are no longer just perimeter devices for the data center, but should be weaved into the fabric of the network from edge to edge such as to offer security layered in-depth and ubiquitous. The next evolution of the firewall has to combine dynamic policy-based security with performance, rapid scaling, high availability and application intelligence. Today, increasing attention is paid to network firewall design quality due to regulations such as the Sarbanes-Oxley act, CobiT framework, the Payment-Card Industry Data Security Standard (PCI DSS) and the NIST standard. All these regulations include specific sections dealing with firewall configuration, management and audit. This paper is a humble attempt to examine various types of firewalls operational as on today and cross reference each firewall operation with causes and effects of weaknesses in their operation. In addition, we analyze reported problems with existing firewalls. Detailed analysis and comparison is done in terms of cost, security, operational ease and implementation of Open source packet filter (PF) firewall, Checkpoint SPLAT and Cisco ASA in a testing environment with laboratory generated traffic. Various throughputs and connections statistics were used as benchmark for performance comparison. The results indicated that Cisco ASA outperforms its peers in most performance criterions. Checkpoint SPLAT and OpenBSD PF also provides reasonably good and competitive performance. The results reported in this paper will also be useful in comparing vendors to procure firewall based on one's own organizational business requirements.
Mobile networks are driven by the need to provide more advanced services to mobile or nomadic computing devices, such as security services requiring remote client authentication. In such services, the user's location might be used as authentication factor, in addition to the typical authentication factors, like passwords, or one time tokens combined with the use of a physical device that a person owns, such as a card or a phone. Since the location information itself is subject to forging attacks, additional mechanisms must be used to certify its integrity. We propose LRAP, a novel protocol combining several authentication factors to securely authenticate a mobile user. In LRAP, the user's location can be determined and its correctness is certified by a third trusted party, called Local Element. As use case, we considered the payment service at the self-service gas stations, a widely available service vulnerable to several types of security attacks, and we proposed an LRAP-based service exploiting one time codes and certified position for secure payment operations.
The paper suggests an attack trees based approach to security analysis of information systems. The approach considers both software-technical and social engineering attacks. It extends the approach to network security analysis based on software-technical attacks which was suggested earlier by the authors of this paper. The main difference is in generalizing the suggested approach for information systems and in use of different conceptions, models and frameworks related to social-engineering attacks. In particular, we define conceptions of legitimate users and control areas. Besides, social-engineering attacks and attacks that require physical access to control areas are included to the attack trees used for security analysis. The paper also describes a security analysis toolkit based on the approach suggested and experiments with it to define the security level of information system.
In this paper, we propose a secure routing algorithm based on three simple levels and show its applicability to heterogeneous unstructured overlays. Due to the lack of secure routing protocols for unstructured overlays we have started to investigate a possible solution for these systems. We propose a scalable secure routing protocol for heterogenous environments. Through analytical study and experimental results we demonstrate the secure routing protocols potential in providing a secure topology management service for a broad range of node classes and communication capabilities. We show that it is possible to combine security, scalability and adaptability in one mechanism allowing each node to achieve an adequate level of security to meet the overall system requirements.
In this paper, a novel approach to prevent accidental or deliberate data breaches is presented. The proposed approach provides platform, network and offline security. Data is categorized as sensitive or insensitive, and the corresponding applications are isolated by using virtualization technology. Data theft or accidental loss is prevented by encrypting virtual hard disks and by introducing a multi-lane network architecture. If no connection to a corporate network is available, an offline mode handles data transfer and encryption. Authentication is managed by applying a biometric feature vector in association with a smart card setup. The approach increases security without disrupting the everyday work routines of users. An implementation based on Virtual Box and Java Card is presented. A performance evaluation of the critical components is provided.
Nowadays, it is already a banality that people run their applications in a complex open computing environment including all sorts of interconnected devices. In order to meet the network security challenge in nonprofessional human environments, Intrusion Detection Systems (IDS) have to be designed. Intrusion detection techniques are categorized into anomaly and misuse detection. To describe the outlined problem, we focus solely on externally observable executions generated by the observed system. Thus, we need some sort of tool being able to discover acceptable and unacceptable patterns in execution traces. Such a tool may be the rough set theory. According to the rough set theory, the vagueness of a subset of a finite universe U is defined by the difference of its upper and lower approximations with respect to a partition of U. In this paper, our starting point will be an arbitrary family of subsets of an arbitrary U, neither that it covers U nor that U is finite will be assumed. This new approach is called the partial approximative set theory. We will apply this theory to build an IDS which is simultaneously able to detect anomaly and misuse intrusions.
Security is a critical issue in wireless local area networks (WLANs) for many individuals and organizations worldwide, and is one of the main barriers to its adoption in organizations. This paper reports on the current status of WLAN security practices in Auckland's central business district (CBD) through war-walking field trials. It provides an in-depth analysis of field trial data collected since 2004. Our findings show that businesses are adopting more WLANs in recent years than a few years back and the overall growth of WLAN deployment in Auckland CBD is 406%since 2004. We also observed that about 88%of all WLANs detected in August 2010 utilized encryption technology. This shows that an overall 48%increase in the use of encryption protocols since 2004. Finally, we provide guidelines to help businesses in improving their wireless security practices in Auckland City.
In this paper, we proposed a framework of defense system by applying attack tree and misuse monitor for prevention of insider's malicious behaviors. Recently, a major interest of network security is the threat from insiders who execute their authorization legitimately to leak information on network system. If insider threats his/her system, he/she has caused a severe damage and loss to compromise information assets. Our proposed framework is consisted of 3 prevention modules. It prevents abnormal behaviors by monitoring all activities according to each prevention techniques. The main keys to prevention are attack tree and misuse monitor. An attack tree is conceptual diagrams of insider threats on systems and possible attacks to reach those goals. And a misuse monitor can prevent the misuse of resources by matching the actual running process pattern to the expected processing pattern in pre-defined current insider executed process profile.
In traditional Open Systems Interconnection (OSI) layered model, many security protocols in layers are proposed to provide network security. Because security protocols among layers are lack of cooperation, system performance degrades due to security redundancy and furthermore causes system overloading. Therefore, the paper proposes a cross-layer design network security management (CLDNSM) to protect system security while improve system performance, such as CPU utilization. First, the multiple security-dimension quantification (MSDQ) metric is proposed to evaluate holistic system security. Then, the proposed CLDNSM aggregates system information from layers and uses it to obtain the optimal security settings of layers according to the MSDQ metric. The simulation results show that system performance will be improved without sacrificing security protect compared to OSI layered model by using CLDNSM. Finally, to adapt to dynamic environments, security constraints will be modified automatically in a limited range to avoid system overloads, the simulation results show that the system overloads are under control
The last decade has witnessed the emergence of a plethora of approaches for securing financial transactions over the Internet. During the same period, attacks have matured from isolated exploits to an organized e-criminal industry. In the midst of this evolution stood the End User, whose instances have often been neglected under the assumption that refunding financial losses is all that mattered. This paper analyzes the existing deployments of Internet banking services from the perspective of the End User, whose main goal is completing the online transaction. The sole use on the client side of so-called &#x201C;trusted&#x201D; hardware devices will be discussed and shown to fall short of the requirements for truly secure Internet banking. Evidence will be provided in support of the need to protect the client components using connected devices and applying software hardening techniques to lower the hacking ROI and help rebalance forces in the fight against cyber criminals. A new metric for gauging the effectiveness of security software will be described and applied to measure the practical security of existing Internet banking systems. Finally, a number of guidelines will be provided for assuring that reasonable care is exercised in the design and deployment of Internet banking systems.
There are hundreds of email summarization tools nowadays. One of the challenging issues of email summarization is to determine how to secure email summaries from spoofing and bombing and to provide preventive measures. Email is one of the most ubiquitous applications used on a daily basis by millions of people world-wide, traditionally accessed over a fixed terminal or laptop computer. In the past years, there has been an increasing demand for email access over mobile phones too. Our work focused on providing security review measures and preventing approaches that provide quality email summaries with secure transmissions over the network.
With the growing diversity of malware, researchers must be able to quickly collect many representative samples for study. This can be done, e.g., by using honeypots. As an alternative to software-based honeypots, we propose a singlechip honeypot appliance that is entirely hardware-based and thus significantly more resilient against compromising attacks. Additionally, it can easily keep up with network speeds of 10+ Gb/s and emulate thousands of vulnerable hosts. As base technology, we employ reconfigurable hardware devices whose functionality is not fixed by the manufacturing process. We present improvements to the platform, aiming to simplify management and updates. To this end, we introduce the domain-specific language VEDL, which can be used to describe the honeypot behavior in a highlevel manner by security experts not proficient in hardware design.
We present a simple new technique to secure quantum key distribution relay networks using secret sharing. Previous techniques have relied on creating distinct physical paths in order to create the shares. We show, however, how this can be achieved on a single physical path by creating distinct logical channels. The technique utilizes a random &#x2018;drop-out&#x2019; scheme to ensure that an attacker must compromise all of the relays on the channel in order to access the key.
One of the obstacles to deployment of QKD solutions has been the distance limitation. Solutions using relays have been proposed but these rely on link-by-link key establishment. We present a new technique to extend the distance of a quantum key distribution channel using an active relay. Each relay acts as an intercept/resend device and allows the establishment of an end-to-end key. It has been argued that such relays cannot be used to extend the distance, but we show that with a suitable adaptation of the protocol the effective key distribution distance can be increased.
This paper focuses on an efficient technology for implementing a Mobile Agent System (MAS). The mobile agent is able to hold consumers' requests, migrates between platforms and executes its code autonomously. The agent is required to return results to its owner, so he or she can make the right decisions. Kerberos protocol is one of the best known authentication protocols based on symmetric key. Kerberos is a trusted third-party authentication protocol designed to establish network security. In this paper, we propose a new protocol that is based on enhancements modification of Kerberos and is suited to provide confidentiality, integrity, authentication and authorization. A security analysis of the new protocol is also provided.
The purpose of having a honeypot, such as Nepenthes, that collects malicious software (malware), is to build the capability of capturing malware propagating in a certain infrastructure, or intentionally targeting that infrastructure. When multiple honeypots of this type are deployed, they require a mechanism in which the malware and other related intelligence are reported to a centralized repository to analyze collected malware and study both overall and infrastructure-specific trends. Such a setup also caters for identifying new malware, i.e., malware that are not known by any antivirus provider. This provides a mechanism of malware detection and analysis at the early stages, which allows it to be dealt with before it spreads massively and causes severe damage.
Aimed at the problem of Byzantine Attack in secure network communication, the network security assumptions which may contain Byzantine attacks are analyzed and described firstly. Then a secure random network coding model is proposed for resisting the Byzantine attacks where the CBC (Cipher Block Chaining) technology is combined with random network coding, we prove the correctness and security also, finally realize safety codes.
Due to limitations of power, computation capability and storage resources' wireless sensor networks are vulnerable to many attacks. The paper proposed a novel routing protocol algorithm for Wireless sensor network. The proposed routing protocol algorithm can adopt suitable routing technology for the nodes according to distance of nodes to the base station, density of nodes distribution and residual energy of nodes. Comparing the proposed routing protocol algorithm with other routing protocol algorithm through comprehensive analysis, the results show that the proposed routing protocol algorithm is secure and efficient for wireless sensor networks.
JXTA is a set of open protocols that enable the creation and deployment of peer-to-peer (P2P) networks, allowing the execution of services in a distributed manner. Being a generic P2P middleware, it has slowly evolved in order to appeal a broad set of different applications. Part of this evolution includes providing basic security capabilities in its protocols in order to achieve some degree of message privacy and authentication. However, under some contexts, more advanced security requirements should be met, such as anonymity. In this work, we propose how to adapt JXTA messaging so that services may be anonymously accessed, by taking advantage of JXTA's idiosyncracies and capabilities, in a manner that is completely invisible to the existing protocols.
The construction of a complex secure system composed from individual secure components presents a variety of challenges to the designer. The authors leverage experiences from 50 years in the security R&D community and from the first-hand experience of building several high-assurance (EAL7) systems to shed light on various high-trust security engineering challenges, including those related to secure architecture, secure implementation, and trustworthy development. The authors use an example system lessons learned. The Encryption-box Security System is a trusted hardware foundation that includes hosts, called arbitrary application processors, a trusted network security controller that defines a security policy over network communications, and a trusted encryption gateway. This system of distributed components results in a comprehensive network security architecture. The authors also describe key concepts for security analysis in complex distributed systems, including the security perimeter, the allocation of policies to specific components, and the security policy domain.
This paper describes a competition-style of exercise to teach system and network security and to reinforce themes taught in class. The exercise, called NetSecLab, is conducted on a closed network with student-formed teams, each with their own Linux system to defend and from which to launch attacks. Students are expected to learn how to: 1) install the specified Linux distribution; 2) set up the required services; 3) find ways to harden the box; 4) explore attack methods; and 5) compete. The informal write-up at the end of the lab focuses on their research into defense and attack methods, which contributes to their grade, while their competition score is dependent on their abilities to attack during the competition. Surveys were performed to evaluate the efficacy of the exercise in teaching system security.
Spyware is a serious threat, posing severe privacy and security issues. The best way for users to reduce the threat from spyware is to adopt anti-spyware programs. While previous studies have identified various determinants of anti-spyware adoption, some factors have not yet been examined. Based on the theory of reasoned action, this study presents a parsimonious model for users' intention to adopt anti-spyware programs. Structural equation modeling is used to empirically test a model. Every hypothesis is supported except the relationship between Internet familiarity and knowledge of spyware. This study concludes with discussions and implications for research and practice.
According to the network security problem of the wireless sensor networks, there are several severe challenges, such as limited processing power, storage, bandwidth, and energy. The stream cipher to be suitable for wireless sensor networks is proposed. And it composed of a 128-length linear feedback shift register (LFSR) and an 11-variable boolean function. The stream cipher algorithm is written with 8051 assembly code, and compared with the RC5 and A5. Through the comparison, the efficiency is satisfied. The code size of the stream cipher used in the wireless sensor networks is 578 bytes and the execution time is 0.024642 second and it faster than RC5 and A5 and uses less code size. Then the statistic testing and the known attack methods are used to test the security of the designed stream cipher. And the results can prove that its security is very high and suitable for wireless sensor networks.
Aimed at the problem of Byzantine Attack in secure network communication, the network security assumptions which may contain Byzantine attacks are analyzed and described firstly. Then a secure random network coding model is proposed for resisting the Byzantine attacks where the CBC (Cipher Block Chaining) technology is combined with random network coding, we prove the correctness and security also, finally realize safety codes.
We study a network security game where strategic players choose their investments in security. Since a player's investment can reduce the propagation of computer viruses, a key feature of the game is the positive externality exerted by the investment. With selfish players, unfortunately, the overall network security can be far from optimum. The contributions of this paper are as follows. 1) We first characterize the price of anarchy (POA) in the strategic-form game under an ffective-investment?model and a ad-traffic?model, and give insight on how the POA depends on individual players' cost functions and their mutual influence. We also introduce the concept of eighted POA?to bound the region of payoff vectors. 2) In a repeated game, players have more incentive to cooperate for their long term interests. We consider the socially best outcome that can be supported by the repeated game, as compared to the social optimum. 3) Next, we compare the benefits of improving security technology and improving incentives, and show that improving technology alone may not offset the price of anarchy. 4) Finally, we characterize the performance of correlated equilibrium (CE). Although the paper focuses on network security, many results are generally applicable to games with positive externalities .
The intrusion response component of an overall intrusion detection system is responsible for issuing a suitable response to an anomalous request. We propose the notion of database response policies to support our intrusion response system tailored for a DBMS. Our interactive response policy language makes it very easy for the database administrators to specify appropriate response actions for different circumstances depending upon the nature of the anomalous request. The two main issues that we address in context of such response policies are that of policy matching, and policy administration. For the policy matching problem, we propose two algorithms that efficiently search the policy database for policies that match an anomalous request. We also extend the PostgreSQL DBMS with our policy matching mechanism, and report experimental results. The experimental evaluation shows that our techniques are very efficient. The other issue that we address is that of administration of response policies to prevent malicious modifications to policy objects from legitimate users. We propose a novel Joint Threshold Administration Model (JTAM) that is based on the principle of separation of duty. The key idea in JTAM is that a policy object is jointly administered by at least k database administrator (DBAs), that is, any modification made to a policy object will be invalid unless it has been authorized by at least k DBAs. We present design details of JTAM which is based on a cryptographic threshold signature scheme, and show how JTAM prevents malicious modifications to policy objects from authorized users. We also implement JTAM in the PostgreSQL DBMS, and report experimental results on the efficiency of our techniques.
This article proposes a robust mathematical method to strategically place trust nodes to compartmentalize a time-critical SCADA network. The trust nodes combine firewall and intrusion detection technology to provide communication network security for protection, control, and SCADA systems. The mathematical technique optimizes the placement of the trust nodes based on the timing requirements of existing systems and the number of trust nodes that are available in the system given constraints, which may arise due to budgetary limitations or the restrictions of existing utility hardware. The intent is to create a planning tool to allow utility system operators to determine the best locations to place trust nodes to increase system security given limited resources and/or hardware constraints. The operational requirements of the environment are translated into a mathematical model. Mixed integer linear programming is used to process this model in search of an optimal solution. Because the problem is provably NP-Hard, a heuristic is also given to quickly find good, but not optimal, solutions. Experiments show promise for the proposed techniques.
Summary form only given. Information Assurance (IA) grew out of the field of computer network security. However, IA is a much broader term than network security and relates to the managing of the risks associated with the use, processing, storage, and transmission of information. The most basic model of IA relies on three properties: confidentiality, integrity, and availability, thus it is often referred to as the CIA model. The wide consensus is that the commonly assumed attributes of sensor nodes, such as low energy, low computational power, unattended operation, and wireless connectivity are considered challenges in implementing IA in sensor networks. Similarly, the broad range of applications, which results in broad range of possibly heterogeneous information modalities, makes the problem even more complex. Though, the relatively constant membership of sensor networks is usually perceived as an advantage. Similarly, the typical massive deployment of nodes can be exploited in the design of IA protocols. In this talk, I will describe an extension of the CIA model and its application to sensor networks. In particular, I will discuss a number of approaches to support IA in sensor networks.
The paper discuss the Wireless LAN 802.11b network security. WLAN security mechanisms including the comparisons of SSIDs, MAC address filtering and the WEP key encryption. based on these authentication methods, etc. By analyzing the weaknesses of WEP and RC4, a simulative platform of software and hardware is designed to crack WEP key. Experiments show that WEP Key can be cracked including SSID enumeration, MAC address spoofing and WEP key cracking by FMS Attack. The paper propose some solutions for Wireless LAN 802.11b network security, for example, WEP key hashing, Dynamic WEP keys, Initialization Vector changes and Message Integrity Check.
It is vitally important for applications in detecting DoS attacks, traffic management, and network security to real-time automatically identify traffic patterns in backbone networks with high speed links carrying large numbers of flows. Our objective is to determine traffic patterns that use up a disproportionate fraction of network resources. This paper first analyzes the major time and space cost in computing high volume clusters under different hierarchical structures, and then proposes a variable hierarchical structure to identify net work traffic patterns in a top-down fashion. We evaluate our model using real trace files from the CERNET backbone link an d demonstrate the improved efficiency of our approach in comparison to previous work on clustering traffic patterns.
Due to limitations of power, computation capability and storage resources' wireless sensor networks are vulnerable to many attacks. The paper proposed a novel routing protocol algorithm for Wireless sensor network. The proposed routing protocol algorithm can adopt suitable routing technology for the nodes according to distance of nodes to the base station, density of nodes distribution and residual energy of nodes. Comparing the proposed routing protocol algorithm with other routing protocol algorithm through comprehensive analysis, the results show that the proposed routing protocol algorithm is secure and efficient for wireless sensor networks.
JXTA is a set of open protocols that enable the creation and deployment of peer-to-peer (P2P) networks, allowing the execution of services in a distributed manner. Being a generic P2P middleware, it has slowly evolved in order to appeal a broad set of different applications. Part of this evolution includes providing basic security capabilities in its protocols in order to achieve some degree of message privacy and authentication. However, under some contexts, more advanced security requirements should be met, such as anonymity. In this work, we propose how to adapt JXTA messaging so that services may be anonymously accessed, by taking advantage of JXTA's idiosyncracies and capabilities, in a manner that is completely invisible to the existing protocols.
Unified Modeling Language (UML), an industry de-facto standard, has been used to analyze dynamically partially reconfigurable systems (DPRS) that can reconfigure their hardware functionalities on-demand at runtime. To make model-driven architecture (MDA) more realistic and applicable to the DPRS design in an industrial setting, a model-based verification and estimation (MOVE) framework is proposed in this work. By taking advantage of the inherent features of DPRS and considering real-time system requirements, a semiautomatic model translator converts the UML models of DPRS into timed automata models with transition urgency semantics for model checking. Furthermore, a UML-based hardware/software co-design platform (UCoP) is proposed to support the direct interaction between the UML models and the real hardware architecture. The two-phase verification process, including exhaustive functional verification and physical-aware performance estimation, is completely model-based, thus reducing system verification efforts. We used a dynamically partially reconfigurable network security system (DPRNSS) as a case study. The related experiments have demonstrated that the model checker in MOVE can alleviate the impact of the state-space-explosion problem. Compared to the synthesis-based estimation method having inaccuracies ranging from $-$43.4%to 18.4%, UCoP can provide accurate and efficient platform-specific verification and estimation through actual time measurements.
Currently,using the network to spread of the virus has become one of the major threats on Internet. Any one of the computer connected to the Internet can be infected anytime. So, it has been a primary and urgent task to analyze the virus propagation properties of computer and put forward corresponding defense strategy in the network information safety field.
Packet classification is a widely used operation in network security devices. As network speeds are increasing, the demand for hardware acceleration of packet classification in FPGAs or ASICs is growing. Nowadays algorithms implemented in hardware can achieve multigigabit speeds, but suffer with great memory overhead. We propose a new algorithm and hardware architecture which reduces memory requirements of decomposition based methods for packet classification. The algorithm uses prefix coloring to reduce large amount of Cartesian product rules at the cost of an additional pipelined processing and a few bits added into results of the longest prefix match operation. The proposed hardware architecture is designed as a processing pipeline with the throughput of 266 million packets per second using commodity FPGA and one external memory. The greatest strength of the algorithm is the constant time complexity of the search operation, which makes the solution resistant to various classes of network security attacks.
A hybrid WDM and SAC/Optical CDMA System is proposed to enhance the data network security and to exploit the capacity of an optical system. Code pulses of SAC/Optical CDMA are overlaid onto a multichannel WDM system. Modified Quadratic Congruence (MQC) code is as the signature address code for SAC/Optical CDMA. In general, OCDMA improves service availability for data of a network. With the use of WDM channels in a hybrid scheme, an eavesdropper faces another challenge for intercepting and decoding the coded signal. The interference that is the main factor limits the performance of the proposed system. So that, the notch filters and APD photo detectors instead of PIN photo detectors are used in order to improve the SAC/OCDMA performance. A simulation is presented as a demonstration of the concept.
The following topics are dealt with: homeland defense; alternative energy sources; green building technologies; mobile communications; microwave technology; electromagnetic compatibility; mobile ad hoc networking; network security; sensor fusion; antenna systems and processing; radio locationing; radar systems and techniques; and medical electronics.
A lot of college wireless networks use software systems and web-based logins to authenticate users. In this paper we find that it is not hard to bypass such authentication. An attacker can use DHCP request to collect information about the users on the network. It makes the attacker much easier to gain unauthorized access to the network facilities. This can be done by putting the network card on monitor mode, and filter the network frames based on the collected MAC addresses. Once any client is disconnected from the network, the attacker can spoof the client's MAC address and connect to the network. The authentication system is going to accept the spoofed MAC address and let the attacker to connect to the network.
Network security situational awareness (NSSA) technology as a new research area in network security plays an important role in changing network security defense model from passive type to active. In order to enhance the capability of perceiving status the network stays in and emergency responding capability, it is especially important to design an index system for large scale NSSA and a situation assessment algorithm. This paper extends the existing hierarchical evaluation method and introduces a new network security assessment algorithm with a high adaptability based on other researchers' work.
IT security has evolved dramatically over the last few decades. Initially, it consisted of password management and basic network security controls. However, as enterprises have come to depend on IT for every aspect of daily operations, the need for security has transcended the datacenter and now is woven through all aspects of IT operations.
Intrusion prevention technologies and mechanisms have been developed to enhance the network security. Model-based approach is one of the most promising approaches for intrusion prevention and intrusion detection, since it can reveal the hidden characteristic of time series. Hidden Markov Model (HMM) is a main time series model. In the implement of the intrusion prevention mechanism, the combination of fast adaptive clustering algorithm and intrusion prevention algorithm is used to redetection, which can adaptively update model, and raise speed of detection. Experimental results with the KDD Cup99 data sets demonstrate that false positive rate of the detection algorithm is lower than conventional model-based detection algorithm, while the detection rate is still kept in a good state.
Lack of customer's trust is one of the important factors to impede China's e-commerce development. How to improve e-commerce trust is the common concern of academics, government and enterprises. The virtual community for customer's e-commerce enterprises is an important way to gain customers trust. Through technologies with high security, convenience and sociality, a virtual community can satisfy customer's demand of information safety, convenient operation needs and social needs, in order to win customers trust.
Great-perceived risk in online shopping has highlighted the importance of brand in virtual space. With the rapid growth of B2C businesses and fierce market competition, "web equity" is an important asset that B2C retailers can use to lower perceived risk and gain competitive advantages online. The main objective of this research is to explore drivers and dimensions of web equity of B2C retailers. Through an empirical study, we identified web awareness, online experience, and web trust as key dimensions of web equity, and web security, web design and order fulfillment are significant factors affecting web trust Additionally, web security, web design, interactivity, order fulfillment, and marketing communication are significant factors affecting online experience. Furthermore, web security, order fulfillment, and marketing communication are significant factors affecting web awareness. The paper also highlights the important role of web equity in mediating the relationship between web equity drivers and purchase intent on the web site. Managerial implications are discussed at the end of this paper.
Distributed denial of service (DDoS) attack is one of the major threats to the current Internet. It is challenging to detect DDoS attacks accurately and quickly. We propose a novel IP Flow Interaction Feature algorithm (FIF) based on multiple features of DDoS attack flows via IP addresses and ports. To increase the detection accuracy in various conditions, we describe the state characteristics of network flows using FIF time series, and a simple but efficient FIF-based DDoS attack detection model (FDAD) is proposed by associating with contextual information in observed FIF time series. Finally, we present a simple alarm evaluation mechanism based on the alarm frequency and time interval. Our analysis and experiment results demonstrate that FIF can well reflect the characteristics of DDoS attack flow and normal flow and can distinguish normal flow from attack flow effectively. FDAD can identify normal flow and abnormal flow with DDoS attack flow quickly, accurately, and reduce false alarm rate drastically.
With the quick development of computer network, the network scale in school campus has been keeping on expanding. Therefore, security problems of network become a focus of research, a well-operated network management becomes the key issue for a normal and effective school campus network This article mainly analyzes the problems and solutions of the campus E-government network security, studies the security issue and its root in school campus network and mainly probes into the key technique of the school campus network security management system, then we provides the security strategy of network According to the practical situation in our campus, we designs and realizes the school campus security management system.
The rapid development of Internet, e-commerce comes with the problem of how to solve security, has become an urgent goals. The aim of this paper is to prove computer network security and the business transactions security. A number of conventional technology solutions are on this basis, Solve the security of computer networks and transactions is the key to improving the security environment and e-commerce laws and regulations, and can truly make people have a nice habit. This would also allow the community of e-commerce activities to be a commendable method, to facilitate the conduct business online, efficient significance.
This article firstly analyzes the situation of network marketing in Domestic and foreign banks, and states the necessity of Chinese commercial banks developing network marketing from the advantages, internal and external pressures and threats, opportunities. Although, Chinese commercial banks face with some problems in the network marketing, such as, commercial credit, network security, laws and regulations, etc, this article seeks some strategies to promote their development of network marketing.
Streaming media protocol-based applications share communication resources with regular Network/Internet traffic. So, they are exposed to any existing security attacks. In the present paper, we measure this relationship between attacks and applications by introducing a probabilistic verification evaluation. Our approach is novel in that, it is based on the concurrency between models. In addition, the performance of these new models are qualitatively and quantitatively evaluated by using the PRISM model checker. The results are promising and characterized by a fairly short evaluation delay and automatic security measurement.
As the Department of Defense transitions to a ubiquitous computing environment, our military operations become increasingly vulnerable to compromise via cyber attacks at echelons as low as the Brigade Combat Team (BCT). There is a need to design a system to facilitate the analysis of a nation state's ability to compromise the confidentiality, availability, and integrity of a deployed tactical network. Research demonstrated that, on these networks, compromises due to security protocols violated by humans are much more common than compromises due to technological vulnerabilities. Therefore, this analysis focuses on developing a simulation modeling approach to analyze the effectiveness of security protocols ithin the fortress?and to track the damage done by various forms of cyber attacks that have successfully breached the network perimeter. Our network model uses agent-based simulation in order to model the flow of information at the packet level with dictated behavior specific to the agents modeled: individual network packets, computer systems, routers, servers, and files. The advantage to using an agent-based, rather than a discrete-event, simulation model in this situation is that agent-based models focus on the relationship between entities from the bottom-up, such as at the network packet level, rather than the entire system from the top-down. The developed simulation model allows us to simulate various network attacks, observe their interaction with network security protocols, assess the resulting damage in terms of the network's availability, and quantify the damage in terms of sensitive information lost.
Deep packet inspection is a fundamental task to improve network security and provide application-specific services. State-of-the-art systems adopt regular expressions due to their high expressive power. They are typically matched through deterministic finite automata (DFAs), but large rule sets need a memory amount that turns out to be too large for practical implementation. Many recent works have proposed improvements to address this issue, but they increase the number of transitions (and then of memory accesses) per character. This paper presents a new representation for DFAs, orthogonal to most of the previous solutions, called delta finite automata ( $delta$FA), which considerably reduces states and transitions while preserving a transition per character only, thus allowing fast matching. A further optimization exploits $N$th order relationships within the DFA by adopting the concept of emporary transitions.?
This paper presents an initiative to involve ECE undergraduate students in the design and deployment of a network infrastructure for an academic laboratory. The project aims at attaining a reliable and secure network for an IC CAD environment. The students focused on employing secure authentication, accounting and storage with single sign-on, based on enterprise-grade, open-source protocols. This initiative proved to be highly motivating and allowed the students to develop knowledge and hands-on experience on the area of network security. The resulting network design and core infrastructure is herein described as well as its deployment in a real microelectronics design environment.
ANSS ?The ANSS integrates industry and government aeronautical simulators to assess and identify network security threats in airborne network environments and provides a security testbed used to test, calibrate, exercise procedures, and assess potential weaknesses and vulnerabilities in a controlled environment, without endangering people or resources.
?Identity infrastructure is key to Airborne Network Security for NextGen ?To promote these capabilities: ?Capitalize on existing capabilities, prototypes and refine designs, ?Participate in standards developing organizations, and??Use appropriate standards
Deep packet inspection plays a increasingly important role in network security devices and applications, which use more regular expressions to depict patterns. DFA engine is usually used as a classical representation for regular expressions to perform pattern matching, because it only need O(1) time to process one input character. However, DFAs of regular expression sets require large amount of memory, which limits the practical application of regular expressions in high-speed networks. Some compression algorithms have been proposed to address this issue in recent literatures. In this paper, we reconsider this problem from a new perspective, namely observing the characteristic of transition distribution inside each state, which is different from previous algorithms that observe transition characteristic among states. Furthermore, we introduce a new compression algorithm which can reduce 95%memory usage of DFA stably without significant impact on matching speed. Moreover, our work is orthogonal to previous compression algorithms, such as D2FA, FA. Our experiment results show that applying our work to them will have several times memory reduction, and matching speed of up to dozens of times comparing with original FA in software implementation.
Packet classification is widely used in various network security and operation applications. Two of the main challenges are the increasing number of classification rules, amount of traffic and network line speed. In this paper, we investigate an approximation algorithm for selecting the top- N most frequently matched subset of rules from the original ruleset. The goal is to obtain Top-N rules that covers as much traffic as possible while preserving the dependency relationships. Through simulations, we show that our approaches the optimal while runs in seconds, allowing online adaptation to changing traffic patterns.
Application of Wireless sensor network (WSN) is increasing in a rapid speed. As sensor networks may interact with sensitive data and operate in hostile unattended environments, it is imperative that security concern be addressed from the beginning of the system. But sensor networks also introduce severe resource constraints due to their lack of data storage and power. Both of these represent major obstacles to the implementation of traditional computer security techniques in a wireless sensor network. There has to be some compromise between the security and the energy. Asymmetric protocol like RSA has not been implemented due to high power constrain and for memory issue. In this paper, we have shown that RSA can be implemented for sensor in an efficient manner by using optimized computation. We have simulated the protocol in NS2.34 platform. The energy requirement gives an optimistic result quite similar to symmetric protocol energy requirement for sensor.
This paper proposes a novel ARP authentication scheme based on an ARP authentication trailer. The scheme provides a method to defend ARP attacks. In addition to avoid ARP attacks, the proposed scheme is backward compatible with existing ARP, no additional header fields are added to the protocol. The scheme supports both dynamic and static IP address assignment. Our experimental analysis shows that the proposed scheme can successfully defend various ARP attacks and can also improve overall network security without the need for complex configuration/installation or an additional server.
Malicious attacks are frequently launched to make specified network service unavailable, compromising end hosts for political or business purpose. Though network security appliances are widely deployed to resist these attacks, there is a lack of dynamic and collaborative platform to flexibly configure and manage all the security elements. In this paper, we present NetSecu, a platform based on Java and Click Router, which can dynamically enable, disable and configure security elements such as firewall, IPS and AV. Furthermore, a collaborate module is implemented to integrate individual NetSecu platform into a Secure Overlay Network, providing collaborative traffic control against DDoS attack. Equipped with collaborate module, NetSecu platforms are organized in a tree hierarchy where each level node is registered to its father node. A Central Management Site acts as the root node for large scale deployment. The policy is distributed from higher level to lower level NetSecu nodes, while security events are aggregated from lower level to higher level. Performance evaluation shows that our NetSecu system can achieve line rate with and without security function. Finally we deploy the NetSecu platform in multiple sites, where our design is fully demonstrated and tested.
Network Security Appliances are deployed at the vantage point of the Internet to detect security events and prevent attacks. However, these appliances are not so effective when it comes to distributed attacks such as DDoS. This paper presents a design and implementation of collaborative network security management system (CNSMS), which organize the NetSecu nodes into a hybrid P2P and hierarchy architecture to share the security knowledge. NetSecu nodes are organized into a hierarchy architecture so they could realize different management or security functions. In each level, nodes formed a P2P networks for higher efficiency. To guarantee identity trustworthy and information exchange secure, PKI infrastructure is deployed in CNSMS. Finally experiments are conducted to test the computing and communication cost.
Security constitutes a crucial concern in modern information systems. Several aspects are involved, such as user authentication (establishing and verifying users' identity), cryptology (changing secrets into unintelligible messages and back to the original secrets after transmission) and security policies (preventing illicit or forbidden accesses from users to information). Firewalls are a core element of network security policies, that is why their analysis has drawn many attention over the past decade. In this paper, we propose a new approach for analyzing firewalls, based on tree automata techniques: we show that the semantics of any process composing a firewall (including the network address translation functionality) can be expressed as a regular set or relation and thus can be denoted by a tree automaton. We also investigate abilities opened by tree automata based representations of the semantics of firewalls.
Web applications are an important target for security attacks. Most of these applications make use of cookies to maintain user state. Many attacks are carried out over these cookies in order to compromise network security. In this paper, we propose an architecture and a method of cookies security. This method aims to enforce cookies with integrity and confidentiality services. It was necessary to review the behavior of Reverse Proxy in order to apply these contributions. The approach has been quantitatively and qualitatively validated. The results of this validation are analyzed in this article.
The appearance of new technologies allows new data processing techniques. Thus, many new data processing techniques make difficult to user to find pertinent information in suitable time, unless knowing what accurately is in search of, where and how getting it. This paper proposes a pervasive network based information filtering system that integrates user profile such as identity, preference and other important data. User profile is embarked in a RFID-SIM card in order to guarantee its privacy, flexibility, mobility and confidentiality. The overall system objectives are privacy, security and providing pertinent information to the user according to his profile at anytime, anywhere, and in any form. The design and implementation of the system is also presented.
Microgrids are low voltage networks usually located at the consumer end of the distribution system. It typically consists of consumer loads, energy storage and small generation systems and is capable of islanding to protect itself against grid supply interruption. With the increased awareness of clean energy power systems, renewable technologies such as solar PV, wind turbines and fuel cells are gradually emerging within the power system network, and control and management for these equipment are necessary to ensure the stable operation of microgrids. However, existing centralized control systems are unable to handle the large number of renewable components and thus, a decentralized control scheme called Multi-Agent System (MAS) is introduced to manage these components. The implementation of distributed control will include JADE as the platform for agent communications as well as developing customizable agents for specific microgrid requirements such as ancillary services, power trading and negotiation and network security.
In this paper, we introduce a novel approach to reach ability analysis of dynamically routed networks. The goal is to determine the network-wide reach ability using static analysis of configuration files gathered from forwarding devices. We describe a method that can compute the reach ability in networks with a mix of static routing configurations, distance vector routing protocols, filtering routing updates and redistributions. The method computes a network-wide approximation of distributed routing information using the standard graph algorithms. Thus, for any network state, we can determine a set of active paths used for packet delivery. The outcomes of the method can be, for instance, used during the conformance checking of distributed access control lists against network security policies.
Due to the serious network security problems in recent years, a large number of malware features have been emerged, which leads to increasing time-complexity and space-consumption for malware detection systems. Moreover, irrelevant and redundant features may decrease the detection rate. Feature selection, as an important data mining phase and technology, can effectively reduce the redundant and irrelevant features in the original large feature space, thereby can increase the detection rate and reduce the false positive rate for malware detection model. This paper proposes a class driven correlation based on feature selection method, which can select corresponding features for different classes of data respectively. Then this method uses correlation based feature selection method to eliminating redundant features. Experimental results indicate that the approach can not only reduce the complexity of malware detection system, but also increase the detection rate as compared to other methods.
Resiliency and security in critical infrastructure control systems in the modern world of cyber terrorism constitute a relevant concern. Developing a network security system specifically tailored to the requirements of such critical assets is of a primary importance. This paper proposes a novel learning algorithm for anomaly based network security cyber sensor together with its hardware implementation. The presented learning algorithm constructs a fuzzy logic rule base modeling the normal network behavior. Individual fuzzy rules are extracted directly from the stream of incoming packets using an online clustering algorithm. This learning algorithm was specifically developed to comply with the constrained computational requirements of low-cost embedded network security cyber sensors. The performance of the system was evaluated on a set of network data recorded from an experimental test-bed mimicking the environment of a critical infrastructure control system.
Many computational intelligence techniques for anomaly based network intrusion detection can be found in literature. Translating a newly discovered intrusion recognition criteria into a distributable rule can be a human intensive effort. This paper explores a multi-modal genetic algorithm solution for autonomous rule creation. This algorithm focuses on the process of creating rules once an intrusion has been identified, rather than the evolution of rules to provide a solution for intrusion detection. The algorithm was demonstrated on anomalous ICMP network packets (input) and Snort rules (output of the algorithm). Output rules were sorted according to a fitness value and any duplicates were removed. The experimental results on ten test cases demonstrated a 100 percent rule alert rate. Out of 33,804 test packets 3 produced false positives. Each test case produced a minimum of three rule variations that could be used as candidates for a production system.
The 2011 IEEE Symposium on Computational Intelligence for Security and Defense Applications will present a wide range of applications to very challenging problems in the security and defense domains. IEEE CISDA 2011, the fourth such forum (the first was held in 2007), aims to present the most recent results on computational intelligence technologies and their applications to security, defense and military problems. The following topics will be discussed at the symposium: mine detection, complex adaptive systems, radar systems, modeling and simulation of military operations, network security, and maritime applications.
Buffer overflow attack is the most common and arguably the most dangerous attack method. The buffer overflow detecting will play a significant role in network security filed. Various solutions have been developed to address the buffer overflow vulnerability problem. The paper presents a method that combines static analysis with dynamic test. By using the method we can identify a lot of potential weakness locations. A buffer overflow vulnerabilities testing system was developed. Using the system some PE-format files and dynamic link library files are detected respectively. The experiment results show that the method is feasibility and availability.
The value of a Data collection mechanism like Honeypot/Honeyntes lies in being attacked and probed[1]. Hence the efficiency of these resources depends upon the amount and value of data collected by them but then there is no appropriate measure present to quantify the value of data collated by these systems. Most of the honeynet projects proves the efficiency of their honeynet systems based upon the volume of data collected but then the volume of data in it self could be a misleading parameters as in the case where a honeypot collects a high volume of the data but the data lacks in the diversity as it collects the same attacks in a given time frame again and again from different data sources. In this paper we have done efforts to 1) introduced the diversity index which is commonly used in the ecological studies as a measure to quantify the value of data in terms of diversity of the data 2) and to prove that the diversity of the data collected by a distributed honeynet is greater than that of a honeynet deployed at a single location.
This paper describes a case study of Honey pot deployment in an organizational network. As per Wikipedia oney pot is a trap that is set to detect, deflect, or in some manner counteract attempts at unauthorized use of information systems?[05]. These traps could be any digital resource ranging from a single computer to a network of such computers or a network application that appears to be a part of organizational network resources but is actually a fake resource with no production traffic. Further these resources are closely monitored and the traffic to and from these resources is well under the control of the administrator. In the experiment performed in this paper, such a trap is laid in the form of a low interaction honeypot honeyd [01] in the perimeter security of an organizational network. The results of deployment are presented and further various props and cons of such deployments are brought about.
The job of the network defender increases in complexity daily with the discovery of new vulnerabilities and new forms of attack. Limited only by their imagination, attackers have a seemly endless repertoire to bring to bear. Defenders must be able to accurately represent the information vital to their trade in a way that supports understanding and enables prediction. This paper presents a new approach, the application of the technique of storyboarding to network defense.
Basic VPN network protections are initially handled by a firewall system. However, this protection is usually insufficient and needs additional improvements. An improved method for detection and elimination of the security problems is setting up the IDS system for timely notification of malicious actions on the network. The concept of a firewall and IDS system was used in this paper as a joint mechanism for improved VPN network security. The proposed security system consists of firewall, syslog server and email server and is implemented in real environment and tested against some frequent DoS attacks. The easiest attacks were realized through the frequent (FTP, HTTP, RDP) open ports to the VPN network and successfully detected. Malicious activity system alert is a mechanism for starting an active response to IDS attack and for blocking the attacker, as well as establishing the full functionality of the network.
In this article, we present a cross-domain protocol model based on hypercube, analysis of the theorem and simulation results. Utilizing the structural features of hypercube, the model supports mutual authentication between the entities in different trust domains. Compared with traditional cross-domain authentication model, the proposed model reduces the register workload among authentication servers and simplifies the distributed and management of shared keys among authentication servers. The model could also avoid network bottleneck and single point collapse problems. Analysis shows that the protocol model is secure and reliable. Therefore, the application servers can effectively carry out access control to ensure network security.
This paper examines four types security countermeasures and their influence on information misuse or abuse behaviors of government internal officials' in the context of e-government information sharing. A research model was developed based on the General Deterrence Theory. Analysis of the data collected in a survey of 21 government agencies in Shanghai with 124 questionnaires who are specialized in the processes of government information sharing, including information collection and storage, information processing, information transfer and information usage. First, the findings indicated that static information security policies only can take effects when the policies are conveyed to employees clearly and the punishment is carried out well. Second, security awareness practice is one of the most important measures for internal officials. Third, when employees realize the organization is monitoring their information system behaviors and know consequence of information misuse or abuse behavior, the countermeasures can play more effects. Fourth, preventative software is still the strongest measure for e-government security.
Many office devices have a history of being networked (such as printers) and others without the same past are increasingly becoming networked (such as photocopiers). The modern networked versions of previously non-networked devices have much in common with traditional networked servers in terms of features and functions. While an organization may have policies and procedures for securing traditional network servers, securing networked office devices providing similar services can easily be overlooked. In this paper we present an evaluation of privacy and security risks found when examining over 1,800 networked office devices connected to a large university network. We use the STRIDE threat model to categorize threats and vulnerabilities and then we group the devices according to assessed risk from the perspective of the university. We found that while steps had been taken to secure some devices, many were using default or unsecured configurations.
We present a methodology for identifying sensitive data in packet payloads, motivated by the need to sanitize packets before releasing them (e.g., for network security/dependability analysis). Our methodology accommodates packets recorded from an incompletely documented protocol, in which case it will be necessary to consult a human expert to determine what packet data is sensitive. Since expert availability for such tasks is limited, however, our methodology adopts a hierarchical approach in which most packet inspection is done by less-trained workers whose designations of sensitive data in selected packets best match the expert's. At the core of our methodology is a data reduction and presentation algorithm that selects candidate workers based on their evaluations of a small number of packets; that solicits these workers' designations of sensitive data in a larger (but still minuscule) subset of packets; and then applies these designations to mark sensitive data in the entire data set. We detail our algorithms and evaluate them in a realistic user study.
This talk discusses how cybersecurity data (e.g., incidents, intrusion detection system alerts, network flows and malicious activity against a large range of honeypots) can be analyzed to evaluate the security of organizational networks. Since data are highly sensitive, organizations are reluctant to share them. In many organizations, the security team who can collect such data is not willing to share them even within the organization. The University of Maryland (UMD) plays a significant role in cybersecurity research due to a collaboration between the Office of Information Technologys (OIT) security team and UMD researchers. The result of the collaboration is access to and analysis of all security related data collected on UMD networks. The talk will review some studies conducted using the data provided by OIT. First, organizations face increasing challenges in addressing and preventing computer and network security incidents. Being able to understand and predict trends in incidents can aid an organizations ability to allocate resources for the prevention of such incidents, as well as the evaluation of mitigation strategies. We compared non-homogeneous Poisson process software reliability growth models and time series models with a large set of security incident data. Based on the over 12,000 incidents recorded since 2001, these models were compared for their prediction capability for the number of incidents. Second, security administrators lack network visibility, i.e., they often do not have the tools to monitor their networks in detail. Even though about 40,000 IP addresses are linked to UMD users, the UMD network consists of more than 130,000 IP addresses. We developed a tool called Nfsight that identifies clients, servers, and scanners solely based on network flows. Various heuristics have been applied and combined using a Bayesian method. Nfsight is currently monitoring the UMD network and the security team has integrated Nfsight into their security tool suit-
An integrated heterogeneous wireless system (IHWS) combines different wireless access technologies together and provides a user with ubiquitous network connectivity. Security in such network is extremely critical for its successful implementation. However, each underlying network in an IHWS has a unique security scheme that cannot be adopted by other networks. Several security algorithms that have been proposed, aim at achieving inter-network security but fail to address mobility and scalability requirements of the network. In this paper, we propose a novel security scheme that addresses the limitations in the existing security protocols for IHWS networks and prove its efficiency through extensive simulations.
This position paper outlines a staged approach to search-based application security testing. In the first stage one searches for candidate tests in the input space that have a chance of leading to good security tests. In the second stage one selects individual candidates and uses them to select and parametrize specialized search techniques. This approach has its roots in exploratory security testing. In the first stage, the fitness of tests depends on their ability to provoke vulnerability symptoms at all, and on their relation to other tests in a test suite. In the second stage, the fittest tests are those that come closest to an exploit of a specific type of vulnerability. To evaluate the performance of such a staged approach one might use web application vulnerability scanners as a baseline.
iSCSI is a standard network storage protocol. The working theory and security mechanism of iSCSI were studied and the threat of this protocol was pointed out. In order to improve performance, we put forward a secure iSCSI scheme based on SSL. A secure iSCSI system was implemented by adding SSL functions to iSCSI protocol. The results of experiments showed that throughput of the security iSCSI based on SSL increased 25%and CPU utilization rate decreased 50%, compared with the secure iSCSI based on IPSec.
With an ever-greater increase in network bandwidth, network speed and network traffic, network attack techniques are constantly changing and improving, making it formidable for the traditional network security defense measures to keep pace with this challenge. In this paper, a theoretical analysis is made first of both the traditional intrusion detection and the data stream mining, and then, a research is conducted into a network security defense technique based on the integration of data stream mining and intrusion detection, thereby coming up with an algorithm in the light of data stream clustering mining through a sliding and damped window. And this algorithm is applied to the intrusion detection systems so as to approach the traditional problem of inadequate real-time intrusion detection. Through analysis and simulation, it turns out that the algorithm has a lower requirement for operating environment but a higher clustering quality, thus facilitating good reference to improvement in the performance of intrusion detection.
With the development of various wireless personal area networks (WPANs), the issue of security has become a crucial problem for their applications. Jamming is one of the most important methods of attack to deprive or reduce the communication service of WPANs. Most existing jamming attacks can cause negative interference, but the attack strategies are usually not adjusted against the countermeasures that are currently taken. This paper proposes an effective dynamic jamming attack (EDJam) in an 802.15.4-compliant WPAN. In this attack, a jammer who is aware of a change in the network defense strategy, e.g. the use of a dynamic retransmission mechanism, may choose a better strategy to make more damage to the network with less cost. Similarly, a well-protected network can change its defense strategy against the EDJam. This procedure of competition between the EDJam attacker and defending networks is modeled and formulated as a Stackelberg game, and a unique Nash Equilibrium point is derived in analytical format. Based on an equilibrium analysis, we discuss the condition under which a defense strategy will increase the utility of the network and a dynamic retransmission mechanism defense strategy is proposed accordingly. The simulation results show that EDjam can be more cost-efficient than continuous, random and fixed-period jamming.
Many network security applications in today's networks are based on deep packet inspection, checking not only the header portion but also the payload portion of a packet. For example, traffic monitoring, layer-7 filtering, and network intrusion detection all require an accurate analysis of packet content in search for predefined patterns to identify specific classes of applications, viruses, attack signatures, etc. Regular expressions are often used to represent such patterns. They are implemented using finite automata, which take the payload of a packet as an input string. However, existing approaches, both non-deterministic finite automata (NFA) and deterministic finite automata (DFA), do not deal with compressed traffic, which becomes more and more popular in HTTP applications. In this paper, we propose an efficient algorithm for regular expression matching to implement deep packet inspection on compressed traffic. Based on the observations of DFA, we design a scheme to skip most of the matching process in the compressed parts of traffic. To the best of our knowledge, this is the first effort to design an efficient regular expression matching on compressed traffic. We evaluate our algorithm using rule sets provided by Snort, a popular open-source intrusion detection system. The evaluation results show that our approach can reduce the number of state access in the DFA significantly.
Identifying and classifying different network applications is very important for trend analysis, dynamic access control, network security and traffic engineering, while traffic classification is able to classify applications effectively. Current popular methods of traffic classification mainly include machine learning algorithm based on supervised or unsupervised and the method based load. In practical applications, the above methods have high complexity or low accuracy degree, so we propose a semi-supervised support vector machine method only based on flow statistics to identify and classify network applications. In this method, SVM, "constant" flow and co-training algorithm are the key core to obtain a classifier rapidly. The classifier got by this method has three advantages contrast to the previous classical methods: 1) high classification degree; 2) high generalization performance; 3) rapid computational performance. As a proof of concept, we implement the classification algorithm based on open-resource, and show the characteristics and feasibility of our method in the campus and resident network.
Network security assessment is critical to the survivability and reliability of distributed systems. In this paper, we propose a novel assessment approach that supports automatic vulnerability assessment utilizing Bayesian attack graphs. We also integrate several major vulnerability database into a comprehensive database and build a customized vulnerability scanner to assist attack graph generation. Different from existing solutions that manually assign probabilities to a Bayesian attack graph, we design a set of quantitative metrics to automatically analyze vulnerability and evaluate the proposed approach with real-world examples. Our results show the promising capability of the proposed approach in further improving assessment quality.
Traditional network security protocols depend mainly on developing cryptographic schemes and on using biometric methods. These have led to several network security protocols that are unbreakable based on difficulty of solving untractable mathematical problems such as factoring large integers. In this paper, emph{{underline{S}ecurity of underline{N}etworks underline{E}mploying underline{E}ncoding and underline{D}ecoding}} (textbf{SNEED}) is developed to mitigate single and multiple link attacks. Network coding and shared capacity among the working paths are used to provide data protection and data integrity against network attackers and eavesdroppers. textbf{SNEED} can be incorporated into various applications in on-demand TV, satellite communications and multimedia security. Finally, It is shown that textbf{SNEED} can be implemented easily where there are $k$ edge disjoint paths between two core nodes (routers or switches) in an enterprize network.
We mine the logs of network traffic data to find the contexts of attacks; we call them attack patterns. We propose an iterative algorithm for discovering attack patterns via a feedback mechanism, with the degrees of belief for attack instances propagated to the next iteration to further refine the search. Our simulations verify that the algorithm achieves accuracy in discovering attack patterns. Our attack pattern discovery has the additional advantage of being an unsupervised algorithm, e.g., it does not require a priori user-defined thresholds.
Intrusion detection technique has become the focus in the area of network security research. Various soft computing approaches have been applied to the intrusion detection field. The paper incorporate fuzzy logic and genetic algorithms into the classifying system based on fuzzy association rule to extract both accurate and interpretable fuzzy IF-THEN rules from network traffic data for classification, and utilize genetic algorithms to optimize the classifier. The experiments and evaluations of the proposed method were performed with the KDD Cup 99 intrusion detection dataset. Results indicate the high detection accuracy for intrusion attacks and low false alarm rate of the reliable system.
With the advent of open markets trading of power as a commodity, secure operation is an enduring concern of the independent system operator. A comprehensive security-constrained unit commitment (SCUC) methodology to coordinate a secure and economic generation pattern for the day-ahead market is an urgent requirement. An ineffective coordination of electric power transfer over the transmission lines could result in congestion, and inadvertently an outage cause through line overloading. In the current practice, the SCUC is solved by decoupling the cost evaluation aspect from the network security aspect. The primary frequency regulation reserve deployable in a manner after a (n - 1) contingency has not been fully utilised in the present approaches. A unified approach to the solution of the SCUC using a linear programming methodology with an extended DC network model embedded in the main algorithm is presented. The line-flow-based power flow equations are used to couple the economic dispatch (ED) and the network security process, and the proposed SCUC will include primary reserve as a part of the security constraint. Unlike the traditional SCUC programs that solve the ED first and then check it with the network for any violation, the proposed method solves the optimisation by fixing the line limits to solve the ED from the UC. A six-bus three generator and a modified IEEE 30-bus nine generator systems illustrate the implementation of the proposed SCUC formulation.
Wireless sensor networks are being used in many commercial and military applications to collect real time and event driven data. Deployment nature of sensor networks makes them vulnerable to security threats. Due to the resource limitations traditional security measures are not enough to protect sensor nodes. Research in sensor network security domains has produced several security solutions. In this paper we have observed three recently introduced security mechanisms (1) TinySec (2) MiniSec, and (3) TripleKeys. We have studied these security mechanisms in terms of packet overheads and compared the packet transmission time, average latency and energy consumption. Our comparison shows that the packet overheads in TripleKeys are lesser compared to other two schemes. We have then used the 38 bytes packet size of TripleKeys for further analysis and calculated the packet delivery ratio, latency and energy consumption. We have observed that packet delivery ratio decreases when we increase the number of nodes while latency and energy increases.

Malicious web pages that launch drive-by-download attacks on web browsers have increasingly become a problem in recent years. High-interaction client honeypots are security devices that can detect these malicious web pages on a network. However, high-interaction client honeypots are both resource-intensive and unable to handle the increasing array of vulnerable clients. This paper presents a novel classification method for detecting malicious web pages that involves inspecting the underlying server relationships. Because of the unique structure of malicious front-end web pages and centralized exploit servers, merely counting the number of domain name extensions and Domain Name System (DNS) servers used to resolve the host names of all web servers involved in rendering a page is sufficient to determine whether a web page is malicious or benign, independent of the vulnerable web browser targeted by these pages. Combining high-interaction client honeypots and this new classification method into a hybrid system leads to performance improvements.
Using high-level security policy rules to regulate low-level system, the security management system with a high level of expansibility and flexibility was made. For purpose of managing network security policy duly and flexibly in the complex network environment, and resolving its issue efficiency, a dynamic and self-adaptive security policy realization mechanism is proposed. The accident monitor and policy life-cycle are put forward, and the impact of safety equipment or user requests, such as system resources found on the flow control can be calculated automatically. The system can independently carry out a dynamic, flexible and real-time to adjust and control in the network environment and security needs change. The distribution model is given to response policy request rapidly, take the appropriate policy dissemination methods, and reduce PDP computing tasks, system resource consumption, which introduces the concepts of issue affecting factors, security domain addresses allocation, etc. Expression and making ways of the structure-dissimilarity policy faced on attribute characters and operation are analyzed emphatically. The effectiveness of the proposed model and algorithms is proved by experiments.
Many research efforts on application of ontology in network security have been done in the past decade. However, they mostly stop at initial proposal or focus on framework design without detailed representation of intrusion or attack and relevant detection knowledge with ontology. In this paper, the design and implementation of Ontology-Based Knowledge Representation for a Peer-to-Peer Multi-Agent Distributed Intrusion Detection System (Ontology-Based MADIDS) are introduced. An example which demonstrates the representation of an attack with ontology and the relevant detection process is also presented. In Ontology-Based MADIDS, ontology technique enables peers in the system and agents in one peer to share common understanding of information. In addition, benefited from agent technology and P2P architecture, agents in Ontology-Based MADIDS not only detect attacks on a single host but also in a distributed domain. These features make the Ontology-Based MADIDS more flexible and robust.
Monitoring Internet traffic is critical in order to acquire a good understanding of threats and in designing efficient security systems. While honeypots are flexible security tools for gathering intelligence of Internet attacks, traffic collected by honeypots is of high dimensionality that makes it difficult to characterize. In this paper, we propose the use of principal component analysis, a multivariate analysis technique, for characterizing honeypot traffic and separating latent groups of activities. In addition, we show the usefulness of principal component plots in visualizing the interrelationships between the detected groups of activities and in finding outliers. This work is demonstrated through the use of low interaction honeypot traffic data from the Leurr&#x0E8;.com project, a world wide deployment of low interaction honeypots.
Topics include a new blacklisting technique, a network architecture that could reduce data-center costs, a computing approach for the disabled that uses the tongue as a controller, and attempts to rein in offensive behavior in virtual worlds.
In this paper we study on the prediction technique of network situation awareness. It has two levels: the high-level situation and the low-level next attack step. The first one includes the indexes and the evaluation results of the network security situation, they are figure form, we use the RBF network to predict them for RBF&#x2019;s self-learning character. Then we use the weighted attack graph to predict the next attack step. The weights represent the probability; the biggest weights indicate the most possible next attack step. The simulations show these prediction methods can offer different prediction capability to satisfy the prediction need of the network situation awareness.
Neural networks approach is one of the most promising methodologies for intrusion detection in network security. An integrated intrusion detection system (IIDS) scheme based on multiple neural networks is proposed. The approaches used in IIDS include principal component neural networks, growing neural gas networks and principal component self-organizing map networks. By the abilities of classification and clustering analysis of the above methods, IIDS can be adapted to both anomaly and misuse detections for intrusive outsiders. The training stage is a mixture of supervised manner and unsupervised one. Furthermore, IIDS uses the buffering and spoofing principles of address resolution protocol (ARP) to capture and refuse the insider intruders trying to log on a local area network (LAN). Therefore, IIDS is able to detect the intrusions/attacks both from the outer Internet and an inner LAN. Experiments are carried out to illustrate the performance of the proposed intrusion detection system by using the KDD CUP 1999 Intrusion Detection Evaluation dataset.
Comparative study of CMOS SRAM 8K for different SOI structures (SIMOX, Smart-Cut and Dele-Cut) and SOS structures has been carried out. Radiation tests have been provided using laser and x-ray simulators in order to estimate CMOS SRAM stability to radiation pulse and total radiation dose. The tests have been carried out in active mode. The level of information safety for SRAM samples on SOI structures was three times higher than that for the samples on SOS structures. Research of the influence of total radiation dose has shown that the level was of 6&#x00B7;10<sup>4</sup> units for SOS and SOI structures.
Packet classification algorithms are widely used in network security devices. As network speeds are increasing, the demand for hardware acceleration of packet classification in FPGAs or ASICs is growing. Nowadays hardware architectures can achieve multigigabit speeds only at the cost of large data structures, which can not fit into the on-chip memory. We propose novel method how to reduce data structure size for the family of decomposition architectures at the cost of additional pipelined processing with only small amount of logic resources. The reduction significantly decreases overhead given by the Cartesian product nature of classification rules. Therefore the data structure can be compressed to 10%on average. As high compression ratio is achieved, fast on-chip memory can be used to store data structures and hardware architectures can process network traffic at significantly higher speed.
Applications to evaluate Internet quality-of-service and increase network security are essential to maintaining reliability and high performance in computer networks. These applications typically use very accurate, but high cost, hardware measurement systems. Alternate, less expensive software based systems are often impractical for use with analysis applications because they reduce the number and accuracy of measurements using a technique called interrupt coalescence, which can be viewed as a form of sampling. The goal of this paper is to optimize the way interrupt coalescence groups packets into measurements so as to retain as much of the packet timing information as possible. Our optimized solution produces estimates of timing distributions much closer to those obtained using hardware based systems. Further we show that for a real Internet analysis application, periodic signal detection, using measurements generated with our method improved detection times by at least 36%.
Vulnerabilities in web applications expose computer networks to security threats. In fact, a large number of websites are used by attackers as hopping sites for attacking other websites and user terminals. These incidents prevent service providers from constructing secure networking environments. To protect websites from attacks based on vulnerabilities of web applications, security vendors and service providers collect attack information using web honeypots, which masquerade as vulnerable systems. To gain full access and to launch further network attacks by executing malware, such as a downloader, vendors and providers use high-interaction web honeypots, which are composed of real vulnerable systems and surveillance functions. However, conventional high-interactive web honeypots can collect only limited information and malware from attacks, whose path to the destination URL does not match the path structure of the web honeypot, due to the fact that these attacks are failures. To solve this problem, we propose scheme in which the destination URLs of these attacks are corrected by determining the correct path from the path structure of the web honeypot. Our Internet investigation reveals that 97 percent of attacks are failures. However, we confirmed that about 50 percent of these attacks will succeed with our proposed scheme. With our proposed scheme, we can use much more information to protect websites than with conventional high-interaction web honeypots because we can collect complete information and malware from these attacks.
Broad network bandwidth and deep inspection impose great challenge for the capability of 10Gpbs network security monitoring. Proper scheduling policies can improve system capability without requiring additional resources. LAS, a size-based scheduling policy which can achieve optimal mean response time by giving preferential analysis to short flows, is widely used in various aspects of network field. Due to the high variability property of Internet traffic, LAS favors short flows without penalizing large flows very much. Unfortunately, the inspection of large flows can not be guaranteed in those network intrusion detection systems on 10Gbps links, which are usually heavily loaded, or even overloaded. Although tiny in percentage, large flows comprise more than 50%of the total load, and therefore can not be ignored, especially when specified by users as critical. How to avoid starving large flows while still giving higher priority to short flows is a dilemma we have to face in practice. In this paper, we propose a QoS-supported three-level scheduling policy (QTL), which can remedy LAS' defect. The experimental results show that our QTL scheduling policy has approximately the same performance as LAS for short flows, and meanwhile exhibits greatly enhanced processing capability for large flows.
Several publish/subscribe (pub/sub) and data-oriented networking proposals have been presented to overcome limitations of the current message- and host-centric Internet. However, security issues of these solutions have not been addressed comprehensively. In this paper we examine roles of actors comprising an inter-domain pub/sub network, together with security requirements and minimal required trust associations arising from this setting. We then introduce and analyze a security design for a clean-slate pub/sub network architecture that secures both the control and data planes. The solution addresses availability and data integrity while remaining scalable and usable.
A new network-worm-control technology is proposed after having fully analyzed the differences between worm and normal connection requests. Considering the worm characteristic of attacking unique port and dispersing IP addresses, the method uses multiple data sets according to the different ports to avoid the influence among the ports. Aiming at the normal connection characteristic of ephemeral bursting out, the method takes advantage of two-stage leaky bucket to control the output of delay queues. The method can not only shorten the period of staying in the delay queue of normal requests, but also prevent the worms. The analysis shows that this method plays a significant role before the outburst of the worms.
The spreading worms have greatly affected the network infrastructure security. After the CodeRed, there have been many new worms reported. To take countermeasure against the spreading worms, in this paper, a correlation based method is proposed and applied in the analysis. Results indicate that the spreading worms could cause dramatic changes in the flow size distribution. This method provides new insight into the worm detection and traffic anomaly discovery.
Many network security applications in today's networks are based on deep packet inspection, checking not only the header portion but also the payload portion of a packet. For example, traffic monitoring, layer-7 filtering, and network intrusion detection all require an accurate analysis of packet content in search for predefined patterns to identify specific classes of applications, viruses, attack signatures, etc. Regular expressions are often used to represent such patterns. They are implemented using finite automata, which take the payload of a packet as an input string. However, existing approaches, both non-deterministic finite automata (NFA) and deterministic finite automata (DFA), have limitations; NFAs have excessive time complexity while DFAs have excessive space complexity. In this paper, we propose an efficient algorithm for regular expression matching to implement deep packet inspection on multi-core architecture. A regular expression is split into NFA-friendly components and DFA-friendly components, which are then assigned to different cores. This hybrid method combines the merits of NFA and DFA implementations, and efficiently takes advantage of multi-core architecture. We evaluate our algorithm using rule sets provided by Snort, a popular open-source intrusion detection system. The simulation results show that our approach outperforms existing NFA/DFA and hybrid approaches. Furthermore, our algorithm performs well on the important issues on multi-core architecture design, such as load balancing, data locality and communication between cores.
As more and more organizations rely on the Internet for their daily operation, Internet security becomes increasingly critical. Unfortunately, the vast resources available on the Internet are attracting many malicious users and organizations, including organized crime syndicates. With such organizations disguising their activity by operating from the machines owned and operated by legitimate organizations, we argue that responsible organizations could improve overall Internet security by strengthening their own. By improving their organizational etiquette, legitimate organizations will make it more difficult for malicious users and organizations to hide. Towards this goal, we propose a system to identify and eliminate malicious activity on edge networks. We use a year-long trace of activity from an edge network to characterize the malicious activity at an edge network and demonstrate the potential effectiveness of our system.
With expanding network infrastructures, increasing vulnerabilities and uncertain malicious activities, cyber security research has begun to provide situation assessment beyond Intrusion Detection Systems (IDSs). A key goal of cyber situation assessment is to efficiently and effectively project the likely future targets of ongoing multistage attacks. This work presents two ensemble techniques that combine real-time projection algorithms modeling the behavior, capability, and opportunity of malicious activities in a network. Sugeno fuzzy inference system and Transferable Belief Model are used to combine supporting evidence and resolve conflicts between the algorithm outputs. The two ensemble techniques are analyzed and compared using simulated attack datasets generated for varying network environments and attack parameters. The results are discussed to reveal the benefits and limitations of individual algorithms and ensemble techniques.
Wireless Community Networks (WCN) are formed by the integration of user-operated wireless sensor networks that are internetworked by wireless mesh networks available within urban communities. WCNs enable novel applications for the members of the community. These include different sensing applications, where individuals contribute sensor data for further use within their community at large or with well-defined restrictions to certain users. Sensing application scenarios for WCNs differ from traditional sensor network applications with respect to their security and privacy requirements. In this paper, we define three representative scenarios-personal sensing, designated sensing, and community sensing. These scenarios are then studied with respect to their privacy and security implications. In particular, we identify main research questions and highlight the challenges of using various security and privacy approaches from networking and cryptography to make sensing applications in WCNs security and privacy aware.
The following topics are dealt with: wireless communication; signal processing; network security; data privacy; grid computing; Internet services; protocols; peer-to-peer network; multimedia computing; optical network; and ad hoc network.
Distributed Denial of Service (DDoS) attacks have become one of the most serious threats to the Internet. In this paper, we propose Mantlet, an overlay-based approach to detect and mitigate DDoS attacks. Mantlet combines three innovative mechanisms for anti-spooflng, attack detection and mitigation, respectively. To circumvent IP spoofing, we first propose a probing mechanism named Bypass Check to authenticate the clients of TCP or UDP services. Then, Cumulative Sum (CUSUM) is adopted to detect DDoS attacks based on the abrupt change of sequential packet symmetry, the ratio of received to transmitted packets of a service. After detection, the suspicious flows that contribute to asymmetry are segregated and experience preferential dropping test (PDT). A suspicious flow is confirmed as malicious if it is unresponsive to packet drops. Finally, we implement Mantlet with Click and perform experiments on PlanetLab. The experimental results validate our analysis and show that Mantlet is applicable to not only TCP services but also UDP services.
Recently with rapid development of broadband network and IT technology, consumers are capable of wirelessly connecting to the internet through Access Point (AP) or 3G to conveniently browse the Internet or even receive a digital multimedia. The advancing of this technology is providing consumer with an easy access to Internet anywhere to receive services. One of the services that is attracting user in these days is IPTV services. IPTV is known as Internet Protocol TV that is capable of delivering a high quality of multimedia such as data, voice, and video to consumer homes through wired set-top box (STB). But with the advancing of mobile devices technology, users would want to receive their services through mobile devices at home or outside. However, the use of wireless environment has many risks and weaknesses when you compared it to existing wire networks. For instant, in wireless environment, mobile devices have vulnerabilities such as denial-of-service attack (DoS attack), man-in-middle attack, replay attack, and so on. Therefore consumer security and privacy are essential security issue as well as protecting digital content when users connect to wireless environment. Therefore, a secure migration of IPTV services from a set-top box (STB) to mobile devices at home or outside for pay per view video mechanism is proposed in this paper.
IPv4 is a foundation of Internet communications. Designed many years ago the protocol is inadequate to modern networks. New sixth version is replacing the older one. It is often repeated that IPv6 was designated to solve some performance problems. This statement is true only to some extent. IPv6 deployment (especially in the transition phase) will have large impact, not always positive on many aspects of Internet services: network performance, data security, economy. As number of IPv6 networks grow, understanding of performance issues becomes more important. The paper attempts to present comprehensive survey on positive and negative IPv6 impact on performance with a special emphasis on IPv4 to IPv6 transition phase. The survey is accompanied by brief analysis of third party test results.
This report investigates whether a vulnerability found in one web framework may be used to find a vulnerability in a different web framework. To test this hypothesis, several open source applications were installed in a secure test environment together with security analysis tools. Each one of the applications were developed using a different software framework. The results show that a vulnerability identified in one framework can often be used to find similar vulnerabilities in other frameworks. Cross-site scripting security issues are the most likely to succeed when being applied to more than one framework.
Today's fast growing use of the Internet for commercial transactions has increased the demand for data protection and network security. New methods of data encryption / decryption are constantly being developed, with the aim of finding more secure, reliable and flexible systems to protect transmitted data from any possible attacks. The present work evaluates the one-dimensional New Mersenne Number Transform (1-D NMNT) for security applications by analyzing its avalanche and diffusion percentages. It is recommended for security applications, as it is sensitive; changing a single element at the input makes drastic changes in the output elements and vice versa, it is parameterized; provides variable block length, has long transform length (power of two), is error free, its inverse is the same with a scale factor of (1/N) which simplifies implementation of both encryption and decryption, and finally, fast algorithms can be applied to it to speed up processing.
Recent years have seen substantial development in computer and network security design. This has been manifested as an every increasing range of new protocols, new encryption algorithms, new methods of authentication, smarter firewalls and intrusion detection techniques, new anti-malware products and many more. During the same period of time increasing demands for more trustworthy network infrastructure have seen the development of sophisticated analysis tools necessary to meet the operational requirements of law enforcement agencies. These include tools for e-discovery, commercial intelligence and national security. Thus the industry has seen equally significant developments in computer forensic tools where methods of searching for and detection of, malicious activity for presentation as evidence and provision of trust have become ever more sophisticated. To a considerable degree the science of security and forensics have seen both rapid but separate developments. This paper proposes that there are areas in common between these two important fields of endeavour and sets out techniques and ideas which demonstrate how they can overlap and work together in order to provide improved security and trustworthiness in critical infrastructures. In particular this paper addresses computer security and forensic analysis from a real-time perspective such that security events can be monitored in a live network while sound forensic data collection, storage and processing can be carried out in a manner which supports real-time security and at the same time still meeting the requirements of sound evidence.
The IEEE 802.16 standard, the service data units deemed to exceed the granted bandwidth may be delayed or dropped. In addition, nodes would behave maliciously by requesting more resources than their requirements just to deny other nodes from having access to the network. In this paper, we propose a trust-aware adaptive monitoring scheme that can be integrated with the basic architecture of WiMAX network to monitor if a service flow exceeds its maximum sustained traffic rate and whether the resources allocated to subscriber stations are significantly used or not. To detect and react to malicious behaviors, the scheme gradually establishes a trust relationship between the different entities of the network. We have evaluated the performance of the proposed scheme using the network simulator NS-2 while measuring several network parameters such as throughput.
Currently, the internet becomes an important information infrastructure, and internet security problem have become strategic issues related to the country safety. Therefore, it is important theoretical and practical value to make network security risk assessment theory and key technology research. As the internet has the characteristic of complex nonlinear systems, we lead nonlinear system analysis and forecasting techniques into the quantitative risk assessment of network security. It aims at exploring the complexity and uncertainty relationships among the elements in risk element and to establish a framework and method for quantitative risk assessment. In this paper, we introduce nonlinear system theories on chaos and fractal for complexity analysis of network threat frequencies and to reveal the essential characters in the network time series. Secondly, we study nonlinear chaotic prediction methods for network threat frequencies. Finally, we analyze quantitative characteristics of network threat behavior.
This paper presents four kinds of security attribute to assess the PC terminal whether it is safe and reliable. The terminal will be measured by its credibility before accessed to network, and then assigned to different levels of security network environments basis on the value of credibility. By using self-designed trusted hardware platform - ENSS (Embedded Network Security System) PCI card, programming network driver and tailoring the architecture of TNC (Trusted Network Connection), this paper achieved network security accessing and access control policies to construct trusted network environment.
Along with the development and application of computer network, the network security and stability has become an urgent task. In this paper, a novel model of network security situation awareness is presented based on association rules algorithm. To aim at the shortcomings of the classic association rule (Apriori algorithm), some improvements are carried out on it and the improved method is applied into network security situation awareness. Through the data mining method, the association rules can be found for the reflection of large-scale network security situation. By the influence of the attack to the sequence of entropy, the association rules are divided into normal and abnormal space in order to do the clustering. In accordance with the clustering, the level of network security awareness can be obtained.
Along with the internet fast developing recently, viruses, leaks and spy wares emerge in endlessly; worms, Trojans, and net thefts increase year by year. Maintaining Internet security becomes more and more important. Denial of service attack is among the hardest security problems to address because it is easy to launch, difficult to defend and trace. For the purpose of tracking IP address of the attackers, many methods and ways were proposed by experts. This paper systemically deals with a state of art about the most techniques of IP tracking for Denial of Service (DoS) and Distributed Denial of Service (DDoS) till now, classifies them and analyzes their respective advantages and disadvantages. The majority of these methods can be implemented basically by victim and police department.
Network security plays an important part in LAN system. The 802.1X protocol provides a port-based network access control to the LAN. In this paper, three cases of user name attacks against the 802.1X are given firstly. Then, the design flaws of 802.1X system are analyzed. At last, an improved authentication process is presented and how to prevent the user name attacks is discussed.
The research on network security architecture is a indispensable part in network security research. Layered Network Security Architecture (LNSA) is proposed in this paper. Firstly, LNSA is described with the concept of component. Then Stochastic Petri net is used for formal modeling of LNSA. Subsequently, the Stochastic High Lever Petri Nets is used to predigest the formal modeling and its corresponding Markov chain is used to calculate the steady probability. Then the steady probability is used to evaluate the network security status, as time is used to describe Hacker's intrusion ability and security system's protection ability. Obviously, the result is of important guiding significance in the actual network security analysis and application.
Security of identity authentication in mobile-commerce system is a very important task, and its safety assessment is especially complicated. The Analytic Hierarchy Process (AHP) is an effective tool on transformation of question from half-qualitative and half-quantitative to quantitative. In this paper, applies AHP method to the safety assessment, establishes an index system according to the international standard of network security, constructs a hierarchy model, and establishes a judgment matrix. The method is proved to be feasible by the case study of the safety assessment of identity authentication in mobile- commerce in a special environment.
This paper presents a cooperative anti-worm system model based on distributed honeypots for local area network(LAN). This model deployes honeypot systems in DMZ, at the back of firewall and in the internal subnets respectively. Honeypot systems cooperate with intrusion detection system (IDS) and firewall to prevent the worm attack from outside or inside LAN by the monitor center. Honeypots are not only able to lure a variety of network worms and collect new worm data, but also able to take measures to prevent worms from further spreading. The monitoring center is mainly responsible for further analyzing the suspicious data send back by each honeypot system and extracting new type of worm attack patterns and then sending them to the firewall and ID agents. The firewall and ID agents accept the feedback from the monitoring center to update their own rules, so they are able to respond to the new type of worms. By collaborating between honeypots and other security systems, the system is able to quickly respond to a variety of worm attacks from outside or inside LAN and provide a lot of evidence for administrators.
Traditional IDSs have take network factors and distributed environment into account, but the conception of real Distributed-IDS is not so strong. This paper introduces a modified federated peer to peer architecture that can be implement easily. With the help of multicast reflector and modified shaker protocol embedded inside, the suggested scheme can perform better intrusion detection by sharing information between low-level IDS agents.
Denial of service attacks and distributed denial of service attacks has become the network of one of the most serious problems. Next article IPv6 denial of service attacks against the latest side measuresIP tracking method in detail and in-depth study will be needed to track the mechanism part of the definition of data bits to move to IPv6 header, gives the probability of packet marking scheme of the implementation method, experimental tests show that you can achieve better tracking of work..
This paper introduces the concept and the classification of Web mining, analyzes the Web mining process and the analysis techniques often used. We propose the system architecture based on Web mining of E-commerce application, and discuss the analysis model of Web Mining and the applications in E-commerce. Web mining can find potential customers and achieve website structure optimization. It can provide customers with personalized service, realize business intelligence and Web use describing, and has certain promoter action to the network security. It has important directive significance for the development of E-commerce to study the E-commerce application based on Web mining.
Applications of Artificial Immune System (AIS) has been widely applied in various engineering, including network security, pattern recognition, combinational optimization, machine learning and fault diagnosis, etc. Fault diagnosis is another AIS application field directly mapped from the theory of immunity after information security and has made certain achievements in research. Real-valued Negative selection algorithms (RNSA) of AIS generate their detector sets based on the points of self data. Self data is regarded as the normal pattern of behavior of the monitored system. This paper provide a new fault detection method based on RNSA of artificial immunity. It can effectively overcome the deficiency of the various fault detection methods of today that cannot implement fault detections because there are only normal samples and not enough fault samples and short of the function of continuous learning. The test result shows that, by increasing a certain number of training samples, the accuracy of fault diagnosis has made great changes. This way has obvious advantage in robustness and accuracy in detection and shows a favorable prospect of application.
The following topics are dealt with: peer-to-peer network; wireless network; computer network security; computer network management; and BitTorrent.
With the wide availability of digital content and proliferation of Web2.0 applications, supporting secure and efficient identification of digital content becomes a more and more important issue. In early 2010, the Open Mobile Alliance (OMA) ratified a new standard known as Secure Content IDentification Mechanism (SCIDM) to facilitate the management of digital content. As the main contributors to the SCIDM standard, the authors provide an overview of this standard as well as a use case in this paper.
The following topics are dealt with: wireless communication system; network security; optical communication system; broadband communication system; signal processing; neural network; computational intelligence; power semiconductor devices; power convertors; microwave technology; nonlinear circuits; memristors; and memristive systems.
In this paper, we use mathematical method to study the model of complex network, analysis the network characters, use statistics, entropy and visualization reasoning analysis to get these characters. Then we give the three layer model for service, host and network to present network, and set up the system of network characters management.
Fast-flux based service network is first introduced as a technique used to improve the reliability and quality of a service system. But recently, this technique is also adopted by cyber-criminals and Internet miscreants to evade identification and to frustrate law enforcement and anticrime efforts aimed at locating and shutting down the illegal service. Previous work on this topic focuses mostly on detecting and measuring existing fast-flux based service networks ignoring the most import property provided by them-high availability under countermeasures. This paper attacks the availability problem of fast-flux based service network under countermeasures. In this paper, counter-system is taken into account and a model of these networks under countermeasures is presented. Based on this model, two evaluating factors of availability are proposed. By using reliability theory and queuing theory, this paper also performs a study of typical fast-flux based networks and their availability. Results show that both employed strategies and users' network environment have significant impacts on the availability of fast-flux based service network.
To use the network services provided by multiple servers in mobile wireless network, a hash function and smart card based multi-server authentication scheme without verification tables and servers' public keys is proposed. The new protocol has many advantages, such as no encryption, signature, verification tables, timestamp, and public keys directory.
Peer-to-Peer (P2P) networking is beneficial when removing a centralized server. On the other hand, new mechanisms are required to compensate for the central authority, especially for network security and dependability. In this paper, we propose a new fuzzy reputation (Fuzzy-Rep) model to improve security and dependability of P2P e-commerce. The model employs fuzzy logic inference rules to assess transactions through evaluating the direct trust, indirect reputation, peer's comprehensive reputation, rewards and penalty mechanisms which can resist vicious trust recommendation and fraud.
Summary form only given. Cross platform /network services (CPS) are a new breed of enterprise services that have spawned from the Internet to also include metropolitan 3G, WiFi, and WiMAX networks today. While the security of each separate network has been well addressed, the issue of end to end security of such cross platform networks is largely unexplored. There are fundamentally two core reasons as to why it is significant to study this issue. Firstly, information between entities in separate networks has to undergo a translation when they cross over from one network to another. Such translation today is not security-aware, and it can be seriously taken advantage of by attackers. Secondly, when there is chain of communication traversing across multiple networks, it is very easy for attackers to attack the weakest link in the chain, and hence cause significant damages to otherwise well protected networks in the chain. In this talk, I will comprehensively introduce our project to deal with the security of cross platform networks. Our objectives are to highlight the core vulnerabilities of cross platform networks, and to securely defend them against attacks that can exploit such vulnerabilities. Towards this extent, our project will focus on securing the following cross networks: lnternet-3G networks, Internet-WiFi networks, Internet-WiMAX networks, and lnternet-3G-WiFi-WiMAX networks. We have designed and implemented the secure measurements to protect the representative applications (e.g., Voice/ Video conferencing; Messaging services like email, instant messaging; Remote working; Web services etc.) and against the attacks (e.g., Denial of Service, Information stealing and Malicious code injection etc.) for the above applications.
Based on chaotic Tent map, this paper proposed a new 5L (L?2) bits extensible Hash function with symmetric keys. The structure only performed Boolean algorithm and shift operation. In order to generate fast diffusion, confusion and avalanche effect, Chaos series were embedded in the whole algorithm. The security against statistical attack, birthday attack were analyzed in detail. Many simulations were shown that the Hash value was highly sensitive to initial condition and parameter, and also to a message bit. Performance comparisons with MD5 showed that this scheme was a feasible scheme, and can be applied in e-commerce.
In this paper, we propose a trust management framework for network virtualization environments. The proposed framework helps distinguish among the infrastructure providers based on their past experiences and feedbacks from service providers. We describe the main components involved in gathering the feedbacks and managing the reputation data. We detail the proposed trust system and we define the concept of Degree of Involvement of an infrastructure provider in terms of nodes and links. We also perform a mathematical analysis of the proposed system. Simulation results confirm the effectiveness of the proposed trust management system in increasing the service providers' satisfaction and reducing the selection of infrastructure providers that do not fulfill their Service Level Agreement (SLA).
As a widely accepted model of multi-step network intrusions, attack graph has been applied to topological vulnerability analysis, network hardening, alert correlation, security metrics, and so on. A major challenge faced by attack graphs is the scalability: Even the attack graph of a moderate-sized network is typically incomprehensible to the human eyes, whereas that of large enterprise networks usually has an unmanageable size. Such a complexity, however, is not entirely unavoidable. In this paper, we shall show that an attack graph may contain much redundancy due to the similarity between different hosts' configurations. We then present a novel representation of attack graphs based on reference encoding. Specifically, subnets of hosts with similar configurations are represented using reference hosts while textual rules are employed to describe minor differences. The compression process is lossless and the resultant attack graph can directly provide useful insights. The effectiveness of the proposed model is illustrated through a case study and simulation results.
Next generation networks anticipate an increasing amount of network traffic from a wide range of emerging network applications. The features of packet flows (such as the minimal packet inter-arrival time and the number of packets with non-zero options in TCP headers) are used frequently in determining the traffic type and applying security policies. However, the extraction of flow features is difficult due to the increasing line rates, a broad range of network protocols, and a variety of complex flow features. In this paper, we leverage the multi-core processors to speed up the feature extraction process. We design an open source parallel software tool, aiming for processing network packet flows in real-time. We implement the software in four different designs including serial, parallel, pipelined and hybrid architectures. We evaluate the performance of the parallel software tool through measurement experiments. Our experimental results show that each method increases the packet processing throughput by 5-7%in comparison with the previous method. And finally the implementation based on the hybrid architecture improves the packet processing performance by 19.3%than the implementation based on the serial architecture.
Security problem poses a great challenge for running mission-critical cyber-physical systems in wireless networks. With the disregard of security factors, existing message scheduling schemes expose critical messages to security threats, especially by confidentiality attacks. Incorporating confidentiality improvement into message scheduling, we investigate the problem of scheduling periodic messages with both time-critical and security-critical requirements. Risk-based security profit model is built to quantitatively measure the security quality of messages. For a set of periodic messages, a Security-Slack based Heuristic Algorithm (SSHA) is proposed to judiciously allocate a slack time to the most suitable confidentiality level for each security-critical message. Experimental results evaluate the efficiency of SSHA. Compared to other heuristic schemes, specifically, the average security improvement can achieve up to 57.8%for SSHA.
Most successful cyber attacks begin with successful information gathering. Now more and more Web servers try to hide their identities by removing product tokens in the erver?header in their responses discreetly, but that fails because of some Web fingerprinting tools. Some tools try to defeat these fingerprinting tools by changing Web servers' headers order or adding/removing some headers, but, as analyzed in this paper, those measures cannot change Web servers' inner behavioural characteristics and so fail in anti-fingerprinting. In this paper we argue that eliminating compliance variation among Web servers is a better way against Web server fingerprinting.
From the study of the two major characteristics of complex networks as a starting point, this paper presents a risk-conductivity as a weight side between neighbor nodes to construct a weighted complex network. Further, the concepts of weighted degree coefficient and weighted aggregation coefficient as well as their computing methods are given, and for the cluster analysis, a weighted fuzzy C-means clustering algorithm based on the weighted complex network features is formed by putting the two characteristic values. Finally, this method applied to assess a campus network security is verified by doing an experiment.
Methods for the assessment of the dynamic network security, with regard to the installed protection system, are introduced. Currently, an accordant online system is under development together with European Universities and European Transmission System Operators. Here, the purpose is the observation of the security of the overall system, consisting of primary network and the superimposed interacting protection system including its communication. The assessment of the security of the installed protection system with its actual characteristics and settings is of special importance.
Preliminary results from an investigation on the application of dynamic power flow controllers to a stretch of the Qatar Transmission System (QTS) along the western coast of Qatar. The main objective of applying the power flow controller is to maintain balance of power flow between the networks or lines that supply to this weaker region of the QTS, and in turn support network security should the demand in this region grows. The paper will investigate the impact on steady-state power flow control using a Unified Power Flow Controller (UPFC), and the potential application of a Generalized Power Flow Controller (GUPFC), in the QTS to check if these controllers provide a viable solution looking into the future. In controlling of the power flow controller, this paper looks into the possibility of using phasor measurement units (PMUs) and synchrophasor based control. An assessment on the existing infrastructure of the QTS and necessary improvement needed to support implementation of the power flow controller and its control will be included in this paper along with a preliminary cost analysis. The modeling work and preliminary studies in this paper were conducted with PSS/E.
This paper describes the Intelligent Energy Management (IEM) System of the Future Renewable Electric Energy Delivery and Management (FREEDM) System. The IEM system is comprised of three main subsystems: the solid state transformer (SST), the reliable and secure communication (RSC) network, and distributed grid intelligence (DGI). Each of the subsystems is briefly described and the primary challenges associated with each one are highlighted.
Pricing for the use of the networks is essential in the way that it should be able to reflect the costs/benefits imposed on a network when connecting a new generator or demand and to provide forward-looking message to influence the site and size of future network customers. Studies have been extensively carried out over the years to achieve this pricing goal. Few methodologies that can directly link nodal generation/demand increment to network long-run marginal/incremental costs. Even fewer consider network security in their pricing methodologies, considering it is one of the most important cost drivers. All networks are designed to be able to withstand credible contingencies, but this comes at a significant cost to network development. This paper proposes a new approach that can establish the direct link between nodal generation/demand increment and changes in investment cost whilst ensuring network security. The investment cost is reflected by the change in the spare capacity of a network asset from a nodal injection, which is in turn translated into an investment horizon, leading to the change in the present value of a future investment cost. The security is reflected in the pricing through a full N-1 contingency analysis to define the maximum allowed power flow along each circuit, from which the time horizon of future investment is determined. This paper illustrates the implementation of the proposed pricing model for a system whose demand grows either at a uniform rate or at variable growth rates. The benefits of introducing security into the long-run pricing model are demonstrated on the IEEE 14 busbar system and a practical 87-busbar distribution network.
WiMAX has been proposed as an attractive solution to address the infrastructure costs and physical limitations of wired networks. However, MAC management frames are not encrypted in WiMAX networks. Many of them are not even authenticated. In this paper, we analyze how sniffing and DoS attacks based on unauthenticated management frames could be launched. To prevent these potential attacks, three different authentication mechanisms are proposed. The performance of these mechanisms is also analyzed in terms of additional bits and processing time.
Sensor networks, by their very nature, are vulnerable to network security threats, with Denial of Service by radio jamming being one of the most common. It is more useful to detect jamming reactively than to proactively avoid it. We describe a method of countering jamming wherein some nodes just outside the jammed region form groups and transmit their locations to the Base Station for localization. These nodes are downstream in the hop-count based gradient broadcast routing sense. We also estimate the exact position of the jammer using the centre of curvature method. Simulation results show that the error in estimation of the jammer location is as low as 1.03m, accurate enough for real-time response.
Anomalous activities such as flash crowd/event and denial of service (DoS) overload a pool of servers that hosts web contents. This is a great challenge for the 24 by 7 provision of Web contents and may result in interruption of the Web services. Therefore, it is very important that the occurrence of such activities is monitored so that in case of the occurrence of such anomalies, a remedial action can be in placed. In this paper, we introduce a novel technique to model the excessive legitimate or illegitimate requests (to a server farm/cluster) in terms of the concepts and terminology of Network Calculus. Network Calculus deals very well with flow rates (such as the rate of arrival and service patterns) and the capacity of a system. Simulation results show that the model successfully identifies anomalies such as DoS and flash events, and this indication can be used to start the remedial steps.
Recording raw network traffic for long-term periods can be extremely beneficial for a multitude of monitoring and security applications. However, storing all traffic of high volume networks is infeasible even for short-term periods due to the increased storage requirements. Traditional approaches for data reduction like aggregation and sampling either require knowing the traffic features of interest in advance, or reduce the traffic volume by selecting a representative set of packets uniformly over the collecting period. In this work we present RRDtrace, a technique for storing full-payload packets for arbitrary long periods using fixed-size storage. RRDtrace divides time into intervals and retains a larger number of packets for most recent intervals. As traffic ages, an aging daemon is responsible for dynamically reducing its storage space by keeping smaller representative groups of packets, adapting the sampling rate accordingly. We evaluate the accuracy of RRDtrace on inferring the flow size distribution, distribution of traffic among applications, and percentage of malicious population. Our results show that RRDtrace can accurately estimate these properties using the suitable sampling strategy, some of them for arbitrary long time and others only for a recent period.
The rise of attacks and incidents need additional and distinct methods of response. This paper starts a discussion by differentiating the type of operation mode such as Intrusion Detection Systems (IDSs), Intrusion Prevention Systems (IPSs) and Intrusion Response Systems (IRSs). Using characteristics of response and attack time frame, a response model is proposed to distinguish between active and passive response options. The characteristics of response include level of operations, speed and time of response, ability to learn and ability to cooperate with other devices. This paper uses the attack time frame as a response model to show the relationship between active and passive response. Furthermore, the Response Model for Intrusion Response Systems shows some other different approaches and stages of active response. Finally, in order to investigate the most common response used by security practitioner and to justify the response model, studies involving 34 samples products from both commercial and non-commercial are analysed. As a result, this paper shows a clear distinction between the options of responses.
Distributed cache technology is being more and more considered to apply in distributed system to solve the system's data access performance bottleneck. Because there are some defects in distributed cache system such as the data backup strategy, we designed and implemented the partition replication-based distributed cache system. Partition replication method reduced the network data transmission load during replication and improved overall system performance.
Sensor networks can be used to monitor and control pipeline infrastructures. This paper discusses and compares different sensor network architectures that can be used for monitoring underwater pipelines infrastructures. These architectures are underwater wired sensor networks, underwater acoustic wireless sensor networks, RF (Radio Frequency) wireless sensor networks, integrated wired/acoustic wireless sensor network, and integrated wired/RF wireless sensor networks. The reliability characteristics, advantages, and disadvantages among these architectures are discussed and compared. Three reliability factors are used for the discussion and comparison: the network connectivity, the continuity of power supply for the network, and the physical network security.
Anonymity is a fine security characteristic for supplying user identification and key agreement along the user's login operation. Hsu and Chuang recently suggest A Novel User Identification Scheme with Key Distribution Preserving User Anonymity for Distributed Computer Networks. This paper presents a weakness of security that is found in Hsu and Chuang protocol. It is shown that this protocol attacker can freely impersonate the user or service provider. This occurs because an attacker can get the secret key of user (or the service provider) after successful execution of the key generation phase. It is shown a progress to improve and repair the security flaws of the Hsu and Chuang protocol and suggest that the proposed scheme is secure reply attack, impersonation attack and deniable of service(DoS).
Data encryption is important to E-government information safety. This paper introduces the general model of date encryption, points out using different arithmetic according to the degree of cryptograph. After analyzing the disadvantage of the model, it adds the identity validation part to revise the model. Then it explains the procedure of data encryption in E-government, such as how to do identity validation, and how to manage the secret keys.
e-Government refers to the application of information and communication technology (ICT) by the government with the aim of exchanging information and services among different arms of government, and with its citizens and businesses. Through distributing questionnaire survey to departments of Nanchang municipal administration, the author sueveyed the status and issues of e-government in Nanchang city relating to network infrastructure, application systems, data sharing and information safety. The research findings show that the creative construction model let e-government in Nanchang having possession of a good network infrastructure. However, there were some issues that sharing information resources was not enough, most of systems were applied only inside the municipal departments, investment of operation and maintenance were too low and not reasonable investment structure and information safety measures had brought great hidden dangers in e-government. It is conclude that the level of Nanchang e-government is situated between stages of the enhanced and interactive. Finally as for the issues, the author put forwards some countermeasures to promote devement of e-government of Nanchang municipal administration.
It applies artificial immune theory to the intrusion detection model and puts forward a new method different from traditional affinity match method. It provides the dynamic equation of the ripe cell and memory cell, and sets up a kind dynamic match algorithm. The experiment simulation result indicates that adopting this algorithm improves the efficiency and accuracy of measurement, makes it more effective to the discernment rate of the antigen. It has offered a kind of effective scheme for the fact that structure a new generation's high-efficient and rational network security system.
The electronic of government documents transmission and processing has greatly improved working efficiency. However, because the network has openness, connectivity and other features, resulting the network vulnerable to be attacked by hacker and viruses, and also resulting in information of electronic documents disclosure, counterfeiting, tampering, repudiation and many other issues. This paper firstly points out the status of secure transfer of electronic documents under e-government environment and security needs, on this basis, the authors propose the solution to the problem on the secure transfer of documents under e-government environment, building computer and network systems security and hybrid encryption technology, combining an integrated encryption system to ensure the security delivery of electronic document.
This paper creates a new E-Government security framework based on Immune Agent, and it describes a new deployment policy of E-Government network system based on Immune Agent. On the theoretical basis, this thesis elaborates the detailed design and implementation of a particular E-Government network system. To meet the needs of practical security and provide strong protection, the system is developed by adopting the new model and algorithm. It is already been in use.
The network honeypot technology is a new type of proactive defense technique in the network information security. This paper introduces the idea of honeypot in the network to the field of management and explores the concepts and functions of management honeypot. Through the comparison between such existing mature technologies as flexible management and 6 Sigma management and management honeypot, it draws a preliminary conclusion that management honeypot has some comparative advantages. Meanwhile, the paper discusses the application of the ideas of management honeypot to the process in the daily management, new problems management and after-event management. Accordingly, it is assumed that the management honeypot is of significant value in the management activities.
One of the main privacy problems in network communication is lack of anonymity. Much work has been done on this problem in recent years. However, it is a challenge to improve anti-attack ability and degree of anonymity. A new anonymous multiplexing communication protocol, based on IPv6, is proposed in this paper. It utilizes the secret sharing technique to establish multiplexing communication, solves the secret sharing problem between the sender and the last hop, and increases the reliability and security of the overall system. When a sender communications with a recipient, this novel protocol is able to dynamically adjust other safer links to complete the communication, if a small quantity of communication links is attacked. Various evaluations via present attack models demonstrate that the new protocol, able to achieve recipient anonymity when reducing the communication delay, is superior in security and reliability over the traditional anonymous communications.
In recent years, network financial services in china have developed very quickly. However, through empirical analysis, this paper argues that there are still many problems in the development of network financial services of china, such as laws and regulations problem, security problem, network infrastructure problem, etc. To solve these problems, this paper considers that we should formulate and improve relevant laws and regulations, strengthen the network security, speed up the network infrastructure, enhance the innovation of network financial services, and reinforce the supervision.
This paper introduces network security System based on intrusion detection technique. Describe the definition and the classification of intrusion detection introduce the gengric intrusion detection model, Then we design a network security system based on intrusion deception.
Regular expression matching is an important application in the area of network security. In this paper, we analyse the limitation of the existing methods and show the classification of counting constraints patterns. We then present a new representation for deterministic finite automata, called Bitmap Shift Finite Automata (Bs-FA), which introduces condition function and bitmap structure. Bs-FA handles counting constraints patterns effectively and considerably reduces memory space requirement of states by evaluating this method on signature sets used in Snort. Overall, for all signature sets and compression methods evaluated of counting constraints patterns, Bs-FA offers the best memory reduction.
Considering the problem in network construction of colleges and universities and combining the features of VPN technology, this article proposes that VPN can be applied in the network construction. The application can provide low-oschial, efficient and safe network data transmission It can offer reliable functions such as mobile handling official business conges interconnection and interschool exchanges. The article provide a kind of high security network solution based on VPN technology.
To reduce the work load of network administrators, e.g., fault detection/recovery, performance analysis and security maintenance etc., knowledge-based intelligent support system for network administrator based on active information resource (AIR-NMS) have been proposed. In this paper, we describe some design pattern and some implemented examples of activated information resource (AIR) on network administration. A design pattern consists of the classification policy of information resources such as state of network equipments and representation of information utilization knowledge such as expertise and heuristics of administrators. Based on these AIRs, we present a prototype architecture of cooperation among AIRs and integration of functions. The architecture realize the flexible/scalable fault recovery support processing in form of parallel distributed procedure.
There are several weaknesses that make home wireless networks prone to unauthorized access. The weaknesses include misconfiguration, use of default settings, and many others as a result of general lack of user education. This paper proposes a framework for educating home wireless users. It examines the nature of education frameworks and suggests a structural guide for effective education that could be used by information technology trainers and consultants.
The media often reports shocking stories regarding attacks to the networks of well-known companies and organisations with headlines announcing either millions of pounds worth of financial loss or exposing public security issues. The most common attacking methods take advantage of the weakness of operating systems, the bugs found in certain applications such as Internet Explorer or Adobe Reader, or incomplete security deployment. As such end users and network administrators have to cope with increasingly complicated network environments in order to ensure that they are under reasonably safe protection. However, the very nature of a network itself means making information accessible. As a matter of fact, networks are themselves designed to be "open". From a security point of view, the inherent nature of the network is insecure. Due to the complexity of the network nowadays, end users including home and business users have been aware of the importance of having a safer network environment and have taken action to protect themselves by applying the defences provided by computer security software & hardware vendors. Most of this just simply involves deploying off-the-shelf security applications and solutions such as firewalls, IDS, IPS, antivirus, anti-spam and so forth. Here users actually remain oblivious to their specific safety requirements, or they do not possess enough knowledge or sufficient resources to undertake appropriate requirement analysis. Such analysis also involves significant and untenable investment in terms of time, money and labour. Consequently the end result is often a much less safe network environment than required. It is important for end users to comprehend their specific safety requirement, and particularly to have access to an effective assessment of their current network security status so that they are able to proportionally apply the right security solutions needed to improve and enhance their network security. This paper introduces a- - straightforward assessment method which can provide quick, reliable, cost effective, accurate and quantified assessment result suitable for the live application environment by using AHP and risk weight system.
This paper presents a causal assessment model based on Bayesian Belief Networks to analyze and quantify information security risks caused by various threat sources. The proposed model can be applied to a variety of information security evaluation tasks, risk assessment, software development projects, IT products, and other decision making systems. This unique concept can also be used for the determination of joint risk propagation and interdependence structures within computer networks, information systems, and other engineering tasks in general. By this manner, we can facilitate the determination of probabilistic outputs caused by some precalculated input probabilities or by marginal/joint probabilities found so far within the chain of an interdependence structure.
An enhancement of data networks security using a hybrid WDM and SAC/Optical CDMA System is proposed. Secure transmission for dedicated users is achieved by overlaying a covert channel onto a multichannel WDM network. The covert channel is spectrally encoded using spectral amplitude coding (SAC) with unipolar Modified Double Weight (MDW) as the signature address code. OCDMA improves service availability in the presence of physical infrastructure attacks. With the use of WDM channels, the difficulty for an eavesdropper to intercept and decode the secure signal is increased. A simulation is presented as a demonstration of the concept.
With increasing Internet popularity, network security has become a serious problem recently. Therefore, a variety of algorithms have been devoted to this challenge. Genetic Network Programming is a newly developed evolutionary algorithm with directed graph gene structures, which has been applied to data mining for intrusion detection systems and has shown that it provides good performances in intrusion detection. In this paper, a hybrid rule mining algorithm based on Fuzzy GNP and probabilistic classification has been proposed. Hybrid rule mining uses fuzzy class association rule mining algorithm to extract rules with different classes. Then, using different class rules and the classification of data is done probabilistically. The hybrid methods showed excellent results by the simulation experiments.
Modern living is more and more dependent on the intricate web of critical infrastructure systems. The failure or damage of such systems can cause huge disruptions. Traditional design of this web of critical infrastructure systems was based on the principles of functionality and reliability. However, it is increasingly being realized that such design objectives are not sufficient. Threats, disruptions and faults often compromise the network, taking away the benefits of an efficient and reliable design. Thus, traditional network design parameters must be combined with self-healing mechanisms to obtain a resilient design of the network. In this paper, we present RNEDE a resilient network design environment that not only optimizes the network for performance but tolerates fluctuations in its structure that result from external threats and disruptions. The environment evaluates a set of remedial actions to bring a compromised network to an optimal level of functionality. The environment includes a visualizer that enables the network administrator to be aware of the current state of the network and the suggested remedial actions at all times.
Intrusion response models and systems have been recently an active field in the security research. These systems rely on a fine diagnosis to perform and optimize their response. In particular, previous papers focus on balancing the cost of the response with the impact of the attack. In this paper, we present a novel attack response system, based on the assessment of the likelihood of success of attack objectives. First, the ongoing potential attacks are identified, and their success likelihood are calculated dynamically. The success likelihood depends mainly on the progress of the attack and the state of the monitored system. Second, candidate countermeasures are identified, and their effectiveness in reducing the pre-calculated success likelihood are assessed. Finally, the candidate countermeasures are prioritized.
In this work, we design a system that can automatically detect what is critical in data systems based upon the content and context of the information. After this process has been performed, the information it provides can be used for insider threat detection. If a DBMS is used for data access, historical logs are generally kept and our method uses these logs to detect the typical level of criticality of data that each user uses during normal work conditions. If a user suddenly attempts to access data that is much more critical than was typically accessed in the past, this is a potential sign that the insider is acting maliciously. Few attempts at locating critical data exist in the computer security literature and we argue in this work that our novel design fulfills this need in a manner that is extensible and applicable to a wide range of problems. Our results show that our design requires limited computing resources, and with proper training can be very effective at locating critical data and aiding in mitigating insider threats.
This paper introduces a mechanism - UDT-Authentication Option (AO) to secure UDT -a new UDP-based data transport protocol. We describe this mechanism through the introduction of UDT extension to achieve security. We evaluate UDT-AO through the use of existing message authenticity for other protocols such as TCP. We review existing message protection that can act like a signature for UDT segment incorporating information known only to the connection end points. Since UDT is operating on UDP for high speed data transfer, we propose the creation of a new option in UDT that can significantly reduce the danger of attacks on applications running UDT. This can maintain message integrity during data transmissions on high speed networks.
Visual spoofing in Unicode-based text is anticipated as a severe web security problem in the near future as more and more Unicode-based web documents will be used. In this paper, to detect whether a suspicious Unicode character in a word is visual spoofing or not, the context of the suspicious character is utilized by employing a Bayesian framework. Specifically, two contexts are taken into consideration: simple context and general context. Simple context of a suspicious character is the word where the character exists while general context consists of all homoglyphs of the character within Universal Character Set (UCS). Three decision rules are designed and used jointly for convicting a suspicious character. Preliminary evaluations and user study show that the proposed approach can detect Unicode-based visual spoofing with high effectiveness and efficiency.
Recently, the variety and vastness in networks have increased rapidly. To keep networks stable and reliable, network administrators have to understand the nature of network traffic flows. In this paper, we propose a method to analyze network traffic using firewall logs. The characteristics of our method are 1) the use of the aggregate flow information, and 2) the use of cardinality information of aggregate flows. Here, the cardinality information shows the number of servers/clients, and contributes to finding P2P software and Intranet viruses. The experimental results confirm that the session level cardinality information acquired by the proposed method can find P2P software and other types of applications.
A Honeypot is a system that aims to detect and analyze malicious attacks attempted on a network in an interactive manner. Because the primary objective of a honeypot is to detect enemies without being known to them, it is important to hide its existence. However, as several studies have reported, exploiting the unique characteristics of hosts working on a consecutive IP addresses range easily reveals the existence of honeypots. In fact, there exist some anti-honeypot tools that intelligently probe IP address space to locate Internet security sensors including honeypots. In order to tackle this problem, we propose a system called DarkPots, that consists of a large number of virtualized honeypots using unused and nonconsecutive IP addresses in a production network. DarkPots enables us to deploy a large number of honeypots within an active IP space used for a production network; thus detection is difficult using existing probing techniques. In addition, by virtually classifying the unused IP addresses into several groups, DarkPots enables us to perform several monitoring schemes simultaneously. This function is meaningful because we can adopt more than one monitoring schemes and compare their results in an operating network. We design and implement a prototype of DarkPots and empirically evaluate its effectiveness and feasibility by concurrently performing three independent monitoring schemes in a high-speed campus network. The system successfully emulated 7,680 of virtualized honeypots on a backbone link that carries 500 Mbps - 1 Gbps of traffic without affecting legitimate traffic. Our key findings suggest: (1) active and interactive monitoring schemes provide more in-depth insights of malicious attacks, compared to passive monitoring approach in a quantitative way, and (2) randomly distributed allocation of IP addresses has an advantage over the concentrated allocation in that it can collect more information from malwares. These features are crucial in monitoring th-
Malware is code that has malicious intent and is designed for malicious purpose such as stealing confidential data, or obtaining root privileges on a system. The current approach to deal with malware threats such as virus and spyware is to use host based anti-malware software. However, this approach leads to many vulnerable machines since many users don't update their software, their virus signatures, and some even disable their software to avoid the system performance degradation caused by these software. Host based security software require a good deal of administration, with consistent needs for reconfiguration, management, and report analysis. With security administrators supporting an ever growing number of users, such an approach has become impractical. In this paper, we present a novel network based malware detection architecture that uses host security vectors to protect host machines without any intervention from hosts. This architecture provides another layer of security and can complement existing host based solutions. Only central detection server needs to be actively managed instead of individual hosts - hence providing more manageable solution for large IT infrastructures.
The widespread use of Web applications, in conjunction with large number of vulnerabilities, makes them very attractive targets for malicious attackers. The increasing popularity of Web 2.0 applications, such as blogs, wikis, and social sites, makes Web servers even more attractive targets. In this paper we present empirical analysis of attackers activities based on data collected by two high-interaction honeypots which have typical three-tier architectures and include Web 2.0 applications. The contributions of our work include in-depth characterization of different types of malicious activities aimed at Web servers that deploy blog and wiki applications, as well as formal inferential statistical analysis of the malicious Web sessions.
Summary form only given. It is well known that despite all of its advantages the digital revolution also leads to large variety of new risks. One principal issue in this context is the growing dependence of our modern information society from the availability and correct (proved) function of modern communication services. First, I'll give a short overview on threats in communication networks (grids, clouds, etc), protocols and secure personal devices. Then I'll discuss current network security approaches based on anonymous message exchanges within communicating systems. Cryptography was first used to ensure data confidentiality, it has been emocratized?by ensuring the safety of telecommunications services, thereby extending its scope to authentication of a person or device, or a message, non-repudiation, integrity but also the anonymity of transactions. The anonymity is sometimes quite important in the new telecommunication and mobile networks services, much more than just message confidentiality. The talk will focus on some examples and new approaches developed in our research laboratory to deal with anonymity in routing protocols for mobile communicating systems.
Nowadays, web servers are suffering from flash crowds and application layer DDoS attacks that can severely degrade the availability of services. It is difficult to prevent them because they comply with the communication protocol. Peer-to-peer (P2P) networks have been exploited to amplify DDoS attacks, but we believe their available resource, such as distributed storage and network bandwidth, can be used to mitigate both flash crowds and DDoS attacks. In this paper, we propose a server initiated approach to employ deployed P2P networks as distributed web caches, so that the workload directed to web servers can be reduced. In experiments, we use Kad as the particular P2P network for the realization of a large-scale distributed web cache. We performed comprehensive evaluation on the feasibility, efficiency and robustness of our scheme, through experiments and simulations on the prototype we implemented. The evaluation results show that our scheme can increase the capacity of the protected web servers at least 10 times at the same cost of connection and bandwidth consumption. The web contents cached in Kad remain reachable even under churn of peers and targeted DoS attack, and the access latency is comparable to normal direct access to web servers. It also achieves good load balancing under the heavy-tailed distribution of object popularity.
In this paper, we formulate intrusion response problem as a factored Partially Observed Markov Decision Process (POMDP) model. Furthermore, a hierarchical planning algorithm is presented to decompose overall POMDP into some small sub-POMDPs and compute global optimal response policy according to MLS heuristic criterion. Meanwhile, reachable attack intention is defined and used to identify false alerts and compress belief state space. Finally, some experiments were performed to compare proposed algorithm with previous approaches and the results show that our approach have a good performance in response accuracy to different attack scenarios and robustness against false alerts.
This article related to network data transmission technology, bar code technology, serial communications technology, Windows authentication interface technology, data encryption and decryption techniques are studied; the final design and implement a set of ublic Computer Lab Management System.?The system's feature is the use of low cost, good compatibility, management model and has better management efficiency. Thesis on the public computer room of the scientific management, software process of interpretation, computer integrated control, network security of data transmission has a certain reference value.
According to network security problems, a new more initiative distributed network of safety warning model P2DWR2 is developed in this paper, based on the available P2DR model. The key technologies of P2DWR2 model, i.e. deployment of security policy, cooperation of security components etc., were discussed. The new model compensates the insufficiencies of P2DR model, such as low automatic level and requirement of human involvement to complete the security incident response deficiencies. P2DWR2 model provides the policy uniform defines, automatic distribution, self-adaptive management functions and so on. And it makes the core effect of security policy realized.
With the wide usage of internet in many fields, networks are being exposed to many security threats, such as distributed denial of service (DDoS) attack, worm/virus, and so on. So prevention failure of network security leads to the revealing of information or interruption of network services, thereby results in the enormous economic loss. In this paper, we propose an effective method for defense against IP spoofing attack, which is based on traceroute and the cooperation with trusted adjacent nodes. We present the results of the experiment, compare the method with others. The result demonstrates that the method can effectively and steadily detect the IP spoofing attack, thereby blocks it.
In this paper, a novel image cheat-encryption algori-thm based on hyper-chaotic Chen-system is proposed. The hyper-chaos is employed to encrypt plain image into cheat-like image. The cheat-image is transferred in network and easily neglected by attackers. If cheat succeeds, it is impossible to decrypt. When the cheat-image fails to cheat the attackers, security and performance analysis shows the cheat-encryption method still has the same confidentiality as the cover-encryption.
In this paper, we present a periodic authentication scheme of integrity measurements based on TNC architecture. In TNC architecture, server authenticates security posture of endpoint before an endpoint is allowed to access the network. After connected, the endpoint will not be supervised any more. Our scheme is the first to extend this before-access authentication to all-time authentication over endpoints. In fact, we show that by our scheme, the security posture of endpoint would be always monitored therefore the security of network can be improved greatly by cutting off malicious endpoint's connection. We also optimize this scheme to limit its impact on network's performance. Results show that these optimizations make the scheme more efficient and the scheme achieves a desirable effect in practical network.
Security is one of main challenges that obstruct the wide adoption of mobile P2P systems. Current decentralized trust-management research focuses mainly on providing software resource level security schemes by trust models and algorithms, whereas potential merits of hardware-based security mechanisms to further secure application exposure have not been considered so far. This paper presents a new trust management solution towards establishing a security mobile environment for mobile P2P systems. The proposed architecture for developers can be utilized as a foundation for their own secure mobile P2P applications. This unique feature of the proposed architecture is that it combines software resource level security features offered by trust models and algorithms, with the hardware-based security mechanisms offered by Trusted Computing Platform.
This article describes the works Principle and its structure of the Intrusion Prevention System and the Intrusion Detection System. Pointed out the Intrusion Prevention Systems to the traditional advantages and disadvantages of Intrusion Detection System. The shortcomings of current Intrusion Protection Systems are discussed and a number of future researches in this field are suggested.
With the development of Internet, e-commerce, with its advantages of high efficiency and low cost, has become important business mode. But security has always been one of the largest barriers to e-commerce. This paper analyzes the security technology of E-commerce system, and proposes a secure model to improve e-commerce system.
SIP-based instant messaging (IM) is currently a quite hot issue in the SIP world. Two approaches, pager-mode IM and session-mode IM, have been explored for implementation. This paper provides an in-depth analysis of the first method through an example, and makes improvement to the second by presenting specific signaling and media messages. Solutions to security problems are also discussed in detail for the two methods, respectively.
With the increase of network intrusion and hacking, the network security has become a main problem in computer science and attracts much attention from people. To solve these problems, this paper first analysis the advantages and disadvantages of the traditional security technologies in solve these security problem. Next, introduces an intrusion management system, which can make up for these deficiencies. Then it has a further study of the basic principle of the intrusion prevention system.
This paper proposes a new authentication mechanism to meet the demand of security for P2P-SIP network. Based on an improved certificateless signature, it achieves mutual authentication through signal interaction between end to end users. From the analysis, this scheme has high performance for defending fabricating identity, tampering and counterfeiting message. It is efficient to hold back replay attack, man in the middle attack and unauthorized data access.
Key management is challenging in wireless sensor networks (WSNs). Most existing key management schemes mainly consider using symmetric cryptosystems in homogeneous sensor network, but they often do not provide a satisfied storage performance and cause large overhead in establishing shared keys for all pairs of neighbor sensors. In this paper, we propose a key management scheme using for heterogeneous sensor networks (HSNs). Our scheme is based on pairing-based cryptography and has four types of keys according to networks security needs. The scheme reduces the communication cost and key storage spaces of nodes. In fact sensor nodes can agree shared keys without any interaction and no need to store any keys of the other nodes before deployed. Analysis and simulation indicate that the scheme is efficient, and it enables authentication, and robust against different attacks such as replay attack, masquerade attack, Sybil attack etc.
In accordance with the security problems during the process of constructing campus network, combining with the campus network features and the concrete security requirements, an idea of constructing a trusted campus network has been put forwarded. In the light of the present security situations of huge amount of users and difficult identification of all the characters, a feasible schema of constructing a trusted campus network based on certificate authority has been advanced. At last, future work has been addressed.
The internet-based distance education is one of the important ways to establish Lifelong Learning System; the design of network course is an important research topic of it. Based on analyzing present situation and existing problems of network course in China, this paper introduces the finite automata theory, and in order to guarantee the security of information, puts forward a method of using the finite automaton to monitor and filter the text information inputted by using the synchronous or asynchronous communication tools provide by network course. The results show that our method is effective.
This paper provides brief summary of threats and vulnerabilities of VoIP systems and architecture and provides overview of most used methods how to mitigate them. It also includes some interesting projects running nowadays that reflect latest development in this area.
Using communication services like voice services, chat services and web 2.0 technologies (wikis, blogs, etc) are a common part of everyday life in a personal or business context. These communication services typically authenticate participants. Identities identify the communication peer to users of the service or to the service itself. Calling line identification used in the Session Initiation Protocol (SIP) used for Voice over IP (VoIP) is just one example. Also, further mechanisms rely on identities, e.g., white lists defining allowed communication peers. Trusted identities prevent identity spoofing. They are a basic building block for the protection of VoIP communication. However, providing trusted identities in a practical way is still a difficult problem. Identity cards have been introduced by many countries supporting electronic authentication and identification of citizens, e.g., the German lektronischer Personalausweis?(ePA). As many German citizens will possess an ePA soon, it can be used as security token to provide trusted identities. Authentication and identification are important building blocks in the protection of VoIP communication, keying material established during authentication can be used for further protection of the communication. This paper describes how identity cards can be integrated within SIP-based voice over IP telephony to reliably identify users and authenticate participants using as example the German ePA.
Network worm propagation is one of the most interesting and complex topics in the field of network security. The current study of network worm propagation is mainly carried out in the simulation environment since it is unpredictable and harmful to do the experiments in the real world. Simulation of network worm propagation needs to meet the special requirements of the large-scale individual, strong randomness, high concurrency. Compared with a variety of popular simulation software, NetLogo which developed by Northwestern University is chosen as the platform. The Simulation of classic worm propagation model, complex network and worm active/passive dissemination is designed and implemented in this paper. Moreover the evolution process can be configured, controlled and observed. The experimental results show that the simulation is better than the traditional MATLAB simulation.
In this paper, university network security evaluation system based on fuzzy AHP evaluation model, with examples given using the evaluation model for network security assessment procedures and methods. Evaluation results show that the proposed approach is feasible and guidance.
Because of the confidential and important information contained inside the local area network (LAN) of the companies or government departments, the network interior's security problem highlights gradually. On the basis of the fact that firewall and intrusion detection systems can not effectively protect the security in LAN, this paper proposes a multi-Agent network security audit system model, and at the same time, an improved detection algorithm based on information entropy is introduced into the audit system. During the system's running time, the improved algorithm is used to simulate and detect the DDoS attack. The experiment results prove that the multi-Agent system running in LAN can audit part of the intrusion effectively and improve log data analysis intelligence.
To resolve the question: penetrate firewall and accurately identify the remote host operating system, research deeply on the ARP protocol and the TCP/IP protocol stack, provide a mothed about host detection through firewall, describes the TCP port scanning process, design a fingerprint storehouse, TCP bag first part and operating system detecting process, develop the network security assessment system based on this way. Test results show that this system can penetrate firewall and accurately identify the target host operating system.
This article discusses the issue of increasing the security of computer networks based on the constant verification of the level of information security. Simulation of the network, based on accurate data derived from the security audit, allows for constant monitoring of the level of network security. Using hardware virtualization to build models to avoid the restrictions of the programming model and apply the full range of information resources impacts for verification of security level is proposed.
Ensuring security of e-government applications and infrastructures is crucial to maintain trust among stakeholders to store, process and exchange information over the e-government systems. Due to dynamic and continuous threats on e-government information security, policy makers need to perform evaluation on existing information security strategy as to deliver trusted e-government services. This paper presents an information security evaluation framework based on new fuzzy multi criteria decision making (MCDM) to help policy makers conduct comprehensive assessment of e-government security strategy.
In the paper, an optimized data network security system based on 4G is designed. Abiding by the information system safety theory, the paper follows the methods of systems analysis and overall balance, and raises the principles systematically that by study and analyze the network venture of Power Supply Bureau in the process of informatization. And, according to the No.30 order of The State Economic and Trade Committee, the Power Supply Bureau data network is designed. The project it referred is based on the real requirement and technology practice when rebuilding Supply Bureau network, and bring out the guidance effect for generalizing the new unified operation system, planning the network flat and disposing the network safe policy.
With the popularity of the network, the current internet protocol IPv4 has been exposed more and more problems, IPv6 as the new internet protocol arises at the historic moment. The new generation of internet protocol IPv6 serves as the next generation of the internet standard, IPSec will also serve as the network security standard to enforce in IPv6. This paper analysises the satety of IPSec protocol, in the IPSec security architecture, using Authentication Headers protocols AH and Encapsulating Security Payload protocol ESP can successfully carry out security package for packets, the transmission mode and tunnel mode of AH and ESP allow the application methods of IPSec more flexible. This paper make policy configuration for IPSec in IPv6 protoocal, and verified by the network sniffer software the security of IPSec provided.
CAN is a communication protocol largely used in automotive and industrial appliances because of the simple procedure for its parameterization and the low cost of its circuitry. The native CAN, however, does not assure the fault-tolerance level required by safety-critical appliances and somewhat sophisticated protocols have been introduced for their networking. In an attempt to keep on the advantages of CAN, several application protocols have been developed to supplement the native CAN with the aim of giving the CAN networks safety features. This paper continues such a research trend by proposing a CAN application protocol termed time-triggered bus-redundant CAN (TTBR-CAN) that outdoes the performance of the existing ones. After describing the architecture of TTBR-CAN, details are given on its implementation and experimental results are reported to demonstrate its effectiveness.
Up to now, building automation systems were considered as virtually closed environments. If at all, they had to provide some well dedicated dial-in connections for remote management. With the introduction of interconnections to foreign networks (e.g., Web gateways to the Internet) and wireless technologies, the assumption of physical isolation is not longer valid. In parallel, building automation systems of the next generation shall also be home for tight integrated services with seamless interworking nodes of formerly separated systems. When security-critical integration is considered, this promises synergies, but significantly tightens requirements on the protocol stack. This paper summarizes the demands to be met by nodes participating in such an environment and presents necessary secure services that are part of the protocol architecture.
We notice a very strong ll IP?trend characterizing today's communication technology. There is a powerful IP backbone in place, but, in some cases, the ast mile problem?remains to be solved. One of the solutions for this problem is represented by IP communications over satellite. This paper analyses the compatibility between a recently proposed security enhancement for IP communications over satellite DVB and the multicast communications. We discuss the importance of multicast services and we show that the recently proposed security enhancement has built in multicast support, i.e. only a small upgrade in the receivers is needed to allow multicast communications.
On communication networks, deep-packet inspection (DPI) techniques are used to handle various requirements, such as network security, network quality-of-service and so on. DPI is a technique to scan packets to detect specific patterns from those packets and to identify those packets according to the scanning result and the purpose of requirements. String matching is a key element of DPI and is especially used for scanning application specific data of packet payload. In this paper, I propose a string matching architecture to process multiple characters at a single cycle and a technique to speed up the string matching process. The evaluation results show that the memory efficiency and the processing speed can be enhanced by the proposed approach, compared to the previous works.
We investigated the distribution of malware on websites by constructing web honeypots carrying vulnerable web applications. With the diffusion of web services caused by the appearance of a new architecture known as cloud computing, a large number of websites have been used by attackers as hopping sites to attack other websites and user terminals. To construct hopping sites, many attackers force victims to download malware by using vulnerabilities in web applications. To protect websites from these attacks, conventional methods, such as using anti-virus software, filter files from attackers using pattern files, which are generated by analyzing conventional malware files collected by security vendors. However, it is difficult to define malware since software files become malicious depending on the situation. In addition, it is difficult to detect malware, which is different from known malware analyzed by security vendors. Recently, variations in malware continue to increase as new types of malware constantly appear. To reveal the actual situation and critical detection ratio of such conventional methods, we investigated the detection ratio of anti-virus software by using malware collected by web honeypots, which collect attacks on websites by using actual vulnerable web applications. Our investigation revealed that anti-virus software fail to detect many malware files, and that traffic patterns to web honeypots are useful for detecting malware files on websites.
The ESD protection program set up in the electronics manufacturing facilities especially in China region (evident but not limited to that area) appears to have common weaknesses. They have a lot of ESD protection in place but do not know whether this is the right method at the right place. Nevertheless they have enough backup solutions in place so that they can handle ESDS without big problems. With the future limitations expected for on-chip ESD protection, it is necessary to increase their knowledge and to enable them to assess the ESD risks of their production lines in the right way.
Multi-core architectures are commonly used for network applications because the workload is highly parallelizable. Packet scheduling is a critical performance component of these applications and significantly impacts how well they scale. Deep packet inspection (DPI) applications are more complex than most network applications. This makes packet scheduling more difficult, but it can have a larger impact on performance. Also, packet latency and ordering requirements differ depending on whether the DPI application is deployed inline. Therefore, different packet scheduling tradeoffs can be made based on the deployment. In this paper, we evaluate three packet scheduling algorithms with the Protocol Analysis Module (PAM) as our DPI application using network traces acquired from production networks where intrusion prevention systems (IPS) are deployed. One of the packet scheduling algorithms we evaluate is commonly used in production applications; thus, it is useful for comparison. The other two are of our own design. Our results show that packet scheduling based on cache affinity is more important than trying to balance packets. More specifically, for the three network traces we tested, our cache affinity packet scheduler outperformed the other two schedulers increasing throughput by as much as 38%.
The development of Network Intrusion Detection Systems (NIDS) is nowadays a powerful solution to defend against various network security threats. There has been a lot of research effort devoted to hardware-based NIDS, because of (1) the massive amount of computation performed by regular expression matching algorithms and (2) the gigabit per second performance requirement of modern NIDS. Hardware-based NIDS take advantage of parallelization inherent in FPGAs, ASICs or network processors to support very high network speeds, while software approaches fail to do so.
Packet classification is widely used in various network security and operation applications. Two of the main challenges are the increasing number of classification rules, amount of traffic and network line speed. In this poster, we investigate an approximation algorithm for selecting the top-N most frequently matched subset of rules from the original ruleset. Through simulations, we show that our approaches the optimal while runs in seconds, allowing online adaptation to changing traffic patterns.
Regular expression matching is the time-critical operation of many modern intrusion detection systems (IDS). This paper proposes pattern matching algorithm to match regular expression against multigigabit data stream. As usually used regular expressions are only subjectively tested and often generates many false positives/ negatives, proposed algorithm support the possibility to reduce memory requirements by introducing small amount of faults into the pattern matching. Algorithm is based on the perfect hashing and is suitable for hardware implementation.
Many hardware architectures have been designed to accelerate regular expression matching in network security devices, but most of them can achieve high throughput only for strings or small sets of regular expressions. We propose new NFA Split architecture which reduces the amount of consumed FPGA resources in order to match larger set of regular expressions. New algorithm is introduced to find non-collision sets of states and determine part of nondeterministic automaton which can be mapped to the memory based architecture. For all analysed sets of regular expressions, the algorithm was able to find non-collision sets with 67.8%of states in average and reduces the amount of consumed flip-flops to 37.6%and look-up tables to 63.9%in average.
Wireless sensor networks (WSNs) have many applications in home and industrial automation, and the management and integration of WSNs into Internet and IP-based networks is still getting tremendous interesting. This paper presents a micro SOA-model as part of a 4-layered, SOA-based architecture targeting resource-constrained devices with 48 KB of ROM and 10 KB of RAM. The key idea in this model is trying to implement SOA concepts using WSN protocols instead of trying to port the SOA protocols from IT-business world. Our micro model is based on the mIP protocol and it uses the HTTP philosophy instead of the HTTP protocol itself. The middleware layer in our architecture manages the simultaneous access to the WSN and filters the verbose information in a HTTP request to the WSN. It also manages the events and the service description of all devices in the network as well as it provides many other tasks like DNS, firewall, security, and authorization. Furthermore the paper presents a new idea for compressing XML and JSON messages based on a new concept of exchanging a common vocabulary between the networked embedded devices. Our results show that using JSON format instead of XML, or applying our zipping algorithm on them has many advantages in terms of network overhead and power consumption.
Nowadays, many computers have been infected with the computer anomalies or viruses. The availability of network security visualization tools greatly facilitate to detect, perceive and defend computer users from being affected by these anomalies. Many of the network security visualization tools are designed particularly for users with advanced network security awareness even though the tools are indispensable by various types of computer users. We proposed an expert-aware approach to designing a system which formulated with a large amount of network data or high-dimensional data and adaptive for different types of users. In the preliminary phase, we proposed and implemented initial pre-expertise classification system which provides a default setting for the expert-aware network security visualization tool. The tool will learn from continual user feedbacks in order to statistically satisfy the needs of majority tool users. The expert-aware approach looks at the users' expertise level in network security and adapts the visualization views that are best suitable for the user. Initial results of the implementation of the system show that it is capable of representing several of network security data not only on two-dimensional space on a computer but also beyond that space. Systems features, such as system effectiveness and efficiency of data visualization have been improved. Our experiments in a network lab suggest that the tool can be further improved as the tool for distribution to a wide range of computer user.
A firewall's proper functioning is critical to the network it protects. Thus, a firewall should be tested with respect to its intended security policy. We propose a feedback control based approach for test case generation to detect mismatches between firewall's expected and executed behavior. In the proposed approach, abstract test cases are generated from firewall decision diagrams and instantiated with the test input values chosen using the proposed feedback control based selection algorithm. A case study is presented to validate the presented approach.
Recently, DoS (Denial of Service) detection has become more and more important in web security. In this paper, we argue that DoS attack can be taken as continuous data streams, and thus can be detected by using stream data mining methods. More specifically, we propose a new Weighted Ensemble learning model to detect the DoS attacks. The Weighted Ensemble model first trains base classifiers using different data classification algorithms (i.e., decision tree, SVMs, and Naive Bayes) on multiple successive data chunks, and then weights each base classifier according to its prediction accuracy on the up-to-date data. Experimental results on the benchmark KDDCUP'99 dataset demonstrate that our new Weighted Ensemble model is able to successfully detect DoS attacks.
Due to the diversity of network services and the unpredictability of their behaviors, there is an increasing need for tools that can aid in the global management of IP networks. Being able to predict network data can be very useful to anticipate network upgrading decisions or changes on the network functional operation. This paper proposes a practical approach, based on neural networks, that is able to predict network traffic in a specific network link. In order to improve the prediction capabilities of the different neural network models, sliding window and multi-task learning mechanisms are introduced and tested. By applying this prediction framework to different network links, it will be possible to predict the evolution of the global network traffic and use this information for network security, management and planning purposes. The results obtained by applying the proposed model to realistic network scenarios show that this concept can achieve excellent performance in the prediction of the network traffic on the selected links. The prediction is accurate even when there are significant changes in the number of users and their respective profiles. Moreover, the proposed prediction approach is generic and can be used to predict different network data with a very satisfactory accuracy, even with simple and small NN models.
For existing probabilistic marking technologies for IP traceback, such as Probabilistic Packet Marking (PPM), TTL-based Packet Marking (TPM) and Dynamic Probabilistic Packet Marking (DPPM), it is difficult to reconstruct attack path(s) fast and defend against spoofed marks. In this paper, we present Adaptive Probabilistic Marking scheme (APM), where the TTL value of each packet is set to a uniform number at the first-hop router, and each router deduces the distance that each packet has already traveled, and then adaptively marks the packet with the probability inversely proportional to the distance. We theoretically prove that, in APM, the victim requires the fewest packets for a successful traceback, the effect of spoofed marks can be eliminated. NS2 experiments show, in APM, the time for the victim to collect all the obligatory marks for the path reconstruction is reduced by more than 20%compared with existing schemes, and spoofed marks cannot reach the victim.
In this paper, we propose a new approach, derived from the Trusted Computing Group specifications, to bring trust within Virtualized Networks defined in the European project 4WARD. Many players are involved in this type of networks. Some of the actors may compete with each other while they share the same physical networking resources. This paper focuses on the trust deployment in a physical node. A chain of trust is established from the root of trust to high level applications. The root of trust is based on a small piece of software stored on a hardware chip.
Web Services are more and more used in designing and building systems in open and dynamic distributed environments. The security of these transactions is becoming a critical issue. This paper proposes a security testing method for stateful Web Services. We define some specific security rules with the Nomad language. Then, we construct test cases from a symbolic specification and test purposes derived from the previous rules. We present some experimentation results based on roughly 100 Web Services and we show that 11 percent have vulnerabilities, using the rules introduce in the article.
In terms of storage and processing, is widely known that organizations infrastructure is misused, being idle most of the time. Consequently, there is an emerging need for solutions that take advantages of the idle capacity of this infrastructure. In this context, the P2P architecture proves to be an efficient way to make a good use of this capacity by implementing distributed backup systems. This work presents a proposal for an algorithm that can be used in P2P networks to quantify as well as ensures the availability of restoring files anytime in percentage terms, so that any hardware failure does not affect the backup operation. The algorithm proposed improves reliability on P2P network, making possible to validate using percent values the availability of a peer.
We have focused on a particular mechanism of providing network security: firewall technology. Firewalls provide a false sense of security because they have inherent flaws that are continuously exploited by hackers. Current firewalls lack in providing adequate security against insiders. Literature suggests that these limitations arise from the deficiencies in firewall design. This paper presents a model of a firewall called disarming firewall. The model is composed of different components, each serving different purpose. The firewall protects against malicious insiders by limiting the attacking capabilities of each internal host. Knowing that obtaining knowledge of end systems is a precursor of an attack, the firewall hides the identity of OS and server software placed in DMZ from internal as well as external users. Another problem solved by the firewall is the general laziness in applying patches to the software. The auditing system of firewall actively monitors all systems in the perimeter and applies patches as soon as they are released.
Today with the tremendous growth in the era of networking, the services provided to users have become diverse. In such a distributed environment, multiple users can access multiple services simultaneously. It has therefore become essential to provide security to the services being provided along with the identity of the users. Various security protocols have been proposed and used over the years and have evolved into better versions with time. In this paper, we present a simple way to perform symbolic model checking of a widely used security protocol called the Kerberos protocol, mainly a network authentication protocol, using the NuSMV model checker. It also demonstrates how the use of the freshness concept helps to overcome a common security attack called the Replay attack in the protocol.
This paper explores the use of TCP fingerprints for identifying and blocking spammers. Evidence has shown that some bots use custom protocol stacks for tasks such as sending spam. If a receiver could effectively identify the bot TCP fingerprint, connection requests from spam bots could be dropped immediately, thus reducing the amount of spam received and processed by a mail server. Starting from a list of known spammers flagged by a commercial reputation list, we fingerprinted each spammer and found the roughly 90%have only a single known fingerprint typically associated with well known operating system stacks. For the spammers with multiple fingerprints, a particular combination of native/custom protocol stack fingerprints becomes very prominent. This allows us to extract the fingerprint of the custom stack and then use it to detect more bots that were not flagged by the commercial service. We applied our methodology to a trace captured at our regional ISP, and clearly detected bots belonging to the Srizbi botnet.
With the development of science and technology, computer networks have been widely used in political, military, financial and commercial areas of sectors brought to mankind of its great convenience and benefits, the same security of also become increasingly prominent as virus. The emergence of the virus attack and defense of radiation research into information security a top priority. In this paper, information security, computer network problem, a virus attack and defense of the model of radiation and signal processing, in this based on the proposed radiation attacks the virus identification technology and security cable protection radius. In this paper, radiation attacks the virus program, and then conducted a computer network communication protocol analysis, attack radiated from computer virus database.
Skype traffic recognition is a challenging problem due to the encryption and dynamic port number. Accuracy and timely traffic classification is critical in network security monitoring and traffic engineering. In this paper, we propose an online recognition method based on SVM (support vector machine) machine learning method. As the feature set is optimized instead of redundant, our method is able to compute faster and more accuracy. Experimental results on Collage campus data sets show that our method performs better on both speed and efficiency. Moreover, the robustness of our method is demonstrated on the other non-Skype traffic such as MSN (Microsoft Service Network), PPLive (Peer to Peer LIVE) application.
The popularity of using Internet brings some risks of network attacks. The technology of intrusion detection is an important component of network security. The traditional Intrusion Detection System (IDS) generally has the following disadvantages: the first one is that it could not detect the new type attacks even the variation of existed attacks; the second one is the detection time is so long that the real-time capability of the system is low. In this paper we proposed a new model based on LLE and BVM to solve the problems mentioned above. This model is different from traditional IDS, we use BVM classifier to differentiate the normal and abnormal attacks, and we added a pre-process module before the classifier. In the pre-process module, we use LLE to extract the main features of the intrusion data, which is the main part of the pre-process module, and then the principal components of the data are used as input of BVM classifier that differentiates the normal and abnormal actions. Applying this proposed system to KDDCUP99 data, experimental results clearly illustrates that this model has a remarkable performance in detecting both existed instruction and new ones.
In mobile peer-to-peer networks, proposed service discovery protocols disregard the exposure of the participating peers' privacy details (privileged information). In these methods, the participating peers must provide their identities during the service discovery process to be authorized to utilize services. However, a peer may not be willing to reveal its privileged information until it identifies the service providing peer. So these peers face a problem, should the service requesting or the service providing peer reveal the identity first. The protocol presented in solves this problem to some extent and discover the services available in the service requester's vicinity in a single-hop time sync peers only. In this paper, we propose a privacy-preserving model based on challenged/response idea to discover the services available in the mobile peer-to-peer network even when the moving service requester and the service provider are at a multi-hop distance away. The performance studies shows that our protocol does preserve the privacy in a communication efficient way with reduced false positives in comparison to one other recently proposed protocol.
We describe a novel method for over-the-air automated authentication and verification of machine-to-machine (M2M) wireless sensor networks using the existing authentication assets of a cellular telecom operator. We extend the standard Generic Bootstrapping Architecture (GBA) provided in the 3GPP specifications to implement our solution with minimal additional hardware and software requirements.
A Home Network, or Customer Premises Network (CPN), is a special case of a local area network. It is used for communication between a small number of devices deployed within a home or a small office: computers and printers, Set-Top Boxes, IP-phones and other connectivity gears (e.g. wireless access points). The key element of the CPN is its interconnection, through a (ultra) broadband provider, to an external Next Generation Network (NGN) or public networks such as the Internet. Many Standard Development Organizations (SDO) have addressed the security of the NGN and IMS, whereas the security of the CPN and its interconnection with the NGN is still in its infancy. This paper is aimed to describe the security issues that Service Providers and Operators have to face interconnecting ecure?core NGN to the customer's home networks or CPN and to present the main countermeasures defined by ETSI Tispan.
Mobile ad-hoc networks (MANET) has highly dynamic topology, open access of wireless channel and unpredictable behaviors; however, absence of effective security mechanism render MANET more vulnerable to positive attacks. Conventional assessments always require large sample data satisfy specific distribution and establish models through subjective recognition, thus lack common applicability, objectivity and reliability. To solve this problem and make accurate assessment, we propose RAPCA-PP model on basis of Projection Pursuit theory to realize both risk assessment and attributes analysis. Due to Projection Pursuit's theoretical merits, RAPCA-PP is thoroughly data-driven; it can be applied to conditions with small sample quantity, incomplete data and no-prior experience. Using RAGA for solution, RAPCA-PP shows well convergence. Compared with Grey Relations Projection, it demonstrates both better accuracy and higher discrimination. Moreover, our model can analysis attributes by importance and eliminate redundancy. Experiment shows that assessment with eliminated attributes can also correctly reflect each node's performance. RAPCA-PP proved to be suitable for real MANET working scenarios.
Cloud computing brings opportunities for network forensics tracing Internet criminals in the distributed environment. We may use the new ay-as-you-go?model of the cloud computing to deploy the on-demand cyber surveillance sentinels and conduct distributed trace back in complicated cyber crime scene investigations. To trace criminals abusing anonymous communication networks such as Tor, law enforcement can deploy high-bandwidth Amazon EC2 sentinels into the Tor network. Some sentinels are configured as Tor entry guards and others work as Tor exits nodes. With the high bandwidth and appropriate number of such sentinels, we can achieve a required probability that a Tor circuit passes through an entry sentinel and an exit sentinel in order to capture the suspects. The proposed ay-as-you-go?traceback model is cost-effective since the investigation may last for just hours with effective traceback techniques. Our experiments demonstrate the feasibility of this new traceback strategy through the cloud.
Two-way Time-of-Arrival (TOA) distance-ranging is well-suited for use in IEEE 802.11 MANETs and wireless mesh networks because it is simple, efficient and does not require precise time synchronization between network stations. Despite its utility we show that this distance-ranging procedure is completely insecure and demonstrate how it can be subverted by a simple but highly effective attack. This attack allows the adversary comprehensive and fine-grained control over the distance reported by the procedure. Such adversaries can appear to be either much further away or much closer than they are in reality. We demonstrate the attack experimentally and also show how it can be implemented using ordinary wireless network interfaces. Finally, the necessary and sufficient conditions for the secure use of two-way TOA distance-ranging procedure in IEEE 802.11 wireless networks are identified.
Mobile ad hoc network is a new multi-hop wireless communication network. It has wide application, but it is exposed to kinds of security threats. Key management is vital important in ad hoc network security research field. Firstly the related work about key management in mobile ad hoc network is analyzed, and then an algorithm is proposed based on identity-based generalized signcryption. Lastly a suitable key management scheme is put forward based on generalized signcryption and threshold cryptograph. This scheme not only saves the user's storage, but also decreases the communication and computation overhead because of generalized signcryption. The detailed key generation and key update schemes ensure the scheme's availability.
Radio waves are the medium used by sensors to communicate and exchange data. The unconstrained accessibility to any information carried over this medium is a security issue in many sensor-based applications. Ensuring protected wireless communications is a problem that has received a lot of attention in the context of ad hoc networks. However, due to hardware constraints of sensors along with multi-hop communication, most of these solutions turn out to be useless for sensor networks. This paper provides basic building blocks to establish secure communication by exchanging secret keys between neighbor nodes without any use of cryptography methods allowing an gain in efficiency. This paper also proposes a second algorithm that extends the secret key establishment to nodes that are not direct neighbors. Among the interesting features of the proposed algorithms we can note a low overhead and the absence of initial configuration.
Stainless steel hot strip line integrated with mechanical electric hydraulic equipments is a complicated system. Automatic control system is very important for improving mill efficiency and products quality. The characteristics of hot strip line and its requests for automation, the idea of control system design, system structure and configuration are explained in detail. TDC(Technology drive control) system is the latest multiprocessor control system, and it is very applicable to real time multitask and high speed calculation system such as hot strip rolling. The hot strip rolling control system based on TDC is developed by ourselves. Multilevel system, high speed fabric Ethernet between levels, high speed global data memory network between controllers, high speed Profibus_DP network between controller and equipments, hot machine backup strategy of L2, fabric Ethernet redundancy of L1 and other innovative design ideas, L2 main mathematic models, L1 key control models are all applied in the system. Automatic control system designed according to the method has been applied on several hot strip mill lines.
The negative impact of network partitions on the availability of key management services in Wireless Local Area Network (WLAN) is alleviated by a robust key management scheme based on the distributed certificate authority (DCA). Auxiliary shares are used to increase the redundancy of shares in the networks thereby ensuring robustness and security at the same time. Simulations show that the average failure ratio for DCA requests is reduced by more than 50%. A security analysis and simulations show that the approach ensures system robustness and thwarts common attacks.
For end-to-end encryption and special communication protocol-based Skype system, it is not feasible to recognize the Skype protocol by using traditional port-based or payload-based identification methods. By detailed analysis of Skype protocol, this paper explores the general characteristics and special behavior characteristics under certain circumstances of Skype application and proposes a connection tracking-based strategy that can efficiently recognize Skype application and in further step block it. The experimental result indicates that the proposed method can efficiently block the current popular version of the Skype application, thus, achieve the high security of network.
In view of the growing number of network security threats and the current intrusion detection system is often helpless in the face of some attacks, this paper analyzes and compares the traditional network security technology and network intrusion detection system's theories, gives a new design based on the distributed intelligent network intrusion detection system. The new system includes four modules: control center, mobile agent, knowledge base and honeypot deploy module. The system uses mobile agent for efficient intrusion detection, and integrates honeypot technology to update the knowledge base and manage the system resources dynamically. It possesses a certain flexibility and security.
In order to solve the problem that algorithm SVM (Support Vector Machine) is very slowly for intrusion detection systems, a novel algorithm based on SVM divided up by clusters was proposed. In the method, Training set is divided into many subsets by clustering algorithm, and these subsets are classified by the decision function SVM. Detection Experiments with the algorithm on intrusion detection data were completed, the results show that the method can find the intrusion actions quickly with a high precision.
This paper firstly presents a review of the goals of the security of e-government system, and meanwhile analyst the risk. Then, the paper discusses the measures of improving the security from managerial perspectives. At the end part, the paper propose that to effectively lower the trade cost, government should grade the security levels, and make more openness to the public.
Due to unreliable wireless media, dynamic network topology and lack of infrastructure of mobile ad hoc network, it is very vulnerable to security threats and can be attacked more easily. Mobile ad hoc network security is becoming the problem which restricted it to be applied in actual world. The infrastructure-less nature and network dynamics of mobile ad hoc networks make the conventional PKI (Public Key Infrastructure) less suitable. In this paper, we present these challenges in detail, identify the requirements for such solutions, and propose an improved PKI solution for mobile ad hoc networks.
The application and research of XML technology are mature, however, object-oriented XML technology is under development, and now the object-oriented XML research is concentrated on database. In this paper, we will study the application of object-oriented XML and its primary technology Extended DTD. First of all, analyze the internal mechanism of using Extended DTD to realize the object-oriented features of XML; then discuss how to use object-oriented XML in the network security management platform to handle network security event data and make it more flexible; finally, on basis of that, study the implementation of an Extended DTD parser which specific to network security event data.
The current network security technologies are mostly passive defense technology, but intrusion detection technology as an active and dynamic network security defense is a new direction in the development of network security technology and it is also a necessary complement to passive defense technology. In this paper, sparse neural network is applied to the field of intrusion detection. Sparse neural network simulates connected structure of the human brain, so it have advantages in shortening computing time, improving generalization ability and even reducing hardware implementation difficulty. Compared with intrusion detection method based on BP neural network, the detection rate by using intrusion detection method based on sparse neural network is higher.
In this paper, we introduce an Intrusion Detection system (IDS) based Hybrid Evolutionary Neural Network (HENN). A brief overview of IDS, genetic algorithm, and related detection techniques are discussed. The system architecture is also introduced. Factors affecting the genetic algorithm are addressed in detail. Unlike other implementations of IDS, Input features, network structure and connection weights are evolved using genetic algorithm in HENN. This is helpful for identification of complex anomalous behaviors. Experimental results show that the proposed IDS can efficiently improve the detection rate and correctness rate.
Public security system is a special system that should aim at high speed, reliableness and comprehensiveness, fully integrated by video conferencing, data collaboration, multi-channel video surveillance and other operations. As a result, the public security system design must take full account of the industry users' individual character and special function requirements. In this paper, based on the existing network security system and SIP protocol, a video conferencing system is designed and implemented, which can help SIP further study and application in public security system effectivelly.
To improve the network active defense ability with intelligence defense, prevention, and authentication methods in campus network. the concept of intelligent active defense system is proposed based on web access control and 802.1X admission control, the two technologies have their own disadvantages whatever web technology or 802.1X access control, but if we develop their own advantages of web and 802.1X , we will get greater convenience not only easy to deploy but also improve network security. after the active defense system architecture is given and its functional module and authentication flowchart is described in detail, some methods and suggestions are provided for the application deployment and specific problems on network security management.
More than 90 independent honeypots have observed malware traffic at the Japanese tier-1 backbone. Typical attacks were made by multiple servers, coordinating to send many kinds of malware. This paper aims to discover some frequent new sequential attack patterns of malware. It is not easy to identify particular patterns logs of one year because the volume of dataset is too large to investigate one by one. To overcome the problem, this paper proposes data mining algorithm, the PrefixSpan method. We implement the PrefixSpan algorithm to analyze the malware footprints and show the experimental result. The result of analysis shows that the attacks are performed by multiple sequential attack patterns within a short amount of time.
Dark net monitoring is an effective method to analyze malicious activities on networks including the Internet. Since there is no legitimate host on darknets, traffic sent to such a space is considered to be malicious. There are two major issues for dark net monitoring: how to prepare unused address space and how to configure network sensors deployed on the network. Preparation of monitoring addresses is difficult, and it have not been obvious yet what an appropriate configuration is. To solve the first issue, we proposed a method for network monitoring by exploiting unused IP addresses on segments managed by DHCP server, where is a real-operated network. By assigning these addresses, we can easily obtain IP addresses for monitoring and enable network monitoring on production network. Furthermore, we conducted real dark net monitoring experiments and clarified what kind of information could be obtained. We deployed several types of sensors on real-operated network and captured dark net traffic. After analyzing the traffic, we compared the data between each sensor. We found that there were dramatic differences between the data collected by each sensor and our proposed method was useful for real network monitoring.
The design of an efficient collaborative multi-robot framework that ensures the autonomy and the individual requirements of the involved robots is a very challenging task. This requires designing an efficient platform for inter-robot communication. P2P is a good approach to achieve this goal. P2P aims at making the communication ubiquitous thereby crossing the communication boundary and has many attractive features to use it as a platform for collaborative multi-robot environments. In this work, we presented the JXTA Overlay P2P system and implemented P2P robot control system for biped walking robot. Since JXTA-Overlay is able to overcome Firewalls, Routers and NATs, it is possible to control end-devices in a WAN without changing the network security policy. We used JXTA-Overlay for the control of biped working robot and evaluated the proposed system by many experiments. Experimental results have shown that the proposed system has a good performance and can be used successfully for the control of biped robot.
Due to the increasing threat of attacks and malicious activities, the use of firewall technology is an important milestone toward making networks of any complexity and size secure. Unfortunately, the inherent difficulties in designing and managing firewall policies within the modern highly distributed, dynamic and heterogeneous environments might greatly limit the effectiveness of firewall security. It is therefore desirable to automate as much as possible the firewall configuration process. Accordingly, this work presents a new more active and scalable fire walling architecture based on dynamic and adaptive policy management facilities, thus enabling the automatic generation of new rules and policies, to ensure a timely response in detecting unusual traffic activity and identify unknown potential attacks (0day). The proposed scheme, structured in a multi-stage modular fashion, can be easily applied in a distributed security environment, and does not depend on specific security solutions or hardware/software packages.
The following topics are dealt with: broadband, wireless computing, communication and applications; ad hoc networks; sensor networks; parallel and multi-core systems; mesh networks; practical security and privacy application; optical networks; networks algorithms and protocols; distributed schemes and protocols; wireless networks; disaster information systems; network security, privacy and trust; distributed algorithms and systems; multimedia and Web applications; wireless communication; digital signature; computer forensics; authentication and privacy; cloud and wireless security; image secret, sensor network and computer security; fault detection and diagnosis; intelligent networks; human behavior understanding; image and video analysis; scattering and diffraction; wireless modelling; wireless technologies; wireless traffic control and resource allocation; intelligent algorithms; network performance analysis; network services; teleoperation and robot control; robotics; agent technology and multiagent systems; symbolic computing; and wireless sensor networks protocols.
The current ubiquity of information coupled with the reliance on such data by businesses has led to a great deal of resources being deployed to ensure the security of this information. Threats can come from a number of sources and the dangers from those insiders closest to the source have increased significantly recently. This paper focuses on techniques used to identify and manage threats as well as the measures that every organisation should consider to put into action. A novel game-based onion skin model has been proposed, combining techniques used in theory-based and hardware-based hardening strategies.
In recent years, distributed systems are connected by VPN (Virtual Private Network) through the Internet, and construct complicated information systems. These information systems bring benefit and security risks to many users. Representative security risks, vulnerabilities are closely related to application software installed in information systems. If a malicious adversary identifies the application software, he can seek the vulnerabilities easily. Thus, to ensure security of information systems, it is necessary to conceal application software installed in information systems. On the other hand, some attempts have been proposed to identify application software or protocol without scanning the payload. These proposed methods can analyze encrypted traffic, because the methods scan traffic patterns such as packet sizes and transmission intervals. While there are some legitimate uses for encrypted traffic analysis, these methods also raise problems about the confidentiality of encrypted traffic. Many researchers proposed countermeasures against traffic analysis to ensure anonymity in a public network. They indicated how to alter traffic patterns in the main. However, a few researcher indicated how to implement the method. Indeed, though previous VPN applications protect payloads against an eavesdropper, do not conceal side channel information including traffic patterns. Our work applies these proposed countermeasures and shows how to implement a secure VPN application that conceals traffic patterns. To alter traffic patterns, it is necessary to control packet sizes. Many popular application based VPN encapsulates packets by TCP or UDP. However, TCP cannot control packet sizes strictly. Though UDP can control packet sizes without difficulty, does not ensure reliable data transmission. A secure application based VPN requires a protocol that can control packet sizes strictly and can ensure reliable data transmission in untrusted networks. SCTP (Stream Control Transmission-
Advances in wireless communications along with improvements in hardware have allowed smart phones to interact with computers in an open environment where virus can diffuse. While many studies have modeled the spread of malware, little has been done to take into account different types of devices that may exist in an ad hoc wireless network. In this paper, we develop a stochastic model that incorporates diversity of entity as well as interactions between different classes of network items understand to their impacts on the spread of a virus propagation. These models describe nodes in a network through states from susceptible to infected and recovered. The proposed model is able to depict the variations and dynamics of malware in a heterogeneous wireless ad hoc network. We have also conducted numerical simulations to understand changes and the equilibrium of a network under malware dissemination.
This paper discusses the often overlooked issues and key vulnerabilities evident in Web facing technologies. The process of uncovering these issues and vulnerabilities is known as footprinting. We describe how organisations leak key information from their Web facing systems. Essentially the paper describes how individuals may target an organisation's Internet systems using general purpose tools and techniques to obtain a digital footprint of that organisation and its system security posture. In particular, the paper highlights that footprinting is often a precursor to an attempted breach of the system perimeter by individuals with ulterior motives. Footprints are regarded as the first stage of a system compromise conducted by individuals wishing to escalate key system privileges with a view to exploiting known system vulnerabilities. The paper also states that footprinting is an activity that is often overlooked by administrators when hardening their systems and security profile. As a consequence of this we highlight the need for system administrators to have an awareness of the nature and type of footprints that their web facing systems provide the external environment and the consequences of failing to adequately control information leakage. The paper concludes by highlighting the need for individuals in various roles within an organization to be able to clearly identify and stem information leakage from their web facing systems in order to minimize the impact on systems' security and vulnerability.
In the literature, the notion of Race Condition deals with the interference between two processes A and B carrying out three interactions involving a shared object. The second interaction of the concurrent process B interleaves with the first and the third interactions of process A. Preventing Race Conditions attacks between concurrent processes is still an open problem. Many limitations remain such as preventing only Race Conditions on a file system or controlling only direct interactions with the shared context. That paper covers those various problems. First, it gives a formal definition of direct and indirect information flows at the scale of a complete operating system. Second, it proposes a general formalization of Race Conditions using those information flows. In contrast with existing formalizations, our definition is very effective and can be implemented on any operating system. Third, it provides a Mandatory Access Control that enables to prevent general Race Conditions at the scale of a whole Linux operating system. The Race Conditions can be easily expressed as a Security Properties policy. A honeypot experimentation provides a large scale evaluation of our dynamic MAC enforcement. It shows the efficiency to prevent both direct and indirect Race Conditions. Performances are encouraging us to follow our approach of a dynamic MAC for enforcing a larger range of security properties.
Computer software is typically developed according to software engineering methodologies. However, with the introduction of the Internet and the World Wide Web, protecting data has become a topic of importance. In order to protect data from hackers and saboteurs in a global society where e-commerce, e-business, and e-sharing are the orm? professionals should have sound knowledge in methods to protect data. Consequently, the area of information assurance (IA) has become one of great significance and it is important that the next generation of technologists are trained in development techniques that can ensure the confidentially and integrity of information. Traditionally, courses in secure software development are offered at the graduate level or in a stand-alone software security course at the undergraduate level. The aim of this paper is to present a paradigm for introducing software security to undergraduates in a traditionally taught software engineering course. The paper also presents challenges and future work.
In this paper, we introduce a method and a tool for systematically assessing the security of complex systems. We gather data from interviews, network documentation as well as active and passive network measurements and combine them in a semantic model with our tool, Graphingwiki. We tested our methodology on an active large-scale VoIP system. The multifaceted data gathering and analysis method was fast and extensive and proved to be effective in finding the weaknesses of the system. The method provided a repeatable method of measuring and ensuring the security of the system. The largest benefit of the method is in quick and efficient data collection from several, differing, data sources and the resulting analyses of the combined data.
Network protocols do not solely rely on the lower layers of the stack for security. Many of these protocols rely on a combination of layers to develop a more secure channel for data transmission. UDT, like other new next generation high speed protocols, relies on Transport, on IP, and on Network layers for data delivery and protection. However, like other existing protocols, UDT has a socket interface to link with API, a feature that makes it flexible in its implementation. This paper contends that UDT's user interface provides a means of protection by using another application service interface to cater for its security requirements. By implementing GSS-API, UDT can achieve authentication, confidentiality, and integrity during data transmission. This work, provides another way of securing high speed network protocols such as UDT when implemented in various network environments.
Botnets have become one of the most prevailing threats to today's Internet partly due to the underlying economic incentives of operating one. Botnet toolkits sold by their authors allow any layman to generate his/her own customized botnet and become a botmaster; botnet services sold by botmasters allow any criminal to steal identities and credit card information; finally, such stolen credentials are sold to end-users to make unauthorized transactions. Many existing botnet countermeasures meet inherent difficulties when they choose to target the botmasters or authors of toolkits, because those at the highest levels of this food chain are also the most technology-savvy and elusive. In this paper, we propose a different, bottom-up approach. That is, we defame botnet toolkits through discouraging or prosecuting the end-users of the stolen credentials. To make the concept concrete, we present a case study of applying the approach to a popular botnet toolkit, Zeus, with two methodologies, namely, reverse engineering and behavioural analysis.
Today's highly dynamic, distributed usage environments, need mechanisms to protect network resources against unauthorized access and unauthorized use. Current contributions in this area follow two main directions: a) users are known and thus service is granted on the basis of user's identifications, user's role, etc. b) users are unknown and thus service is granted on the basis of risk evaluation, caused by granting such a service. We deal with a case, where we do not care about the users' identifications, but rather with local operating conditions of the services. We suggest the design of an architecture for the usage control of SOAP-based services, with dynamic usage restrictions specified only by a dedicated security module involved in the architecture. We present a case study to illustrate the work.
Online social networks are used by more and more people. While they enable rich communication, posting information on such networks can degrade one's privacy and anonymity. A metric based on probability and entropy has been developed for measuring the degree of information revelation caused by posting to social networks. It can be used to measure the loss of privacy as well as the loss of anonymity.
With the proliferation of P2P application, it is critical to consider how these systems can be run in a trust environment. We present a lightweight time-window based effective dynamic trust mechanism(Tw-Trust) considering the peer's local and global trustworthiness. Simulation results show that Tw-Trust has the advantages in countering strategic altering behavior and dishonest feedbacks of malicious peers.
Today VoIP (Voice over IP) and video conference are very common in internet. But the other side, network security is more complicated than before, our network infrastructure set firewalls become a basic network security equipments. Not only enterprises will set up firewall system, most of SOHOs and individual users also set at least one firewall in their network. In order to allow video conference environment normally operates in a firewall environment, IETF and the video conference industry have already propose a lot of solution. This article is going to present some common frameworks to measure and analysis video conference normally working under the DDoS attack just only using a network firewall.
Installation of automatic monitoring system for engineering applications is wide range of technology, through the installation of automatic monitoring system in dangerous places or remote areas. To ensure the safety and efficiency of monitoring, using the wireless network timing of return measurement data results. For monitoring and related security personnel can focus on measurement data, making timely and effective decision. This paper studies the use NS2 ( Network Simulator 2) simulation, the wireless sensor network to collect the file transfer to the gateway node. At the same time as the simulation node cause a denial of service attack by external damage nodes, damage on the node after the routing conditions and under attack by the different queue management, Simulation and analysis of the queue in the attack packet loss under normal conditions.
Vulnerabilities do enormous harm to personal computers. What is worse, with emergence of much more applications of computer networks nowadays, network security caused by vulnerabilities has become an important issue. And attacks turned more proficient, making it much more difficult to protect our computer networks. In view of the security issue of complex network, effective identification of attack paths contributes to network security defense a lot. In order to obtain the paths, attack graphs are used. And to construct it requires preconditions like vulnerabilities information, network configuration, and etc. In this paper, we present a simplified model to construct the attack paths. We discuss the construction of attack paths taking advantage of logic programming and corresponding preconditions generated by the logic rule and facts.
Distributed Denial of Service (DDoS) attacks pose an increasing threat to the current internet. The detection of such attacks plays an important role in maintaining the security of networks. In this paper, we propose a novel adaptive clustering method combined with feature ranking for DDoS attacks detection. First, based on the analysis of network traffic, preliminary variables are selected. Second, the Modified Global K-means algorithm (MGKM) is used as the basic incremental clustering algorithm to identify the cluster structure of the target data. Third, the linear correlation coefficient is used for feature ranking. Lastly, the feature ranking result is used to inform and recalculate the clusters. This adaptive process can make worthwhile adjustments to the working feature vector according to different patterns of DDoS attacks, and can improve the quality of the clusters and the effectiveness of the clustering algorithm. The experimental results demonstrate that our method is effective and adaptive in detecting the separate phases of DDoS attacks.
This paper investigates distributed denial of service attacks using non-address-spoofing flood (NASF) over mobile ad hoc networks (MANET). Detection features based on statistical analysis of IDS log files and flow rate information are proposed. Detection of NASF attack is evaluated using three metrics, including detection ratio, detection time and false detection rate. Thus, the proposed framework address important issues in forensic science to identify what and when does the attack occur. Different NASF attack patterns with different network throughput degradations are simulated and examined in this paper.
The paper examines the advice and support provided by seven major Internet Service Providers in Australia through late 2009 and early 2010 in relation to computer and network security. Previous research has indicated that many end-users will attempt to utilise the support provided by Internet Service Providers as a simple and effective method by which to obtain key information in regards to computer security. This paper demonstrates that in many cases the individuals working at the help desk are either reluctant to provide IT security support or have insufficient skill to provide the correct information.
Elliptic curve cryptography (ECC) provides solid potential for wireless sensor network security due to its small key size and its high security strength. However, there is a need to reduce key calculation time to satisfy the full range of potential applications, in particular those involving wireless sensor networks (WSN). Scalar multiplication operation in elliptical curve cryptography accounts for 80%of key calculation time on wireless sensor network motes. In this paper, two major contributions are made: (a) we propose an algorithm based on 1's complement subtraction to represent scalar in scalar multiplication which offer less Hamming weight and will significantly improve the computational efficiency of scalar multiplication; and (b) we present a fuzzy controller for dynamic window sizing to allow the program to run under optimum conditions by allocating available RAM and ROM at the sensor node within a wireless sensor network. The simulation results showed that the average calculation time decreased by approximately 15%in comparison to traditional algorithms in an ECC wireless sensor network.
The following topics are dealt with: network security, cyber security and knowledge engineering.
Users' anonymity and privacy are among the major concerns of today's Internet. Anonymizing networks are then poised to become an important service to support anonymous-driven Internet communications and consequently enhance users' privacy protection. Indeed, Tor an example of anonymizing networks based on onion routing concept attracts more and more volunteers, and is now popular among dozens of thousands of Internet users. Surprisingly, very few researches shed light on such an anonymizing network. Beyond providing global statistics on the typical usage of Tor in the wild, we show that Tor is actually being is-used, as most of the observed traffic belongs to P2P applications. In particular, we quantify the BitTorrent traffic and show that the load of the latter on the Tor network is underestimated because of encrypted BitTorrent traffic (that can go unnoticed). Furthermore, this paper provides a deep analysis of both the HTTP and BitTorrent protocols giving a complete overview of their usage. We do not only report such usage in terms of traffic size and number of connections but also depict how users behave on top of Tor. We also show that Tor usage is now diverted from the onion routing concept and that Tor exit nodes are frequently used as 1-hop SOCKS proxies, through a so-called tunneling technique. We provide an efficient method allowing an exit node to detect such an abnormal usage. Finally, we report our experience in effectively crawling bridge nodes, supposedly revealed sparingly in Tor.
The process of analyzing available network forensics evidence to determine their meaning and significance can be very involved. It is often necessary to develop a timeline of significant events to obtain an overview of what occurred, to create relational diagrams showing which users are connected to which systems, or to correlate and analyze data to find noteworthy patterns of network traffic. However, there is a lack of statistical analysis of network traffic for security incident determination, especially the Denial of Service (DoS) attack in mobile ad hoc network (MANET). In this work, we focus on the "analysis" part of network forensic investigation. Specifically, we study one type of DoS attack, called distributed DoS (DDoS) flooding attack in MANET. We present a quantitative model to characterizes this attack and its traffic statistics. We also propose an analytical model for looking for specific patterns of the attack traffic, aiming to achieve: (1) Determine if there is an anomaly in the traffic and whether the anomaly is the DDoS attack (2) Determine the time when the attack is launched.
Privacy and security solutions require the protection of personal information so that it may not be disclosed to unauthorized participant for illegal purposes. It is a challenge to address these issues in networks with strong constraints such as Ad Hoc network. The security increase is often obtained with a quality of service (QoS) decrease. We propose in this paper a solution that provides at the anonymity, security to Ad Hoc network with a limited impact on QoS. This method could be efficient against some viral attacks. We also give some security proofs of our solution for Ad Hoc networks.
The following topics are dealt with: computational aspects of social networks; swarm-based computing; social network security; fast logic computation; middleware and component technology; smart home; complex networks and granular computing; intelligent systems; pattern analysis and optimization; business management applications; network control and intelligent computation; synergy between AI and brain science; image processing/pattern recognition; knowledge management & technical innovation; neural networks and connectionist models; evolutionary algorithm; fuzzy logic; ontologies and multi-valued logic in semantic networks; multimedia information processing; network based system design; and wireless sensor network and Internet technique.
The design of an efficient collaborative multirobot framework that ensures the autonomy and the individual requirements of the involved robots is a very challenging task. This requires designing an efficient platform for inter-robot communication. P2P is a good approach to achieve this goal. P2P aims at making the communication ubiquitous thereby crossing the communication boundary and has many attractive features to use it as a platform for collaborative multi-robot environments. In this work, we present the JXTA Overlay P2P system and its application for robot control. Since JXTA-Overlay is able to overcome Firewalls, Routers and NATs, it is possible to control end-devices in a WAN without changing the network security policy. We used JXTA-Overlay for the control of robot motors. We evaluated the proposed system by many experiments and have shown that the proposed system has a good performance and can be used successfully for robot control.
Cyber security continues to be an increasingly important topic when considering Homeland Security issues. This area however is often overlooked during a disaster or emergency situation. Emergency management within the US as it currently stands lacks any real cyber situational awareness with respect to the core activities of emergency management such as mitigation, preparedness, response and recovery. As a result critical cyber-infrastructure resources that emergency management personnel rely on is left on the sideline when planning, handling, and recovering from emergencies or natural disasters. As emergency management evolves within the US to handle dynamic man-made, and natural disasters such as terrorist attacks, hurricanes, and floods, these issues must be addressed to mitigate risks. This paper takes the first step in examining the issue of cyber situational awareness within emergency management and identifies several concerns for the emergency management community.
Nation's network infrastructure such as the Global Information Grid (GIG) for the Department of Defense (DoD) and the OneNet for the Homeland Security Department are tran-sitioning to the Internet Protocol version 6 (IPv6) per DoD CIO Memorandum of June 2003 and the Office of Management and Budget memorandum OMB-05-22. There exist IPv6 specific security vulnerabilities in these network infrastructures that need to be mitigated in order to achieve security parity with the existing IPv4 operations. From the perspective of the Homeland Security technologies, the existence of additional security vulnerabilities implies a possibility for two pronged threats. First, the IPv6 specific vulnerabilities reduce the security posture of the network infrastructure itself; second, other critical infrastructure sectors that depend on IPv6 need additional protection. For example, the future supervisory control and data acquisition (SCADA) industrial capabilities would increasingly use the IPv6 infrastructure, as would the voice communications, the voice and video collaboration, and sharing of data such as the image data and surveillance and reconnaissance data. This paper presents three contiguous results. First, it briefly presents the new IPv6 capabilities; second, it presents a brief analysis of the security vulnerabilities arising from these capabilities; and third, it presents a new security model for IPv6 network infrastructures that has the potential to mitigate these vulnerabilities. The new model is based on the end-to-end connectivity that is restored in IPv6, thus allowing the use of host based security (HBS) systems together with the perimeter security devices. However, the use of HBS complicates the security trust management. Therefore the third component of the model is introduced, namely a policy based security management (PBSM) approach. The PBSM approach allows the secure deployment of the host based security systems. It provides the capabilities needed to specify -
Undertaking a comprehensive cybersecurity risk assessment of the networks and systems of a single infrastructure, or even a single organization of moderate size, requires significant resources. Efforts to simplify the assessment instrument usually obscure the ultimate goal of the assessment and the motivations for the assessment questions. This can make it difficult for assessors to justify the questions and can undermine the credibility of the assessment in the eyes of the organizations assessed. This paper describes the use of assurance cases to help address these problems. Viewing an assessment approach in terms of an assurance case clarifies the underlying motivation for the assessment and supports more rigorous analysis. The paper also shows how the assurance case method has been used to guide the development of an assessment approach called the Cyber Resilience Review (CRR), developed for the U.S. Department of Homeland Security.
Future electricity distribution networks with mass deployment of network equipment sensors and instrumentation, millions of smart meters, small-scale embedded generation, and responsive load will generate vast amounts of data requiring near to real-time analysis. Novel ICT solutions will enable near to real-time state estimation across extensive distribution networks. Novel HPC tools and techniques have been recently developed to cost-effectively solve large scale computational challenges in other major scientific and engineering fields that require similarly scalable communications, computation and data analysis. Therefore it is also realistic to develop a new generation of distribution network management systems that exploit novel near to real-time HPC solutions with inherent security and intelligent communications for smart distribution network operation and management. In this paper cost effective scalable HPC solutions will be proposed and initially investigated for realistic distribution network data traffic and management scenarios involving state estimation.
In this paper the dynamical properties of ARP data for anomaly of internet traffic detection are presented. The local network traffic dynamics for single workstation has been observed experimentally on networks with 42 workstations. In the paper RQA (Recurrence Quantification Analysis) measures have been analyzed. Three of them have been explored using RP windowing method: recurrence rate (RR), divergence (DIV) and the longest vertical line (VMAX). The RP windowing algorithm samples the explored ARP time series, as a result each RP window characterized the disintegration of recurrence plots. The RR and VMAX defined the low activity of ARP dataflow. Inversely, the DIV identified the high activity of ARP time series. The authors have proved that both, the recurrence plot technique and the RP windowing method as the tools in the analysis of network traffic anomaly detection are effective and accurate.
Quasigroups are a well-known combinatorial design equivalent to more familiar Latin squares. Since all possible elements of a quasigroup occur with equal probability, it could be an interesting tool for information assurance and network security. Most of the previous implementations of quasigroups were based on look-up table of the quasigroup, on system of distinct representatives etc. which is infeasible for large quasigroups. This paper presents statistical tests which enable us selection of good quasigroups.
The IEEE 802.11 standard uses Wired Equivalent Privacy (WEP) for data encryption in wireless Local Area Networks. So far, different flaws have been discovered in the security of WEP. Frequently changing the encryption key can improve the security of WEP but there is no built-in provision for this in the standard. In this paper first we present and critically review different possible methods of automatic key updating and then propose a dynamic key management technique. The proposing technique works at the application layer. It is an automated encryption key updation method that can significantly improve the security of WEP without requiring any changes in the standard or at the lower layers of the OSI model.
Intrusion Detection is a critical process in network security. Neural networks approach is an advanced methodology used for intrusion detection. Self-organizing Maps (SOM) neural network is getting more attention in the field of intrusion detection. In this paper, a type of SOM - Quantum Growing Hierarchical Self Organized Map (QGHSOM) are made in order to improve the stability of intrusion detection and increase detection rate. The training process of QGHSOM networks can be described in terms of input pattern presentation and quantum states of weight update by quantum rotation gates. The QGHSOM is implemented and applied to the intrusion detection. The validities and feasibilities of the QGHSOM are confirmed through experiments on KDD Cup 99 datasets. The experiment result shows that the detection rate has been increased by employing the QGHSOM.
Supplier is critical for enterprise. In order to solve the problems related to trust of suppliers, we, in this paper, propose a dynamic evaluation model base on expected object for comprehensive trust of supplier called STEM. Base on the expected object of enterprise, the concept of trust vector was introduced into STEM. Many kinds of factor that affect supplier's trust and trust management that is used in network security were taken to this model. After that, a dynamic method was provided to evaluate comprehensive trust of supplier. Finally, the results of simulation experiment indicate that this model is feasible and effective.
A mutual mapping-update Authentication is proposed in the paper, and it provides the function of key agreement. This protocol resolves the security problem in the process of mapping update in the universal network, and provides a safe environment for communication. Meanwhile, it keeps the information of moving node's identity and location secret.
The NRL Network Pump, or Pump, is a standard for mitigating covert channels that arise in a multi-level secure (MLS) system when a high user (HU) sends acknowledgements to a low user (LU). The issue here is that HU can encode information in the imings?of the acknowledgements. The Pump aims at mitigating the covert timing channel by introducing buffering between HU and LU, as well as adding noise to the acknowledgment timings. Here, for the first time, we model the workings of the Pump in certain situations, as a communication system with feedback and use then this novel perspective to derive a upper bound on the rate of the covert channel between HU and LU in the Pump, in specific situations. This upper bound is presented in terms of a directed information flow over the dynamics of the system.
A semi-infrastructured ad hoc network is a wireless MANET subnetwork connected to a structured backbone network (LAN). This kind of network is becoming popular for low cost implementation and practicability issues. But the security is being considered as the major bottleneck of such semi-infrastructured Ad Hoc network. Uncontrolled access medium, dynamically changing topology, mobility of the hosts in the Ad Hoc mode challenges the security issues if the overall organizational network. In this paper a framework has been proposed to enforce Access Control Policy over such network. Both reactive and proactive routing is considered to implement the access control mechanism. The basis of the framework lies on distributed enforcement of the global access policy through different Policy Enforcing Nodes (PEN). The backbone network contains the Global Policy Management Server (GPMS) and Authentication Server. PENs after being selected and authorized by the GPMS take the responsibility to distribute the Access Control Rules to different Ad Hoc nodes. We have considered an underlying trust model is already implemented over the Ad Hoc network and the nodes are capable to handle symmetric key encryption for Message Authentication. The recent advancement of the research in MANET con rms the assumptions are valid.

It is currently a global hotspot that the application of multimedia and network technology to education is coming to be. There are inevitably threats brought about by the reliance on network and security problems of different levels in the modern web-based distance education system. This paper gives introduction to and analysis on the building of network security mechanisms in detail for distance education system in three aspects, including the selection of the user authentication, VPN selection of remote communication supporting, and the server-side performance guarantees. Then a detailed solution is made.
In order to effectively detect the scan attack on high-speed links, this paper improves the commonly used scan detection algorithm TRW (Threshold Random Walk) based on honeypot, and makes a detailed analysis on its performance. The analysis shows that the improved algorithm has better performance on the speed of identifying the scan source and can finish the real-time detection of high-speed link scan. Meanwhile, on the basis of selective system sample, this paper focuses on the analysis of the anomaly detection accuracy of three scan detection algorithms: Snort, TRW, TRWHP (Threshold Random Walk Based on Honeypot). The experimental results show that, at the same sampling ratio, the false positive rates of TRWHP and TRW algorithm are almost the same, however, the false negative rate of TRWHP algorithm can make a remarkable improvement and obtain the better detection performance.
Advanced distribution automation system is an integrate part of smart grid, and a reliable communication network constitutes the core of the advanced distribution automation system. However, the use of commercial on-the-shelf technologies, standard protocols, combined with increased interconnectivity with other networks, has exposed them to wide range of network security problems. Considering the characteristics of the group communication in the distribution network, one of the solutions for avoiding vulnerability is the group data encryption with group key. This paper focuses on the issues of group key management for advanced distribution automation system. Based on a three-tier hierarchal network model, and by utilizing one-way key derivation algorithm, an efficient key management scheme is presented in the paper.
Along with raptly development of wireless join technique, it's network security problem is increasing austerity. This paper introduces several relative mature network joining techniques at present, systemic explains wireless network security problem aiming at distinct techniques, proposes rational advices for security safety, and offers consult for security safety of multiform wireless network joining.
Intrusion detection system (IDS) is a security layer that is used to discover ongoing intrusive attacks and anomaly activities in information systems and is usually working in a dynamically changing environment. Although increasing IDSs are developed in the literature, network security administrators are faced with the task of analyzing enormous alerts produced from the analysis of different event streams. The intrusion detection model needs to be continuously tuned in order to reduce correlative alerts and help the administrator to determine accurate and critical attacks. In this work, an alert correlation detection module is proposed to analyze the alerts produced by IDSs and provides a more succinct and overall view of intrusions. An automatically tuned IDS rules generation module based on fuzzy logic technique is used to block the highly correlative alerts. The experimental results reveal that the proposed work is effective in achieving alert reduction and abstraction.
With the ever-changing threat of network, packet-filtering firewall, an important instrument for resisting threat, has become the effective measure of host-computer protection. Its ability depends on the capability of filtered rules. This paper first describes filtered rules formally, and then tests the rules including the verification of special aims and security analysis based on model checking. The formal analysis and verification make the rules achieve the security administrators' will of packet filtering and protect the system safe.
On the basis of analyzing the practical significance of developing the straight power purchasing for large power users in China and the advantages and shortcomings of this trade pattern, from the viewpoint of economics and marketing, and combining the electricity market theories and bidding algorithm and so on with straight power purchasing for large power users, the mathematical models are built. Large electricity users in China have three kinds of direct trading algorithms. In Consultation-sided trading algorithms, The two sides consultate trading power, price, electricity load, time and other factors, reporting to the electric power dispatching center and trading center together, the algorithm simple. Centralized auction trading algorithms with the conventional focus on the same bidding algorithm. So the paper focus on large users in China the Centralized algorithm for matching-type transactions. Direct purchase of electricity from large users in China centralize matching algorithm-based transactions in order to maximize social welfare as the goal.According to generators and large electricity users to declare, grid congestion situation, consider the requirement to meet the constraint. Access to large users and power producers in the trading platform focused on direct transactions. For different trading hours, the large users to declare the purchase of electricity prices and to buy electricity, power generation companies to declare the sale of electricity prices and electricity sales. According to the parties to declare the purchase electricity sales curve, considering the transmission losses were calculated in different large users and power generation corporate social welfare (the price between the two sides), the premise of meeting the network security constraints, priority is matching the biggest transaction of social welfare to form a transaction matching pairs; in sale purchase offer both sides, based on the principle of social benefits are divided to form t-
In this paper a new algorithm for determination of optimal active power reserve capacity requirements and energy and spinning reserve allocation in power system during the 24 hours is presented. In the proposed method, the optimum amount of reserve capacity per hour is determined proportional to network security level required by the system operator. In this way, it is essential to define a proper index for measuring system security. Thus, we have used Expected Load Not Served (ELNS) to evaluate system security in each hour. The proposed method has been implemented over the IEEE 24-bus test system and the results are compared with a deterministic security approach, which considers certain and fixed amount of reserve capacity in each hour. This comparison is done from economic and reliability points of view. The promising results show the effectiveness of the proposed model which is solved using mixed integer linear programming (MILP) and GAMS software is used.
Hacking is one of the greatest problems in the wireless local area networks. Many algorithms have been used to prevent the outside attacks to eavesdrop or prevent the data to be transferred to the end-user safely and correctly. In this paper ,two type of Symmetric encryption algorithms introduced, they are 1. UMARAM 2. IMAkey Algorithm. These two algorithms a new symmetrical encryption algorithm is proposed that prevents the outside attacks. The new algorithm avoids key exchange between users and reduces the time taken for the encryption and decryption. It operates at high data rate in comparison with The Data Encryption Standard (DES), Triple DES (TDES), Advanced Encryption Standard (AES-256), and RC6 algorithms. The IMAKey is a key generation algorithm based on random extraction of data from an image file. The algorithm is demonstrated in MATLAB and tested for key randomness and effect of image size on key generated. It is used for generating  key in the AES encryption process. The results for same plaintext input and different keys are obtained and analyzed. The new algorithm is applied successfully on both text file, voice message, And Image File.
In less than two decades, computer viruses and worms have grown from an anomaly to an everyday occurrence. In response to such a threat, the academic community has started a set of research projects seeking to understand worm behavior through creation of models. Staniford created a model to explain the propagation behaviors of worms in computer network environments. Liljenstam added a spatial perspective to this model, varying the infection rate by the scanning worms' source and destination groups. These models have been shown to describe generic Internet-scale behavior adequately. However, they often ignore the nuances present on a highly localized network such as a campus-at a local university. In this manuscript, we make and validate the claim that certain real-world constraints, such as bandwidth and heterogeneity of hosts, affect the propagation of worms and thus should not be ignored when creating models for analysis of worm behaviors.
Data security is one of the major concerns in today's scenario. However, due to availability of information related to networks and free software, hackers can easily hack into different computers without even having much in-depth knowledge of the network architecture. It is very hard to protect our valuable data from others and it is impossible to restrict hackers by installing firewalls and other security software. As software is always having some kind of weak points, which can be exploited by technically equipped people, The solution to this problem is by securing the hardware. In hardwired security, we are proposing a hardware control unit attached with motherboard, due to which we can give permission to different users of that computer according to access rights. The users are identified using smart-metric finger print identification algorithm, it require multiple fingerprints in authentication which prevents the re-use of the fingerprint. It also decreases the probability of successful reproduction of phony fingers which will provide a new level of data security.
p-Cycles present a very interesting approach to building survivable networks. They have been widely studied for unicast and recently for multicast traffic. For anycast traffic almost no research have been done. We will discuss a new p-cycles version, called Anycast-Protecting p-Cycles, introduced in our previous paper. We provide detailed description of some new properties, ILP models, results and extended analysis with comparison to classical p-cycle span protection approach. We will show that usage of Anycast-Protecting p-Cycles can be very beneficial.
Most of previous research on survivable networks has been focused on protecting the unicast traffic against random failures. In this paper we propose a new approach, called RA (resistant-to-attack), to provide protection of anycast and unicast communications against attacks on irregular (e.g. scale-free) networks. We use the single backup path approach to provide protection against a single node failure. For anycast traffic, we assume the utilization of different working and backup replica servers. We provide a new metrics of link costs used in working path computations (resulting in omitting nodes of high degree by the working paths), and locating the replica servers at low degree nodes (i.e. of low probability of breaking due to attacks). The ILP model to find the optimal paths for anycast and unicast flows has been formulated and followed by an efficient heuristic algorithm. The results show that our method remarkably reduces (up to 7.47 times) the total number of connections broken due to attacks, compared to the common case of utilizing the metrics of distance to find both working and backup paths.
In order to test the performance, reliability and security of Web-based system, the paper generates the test scripts template and establishes testing model of system login and access control. Then some Web-based systems are tested by automation test tools. The performance, reliability and security problems of Web login process can be traced and diagnosed. The test result shows that Web-based system can be verified and improved by the test script template of multi-users secure login and resources access control.
In network forensic system, there are huge amount of data should be processed, and the data contains redundant and noisy features causing slow training and testing process, high resource consumption as well as poor detection rate. In this paper, a schema is proposed to reduce the data of the forensics using manifold learning. Manifold learning is a popular recent approach to nonlinear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high. In this paper, we reduce the forensic data with manifold learning, and test the result of the reduced data.
Network Security Situational Awareness(NSSA) has been a hot research in the network security domain.The amount of data from network attacks from Intrusion Detection System (IDS),and hosts'vulnerabilities and the hosts'states is very large.If we use the large amount of data as the NSSA elements directly,the algorithm of data processing must collapse or use a very long time. So in this paper,a method of data preprocessing for NSSA based on conditional random fields(CRFs) is proposed.This method takes advantages of the CRFs models which can stitch to sequence data marking and add random attributes.It uses varied connection information and its relativity in network connection information data sequence as well as the feature sets relativity to attack detection and discovery of abnormal phenomenon. It uses KDD Cup 1999 data sets as experimental data and comes to a conclusion that our proposed method is practicable,reliable and efficient.
One challenge in web security education is its interdisciplinary and practical nature. Students need to have the basic knowledge and skills of a web developer to understand many of the web security topics, and some of them are normally covered in multiple advanced courses like Computer Networks and Network Security, or are absent from many existing undergraduate or graduate degree programs. This paper shares our experience of using VMware virtual machines in supporting hands-on web security education, and developing multiple virtual web security lab modules based on the virtual machines. The lab modules are part of our NSF SWEET (Secure WEb dEvelopment Teaching) project, and each of them contains (1) concepts in a nutshell; (2) lab objectives; (3) software setup; (4) detailed lab instructions; and (5) lab evaluations. Comprehensive lab modules have been developed to guide students to build virtual Ubuntu virtual machines with publicly available tools and install all necessary web servers, application servers and database servers on them so they can function as the foundation and platforms of the other course modules. The other covered course modules include cryptography, HTTP and HTTPS protocols, and introduction to Java web technologies.
In recent years, information security and assurance has received considerable attention from the computing community, with universities revamping course offerings in areas such as cryptography, network security, enterprise systems security, secure coding, and digital forensics. Although secure data management (SDM) is a major facet of overall information systems security, it has received less attention than it deserves. However, at the Rochester Institute of Technology, an SDM course has been offered since 2003. This course covers both commercial practice and current research, and has been revised continually to keep the technical content current, and to incorporate active learning approaches; additionally course delivery has moved from a traditional classroom lecture setting to a blended and fully online (distance delivery) setting. On the practical side, student projects involve the design and development of secure database applications on hardened database servers, and include an attack phase when students get to break into other student projects. On the research side, students explore current topics in the literature and incorporate aspects of their research readings into their implemented projects. This paper discusses the SDM course, some measurements of student learning, and other experiences with the course.
The flow of information in the network currently existing attacks and malicious damage, a privacy protection algorithm based on the maximum greedy is proposed, the data privacy acts as weights between nodes in network diagram , change these weights to achieve the protection of important data, by obtaining the shortest path way to ensure that efficiency is not influenced ,the experimental results show that this algorithm not only protects data privacy but also efficiently ensures the effectiveness of data.
The thesis proposes a hybrid intrusion detection model based on the parallel genetic algorithm and the rough set theory. Due to the difficult for the status of intrusion detection rules. This model, taking the advantage of rough set's streamline the edge to data and genetic algorithm's high parallelism, succeeds in introducing the genetic-rough set theory to the instrusion detection. The application of hubrid genetic algorithm in solving the rough set reduction saves computing time. The concludes that model can result in high detection rate and low false detection rate to different types of network via experiments.
This paper described the current network of primary language XML in network applications, introduced its own XML language features and development to illustrate aspects of XML technology in the application of network security and significance. The network security is a systems engineering which is need to carefully consider the security needs of the system, and a variety of security technologies, such as passwords and technology combine to produce a highly efficient, universal, secure network systems. Secondly, this paper analysis of network security architecture and the current network security system for the protection of technical methods used: the network against viruses, configuration, firewall, intrusion detection systems used, Web, Email, BBS's safety monitoring system, vulnerability scanning systems, IP Theft solution, using network monitoring to maintain system security subnet. Finally, the XML technology for network security enabled areas of security, XML has become a field for the safety of a valuable mechanism for exchange of data, related development is related to XML encryption and XML signature.
The following topics are dealt with: data hiding techniques; wavelet transform; management information system; image sequence compressing algorithm; trusted software; case-based reasoning system; education information platform; UML; multiobjective optimization; information retrieval; web mining; CUDA architecture; fuzzy association rules; BP neural network; network security detection method; adaptive particle swarm optimization; linear-structured composed service routing problem; e-commerce system security; fuzzy reasoning; AES; optical flow; image segmentation; multispectral image coding algorithm; multiparallel architecture and support vector machine.
Network intrusion detection is one of the hottest research issues on the network security recently. In the paper, three kinds intrusion detection model are introduced. By using Bayesian methods, Fuzzy Decision Engine and Immunity-Based Model, we can improve the performance of intrusion detection system and effectively reduce the false positive alarm. Based on the three models, some drawbacks of NIDS have been analyzed and a new architecture of IDS has been put forward. In this new architecture, IDS are viewed as a function instead of a product.
Inspired by the change of antibody concentration, network security system can reduce the false alarm rate effectively. In this paper, we discuss the nonlinear dynamics of antibody number in the antibody concentration inspired network security method. According to the conclusion of this paper, we can get the general calculation approach to compute the accumulation of antibody number in the antibody concentration inspired network security method.
The authentication process of wireless local area network authentication and privacy infrastructure, namely, WAI protocol, is researched and analyzed, and the security analysis is made to its the key-agreement process by using CK model. The analysis shows that: WAI can realize security attributes such as mutual key-control, key confirmation and so on, and it can also statisfy the secure goal of wireless local area network. Thus, it can be used to enhance the security of wireless local area network instead of WEP.
With modern computer network technology as the primary means of financial information systems, financial management and development. The author believes that the network environment of financial system faces numerous risks on information technology and modern management level high demands. This article discusses the network security of the financial system, offers a range of preventive strategies.
This network security information in the confrontation as the research object, starting from the information model of confrontation to study the current dynamics of the network defense technology and defense systems. From the dynamic security deployment, attack the information decision-making, the three aspects of public knowledge acquisition to build a dynamic defense system model, through the confrontation of the model simulation results to validate the safety of defense capability.
Multi-parallel architecture for MD5 (Message-Digest Algorithm 5) implemented on FPGA (Field-Programmable Gate Array) is presented in this paper. To accelerate the speed, a general architecture for Host Computer and FPGAs is proposed. The MD5 implementation is presented. Besides the internal parallelization of MD5 modules, FPGAs can be easily duplicated and connected to Ethernet LAN. The design was implemented on Cyclone II EP2C35F672C6. For a single board, a throughput of 4.3 Gbps was achieved with 30,134 logic elements and 12 concurrent MD5 modules, and 13.0 Gbps was recorded with 3 parallel FPGAs. The performance is higher compared to other recently published works.
The following topics are dealt with: Internet technologies; multimedia over wireless networks; coding and channel estimation; routing and mobility; multicast routing; security and privacy.
Although DoS (Denial of Service) attacks making the amount of network traffic increase excessively are threats to availability of communication services in our internet society, the complete prevention and avoidance are impossible because the conventional ways such as fire wall and packet filtering are still infeasible and incomplete. The random port hopping (RPH) by Badishi {it{et al.}} (2005, 2007) provides a robust communication protocol to decentralize influences by malicious attacks and to realize an exact packet filtering with light load. Since the utility and limitation of RPH against general DoS attack patterns are not clear, however, it has not been used yet as the standard communication protocol in the real world. In this paper, we carry out the simulation study and take place the sensitivity analysis of RPH in terms of the communication success rate. After pointing out a drawback of the existing RPH, we also propose a fine-grained RPH algorithm for more general DoS attack patterns.
Assurance network technologies are necessary to realize trustable terminal/infrastructure service applications for new generation networks. This invited paper outlines our project of research titled undamental Concepts of Assurance Network Technologies and Their Case Studies?and shows concepts and technologies of assurance networks and case studies for mobile ad hoc networks.
Conventional risk assessments in Ad-Hoc Networks always require sample satisfy specific distribution with large quantity and establish models through subjective judgment, these methods lack general applicability, objectivity and credibility. Moreover, some models only focus on single time point evaluation and failed to thoroughly reveal dynamic behavioral character. To solve these problems and make correct assessment, we propose RAPCA-PP model on the basis of Projection Pursuit theory to conduct both risk assessment and attribute analysis over dynamic time sequence. RAPCA-PP is completely data-driven and can be applied in small sample, incomplete data and no-prior experience. Compared with Grey Relations Projection, it boasts both better accuracy and higher discrimination. Since RAPCA-PP makes evaluation along time axis, it can reflect MANET's node series behavioral features. Experiments demonstrate that assessment with eliminated attributes can also correctly reflect each node's performance and be utilized for IDS realization. RAPCA-PP proved to be suitable for real MANET working scenarios.
The current mobile wireless standard 802.16e can provide data confidentiality, integrity and mutual authentication in wireless metropolitan area networks (WMANs). However, secure communication can only be provided after successful authentication and a robust security network association is established. In this paper, we propose our solution to patch the current 802.16e standard and address all vulnerable issues with a new simple authentication key-establishment protocol and an efficient way for privacy-preserving. In simple authentication, we smoothly apply the public-key cryptography-based key-establishment technique to the 802.16 MAC protocol. Our solution can provide link-layer data encryption in initial network entry procedure, separate session encryption keys to preserving privacy of different users, and protection for important frames such as management and Extensible Authentication Protocol (EAP) messages.
Online/offline signatures are used in many applications where the signer must respond quickly once the message to be signed is presented. The idea is to split the signing process into two phases: The first phase is performed offline before the message to be signed is available and the second phase is performed online after the message to be signed is provided. Recently, an identity-based online/offline signature scheme was proposed for authentication for routing in Mobile Ad Hoc Network. Unfortunately, we show that this scheme is insecure in the paper, it is universally forgeable. Anyone can produce a forged signature on arbitrary message without the private key of a signer and a valid signature of a message. To overcome the drawback, an improved scheme is proposed. A security analysis shows that the proposed scheme is proved secure against existential forgery on an adaptively chosen message attack under the assumption of Computational Diffie Hellman in the random oracle model.
This paper proposes a secure session key exchange and mutual authentication scheme between mobile machines in WLAN-based ad hoc networks. One of the main security challenges for secure machine to machine communication is to accomplish a simple and efficient session key exchange and mutual authentication. The main idea of this paper is that non-interactive session key exchange uses the mobile machine's ID and mutual authentication without communication with an authentication server whenever a data transmission occurs between the mobile machines. The proposed scheme is secure against man-in-the-middle attack, impersonation attack, and modification attack.
Most current anomaly detection systems employ supervised methods or unsupervised methods. Supervised methods rely on labeled training data, however, in practice, this training data is typically expensive to produce. In contrast, unsupervised method can work without the need for massive sets of pre-labeled training data, but its accuracy is quite low. This paper put forward a semi-supervised cluster algorithm to detect anomalies in network connections, we evaluate our method by performing experiments over network records from the KDD CUP99 data set. The result shows that the feasibility of this method is much better than the others.
The network information system based on multi-level security strategy (MLSS) is adopted by many organizations, as it reflects features of mandatory access control. Meanwhile, the network has also attracted a growing number of Trojan horses' attacks. Considering the relationship between defend ability and security levels, the paper discusses that the Trojan horses' attacks have a dependency on data path in the network and establishes three probability models of single source node to single target node by single-path, single source node to single target node by multi-path and multi-source nodes to single target node by multi-path. Finally the model is applied to a military network information system, and the results are reasonable.
Low-rate denial-of-service attack is a novel category of attacks that are based on exploiting the adaptive behavior exhibited by several network and system protocols. This attack through periodically non-suspicious low-rate attack pulsing, to reduce the performance of the victims. Based on the simulation of low-rate denial-of-service on NS2 platform, we analyze the defense performance of queue management mechanism itself to count the LDoS attack. Under the analysis of experiments result, we give some suggestions about counter technology.
This paper firstly analyzed the current complicated campus network security condition especially its' active protection; then introduced principle of UTM (United Threat Management), and applied Multi-Core UTM technology in BIGC campus network at last made a related analysis and research of this project.
About the risk evaluation of network security, a new assessment method based on Nonlinear Principal Component Analysis (NLPCA) is given. The principle and process of NLPCA-RBF is introduced in detail. At last, its superiority is indicated by example. It not only can reduce the dimension of input vector, but also can reserve the nonlinear characteristic of the network by nonlinearity. It is a new evaluation method of more quickly, more effective, more exact.
The key of intrusion detection system based on artificial immune is generating the Detector set. To generate highly efficient one, we adopt the mutated Nonself-body set to generate candidate detectors and variable threshold R- continues bits matching rule to reduce the number of hole in mature detector, increase the tolerance of the candidate detector and the detector set, to result in mismatch detectors and increase detect ability. Theoretical analysis and experimental results show that the improved algorithm is effective and feasible.
Multilevel security management system can effectively manage the security devices, but how to realize data exchange between multilevel security management is an important problem. This paper designs and realizes data exchange between multilevel security systems.
Extentics is a new discipline that studies the methods for dealing with contradictory problems. This paper gives a solution of attack's recognition and resistance of survivable network based on extension theory. The basic knowledge of extension theory is introduced. The contradictory characteristic of survivability of borderless network is analyzed. The extension model and algorithms of recognition and resistance of attacks is presented. The definitions of element, including message matter-element, defense affair-element, node relation-element and inform mixture-element, are given. The dependent function based on lateral distance with the most advantage point occurs in the left common endpoint, is constructed to solve the problem and its properties are discussed. Finally, an application of worm's recognition and resistance by this method is illustrated.advantage point occurs in the left common endpoint, is constructed to solve the problem and its properties are discussed. Finally, an application of worm's recognition and resistance by this method is illustrated.
Multimedia communication networks, such as the Internet, are heterogeneous in their nature due to the usage of different methods for content distribution. The efficiency of multimedia distribution depends on the variety of communication protocols that are simultaneously running (composed) over different network hosts in order to resolve packet conflicts. A very natural question that arises in such common settings of multimedia networks concerns the degradation of network stability under adversarial attacks that change dynamically network link slowdowns. A packet-switched network is stable if the number of packets in the network remains bounded at all times against any adversary. In this work, we study this question adopting an enhanced adversarial framework, where an adversary controls the rates of packet injections, determines packet paths and manipulates network link slowdowns. Such adversarial attacks can be considered as a type of denial of service attacks. Within this framework, we study network stability under specific compositions of contention-resolution protocols trying to characterise this property in terms of network topologies. The examined network topologies have been proved forbidden for stability when network link slowdowns are fixed and packet paths do not contain repeated edges, but they can contain repeated nodes. Interestingly, our results suggest that dynamic adversarial attacks changing link slowdowns may be worse than adversarial attacks with unit slowdowns for specific protocol compositions. We feel that this study could help on the design and maintenance of trustworthy multimedia networks.
With the rapid development of wireless network technology, the extensive use of mobile office in government, military and financial industry makes information security an urgent and important problem to be solved. However, traditional security mechanisms focus on authentication and authorization without considering endpoint security. TNC is no other than based on endpoint integrity to extend the trusted state of endpoint to the entire network. In this paper a mobile office security solution based on endpoint integrity measurement is proposed, an integral security authentication protocol is designed and formal analysis is offered, thereby improves the safety over mobile office network from the source, and constructs a trusted mobile office network environment.
With the rapid development of wireless communication technology and Internet, the mobile office emerges as a new requirement of the times. At the same time, the security problem of mobile office is increasingly prominent. Based on the existing WLAN security technologies for Infrastructure mode and Ad hoc mode and combined with environmental requirements and characteristic of mobile office, this paper offers two security solutions based on RADIUS Authenticator System and four-handshake mechanism to solve the problems of identity authentication and information protection, and provides a high level security proposal for mobile office.
The wireless local area network (WLAN) technology has a rapid development because its advantages of fast and high-efficient, flexible and easy in net building. Wireless network brings users convenient network service, but it also hides many security problems that hinder WLAN's development. This paper combines the accountability of IP address with trust routing to a second mechanism applied in WLAN to prevent network attack.
In this paper, we analyzes IPTV security threats via the the triple play. That is, the distribution system for IPTV service is analyzed along with the components of the value chain and corresponding IPTV security requirements or security technologies under the triple play, in order to perform a threat analysis and research suitable for the IPTV service environment.
Network attacks occur in high proportion on the internet, thus aside from security as a means of defense there is a need for being able to detect attacks as they occur so that measures can be put in place to tackle them. For this, an intrusion detection system (IDS) is required that has good quality of detection capability. We propose Quality of Detectability (QoD) as a kind of Quality of Service (QoS)-like detection quality mechanism. Two major contributions are given in this paper: (1) the notion of QoD to measure the attack detection capability; (2) an extended Augmented Attack Tree (AAT) and corresponding intrusion detection algorithm with QoD capabilities.
Voice over IP (VoIP) is in wide use today, replacing phone lines in many scenarios. However, often, security isn't considered well enough, even though many security attacks are already known. More research on VoIP security is needed to enhance the level of security of VoIP systems and to show the implications of failing to take appropriate security measures. This paper presents an architecture and implementation of a robust and flexible VoIP test environment for security related tests. Experiences using the implemented environment for different VoIP security tests are shown to demonstrate the suitability of the proposed test environment for research purposes.
Intrusion detection is one of the most challenging problems in network security. Detection of attacks on a particular network is not an easy task. Since recently, several machine learning, pattern classification and evolutionary techniques have been used on KDD99Cup dataset for detecting different kinds of intrusions that exist in the dataset. In this paper, we present a genetic algorithm (GA)-based technique for detecting Remote-to-Local (R-to-L) attacks in the network. The problems in other techniques with accuracy, false positive rates and speed have been resolved by using incremental GA in our proposed mechanism. We extract features from the dataset and build rules upon them to identify the attacks. The speed of training and testing is reduced by using less number of features in the incremental GA. The results of the R-to-L Intrusion Detection System (IDS) are rechecked for confirmation by using two more detection systems. The latter detection systems make sure that a particular record identified by the first IDS is really an R-to-L attack. The overall system's false positive rates are decreased and detection rates are increased. The model is verified on the dataset taken from KDD99Cup which is a standard dataset used for intrusion detection.
A secure digital timestamp can assert the existence of a particular document at a specific point in time. While centralized timestamp servers show dependability drawbacks with respect to node or link failures, e.g., outages of the Internet connectivity, distributed timestamping protocols don't have these drawbacks, but are nowadays rarely seen in practice because of the distributed trust requirements and applicability issues.
Most of the attacks on networks are launched through spoofed IP addresses i.e. Denial of Service attack. Attacker spoofs IP address of legitimate client, sends many useless packets to the victim and acts as a legitimate user. Since the attacks are launched through spoofed IP addresses therefore it is very difficult to identify the source of the attack (Attacker). The researchers introduce a technique to identify the origin of the spoofed user. This technique is known as the IP Traceback. Many traceback techniques are introduced but all have few drawbacks. Some increases the network load and delay, few requires implantation on all the routers of the world, few compromise on privacy of the user. Many techniques do not guarantee single packet IP traceback. All the existing IP traceback techniques require an efficient marking technique. A new single packet IP traceback technique to identify the source of the packet is introduced in this paper. This technique reduces the network delay and does not require any marking technique. Private information of the users will remain intact.
In order to determine Remote to Local (R2L) attack, an intrusion detection technique based on artificial neural network is presented. This technique uses sampled dataset from Kddcup99 that is standard for benchmarking of attack detection tools. The backpropagation algorithm is used for training the feedforward neural network. The developed system is applied to R2L attacks. Moreover, experiment indicates this technique has comparatively low false positive rate and false negative rate, consequently it effectively resolves the deficiency of existing intrusion detection approaches.

A firewall's complexity and processing time is known to increase with the size of its rule set. Empirical studies show that as the rule set grows larger, power consumption and delay time for processing IP Packets particularly on Hardware firewalls increases extremely, and, therefore the performance of the firewall decreases proportionally. This paper present a new FPGA (field programmable gate arrays) based firewall with high performance, high processing speed, low power consumption, and low space utilization in contrast with early submitted paper in some of credible international conferences. First of all, we use Embedded Memories of FPGA instead of external memories, to increase the processing speed and to decrease the mass of signaling and noise creation in connection between FPGA and external memories. Moreover, we have applied pipeline technique to the architecture, and so we achieved high processing speed in addition to low power consumption. Based on this model, only the security rules are updated by admin during the configuration process while other hardware part would remain unchanged. The proposed architecture is written in VHDL standard language and it is simulated and synthesized with ALTERA QUARTUS II software. Finally, for validation of proposed architecture, we implemented the synthesized code on 3 of ALTERA FPGAs families (STRATIX III, CYCLONE III, CYCLONE II), and then results have been compared with earlier results.
In the past few years, social networking websites such as Facebook and Myspace become very popular. The usage rage of social networking websites even exceeds that of Google. Followed by the popularity is many potential networking threats. How to prevent and improve these threats to avoid their expansion has become a major challenge. This paper categorizes social networking websites into three main structures: The social network (SN), the network application service (NAS) and the communication interface (CI). Through literacy review, we explore the potential information security threats (1ST) that may lead by each layer. We then use security characteristics such as confidentiality, integrity and availability to cross-analyze these threats. The analytical results are presented by graphs and tables to demonstrate the distribution of current security threats for social networking websites. We propose a real-time website security protection mechanism based on the concept of proxy. The client side transmits information to the social networking website through proxy. The main function of the proxy is to detect and determine the security threats of the website. These threats include web-based malware, phishing websites and malicious connection. The idea is to integrate many commercial protection software and online security scanning services into a security module, simultaneously execute webpage security threat scan, then scan the information sent by the web server with the security module before sending to the client. If security threats were found in the web page, the system will add this web page to the blacklist and issue a warning to the client side to prevent attack. The functionality of proxy is to segregate the client and the networking threat. Using simultaneous scan of many protection software and online services can increase the recognition rate of security threats. Later one, as long as the client is to receive the webpage in the blacklist, a warning will be iss-
The main goal of NAC is to extend the security of networks to the end-point by measuring the authenticity, integrity and security posture of each end-point prior to granting network access. To do this, the following functional areas must be present: authentication/authorization, assessment of security posture, quarantine and remediation. This paper presents an overview of an in-depth NAC requirement analysis performed against three NAC products based entirely on open source literature. The emphasis of the analysis was to define functional and security gaps across all products and make recommendations to improve the overall security and interoperability of NAC products. This paper identifies: 1) Key design and implementation choices that are required based on stakeholder requirements 2) Areas where NAC does not meet stakeholder(s) requirements 3) Areas that have not been adequately defined for implementation 4) Recommendations to improve the security posture of NAC products. An analysis of each product is performed in the following areas: 1) System Administrator Interface and Policy Settings 2) Authentication 3) Integrity Measures 4) Remediation 5) Security 6) Functional 7) Non-Functional This analysis and research of NAC lead to seven general recommendations for improving the security of NAC products and four recommendations for deploying and implementing them.
In order to improve the precision of network security situation prediction, a combined method for prediction was proposed. For the nonlinearity of network security situation value, the Method predicted the security situation by using ARMA model and Markov model. On this basis, it optimized the predicted results with combined strategy. The analysis of the example indicates that the combined method can effectively improve the precision of network security situation prediction, compared with single Method.
OSPA protocol is vulnerable to the replay attack and the denial-of-service attack. In this paper, we propose a USB-Key based strong Password authentication scheme. This scheme uses USB-Key to verify the user's password and store the security parameter. This scheme can achieve mutual authentication between user and server and user anonymity by using the temporary identity. The security analysis of the scheme proved that the scheme is resistant to replay attack, impersonation attack and DoS attack, and with high security, and can be applied to the users with low compute resource.
IEEE 802.16e Standard as the new released version, renew the security scheme PKMv1 of IEEE802.16d into PKMv2. This paper analyzes the EAP_based authentication mode of PKMv2, and proposes an improved EAP_TTLS _SPEKEY method for EAP_based authentication patter. The improvement has enabled the EAP_TTLS_SPEKEY to overcome the engrained vulnerability of the Man in the Middle attacks in the original EAP_TTLS design.
Using One-time Password (OTP) in a Two-factor Authentication (2FA) system is very popular nowadays. OTP function has been embedded into many devices such as standalone token, PC, PDA and cellular phone. The generated OTP code is entered into the login window to complete a user's authentication procedure. These tokens and devices do bear extra cost to the user. Deployment or support of such OTP tokens can not be fully done through Internet to reduce the expense and work load. And certain designs may compromise network security when token is lost or stolen. We propose a secure way to generate the OTP code by way of a web browser. A user does not need any electronic device on hand to obtain OTP for 2FA login. A new Rubbing Encryption Algorithm (REAL) is proposed as the base technology. Implementation method of such web-based OTP token is presented and analyzed. The token is licensed to a company to promote product sales. It operates through a web-browser with a REAL dynamic session key. It can be integrated into many secure Internet commerce applications as well.
Password based remote user authentication over unreliable network is the traditional method in the Internet. A variety of password authentication schemes have been published in literature for electronic commerce environment; however, none of them is secure against various types of attack. In 2008, Zhu et al. proposed an improved remote login scheme over the Hwang and Yah's scheme. In this paper we show the Zhu et al.'s scheme still has some security flaws and then we proposed an improved remote login scheme on elliptic curve cryptosystem. The proposed technique supports remote user mutual authentication with session key agreement and secure password change scheme. Moreover, the proposed scheme provides different relevant security attributes and bears less computational cost and low storage space.
With the prevalent trend of computer and network in our country, the problem of security about internet information is becoming more and more outstanding. This article analyses the factors about the hidden security troubles of internet information that our country faced with at present, and put out some protecting measures from some aspects, such as technology, management and policy.
Combination the theories of Factors Neural Networks, the network defense situation niching model is defined. And the networks defense situation extraction, comprehension and display is divided from the model. The factors rattan, net and the niching situation chart of defense situation are got through formalization analysis. The simulation experiments and results are given, the results proof that the model is useful for networks defense simulation research and training.
A network security situation awareness model based on cooperative artificial immune system is discussed in this paper. In this model, efficient memory detectors in different computers can share the differences and improve the coverage and scalability of AIS. These detectors spread in the whole system can be organized by a cooperative channel. The function of the cooperative module is sending and receiving the information of cooperative detector, and deciding which collaboration relationship should be used and when to quit. The structure of the model is described in this paper. Furthermore, the primary algorithm of cooperative is expressed.
This paper introduced network honeypot technology, from the honeypot bait, detection technology, recording of a detailed analysis of three aspects of the network honeypot operation mechanism, discussed the value, strengths and weaknesses of network honeypot technology, analyzed the offensive and defensive technology game in a network honeypot system, and looked forward to the core technology trends and the challenges for it.
The IP protocol do not inspect the source address of IP packets, it is vulnerable to forged source address security issues. Base on the source address validation build the trustworthy the next generation Internet architecture, prevent forging the IPv6 source address and eliminate the forged source address network security threats are particularly important. The real address also makes it easier for network auditing and tracking. This paper describes the principles, related protocol and the implementation of IPv6 access network source address validation architecture based on SAVI protocol and deploy SAVI in the actual network environment.
Focused on the network security situation evaluation, a novel hierarchical evaluation system, which is based on Grey Clustering Analysis, is proposed. In this system, network attacks are classified into trong? edium? and eak?three harmful levels by Grey Clustering Analysis to construct a hierarchical index system. Then, the Analytic Hierarchy Process is used to calculate the impact factor of each network attack, and form the evaluation system to calculate the network security situation value. With Grey Clustering Analysis, harmful level ownership of each network attacks is determined, and each network attacks' influence can be truly reflected. Moreover, network security situation value's calculation speed can be improved by Analytic Hierarchy Process. Finally, large amounts of electric power on-site experiments indicated that the evaluation system is well-performed by showing network security situation in both coarse-grained and fine-grained analysis.
With the rapid development of computer technology and network technology, various new systems come into being, and hidden network danger has been a constant concern. Web Service is a new technology due to the rapid development of network security technology research based on web service. This paper gives a brief introduction of Web Service, and elaborates in particular on Web Service network security technology.
E-government is the application of computer and network technology to integrate the government's management and services on the Internet to achieve national administrative organization structure and work flow optimization, reorganization, beyond time, space and departmental restrictions on physical separation, to provide efficient and comprehensive, transparent and standardized, convenient and consistent with international standards of social management and information services for the public. However, there are various e-government potential security threats. In this paper we use the redicate Object?representation of technology to solve the security and sensitivity protection problems in e-government and demonstrate the safety of electronic monitoring data on this basis.
Now, the network security is facing up to serious situation. The premise of popularization and application of cloud computing is to ensure cloud security. In this respect Trend Micro and Rising have maturely researched and developed corresponding products. Trend Micro implements actively defence, and can effectively analysis the virus from the network. But it isn't useful for the local user who is infected through U disk and mobile hard disk. Rising implements passive defence, and can kill the virus in the local machine. However, it can't effectively deal with the most of the virus from the network. The strengths of Trend Micro and Rising are combined, and a new, systematic and tight anti-virus technology solution is put forward. At the same time, details of steps to handle spam mails are given as the example of the anti-virus. Compared with two companies in memory and systematic resource, this is a feasible, more effective and closely technical solutions.
Along with the development and improvement of informatization construction, campus network security problems are increasingly serious. The attack to portal sites, especially the explosion of SQL injection accidents has become one of the most serious problems of it. This paper analyzes the principle and characteristics of SQL injection attacks on portal sites of university, presents methods available to prevent websites from these kinds of attacks, including improvement of the management system, secure coding within the web application, proper database configuration, deployment of IIS, application of network firewalls and IDS/IPS, installation of webpage temper-proofing system and other security techniques.
The file informationization system relates to a unit's normal operation, relates to each staff's destiny and the future, therefore has its particularity, dues to faceing more threats in the security, once it occur the risk, the influence range will be also broader than the ordinary system, therefore completesing the file informationization system's safe guard is especially important. This article mainly from the infrastructure security, the data security, the network security, the using security, the administrative security, the movement security, the authorization and the audit security and so on, linking the common question of digit file network, reposed the corresponding security policy target-oriented.
At present, the network security technology is single function, mutually independent, and difficult for system management. They cannot be trusted completely as the solutions of network intrusion behavior. This article proposed a kind of structure framework which consists of both the honeynet and intrusion detection technologies. By applying this method, we can achieve the purpose of improving network security.
Web transfer protocol developed from the first plain HTTP to SHTTP and HTTPS, now the developing SPDY. It is also widely used in e-commerce web site transfer protocol. This paper analyzes these network protocols, the author tried Apache httpd server with SSL/TSL support, and prospects the future development of web security protocol.
This paper presents a solution to ensure arbitrarily secure communication in a large computer network by using secret sharing and multiple parties mistrusting each other instead of relying on some rusted party?or a eb of trust? In contrast to other solutions that use a PKI and require asymmetric encryption, this concept can guarantee to provide secure communication even after any possible advance in cryptanalysis and even if unlimited calculation power was available to attack it. But this solution requires the computer network to have special properties. It is mainly intended to be used in the S-Network, a repository for reliable publications.
Vulnerability is one of the important factors that cause security incidents and has become a major international threat to network security. Previous work like Common Vulnerabilities and Exposures (CVE) and vulnerability databases has been offered to manage vulnerability. However, they have significant disadvantages in coverage and regional differences. International Vulnerability Database Alliance (IVDA) is proposed as an alliance model which consists of security organizations from different countries. IVDA provides systematic policies and standards to manage vulnerabilities of software in different languages, and achieves agreement with its members to enhance international cooperation and communication. The evaluation of IVDA shows that the international alliance is rational and effective in vulnerability disclosure.
On behalf of the Technical Program Committee, we welcome all of you to the IEEE International Wireless Communications and Mobile Computing Conference (IEEE IWCMC 2011) in the beautiful campus of Bahcesehir University, Istanbul, Turkey! We are indeed delighted that this year's IEEE IWCMC accomplishes its goal under the conference theme aking Wireless Communities,?and continues its tradition of providing the premier forum for presentation of research results and experience reporting on the cutting edge research in the general areas of wireless communications and mobile computing. This year, we received more than 1000 submissions from 51 countries worldwide. Each paper received at least three peer technical reviews, comprised of 49 Symposia Chairs/Co-Chairs and a total of more than 450 TPC members from academia, government laboratories, and industries. After carefully examining all the received review reports, the IEEE IWCMC 2011 TPC finally selected about 35%high-quality papers for presentation at the conference and publication in the IEEE IWCMC 2011 proceedings. The conference program starts on Monday July 4th with a full day Tutorials that is free of charge to all our attendees. Then, each day starts with a keynote speaker chosen from renowned world-class leaders in the area-Dr. Rick Stevens, Dr. Mario Gerla, and Dr. Sajal Das, highlighting the latest research trends in the wireless communications, mobile computing, and networks. This year, the technical sessions reflect the continued and growing interests in a wide range of spectrum, including wireless communications and networks, cross-layer design and optimization, mobile computing, wireless sensor networks, network security, and use of wireless technologies in social emergency applications. We also added a special Workshop this year to address practical aspects of Wireless Communications and Mobile Computing, such as Multihop Wireless Network Testbeds and Experiments, Network and - - Communications for Advanced Society, and Federated Wireless Sensor Systems (FedSenS). There are five special sessions composed of invited papers from renowned experts from around the world. Outstanding papers will be selected for four Special Issues in well known international journals. Our objective in the future is to reduce the acceptance rate further to reach 30%and less. In addition, we would like to reduce the number of Symposia and Workshops as well to meet the conference theme.
Among the challenges facing the Internet, DoS/DDoS are a critical concern for Internet Service Providers. DDoS attacks can cause country-wide infrastructure problems, and can disrupt communications on a national level. Frequently, Botnets are used to carry out source-spoofed DDoS attacks. The problem of tracing such attacks has been the subject of significant inquiry. Here, we leverage the fact that a Botnet requires significant exposure to risk, and investments of time and resources. Thus, as a capital resource, it is likely that a Botnet will, over its lifespan, be used to execute multiple criminal DDoS attacks on different targets. Here, we report on new techniques that leverage information obtained over sequences of source spoofed Botnet led DDoS attacks, demonstrating the efficacy of these techniques at pinpointing potential attacker locations. DDoS attack flow descriptions can be collected in many ways, using a coordinated DDoS sensor agents (e.g. as described by the authors previously in). Here, as a theoretical contribution, we provide formal statement of the attacker localization problem. We develop an new algorithm for localizing attack sources from sequences of DDoS attacks.
The use of application-layer tunnels has become more popular nowadays. By using encrypted tunnels for prohibited application such as peer-to-peer file sharing it is easy to gain access to restricted networks. Application-layer tunnels provide a possibility to bypass network defenses which is even more useful for malicious users trying to avoid detection. The accurate identification of application flows in encrypted tunnels is important for the network security and management purposes. Traditional network traffic classification methods based on port numbers or pattern-matching mechanisms are practically useless in identifying application flows inside an encrypted tunnel, therefore another approach is needed. In this paper, we propose a two-phased method for classifying SSH tunneled application flows in real time. The classification is based on the statistical features of the network flows. The first classification phase identifies the SSH connection while the second classification phase detects the tunneled application. A simple K-Means clustering algorithm is utilized in classification. We evaluated our method using manually generated packet traces. The results were promising; over 94%of all flow samples were classified correctly, while untrained application flow samples were detected as unknown at high precision.
Due to the importance of topology discovery for many tasks of sensor networks, in this paper, we discuss the basic approaches for network topology discovery, point out the security issues of the network topology discovery and propose possible countermeasures.
In the modern digital age the world became a global village. Finger tips availability of all government services is the goal of every government. That's why the governments of today's world are transforming their manual government services to electronic government services frequently. In the meantime the entire digitization of the system is prone to various security issues. Clear steps must be taken to ensure the privacy, authentication, integrity, and availability etc. This paper outlines some essential measures related to the e-government security. It also adds the security index into the e-government assessment method of the Department of Economic and Social Affairs (ESA) of United Nations (UN). The security index is based upon the international standard ISO 27001. Authors believe that the paper is useful for the policy makers, implementers and users of the e-government.
For the hackers' attacks on the network is constantly updated, the traditional defense dealing with unknown invasion become powerless and can not effectively protect the network. This paper proposes the application of honeypots in the LAN system, where the virtual and physical honeypots are placed in a specific location. Honeypots can initiatively lure hackers to attack the internet, take the record of the ways and means of their invasion, and then analyze and study them. If necessary, the invasion records can be extracted to be evidence. In this way, you can learn about the latest attack methods and tools, updating Firewall and intrusion detection knowledge base and to some extent detering the intruder. This thesis focuses on the combination of a variety of defense technology to explore completely the advantages of Firewall, intrusion detection and honeypot technology to enhance the local area network security.
In this paper the problem of the compensation for the action of a misbehaving node in a networked system with acyclic communication graph is considered. A thorough analysis of a diagnostic tool is investigated and a simple multinode strategy is proposed. The solution has the architecture of a collaborative multinode with precise and easy instructions for each node of the monitoring network. Implementing issues are investigated, considering the adaptation of the diagnostic algorithm in the presence of bounded uncertainties. Simulations are made in order to validate theoretical results.
With its 9 cores per chip, the IBM Cell/Broadband Engine (Cell) can deliver an impressive amount of compute power and benefit the string-matching kernels of network security, networkbusiness analytics and natural language processing applications. However, the available amount of main memory on the system limits the maximum size of the dictionary supported by the string matching solution. To counter that, we propose a technique that employs compressed Aho-Corasick automata to perform fast, exact multi-pattern string matching with very large dictionaries. Our technique achieves the remarkable compression factors of 1:34 and 1:58, respectively, on the memory representation of English-language dictionaries and random binary string dictionaries. We demonstrate a parallel implementation for the Cell processor that delivers a sustained throughput between 0.90 and 2.35 Gbps per Cell blade, while supporting dictionary sizes up to 9.2 Million average patterns per Gbyte of main memory, and exhibiting resilience to content-based attacks. This high dictionary density enables natural language applications of an unprecedented scale to run on a single server blade.
Intelligent Operations and Management, in general, and, Intelligent Network Operations and Management, in particular, is increasingly about End-to-End control across multiple networks/services; across multiple layers in network, computing, and software stacks; and, across a variety of time-frames. It is, therefore, a problem of integration and analysis of huge amounts of very heterogeneous data in real time. This talk will contain an overview of AT&T Shannon Labs research, and a discussion of our work in Information Mining and Software Research with applications in telecommunications industry including network operations, network security, IP network management, fraud detection, marketing, and business & consumer markets analysis. The emphasis will be on near real-time analysis and mining of large scale data streams. Selected future research direction will be presented.
After detailed research on Chengguan Railway Line's transportation organization and its traffic control characteristic, we makes improvement for traditional CTC system and design a decentralized and autonomous CTC system named FZy-CTC system in this paper. Then the paper analyses the system software and hardware structure, discusses the key technologies as network security, stage-plan adjustment, logic train number tracking, dispatching command safety select-control, regional interlocking control, GSM-R communication and describes the system functions. Finally current system operation situation shows that FZy-CTC system accomplishes the function of remote and intellectualized control of train operation and shunting operation route with labor intensity relieved and production efficiency improved.
We often receive unwanted information from a variety of electronic systems mainly through emails, electronic boards and messengers, called spam. Spam is the use of electronic messaging systems to send unsolicited bulk messages indiscriminately. Widely varying estimates of the cost associated with spam are available in the literature. However, a stochastic and quantitative analysis of the determinant characteristics of spam traffic is still an open problem. This work fills this gap. A 4-year data sample of real-time inbound traffic between May 2005 and July 2009 was collected to investigate and analyze characteristics of spam traffic through JIRANSOFT's Spam Sniper on the network at Korean Bible University. Our major findings of a statistical analysis of spam traffic are that (i) real-time inbound spam traffic is statistically more correlated (self-similar) when compared to normal traffic, and (ii) the degree of self-similarity measured in terms of the Hurst parameter H and obtained from different estimation techniques is very high.
In this paper, we present a novel network security evaluation method framework, with a comprehensive analysis of the MADM (Multiple Attribute Decision Making) theory. This framework constructs a measurement model of network security, and normalizes the measurement process. It also provides specific evaluation methods to satisfying the practical requirements. Thereinafter, an example of network worm propagation evaluation is illustrated. Compared to existing evaluation methods, our methods are more comprehensive and scientific, and can make the rank preference order of each worm life cycle stage of each worm defense strategy. Our approach makes contribution on the standardization and scientific of the network security evaluation process.
After analyzing the existing research of network security situation awareness, a framework of situation analysis is proposed in this paper. It is an application and reification of the classic situation awareness model proposed by Tim bass. The framework is composed of three core contents, namely, situation information model, event correlation analysis technology and situation assessment technology. The information model defines what is situation and how to express them, the other two technologies are the implement means of acquiring these situation information. The hierarchic information model contains four levels: raw security datas, security entities, assessment report, and mission impact. Along with the rising of the model level, the quantity of the information decreases while the quality increases. The correlation technology focuses on achieving the security entities, that is the second level situation information. The situation assessment technology provides methods and means for acquiring the information belongs to the third and the fourth levels, namely, it is the technical guarantee of creating assessment report and mission impact. The framework provides guidance and technical support for the whole situation analysis procedure, and it is the foundation of the analysis work.
The diffusion of mobile devices and technologies supporting transparent network mobility can have detrimental effects on network security. We describe how an attacker can lever-age mobility in IPv6 networks to perpetrate known attacks while evading detection by state-of-the-art Network Intrusion Detection Systems (NIDSs). We then propose a new defense strategy based on the exchange of state information among distributed NIDSs. We demonstrate the effectiveness of the proposed solution through a prototype implementation, evaluated experimentally in a Mobile IPv6 network.
With the increasing number of security devices and rules in the network, the complexity of detecting and tracing network security configuration errors become a very challenging task. This in turn increases the potential of security breaches due to rule conflicts, requirement violations or lack of security hardening. Most of the existing tools are either limited in scope as they do not offer a global analysis of different network devices or hard to comprehensively use because these tools are not declarative. Declarative logic programming can readily express network configurations and security requirements for verification analysis. In this paper, we use Prolog to model the entire network security configurations including topology, routing, firewall and IPSec. This is implemented in a tool called ConfigAnalyzer, which was also evaluated with large network and policy sizes. The tool allows for verifying reachability and security properties in flexible and expressive manner. It also allows for evaluating security configurations in terms of accessibilities credentials and rules.
Network monitoring is an essential task of network management. Information obtained by monitoring devices gives a real picture of the network in production including transmitted data volumes, top hosts, a list of frequently used applications etc. Deep analysis of data collected by monitoring can reveal network attacks or detect misuse of network services. In addition, Data Retention Act requires each ISP to track user's activities. Protocol IPv6 puts new challenges for network administrators in the context of user identification. Unlike IPv4, an IPv6 address no longer uniquely identifies a user or PC. IPv6 address can be randomly generated and keeps changing in time. PCs with IPv6 stack can also communicate via predefined tunnels over IPv4 infrastructure. That tunneled traffic mostly bypasses network security implemented via firewalls. In this paper, we identify major monitoring and security issues of IPv6 connectivity and propose a solution based on SNMP and Netflow data that helps to uniquely identify users. The solution requires an extended set of monitoring data to be collected from network devices. We present a new data structure based on extended Netflow records. Feasibility of the approach is demonstrated on the Brno University of Technology (BUT) campus network.
The amount of data that floods today's networks is well beyond what security analysts can manage by textual means alone. In an effort to solve this problem, researchers have explored different methods of visualizing network security threats. There is little debate that humans can perceive more information visually than textually. The problem is that the majority of visualization tools in practice or proposed do not take efficient visualization techniques into consideration. As a result, it is difficult to get a high-level view of the network that facilitates rapid isolation of network attacks. We propose the Converged Security Visualization Tool (Cover-VT) to solve the efficient visualization problem. Cover-VT was designed to provide analysts with a high-level view of network threats using geographic information systems. The tool allows for rapid identification of threats by minimizing the cognitive obstacles to efficient threat location. Cover-VT includes the capability to drill-down on a node of interest for additional details and even filter out unwanted data. Cover-VT was designed with usability in mind, making it easy to comprehend while assisting the analyst in rapid threat identification. Many different security tools make up a security analyst's tool kit. Cover-VT was developed as an effective security visualization system that integrates existing security tools and network security systems.
The introduction of self-* properties over distributed network management infrastructures has been proving to be a feasible approach for the new demands of modern network management. Among the properties of the self-* management vision, self-healing figures as key property in improving the dependability of the managed infrastructures. An interesting possibility to materialize self-* support - and self-healing support as well - in network management is through the employment of peer-to-peer (P2P) management overlays. Considering this scenario, we introduce in this paper a self-healing service provided by a prototype P2P-Based Network Management (P2PBNM) system. Such a service is expected to be contracted by human administrators interested in monitoring and recovering their IT infrastructures. In addition, an experimental evaluation of the self-healing service is performed considering a case study where a Host-based Intrusion Detection System (HIDS) needs to be constantly observed and eventually healed to keep the underlying communication network protected.
Honeypot evangelists propagate the message that honeypots are particularly useful for learning from attackers. However, by looking at current honeypots, most of them are statically configured and managed, which requires a priori knowledge about attackers. In this paper we propose a high-interaction honeypot capable of learning from attackers and capable of dynamically changing its behavior using a variant of reinforcement learning. It can strategically block the execution of programs, lure the attacker by substituting programs and insult attackers with the intent of revealing the attacker's nature and ethnic background. We also investigated the fact that attackers could learn to defeat the honeypot and discovered that attacker and honeypot interests sometimes diverge.
Voice over IP (VoIP) and the Session Initiation Protocol (SIP) are establishing themselves as strong players in the field of multimedia communications over IP, leveraged by low cost services and easy management. Nevertheless, the security aspects are not yet fully mastered. In this paper we present an open-source implementation of a VoIP SIP-specific honeypot named Artemisa. The honeypot is designed to connect to a VoIP enterprise domain as a back-end user-agent in order to detect malicious activity at an early stage. Moreover, the honeypot can play a role in the real-time adjustment of the security policies of the enterprise domain where it is deployed. We aim, by this contribution, to encourage the deployment of such honeypots at large scale and the collection of attack traces. We test the capacity of the honeypot to handle a series of known SIP attacks and present results from diverse scenarios.
Intruder is one of the most publicized threats to security. In recent years, intrusion detection has emerged as an important technique for network security. Data mining techniques have been applied as a new approach for intrusion detection. The quality of the feature selection methods is one of the important factors that affect the effectiveness of Intrusion Detection system (IDS). This paper evaluates the performance of data mining classification algorithms namely J48, Naive Bayes, NBTree and Random Forest using KDD CUP'99 dataset and focuses on Correlation Feature Selection (CFS) measure. The results show that NBTree and Random Forest outperforms other two algorithms in terms of predictive accuracy and detection rate.
The network based denial of service attacks (DoS) are still the big challenge to the researchers in the field of network security. This paper handles the popular DoS attack called TCP-SYN flood attack, and presents the design and implementation of an Artificial Immune system for Syn flood Detection, abbreviated by AISD, based on the Dendritic Cell Algorithm (DCA). The AISD system is able to detect the generated SYN flood attack and response to its generator in a real-time. Performance and accuracy of the system have been evaluated through five experiments. Results of the experiments showed the precision of intrusion detection process to the ratio of 100%, with a notable response speed, and this is shows the benefit and suitability of using artificial immune systems to the network security problems.
The problem of secure operation system risk management is difficult to evaluate quantitatively, a quantitatively risk assessment model for the operating system is proposed in this paper. Through introducing the risk matrix, putting the information safety risk assessment as the risk evaluation model by experts matrix. Borda rule and AHP (Analytic Hierarchy Process) assess the risk assessment process, and achieve the quantitative evaluation of the secure operation system risk management, enhancing the objectivity of the operating system risk quantitative evaluation. In conclusion, the application evaluation model is validated by the examples, and the experimental results show that the model can effectively assess the operating system security risk levels.
SSL is applied to network communication security field by enterprise, especially VPN field, the VPN has become very important like IPsec VPN. This paper on the basis of previous studies, Put forward a method based on the SSL tunnel technology and applied to FTP transmission system. Through add a application-layer protocol to solve the problems of tunnel connecting safety, and We have made an in-depth analysis on the encrypted tunnel approach of establishing SSA rather than a simple approach of data encryption in order to provide security for the remote file transfer system in enterprises.
Digital image watermarking is a useful solution to the problem of information security, copyright and network security. In this paper, a new watermarking algorithm is propose based on Singular Value Decomposition (SVD) and discrete wavelet transform (DWT). The algorithm uses a gray image as a watermark, increasing embedded information capacity. The algorithm can satisfy the transparence and robustness of the watermarking system very well. The experimental result based on this algorithm have shown that the watermarking is robust to the common signal processing techniques including JPEG compressing, adding noise, low pass filter, median filter, cropping and so on.
Digital libraries are important information base establishment in university. The securities of its existence and service are paid attention continually. Thinking of the requests of security for digital resources, those factors of affecting digital library securities were discussed deeply, and countermeasures of security for university digital library.
The physical layer of an optical network is vulnerable to a variety of attacks, including jamming, physical infrastructure attacks, eavesdropping, and interception. As the demand for network capacity grows dramatically, the issue of securing the physical layer of optical network cannot be overlooked. In this survey paper, we discuss the security threats in an optical network as well as present several existing optical techniques to improve the security. In the first part of this paper, we discuss various types of security threats that could appear in the optical layer of an optical network, including jamming, physical infrastructure attacks, eavesdropping, and interception. Intensive research has focused on improving optical network security, in the above specific areas. Real-time processing of the optical signal is essential in order to integrate security functionality at the physical layer while not undermining the true value of optical communications, which is its speed. Optical layer security benefits from the unique properties of optical processingnstantaneous response, broadband operation, electromagnetic immunity, compactness, and low latency. In the second part of this paper, various defenses against the security threats outlined in this paper are discussed, including optical encryption, optical code-division multiple access (CDMA) confidentiality, self-healing survivable optical rings, anti-jamming, and optical steganography.
This paper is presenting one possible centralized approach on building trust within the Romanian Educational Network.
Recently, broadband optical access networks were being deployed worldwide to deliver large bandwidth to end users. Currently, the operators have been mainly deploying time-division multiplexing (TDM) passive optical networks (PON) in their networks. As bandwidth demand continues to increase, the industry further selected several next generation optical access (NGA) options to upgrade from current TDM-PON architectures. Besides increasing bandwidth requirement, current TDM-PONs suffers from limited reach and split-ratio. To further reduce the capital and operational expenditure requirements of current TDM-PON, NGA architectures must consolidate the number of central office sites. Long-reach optical access (LROA) architectures are promising solutions that employ optical amplifiers to increase optical budget to support longer reach and higher split-ratio in the distribution network. As more end users are supported over shared physical medium in LROA, access network security is of greater importance. The first part of this article reviews important LROA architectures and their enabling technologies. Moreover, few works in current literature address security weaknesses in LROA. The second part of this article reviews these security weaknesses as well as monitoring and control techniques employed in current TDM-PONs. In addition, a novel class of quasi-passive and reconfigurable devices is presented that addresses aforementioned security weaknesses and is suitable for LROA.
The ability to share information between organizations in an ad-hoc, networked environment frequently found in humanitarian assistance/disaster relief (HADR) operations, remains a challenge today. For example, US Armed Forces participating in HADR operations do not have an organic ability to share information bi-directionally with Non-Government Agencies (NGO) such as Doctors-Without-Borders. It such cases, communications are normally accomplished outside existing military networks.
For today's Internet world appears various network integrity, safety control problems, such as parsing the IP network security mechanism continuity and high credible runaway drawbacks, honesty network of innovation mechanism of strategy points. Full text aims to breakthrough "IP skillfulness" monopoly, show a can explore the new high trusted network era.
The intrusion detection rate is greatly influenced by the parameters of the support vector machine (SVM) model. In order to overcome the parameter limits to improve the identify accuracy of Distributed Denial of Service (DDoS) attack, this paper presents a new detection method based on Kernel Principle Component Analysis (KPCA) and Particle Swarm Optimization (PSO)-Support Vector Machine (SVM). The KPCA was used to obtain the important characteristics of the intrusion data to eliminate the redundant features. Then the PSO was used to optimize the SVM parameters. Experimental results show the proposed approach can enhance the detection rate, and performs better than the PCA based methods.
As part of network security testing, an administrator needs to know whether the firewall enforces the security policy as expected or not. In this setting, black-box testing and evaluation methodologies can be helpful. In this paper, we employ a simple mutation operation, namely flipping a bit, to generate mutant firewall policies and use them to evaluate our previously proposed weighted test case selection method for firewall testing. In the previously proposed firewall testing approach, abstract test cases that are automatically generated from firewall decision diagrams are instantiated by selecting test input values from different test data pools for each field of firewall policy. Furthermore, a case study is presented to validate the proposed approach.
The following topics are dealt with: ubiquitous computing; ubiquitous services; multimedia services; multimedia security; ubiquitous security and multimedia communication.
With the arrival of campus digitalization, the application of campus network promotes continuously and schools are more dependent on campus network information. As a result the information security problem is becoming more and more threatening. This research paper elaborates the present condition of digital campus network information security, analyzes the main threats to it, and puts forward relevant solutions to main problems existing in network information safety for colleges and universities.
As the Internet technology, speech technology progress and development, VOIP gradually become the mainstream of the present and future voice technology. VOIP and operation in IP network application system based on IP other as common security threat and weaknesses, including all the threat from IP network layer. VOIP technology safety problems also gradually revealed, more and more get more extensive attention. This paper makes a detailed analysis discussed several security potential threat and existence of the mode of attack such as: denial of service attack, the capture of data packets with tampering or broker attack, etc. A series of existence for VOIP network security threat put forward different levels of safety measures and strategies intersectorial; At last, it points out that these security measures and strategies exist some drawbacks.
Coal is China's traditional enterprise. Bringing B2B e-commerce transaction model into the coal industry is to adapt to the times and the need of the coal industry to speed up development. In reference to the legal problems such as security, information integrity, and legitimacy problems of users produced under the B2B e-commerce model of coal enterprises, it's necessary to develop and improve network security laws and regulations, to develop e-commerce basic laws, to ensure B2B safety transactions, and to improve existing e-commerce laws and regulations.
HTTP-tunnel is always used by Trojans and backdoors to avoid the detection of firewalls, and it is a threat of network security. HTTP-tunnel traffic is encrypted now, and the only way to detect the HTTP-tunnel traffic is based on statistical features of transport layer. There are a few methods in detection of HTTP-tunnel, and the statistical fingerprinting is an effective method. The method of statistical fingerprinting is instability because the features which the method using is the packet size and the inter-arrival time, and its accuracy is determined by the volume of training set. We suggested a method based on C4.5 algorithm which using the features of packet and flow. Comparing to the algorithm of fingerprint, the C4.5 algorithm had some advantages in stability, accuracy and efficiency in our experiment.
With the development and application of network technology, the issues of network security has become prominent increasingly. Network security risk assessment has become the key process in solve network security. Support Vector Machine(SVM)is one of novel learning machine methods, its advantages are simple structure, strong compatibility, global optimization, least raining time and better generalization. So it has superiority to apply it into network security risk assessment. This paper describes the content and the evaluation indicators of network security risk assessment and the classification of the support vector machine in detail. And then an assessment method of network security risk based on support vector machine is proposed in this paper. Experiment results show that the method Is feasible and effective.
Network marketing channels account for an increasing proportionat the company's business as technology advances and changes in lifestyle. At present, network marketing channels is not concentrated, and the management is not uniform. Enhancing system security and making up for security vulnerabilities are the urgent problems to be solved as. The article introduces the concept of network marketing security, analyzes the major factors of endangering the security of network marketing, and proposes countermeasures.
With the development of the information technology, the network threats are rampant day by day. In order to protect company and organization network, They have deployed a variety of network security products, such as IDS (intrusion detection system), firewalls, VPN and AVS (anti-virus system). Because of the lack of data exchange mechanism, security product cannot share the security information each other and causes the large volume of alarm message or false alarm. In this case, we need a platform or system to solve this problem The security operations center can collect and manage these security events and analyzes the related information, can reduce the false positives and false negatives, and improve security of network system. In this paper, we do some research on the event correlation analysis, introduce the processes of the analysis and the method of rule extraction, and then introduce the system architecture of security operations center.
Through the research summary of the existing network security management technologies, a network security management platform based on distributed architecture is proposed, a unified security strategy configuration and centralized security audit of the security device in the network is completed, so that users of the security management can manage, monitor and audit for the entire network security, which improves the efficiency and enforcement effort of network security management.
Computer network security is an international problem, Economic losses caused by breakage of computer network security system up to several billions of dollars on a global scale every year. After entering the new century, the loss will reach over 200 billion U.S. dollars. Inofrmation thieves in the past 5 years have been growing to 250%, 99%of large companies have been big in the invasion according to the statistics. World-famous commercial websites, such as Yahoo, Buy, EBay, Amazon, and CNN have been hacked, resulting in huge economic losses. Even RSA, specializes in network security web sites, has been attacked by hackers at some time in the past. Effective security policy or program development is the primary goal of network information security. Network security technology include authentication and authorization, data encryption, access control, security auditing. This paper focuses on intrusion detection technology which is the core technology in the security audit that is one of the network security protections of the important part.
Regular expression matching is a key technology of network security. Around this issue, one of the regular expression patterns in real rule sets called counting constraint pattern is analyzed in this paper. Besides, the complex characteristics of counting constrain pattern syntax and the problems of state explosion lead by patterns composition are studied. Based on the analysis of the limitations of existing matching algorithms, aimed at the multiple counting constraint pattern composition, a papilionaceous automata applied to deep packet inspection is proposed. The results show that the compression ratio could reach 94.86%in Linux L7-filter rule sets, and for the majority of Bro rule sets and Snort rule sets, the compression ratio could up to 99%.
With the rapid development of computer technology and network, the modern world is gradually evolved into an electronic world and all information is in the full digital. Especially in universities, the campus network has become an important infrastructure and it is used widely in teaching, office, library management, etc. With the development of the scale of campus network, the network environment is becoming more and more complicated. So network security plays a decisive role in ensuring the campus network operating stably and it has become one restricting factor in university information development. The paper researches and discusses some measures to help network management personnel to make scientific decision-making through analyzing various problems of college computer network and finding the inherent weaknesses of college computer network in order to provide a safe means of teaching and working environment for teachers and students.
Based on analysis on applications by perception control technology in computer network security status and security protection measures, from the angles of network physical environment and network software system environmental security, this paper provides network security system perception control solution using Internet of Things (IOT), telecom and other perception technologies. Security Perception Control System is in the computer network environment, utilizing Radio Frequency Identification (RFID) of IOT and telecom integration technology to carry out integration design for systems. In the network physical security environment, RFID temperature, humidity, gas and perception technologies are used to do surveillance on environmental data, dynamic perception technology is used for network system security environment, user-defined security parameters, security log are used for quick data analysis, extends control on I/O interface, by development of API and AT command, Computer Network Security Perception Control based on Internet and GSM/GPRS is achieved, which enables users to carry out interactive perception and control for network security environment by WEB, E-MAIL as well as PDA, mobile phone short message and Internet. In the system testing, through middleware server, security information data perception in real time with deviation of 3?%was achieved, it proves the feasibility of Computer Network Security Perception Control System.
This article through to the ARP deception attack principle and PPPoE working principle analysis, thought the enterprise network erect PPPoE server on the Internet, but fundamentally solve the ARP deception problem, the complete elimination of the network at present ARP attack. To enhance network security. And PPPoE server set up are given access method.
Hardware-based Virtual Machine (HVM), for its high efficiency and good simulation, has played an important role in the course of building honeypots and debuggers. But some malwares called HVM-Aware Malware can identify the virtualized environment which they are processing in and then stop themselves to avoid being analysed. One of the most efficient HVM-Aware technologies is called "Counter-Based Detection". It's a very risky challenge to HVM-based honeypots and debuggers. In this paper a new mechanism based on operation interception and return value distortion is presented to solve this problem, it can prevent HVM from being identified so that it can protect honeypots and debuggers very well. At last some experiments demonstrate the effectiveness and practicality of the mechanism.
Nowdays firewall technology in the field of the network security has been used widely, however, facing to various means of attacks, the vulnerability and limitations of firewall technology are more obvious. The paper discusses the honeypot technology in the field of the network security technology. According to the existed shortage of honeypot system, the distributed honeypot intrusion system based on intrusion tracking was proposed. The system uses distributed deployment. It expands the network area, and uses the package marking technology to identify sources of real attacks. The existed network is protected better.
In this paper, we try to evaluate performance and major operational firewall in the market today. As the importance of network security in any organization, summarized the main compliance and need to have clean and efficient and robust firewall configuration and management. The best writer knowledge, currently the most bear and reported by the research work has been a theoretical firewall, lack of practical implementation. We tried comparing kinds of firewall based on performance concrete realization. Comparison of various firewall, this paper brings forward a kind of will help to choose appropriate suppliers.
Key management is one of the most challenging security issues in wireless sensor networks where sensor nodes are randomly deployed in a hostile territory However, due to the resource constraints, achieving such key agreement in wireless sensor networks is nontrivial. Many key agreement schemes used in general networks, such as Diffie-Hellman and public-key-based schemes, are not suitable for wireless sensor networks. Several exiting key management schemes have been proposed in literature to establish pairwise keys for wireless sensor networks, but they either can not offer strong resilience against node capture attacks or have overly large memory requirement to achieve high degree of connectivity. In this paper, we prose a novel pairwise key management scheme to enhancing the security. In the proposed scheme, part of keys in the key pool are computed by using hash function and the hash value are as new keys and put back into the key pool. Comparison to the existing approaches, this proposed scheme provides a stronger resilience against node capture attack.
Nowadays, organizations discover that it is essential to protect their valuable information and internal resources from unauthorized access like deploying firewall. Firewall could prevent unauthorized access, but it cannot monitor network attacks. Another network security tool such as intrusion detection system is necessary to perform network activities monitoring. With the recent trend of high-speed networks, a large volume of data should be analyzed and processed with high-speed infrastructure. To promote the performance of network intrusion detection system and reduce the processing time of the traffic, present studies on network intrusion detection system for high-speed network focus on parallel techniques as an alternative. In this paper, a kind of parallelism is proposed to improve the performance of signature based intrusion detection system. The experimental results show that by the use of two signature based network intrusion detection systems running Snort in parallel with a portion of packets and a subset of rules, and distributing the traffic between them, the processing time of the traffic will be reduced. Consequently, the performance of the system will be improved.
At present, the Network Security is the main study point in the research of Network Technology. Vulnerability assessment technology can detect potential security vulnerabilities and assess the security situation of network. It is one of the most important network security technologies. On the basis of the analysis of the system Vulnerabilities, the article construct a Network security Assessment System based on c/s by adopting detection which is on the basis of RDB (relational database) and the assessment method which is on the basis of fuzzy. Thorough the test on the campus, the result shows that this system could work efficiently and portably. In the mean time it helps to have a comprehensive and precise scanning to the user's system.
Unmodeled dynamics are the unavoidable nonlinear effect that can limit control performance in robotic systems. The unmodeled dynamics of the system include uncertainty or unknown and unmeasured states. Meanwhile, it is not available for the control. Based on universal approximation results for radial basis function neural networks (RBF-NN), it has been proposed as an alternative to NN for approximating arbitrary nonlinear functions in L2(R). Adaptive RBF neural network is used to design a compensator for unmodeled dynamics in robotic system. Then asymptotically stability of the system is assured by combining nominal feedback controller and adaptive law of NN. The simulation results show the validity of the control scheme.
With the continuous development of network technology, Web mail technology has become an important means of information transmission and management in government agencies, enterprises and individuals, however, the subsequent problems of protecting web mail security has become a concern to users. This paper presents a free, flexible and not dependent on the mail protocol web mail forensics system model by deep study of Email principle and method on the basis to network forensics, so as to improve the mail safety and accuracy. This paper in the end verified the validity and rationality of the model.
Cognitive Radio Networks (CRNs) emerge as a possible solution for the lack of spectrum, and make different networks merge. However, security becomes the key problem need to be solved. Cognitive radio networks face not only traditional network security intrusion problems, but also unique security risks. In this paper, we investigate security issues in different classes which including spectrum security and security of network merge. Countermeasures are given from three aspects: security mechanism, key management and identity authentication.
Computer security is a hot topic, more and more Internet users are concerning about computer security. Linux is a operating system like Unix. It has all the features of Unix operating system. Linux system has a very strict structure like Unix in security. This paper discusses the network security of Linux system from the technical aspects of network security. The paper introduces methods of protecting network security which set the right to accessing the customer machine and use firewall technology. Describe detailed the Netfilter`s structure and Netfilter`s position in network.
Along with the unceasing expansion of colleges' library network scale, the problems of network security are obviously increased. This thesis describes the advantages of VLAN and its practical significance on library local area network, and analyses the library network application and structure in colleges and universities. Based on the design of the library network in Xinyang Agricultural College, coping with unsafe factor and the colleges' libraries individual features, the paper proposes the available solution on the security issue and the library network partitioning strategy.
Denial of Service (DoS) and distributed denial of service attack (DDoS) is now a common means of attack that affects seriously network security and the quality of online services. This paper analyzes the DoS (DDoS) attack prevention principles and gives an thorough analysis of existing prevention techniques, proposed to prevent DoS (DDoS) attacks in three ways: using a router DoS attack prevention, increase the trusted platform module, increase system defenses.
With the continuous development of university information and network, network security has become an unavoidable problem. And it has become increasingly unable to meet the needs only relying on the firewall technology. Therefore, intrusion detection is more and more important to protect the network security. In this paper, the author analyzed the design and implementation technology of event detection and acquisition module and data analysis module emphatically to build campus network intrusion detection system.
Networking attacks embedded in spam emails are increasingly becoming numerous and sophisticated in nature. Hence this has given a growing need for spam email analysis to identify these attacks. The use of these intrusion detection systems has given rise to other two issues, 1) the presentation and understanding of large amounts of spam emails, 2) the user-assisted input and quantified adjustment during the analysis process. In this paper we introduce a new analytical model that uses two coefficient vectors: 'density' and 'weight'for the analysis of spam email viruses and attacks. We then use a visual clustering method to classify and display the spam emails. The visualization allows users to interactively select and scale down the scope of views for better understanding of different types of the spam email attacks. The experiment shows that this new model with the clustering visualization can be effectively used for network security analysis.
With the popularity of the Internet services, network security becomes critical issue in the Internet world. Especially, the threats of malicious accesses make the firewall systems have to low down performance due to strict inspections. In this paper, we propose an adaptive firewall system in collaboration with DNS (Domain Name System) which introduces querier's IP address notification feature. With such a feature, the proposal system can identify whether each communication flow can be trusted or not by checking the querier's IP address and the DNS query target domain name. Then based on the result of checking, the firewall system adaptively decides specific operation for specific connection. Consequently, the trusted flows go through bypass route of higher bandwidth without heavy packet inspection while untrusted flows will be blocked or restricted by strict packet inspection. Thus, the firewall system totally accomplishes higher throughput.
This paper presents a new cryptosystem based on chaotic continuous cellular automata (CCA) to increase data protection as demonstrated by their flexibility to encrypt and decrypt information from distinct sources. Enhancements in cryptosystems are also presented including (i) the model based on a new chaotic CCA attractor, (ii) the dynamical integration of modules containing dynamical systems to have more complex sequences, and (iii) an enhancement for symmetric cryptosystems by allowing them to generate an infinite number of keys. This paper also presents a process of mixing chaotic sequences obtained from cellular automata, instead of using differential equations, as a basis to achieve higher security and high speed for the encryption and decryption processes using dynamical modular cryptosystems than other recent approaches. The complexity of the mixed sequences is measured using the variance fractal dimension trajectory to compare them with the realization of an unmixed chaotic sequence to verify that the former are more complex. This type of multiscale measure and evaluation never has been done in the past outside this research group.
Secret sharing is important in information and network security and has broad applications in the real world. Since an elegant secret sharing mechanism was first proposed by Shamir in 1979, many schemes have appeared in literature. These schemes deal with either single or multiple secrets and their shares have either the same weight or different weights. Weighted shares mean that different shares have different capabilities in recovering the secret(s) -- a more (less) weighted share needs fewer (more) other shares to recover the secret(s). In this paper, we identify a direct relation between the length (i.e., the number of bits) and the weight of shares and, based on this relation, present a new Chinese Remainder Theorem (CRT) based weighted multiple secret sharing scheme. This scheme can also be naturally applied to other cases such as sharing a single secret with same-weight shares and is remarkably simple and easy to implement. Compared to both Shamir's scheme and Mignotte's scheme -- the representative of existing CRT based secret sharing schemes, the new scheme is more efficient than both schemes in share computation and more efficient than Shamir's scheme (and as efficient as Mignotte's scheme) in secret recovery. One prominent advantage of the new scheme is that the sizes of shares can vary distantly to fit different requirements and constraints of various devices such as sensors, PDAs, cell phones, iPads, hence, the new scheme is able to apply to broader applications involving wireless/sensor networks and pervasive computing.
Multi-pattern matching algorithm and architecture is critical for packet inspection based network security applications, especially for high speed network or large pattern sets. This paper presents a method to optimize the potential memory usage of DFA based algorithms for multi-pattern expression matching by the combining DFA's paths, named isomorphic path combina-tion (IMPC). To achieve IMPC, a novel multi-pattern matching algorithm, called ACS, is proposed, which is based on CDFA. Compared to the algorithms on DFA, our method can reduce 78.6%states for Snort pattern set, which results in one of the most memory efficient methods. The most important is that our method is a kind of optimization and can be embedded to other algorithms as the second step for better results. Finally the architecture based on ACS is proposed and the experimental results show that 47.6%to 84.0%memory space can be saved for different size of pattern sets as compared to the best known architectures. The method is another one based on CDFA. It means that CDFA may be a more proper model for multi-pattern matching than other FAs.
Many network security applications in today's networks a0re based on deep packet inspection, checking not only the header portion but also the payload portion of a packet. For example, traffic monitoring, layer-7 filtering, and network intrusion detection all require an accurate analysis of packet content in search for predefined patterns to identify specific classes of applications, viruses, attack signatures, etc. Pattern matching is a major task in deep packet inspection. The two most common implementations of Pattern matching are based on Non-deterministic Finite Automata (NFAs) and Deterministic Finite Automata (DFAs), which take the payload of a packet as an input string. In this paper, we propose an efficient NFA-based pattern matching in Binary Content Addressable Memory(BCAM), which uses data search words consisting of 1s and 0s. Our approach can process multiple characters at a time using limited BCAM entries, which makes our approach scalable well. We evaluate our algorithm using patterns provided by Snort, a popular open-source intrusion detection system. The simulation results show that our approach outperforms existing CAM-based and software-based approaches.
Resiliency and cyber security of modern critical infrastructures is becoming increasingly important with the growing number of threats in the cyber environment. This paper proposes an extension to a previously developed fuzzy logic based anomaly detection network security cyber sensor via incorporating Type-2 Fuzzy Logic (T2 FL). In general, fuzzy logic provides a framework for system modeling in linguistic form capable of coping with imprecise and vague meaning of words. T2 FL is an extension of Type-1 FL which proved to be successful in modeling and minimizing the effects of various kinds of dynamic uncertainties. In this paper, T2 FL provides a basis for robust anomaly detection and cyber security state awareness. In addition, the proposed algorithm was specifically developed to comply with the constrained computational requirements of low-cost embedded network security cyber sensors. The performance of the system was evaluated on a set of network data recorded from an experimental cyber security test-bed.
Preserving an open Internet is a challenge given potential harms that might come to Internet users. This article suggests some directions and actions that might lead to preservation of Internet users' freedoms to speak and hear while also offering them protection from harm.
In this paper, we present a distributed multicamera face tracking system suitable for large wired camera networks. Unlike previous multicamera face tracking systems, our system does not require a central server to coordinate the entire tracking effort. Instead, an efficient camera clustering protocol is used to dynamically form groups of cameras for in-network tracking of individual faces. The clustering protocol includes cluster propagation mechanisms that allow the computational load of face tracking to be transferred to different cameras as the target objects move. Furthermore, the dynamic election of cluster leaders provides robustness against system failures. Our experimental results show that our cluster-based distributed face tracker is capable of accurately tracking multiple faces in real-time. The overall performance of the distributed system is comparable to that of a centralized face tracker, while presenting the advantages of scalability and robustness.
Program input syntactic structure is essential for a wide range of applications such as test case generation, software debugging, and network security. However, such important information is often not available (e.g., most malware programs make use of secret protocols to communicate) or not directly usable by machines (e.g., many programs specify their inputs in plain text or other random formats). Furthermore, many programs claim they accept inputs with a published format, but their implementations actually support a subset or a variant. Based on the observations that input structure is manifested by the way input symbols are used during execution and most programs take input with top-down or bottom-up grammars, we devise two dynamic analyses, one for each grammar category. Our evaluation on a set of real-world programs shows that our technique is able to precisely reverse engineer input syntactic structure from execution. We apply our technique to hierarchical delta debugging (HDD) and network protocol reverse engineering. Our technique enables the complete automation of HDD, in which programmers were originally required to provide input grammars, and improves the runtime performance of HDD. Our client study on network protocol reverse engineering also shows that our technique supersedes existing techniques.
The recent development of high-security processors and hardware is substantially changing embedded software tools, shedding light on security in the embedded development environment. The process of developing, certifying, and implementing a secure processor has several challenges that can be compared to providing deadbolts for residential properties. The deadbolt itself, no matter how well designed and tested, offers little security if it isn't installed correctly. Even more significantly, the deadbolt is useless if the owner doesn't lock it at night. These problems are similar for secure hardwareif system developers implement them incorrectly and end users don't employ security configurations, the processor's security properties are of no value in the end system. Instituting security policy in the hardware has all of the hallmarks of traditional security software: user authentication (security engineer only), preventing the impact of users or operating code on security settings, command integrity and verification, and keeping security policy audit log settings. In this article, the authors define the embedded end markets affected by embedded software assurance issues, then examine ways in which security and assurance capabilities are partitioned in hardware and software. They then examine the problems inherent to configuring secure hardware and offers a list of considerations and testing issues for providers interested in improving their embedded security environments to support secure hardware and processors.
INTEL CEO Paul Otellini betrayed no doubts when he described the thinking behind the chipmaker??s $7.7bn cash buy of security software specialist McAfee. ??Over the past couple of decades there have been a few critically important inflexions in the computing landscape. During the past decade, a combination of wireless connectivity and energyefficient computing platforms drove an explosion in mobile computing. Today, I believe we are entering a new era in computing platforms that will impact users worldwide," he told an analysts' conference call.
Policy lookup is a very essential function in packet forwarding and network security. As it is becoming a main cause of bottleneck in many network systems, this letter introduces a new policy lookup algorithm called FRFC (Fast table building for Recursive Flow Classification). Although it is based on RFC which shows the best policy lookup speed but the worst policy table update speed, it achieves high performance in table update as well as policy lookup. FRFC divides the whole rule set into smaller sub-rule sets and by doing so, it speeds up the table building time by 50 times compared to RFC when the considered rule set size is 10,000. With the rule set size increasing, the performance gap becomes larger.
The high data rates employed by wavelength division multiplexing transparent optical networks make them most suitable for todays growing network traffic demands. However, their transparency imposes several vulnerabilities in network security, enabling malicious signals to propagate from the source to other parts of the network without losing their attacking capabilities. Furthermore, detecting, locating the source, and localizing the spreading of such physical-layer attacks is more difficult since monitoring must be performed in the optical domain. While most failure and attack management approaches focus on network recovery after a fault or an attack has already occurred, we suggest a novel safety strategy, proposing a prevention-oriented method to aid attack localization and source identification in the planning phase. In this paper, we propose attack-aware wavelength assignment that minimizes the worst-case potential propagation of in-band crosstalk jamming attacks. We define a new objective criterion for the wavelength assignment (WA) problem, called the propagating crosstalk attack radius (P-CAR), and develop heuristic algorithms aimed at minimizing both the P-CAR and the number of wavelengths used. Our aim is to achieve better protection, but without the need for extra resources. We compare our algorithms with existing WA approaches, illustrating their benefits with respect to transparent optical networks security, as well as the associated wavelength utilization.
In a typical enterprise network, correct implementation of security policies is becoming increasingly difficult owing to complex security constraints and dynamic changes in network topology. Usually, the network security policy is defined as the collection of service access rules between various network zones. The specification of the security policy is often incomplete since all possible service access paths may not be explicitly covered. This policy is implemented in the network interfaces in a distributed fashion through sets of access control (ACL) rules. Formally verifying whether the distributed ACL implementation conforms to the security policy is a major requirement. The complexity of the problem is compounded as some combination of network services may lead to inconsistent hidden access paths. Further, failure of network link(s) may result in the formation of alternative routing paths and thus the existing security implementation may defy the policy. In this study, an integrated formal verification and fault analysis framework has been proposed which derives a correct ACL implementation with respect to given policy specification and also ensures that the implementation is fault tolerant to certain number of link failures. The verification incorporates boolean modelling of the security policies and ACL implementations and then formulates a satisfiability checking problem.
We study a network security game where strategic players choose their investments in security. Since a player's investment can reduce the propagation of computer viruses, a key feature of the game is the positive externality exerted by the investment. With selfish players, unfortunately, the overall network security can be far from optimum. The contributions of this paper are as follows. 1) We first characterize the price of anarchy (POA) in the strategic-form game under an Effective-investment model and a Bad-traffic model, and give insight on how the POA depends on individual players' cost functions and their mutual influence. We also introduce the concept of weighted POA to bound the region of payoff vectors. 2) In a repeated game, players have more incentive to cooperate for their long term interests. We consider the socially best outcome that can be supported by the repeated game, as compared to the social optimum. 3) Next, we compare the benefits of improving security technology and improving incentives, and show that improving technology alone may not offset the price of anarchy. 4) Finally, we characterize the performance of correlated equilibrium (CE). Although the paper focuses on network security, many results are generally applicable to games with positive externalities .
The intrusion response component of an overall intrusion detection system is responsible for issuing a suitable response to an anomalous request. We propose the notion of database response policies to support our intrusion response system tailored for a DBMS. Our interactive response policy language makes it very easy for the database administrators to specify appropriate response actions for different circumstances depending upon the nature of the anomalous request. The two main issues that we address in context of such response policies are that of policy matching, and policy administration. For the policy matching problem, we propose two algorithms that efficiently search the policy database for policies that match an anomalous request. We also extend the PostgreSQL DBMS with our policy matching mechanism, and report experimental results. The experimental evaluation shows that our techniques are very efficient. The other issue that we address is that of administration of response policies to prevent malicious modifications to policy objects from legitimate users. We propose a novel Joint Threshold Administration Model (JTAM) that is based on the principle of separation of duty. The key idea in JTAM is that a policy object is jointly administered by at least k database administrator (DBAs), that is, any modification made to a policy object will be invalid unless it has been authorized by at least k DBAs. We present design details of JTAM which is based on a cryptographic threshold signature scheme, and show how JTAM prevents malicious modifications to policy objects from authorized users. We also implement JTAM in the PostgreSQL DBMS, and report experimental results on the efficiency of our techniques.
This article proposes a robust mathematical method to strategically place trust nodes to compartmentalize a time-critical SCADA network. The trust nodes combine firewall and intrusion detection technology to provide communication network security for protection, control, and SCADA systems. The mathematical technique optimizes the placement of the trust nodes based on the timing requirements of existing systems and the number of trust nodes that are available in the system given constraints, which may arise due to budgetary limitations or the restrictions of existing utility hardware. The intent is to create a planning tool to allow utility system operators to determine the best locations to place trust nodes to increase system security given limited resources and/or hardware constraints. The operational requirements of the environment are translated into a mathematical model. Mixed integer linear programming is used to process this model in search of an optimal solution. Because the problem is provably NP-Hard, a heuristic is also given to quickly find good, but not optimal, solutions. Experiments show promise for the proposed techniques.
Unified Modeling Language (UML), an industry de-facto standard, has been used to analyze dynamically partially reconfigurable systems (DPRS) that can reconfigure their hardware functionalities on-demand at runtime. To make model-driven architecture (MDA) more realistic and applicable to the DPRS design in an industrial setting, a model-based verification and estimation (MOVE) framework is proposed in this work. By taking advantage of the inherent features of DPRS and considering real-time system requirements, a semiautomatic model translator converts the UML models of DPRS into timed automata models with transition urgency semantics for model checking. Furthermore, a UML-based hardware/software co-design platform (UCoP) is proposed to support the direct interaction between the UML models and the real hardware architecture. The two-phase verification process, including exhaustive functional verification and physical-aware performance estimation, is completely model-based, thus reducing system verification efforts. We used a dynamically partially reconfigurable network security system (DPRNSS) as a case study. The related experiments have demonstrated that the model checker in MOVE can alleviate the impact of the state-space-explosion problem. Compared to the synthesis-based estimation method having inaccuracies ranging from $-$43.4%to 18.4%, UCoP can provide accurate and efficient platform-specific verification and estimation through actual time measurements.
IT security has evolved dramatically over the last few decades. Initially, it consisted of password management and basic network security controls. However, as enterprises have come to depend on IT for every aspect of daily operations, the need for security has transcended the datacenter and now is woven through all aspects of IT operations.
Deep packet inspection is a fundamental task to improve network security and provide application-specific services. State-of-the-art systems adopt regular expressions due to their high expressive power. They are typically matched through deterministic finite automata (DFAs), but large rule sets need a memory amount that turns out to be too large for practical implementation. Many recent works have proposed improvements to address this issue, but they increase the number of transitions (and then of memory accesses) per character. This paper presents a new representation for DFAs, orthogonal to most of the previous solutions, called delta finite automata ( $delta$FA), which considerably reduces states and transitions while preserving a transition per character only, thus allowing fast matching. A further optimization exploits $N$th order relationships within the DFA by adopting the concept of temporary transitions.
We mine the logs of network traffic data to find the contexts of attacks; we call them attack patterns. We propose an iterative algorithm for discovering attack patterns via a feedback mechanism, with the degrees of belief for attack instances propagated to the next iteration to further refine the search. Our simulations verify that the algorithm achieves accuracy in discovering attack patterns. Our attack pattern discovery has the additional advantage of being an unsupervised algorithm, e.g., it does not require a priori user-defined thresholds.
The quality of information is crucial for decision making in many dynamic information sharing environments such as sensor and tactical networks. Information trustworthiness is an essential parameter in assessing the information quality. In this paper, we present a trust model to evaluate the trustworthiness of information as well as information publishing entities based on information provenance. In our trust model, decision makers can give an evaluation on the information they receive and further adjust the evaluation result to a more accurate value by considering two factors: information similarity and path difference. We introduce Collusion Attacks that may bias the computation and present a mechanism to detect and reduce the effect of Collusion Attacks. Based on the final adjusted information trust, feedback is given to the information publishing nodes to adaptively update their trust scores. Therefore, our collusion-resistant scheme can dynamically assess the trustworthiness of information as well as participating entities in a network and thus effectively enhance the network security. Detailed analysis of the proposed approach is presented along with simulation results.
Machine to machine communication (M2M) has received increasing attention in recent years. However, its development and character cause new security challenge. The security and trust problems are serious in M2M system. A survey to security and trust in M2M system is necessary. Different visions of the M2M security and trust paradigm are reported and reviewed in this paper. They includes typical technology research progress and security product. All these will help to understand security and trustworthy in M2M system.
Weak self-defense ability of network security system would threaten the computer data security. This paper presents the design and implementation of a daemon which can enhance the self-defense ability of the network security system. Using plugin and config file, this daemon has good scalability and maintainability.
IPSec technology is important in computer network security, it is an IP layer security frame agreement formulated by IETF. This paper introduces the principle and the related concept, and discusses the actual application scenes.
The port scanning is one of the important network security scanning technologies and the foundation of network security detection. Detecting the port scan and finding potential aggressive behavior can help effectively intrusion detection system complete the task of early warning, interception and obtaining evidence, etc. With the enhanced processing capacity of operating system and computer, multi-thread technology has been more and more applied to application programming. After introducing briefly scanning technology and multi-thread technology, this paper gives the implementation of TCP port scanning by using Microsoft Visual C++ MFC library. The program enhances scanning efficiency through multi-thread and ping technology. The results of the program are satisfactory.
The flaws of ARP protocol, the kinds of ARP attack and the major existing defensive technologies are analyzed in details. Due to the cost of existing defensive technologies is too high; therefore, we design and develop a light-weight tool based on Linux platform. It is suitable for most users because it can detect and defend the ARP attacks accompany with very small extra overhead. Moreover, it is convenient for administrators to manage network. Detailed design and some key codes are presented in this article.
Risk assessment for network information security is uncertainty. To control these uncertainties is of great significance for effective risk assessment. There is a big subjective of the existing assessment methods, and the conclusions are less clear. Therefore, this paper presents a fuzzy logic based network information security risk assessment method FLNISRAM. In this method, the result is from a comprehensive assessment for assets, threats and vulnerabilities of the network information system. This paper finally takes an online booking system as a case study, and has carried out an evaluation of the security risk.
This paper discusses DNS and its security extension and analyze its lack of security. A DNS system based on Byzantine fault tolerance is proposed .The DNS system is able to tolerate Byzantine fault tolerance using redundant servers and Byzantine intrusion tolerant technique and voting mechanism. With the method of distributing zone table into several DNS servers, using an improved Byzantine fault tolerant algorithm and majority voting mechanism, system can continuously provide services even if partial servers were intruded or faulty and the availability and security of DNS system are enhanced.
Because the trustworthy network is able to mitigate various secure problems, it has attracted researchers' attention worldwide. However, it's hard to wipe any malicious node off. A node may obtain fake trustworthy by mixing malicious transactions with tremendous honest ones. In order to eliminate such a fault, the improved method employs normalized transactions and linear increase of trustworthy. The way to optimize important coefficients is also presented. The simulation result illustrates that the node's malicious behavior can be detected effectively.
Creating randomness number mechanism plays an important role in any network security and security information systems from a cryptography viewpoint. In this paper, an exhaustive scheme is proposed to create a series of number sequences between a functional space of logical functions under complementary and permutation operations to show indirectly from the visualization of matrices. Following an example, an exhaustive model has been established to express the functional space of n-ary logical function. A visual structure is represented as a diagrammatic representation for function sequences. The visual results are represented in W, F and C codes respectively, to provide the basic support for the variant logic framework.
Traditional technology of network access controlling passive defense attacking by external hack, and cannot eliminate menace to network by some malicious network terminal. To solve this problem, authors proposed a new model of network connection based on objects fingerprints and discuss its work mechanism of certification and communication. Simulation results show that the model can not only ensure the trustworthiness of network connection and network communication, but also provide remediation services for those terminal users who fail to meet the security policy demand.
Network confrontation system cultivates confrontation talents and increases confrontation standards. In this article, we research and design a kind of network confrontation training simulation system, discussing the relevant subsystems from the aspect of the key technology and its realization, including interactive confrontation simulation system, confrontation training simulation supported software system, confrontation simulation assessment system, and confrontation training simulation information database. This simulation system can provide a good environment for theoretical researching and simulation training on network confrontation. Improving computer network confrontation ability is the right way and necessary measures to capture information superiority and control network advantage.
This paper is based on wireless sensor network security evaluation model. First, start from the analysis of the current assessment model, then in the analysis of the structure wireless sensor networks, proposed the structure for the safety assessment. Then improve the lack of the AHP, and make the result as a preliminary calculation, constructing the fuzzy neural network model for safety assessment.
To fix the more and more serious leakage problem in remote access to confidential data, the paper designs and implements a secure end-to-end browsing system with mobile composition. It enables mobile-authenticated users to browse confidential files stored at server side using their personal computers securely. The authentication function is in real-time such that the system can stop the browsing function once it detects that the authenticated mobile is out of the communication range of user's personal computer.
With rapid popularization of IPv6 network, network security has become one of the main factors which can impact the development of IPv6 network. DDoS(Distributed Denial of service) attacks are the leading thread to the Internet. In this paper, firstly the security of IPv6 protocol and DDoS attacks are introduced. Then, two main security applications based on IPv6 campus network are introduced. Finally, a test is carried out to see if they are effective to prevent DoS/DDoS attacks.
In the Internet era, the network becomes a sword fighting corruption. The prominent characteristic of networks such as open, virtual, efficient emerge in the field of fight against corruption. However, as a new thing, Network Anti-corruption faces many difficulties. This paper attempts to analyze and discuss the current status, problems and future about network anti-corruption.
The following topics are dealt with: network design; Web semantics; wireless communication; image processing; quality of service; network engineering; network security; Web engineering; information retrieval; cognitive radios; artificial intelligence; data mining and software engineering.
In traditional network security management systems, system administrators could not prevent malicious users from illegal actions in time. In this paper, we first present a User Cooperation Trust Model (UCTM) which not only encourages the good behavior liberally, but also punishes those minatory behaviors decisively. Then based on UCTM we proposed and implemented the trust based P2P Network Security Management System (T2SMS) which dynamically evaluate user's reputation correlated with the user's privilege in the network. With T2SMS, a malicious user will be kicked out from the network system and refused from accessing resources of the network automatically when the user's reputation is lower than some threshold.
A Honeypot is an information system resource used to divert attackers and hackers away from critical resources as well as a tool to study an attacker's methods. One of the most widely used tools is honeyd for creating honeypots. The logs generated by honeyd can grow very large in size when there is heavy attack traffic in the system, thus consuming a lot of disk space. The huge log size poses difficulty when they are processed and analyzed by security analysts as they consume a lot of time and resources. In this paper, we propose a system which addresses these issues. It has two important modules. The first one is logging module which saves disk space by reducing the log size without losing information. The second module is a log analyzer that can process this log to generate reports and graphs for the security administrators. The analyzer is backward compatible and can process the log file produced by honeyd as well. The experimental results show that the space required by log file reduces significantly.
This paper summarized applications of Fire Wall in e-Commerce security system and have set forth choice principle on fire wall; have studied e-Commerce network security technique on Fire Wall; analysed constructing e-Commerce network security technology and applied to actual e-Commerce system design on power.
This paper presents an efficient intrusion detection model for packet processing. Packet capture device detect the network data packets directly after capture them, instead of detecting them after restructuring, thus the efficiency of detection can be improved. Current products of intrusion detection use the packet restructuring mechanism in order to reduce the false negative rate of the system. The model process the packet restructuring mechanism combined with the data packets. This model improve the efficiency of detection system on the basis of ensuring the detection to the integrity of the attacks.
Analyze the characteristics of network security courses and the use of task-driven teaching practical significance. Discuss the detailes to carry out all aspects of curriculum, including classroom teaching, task-driven experimental procedures, evaluation system of the re-establishment and task-driven implementation of teaching effectiveness.
In order to effectively carry out the work of preventive measures for the network crime of teenagers, we make a comprehensive study on the network crime of teenagers. As the network crime of teenagers is a serious social problem, we must work together through social multifaceted, mufti-pronged, comprehensive management. First, the government should increase financial input and strengthen network security technology development and application, and capture the technology high ground. Second, we must strengthen education for young people control measures. Third, we must improve and perfect the legal system network, enhance network quality of law enforcement and judicial personnel, victims networking "mandatory reporting system" to crack down on crime network of teenagers. Fourth, the government should strengthen the monitoring of network information, optimize network resources, and increase the management of Internet cafes for young people to create a healthy Internet environment; Families and schools should take effective measures to strengthen supervision of the conduct of young people using the network to reduce youth crime network.
A honeynet is a solution designed by the Honeynet Project organization to gather information on security threats and it can be used to proactively improve network security. A honeynet captures a substantial amount of data and logs for analysis in order to identify malicious activities and this is a challenging task. The main aim of this work is to identify the best traffic features or parameters that can be used in an anomaly detection technique to identify anomalies in honeynet traffic. In this work, a detailed analysis of feature-based and volume-based parameters is carried out and the most appropriate features for honeynet traffic are selected. Unlike other techniques proposed in the literature, our work combines entropy distributions for feature-based parameters and volume distributions for volume-based parameters to evaluate the different features. The features were evaluated using real honeynet traces released by the Honeynet project organization and other sources.
Sophisticated intrusions are evolving everyday. Hence, requirements are changing towards computer systems that provide more robust solutions. However, new issues, bugs, threats and vulnerabilities are unavoidably introduced into the market each time a new product is designed to meet users' specifications. For these reasons, Vendors, research community, network forensics professionals and other users of Network Intrusion Detection Systems write tons of detection rules to maximally detect attacks. Despite these, numerous attacks still evade intrusion detectors because of insufficient evidence to expose the emerging threats and risks in the usage of intrusion detection technology. Thus, this paper presents a critical review of these problems. The review provides useful guidelines that can be used to enhance efficacy of intrusion detection system and to achieve high returns on investment.
With an emphasis on the role of wireless communications, this paper defines the cyber domain from a technical point of view. It divides the domain into three functional layers: the core, wireless and cross-domain interface. This paper highlights the role and security vulnerabilities of the wireless layer and asserts the need for increasing research in this area.
Denial of Service (DoS) attacks have continued to evolve and impact availability of the internet infrastructure. Many researchers in the field of network security and system survivability have been developing mechanisms to detect DoS attacks. This paper presents a novel dynamic entropy methodology for the study of DoS detecting. When an abnormal factor arises to agitate the current system the entropy must show an abrupt change. We examine the system using network traffic traces containing notorious DoS attacks and the results show that the proposed method can be able to detect anomalies with higher accuracy and lower false alarm rate.
A system of collection and control for gas information based on GPRS is designed for the purpose of gas safe in this paper. The design of data acquisition and control system hardware and software, design of network security data transmission and the software design of the control center are researched mainly by the paper. The remote monitoring and control to gas pipelines and gas storage facilities at places such as gas leakage can be realized by using the system. The innovation is the real-time information gathering of gas can be implanted.
With the global political and economic integration e-government is represented by the government function management. It's electronic, automated, paperless has been widely used. Traditional network security technology has been used in many fields, but it still has some shortage. This paper will give a solution of the shortage. Built a secure WEB e-government framework based on the immune intrusion tolerance. And gives detailed description of its structure and function.
At present there is no good security tool that can directly associate analysis to the multi-step attack on network, and reconstruct invading process to obtain the criminal evidence. So a new approach of network coordinative forensics based on data provenance was presented: Set up a log server with SYSLOG mechanism, obtain logs provenance databases with Perm rewrite technology, position multi-step attacker with where provenance, and reconfiguration attack process with why provenance. Data provenance theory and experiment results proved that the new approach is feasible and effective.
Traditional network access control approaches are inflexible and low efficient, so they cannot be deployed in large-scale network. This paper presents a new network access control system (NACS) based on routing diffusion. Then this system has been designed and implemented. Then two key optimization programs-parallel router and tunnel technology-are proposed to improve the performance of NACS. At last, a large number of experiments are given to verify its high efficiency.
Digital crimes are a part of modern life but evidence of these crimes can be captured in network traffic data logs. Analysing these logs is a difficult process, this is especially true as the format that different attacks can take can vary tremendously and may be unknown at the time of the analysis. The main objective of the field of network forensics consists of gathering evidence of illegal acts from a networking infrastructure. Therefore, software tools, and techniques, that can help with these digital investigations are in great demand. In this paper, an approach to analysing and visualising network traffic data based upon the use of self-organising maps (SOM) is presented. The self-organising map has been widely used in clustering tasks in the literature; it can enable network clusters to be created and visualised in a manner that makes them immediately more intuitive and understandable and can be performed on high-dimensional input data, transforming this into a much lower dimensional space. In order to show the usefulness of this approach, the self-organising map has been applied to traffic data, for use as a tool in network forensics. Moreover, the proposed SOM takes into account the qualitative features that are present in the traffic data, in addition to the quantitative features. The traffic data was was clustered and visualised and the results were then analysed. The results demonstrate that this technique can be used to aid in the comprehension of digital forensics and to facilitate the search for anomalous behaviour in the network environment.
Digital forensics is essential for the successful opposition of computer crime. It is associated with many challenges, including rapid changes in computer and digital devices, and more sophisticated attacks on computer systems and networks and the rapid increase in abuse of ICT systems. For a forensic investigation to be performed successfully there are a number of important steps that have to be considered and taken. Since digital forensics is a relatively new field compared to other forensic disciplines, there are ongoing efforts to develop examination standards and to provide structure to digital forensic examinations. This paper attempts to address the diversity of methodologies applied in digital forensic investigations.
Summary form only given. This article proposes a robust mathematical method to strategically place trust nodes to compartmentalize a time-critical SCADA network. The trust nodes combine firewall and intrusion detection technology to provide communication network security for protection, control, and SCADA systems. The mathematical technique optimizes the placement of the trust nodes based on the timing requirements of existing systems and the number of trust nodes that are available in the system given constraints, which may arise due to budgetary limitations or the restrictions of existing utility hardware. The intent is to create a planning tool to allow utility system operators to determine the best locations to place trust nodes to increase system security given limited resources and/or hardware constraints. The operational requirements of the environment are translated into a mathematical model. Mixed integer linear programming is used to process this model in search of an optimal solution. Because the problem is provably NP-Hard, a heuristic is also given to quickly find good, but not optimal, solutions. Experiments show promise for the proposed techniques.
With increasing wind farm integrations, unit commitment (UC) is more difficult to solve because of the intermittent and random nature of wind power outputs. A robust optimization model for UC is built to deal with the errors on wind power predictions. The robust optimization method is based on scenario analysis, while the probability of each selected scenario is derived mathematically. In the proposed UC formulation, spinning reserve requirement is specially considered to support possible wind power change between any two successive periods. Network security constraints are considered by DC power flow equations and the whole UC formulation is a mixed-integer programming problem. Case studies on the IEEE 30-bus system demonstrate the effectiveness of the proposed method. The influences of wind power on UC results and network security are also discussed.
In principle, a electricity transmission network security standard that is established within a risk-based framework is superior over the deterministic approach, usually presented in the form of an N-k type rule, as it balances more accurately the reliability (and other) benefits against operation and investment costs incurred to deliver these benefits. However, in the absence of advance technology to monitor, predict, simulate and control large-scale networks in real time, deterministic standards have historically delivered a successful operation of massive and complex electric networks. However, nowadays through the usage of new technology such as Special Protection Schemes (SPS), coordinated voltage control technique, wide-area monitoring and control systems (WAMC), Dynamic Security Assessment techniques (DSA), grid friendly controllers for Demand Side Management (DSM) etc, there is a genuine possibility to apply a probabilistic framework for network security. A risk-based framework will not only maximise the value of transmission but also speed up the connections of wind power. Through qualitative analyses we point out various problems of the present deterministic standards in the UK together with identifying key benefits at a regulatory and operational level of implementing a probabilistic standard. In addition, through quantitative analyses we show that current deterministic standards significantly overvalue reliability risks and, in fact, a probabilistic cost-benefit framework can lead to important savings in the cost of constraints without significantly increasing the risk of system interruptions.
The 30 SEED (Security Education) labs cover such topics as vulnerabilities, attacks, software security, system security, network security, Web security, access control, authentication, and cryptography. More than 80 universities have requested the instructor's manual and have adopted a selection of SEED labs for their courses.
Network protocol fingerprinting refers to the process of identifying a protocol implementation by their input and output behaviors. It has been regarded as both a potential threat to network security and also as a useful mechanism for network management. Existing protocol fingerprinting tools share common disadvantages such as being protocol-specific and difficult to automate. This paper proposes a formal methodology for fingerprinting experiments using which we can model a broad spectrum of fingerprinting problems and design-efficient algorithms. We present a formal behavioral model that specifies a protocol principal by its states and transitions, then identify a complete taxonomy of fingerprint matching and discovery problems is identified based on 1) whether the fingerprinting experiment is active or passive and 2) the information available about the specifications and implementations. Algorithms to solve the problems are discussed. In particular, for fingerprint matching algorithm, we propose an efficient PEFSM online separation algorithm for active experiment and concurrent passive testing for passive experiments. For fingerprint discovery problem, there are two cases: if the protocol specification is available as a nondeterministic PEFSM, we apply across verification and back-tracing technique for active and passive discovery, respectively; if no specification is available, we take the machine learning approach and discover the fingerprint by active testing.
For the low-cost hardware-based intrusion detection systems, this paper proposes a memory-efficient parallel string matching scheme. In order to reduce the number of state transitions, the finite state machine tiles in a string matcher adopt bit-level input symbols. Long target patterns are divided into subpatterns with a fixed length; deterministic finite automata are built with the subpatterns. Using the pattern dividing, the variety of target pattern lengths can be mitigated, so that memory usage in homogeneous string matchers can be efficient. In order to identify each original long pattern being divided, a two-stage sequential matching scheme is proposed for the successive matches with subpatterns. Experimental results show that total memory requirements decrease on average by 47.8 percent and 62.8 percent for Snort and ClamAV rule sets, in comparison with several existing bit-split string matching methods.
Through analysis and discussion for embedded network program and security, research of the quantum key distribution technology, quantum key distribution technology is used in embedded network, embedded quantum key distribution network is put forward, the conclusion offers protection for the security of embedded network data communications.
As the Internet has become an enormous interconnected network, the information security today is very important to guarantee confidentiality, integrity and availability of computing resources. Advanced Intrusions Detections Systems IDS should be capable of identifying malicious actions that may compromise these guarantees, as quickly as possible. In this paper, we present a proposal for an IDS based on the wavelet and artificial neural network that is applied to the well know Knowledge Discovery and Data Mining KDD. The experiment showed high detection rate, suggesting that the approach is very promising.
We propose incremental logarithmic time-series technique as a way to deal with time-based representations of large and dynamic event data sets in limited space. Modern data visualization problems in the domains of news analysis, network security and financial applications, require visual analysis of incremental data, which poses specific challenges that are normally not solved by static visualizations. The incremental nature of the data implies that visualizations have to necessarily change their content and still provide comprehensible representations. In particular, in this paper we deal with the need to keep an eye on recent events together with providing a context on the past and to make relevant patterns accessible at any scale. Our technique adapts to the incoming data by taking care of the rate at which data items occur and by using a decay function to let the items fade away according to their relevance. Since access to details is also important, we also provide a novel distortion magnifying lens technique which takes into account the distortions introduced by the logarithmic time scale to augment readability in selected areas of interest. We demonstrate the validity of our techniques by applying them on incremental data coming from online news streams in different time frames.
Given the rapid evolution of attack methods and toolkits, software-based solutions to secure the network infrastructure have become overburdened. The performance gap between the execution speed of security software and the amount of data to be processed is ever widening. A common solution to close this performance gap is through hardware implementation of security functions. Possessing the flexibility of software and high parallelism of hardware, reconfigurable hardware devices, such as Field Programmable Gate Arrays (FPGAs), have become increasingly popular for this purpose. FPGAs support the performance demands of security operations as well as enable architectural and algorithm innovations in the future. This paper presents a survey of the state-of-art in FPGA-based implementations that have been used in the network infrastructure security area, categorizing currently existing diverse implementations. Combining brief descriptions with intensive case-studies, we hope this survey will inspire more active research in this area.
Internet worm attacks pose a significant threat to network security and management. In this work, we coin the term Internet worm tomography as inferring the characteristics of Internet worms from the observations of Darknet or network telescopes that monitor a routable but unused IP address space. Under the framework of Internet worm tomography, we attempt to infer Internet worm temporal behaviors, i.e., the host infection time and the worm infection sequence, and thus pinpoint patient zero or initially infected hosts. Specifically, we apply statistical estimation techniques and propose method of moments, maximum likelihood, and linear regression estimators. We show analytically and empirically that our proposed estimators can better infer worm temporal characteristics than a naive estimator that has been used in the previous work. We also demonstrate that our estimators can be applied to worms using different scanning strategies such as random scanning and localized scanning.
Cybersecurity of the substations in a power system is a major issue as the substations become increasingly dependent on computer and communication networks. This paper is concerned with anomaly detection in the computer network environment of a substation. An anomaly inference algorithm is proposed for early detection of cyber-intrusions at the substations. The potential scenario of simultaneous intrusions launched over multiple substations is considered. The proposed detection method considers temporal anomalies. Potential intrusion events are ranked based on the credibility impact on the power system. Snapshots of anomaly entities at substations are described. Simulation results using the modified IEEE 118-bus system have shown the effectiveness of the proposed method for systematic identification. The result of this research is a tool to detect cyber-intrusions that are likely to cause significant damages to the power grid.
Cyber security analysis tools are necessary to evaluate the security, reliability, and resilience of networked information systems against cyber attack. It is common practice in modern cyber security analysis to separately utilize real systems computers, routers, switches, firewalls, computer emulations (e.g., virtual machines) and simulation models to analyze the interplay between cyber threats and safeguards. In contrast, Sandia National Laboratories has developed new methods to combine these evaluation platforms into a cyber Live, Virtual, and Constructive (LVC) testbed. The combination of real, emulated, and simulated components enables the analysis of security features and components of a networked information system. When performing cyber security analysis on a target system, it is critical to represent realistically the subject security components in high fidelity. In some experiments, the security component may be the actual hardware and software with all the surrounding components represented in simulation or with surrogate devices. Sandia National Laboratories has developed a cyber LVC testbed that combines modeling and simulation capabilities with virtual machines and real devices to represent, in varying fidelity, secure networked information system architectures and devices. Using this capability, secure networked information system architectures can be represented in our testbed on a single computing platform. This provides an experiment-in-a-box capability. The result is rapidly produced, large scale, relatively low-cost, multi-fidelity representations of networked information systems. These representations enable analysts to quickly investigate cyber threats and test protection approaches and configurations.
The smart grid, generally referred to as the next-generation power electric system, relies on robust communication networks to provide efficient, secure, and reliable information delivery. Thus, the network security is of critical importance in the smart grid. In this paper, we aim at classifying and evaluating the security threats on the communication networks in the smart grid. Based on a top-down analysis, we categorize the goals of potential attacks against the smart grid communication networks into three types: network availability, data integrity and information privacy. We then qualitatively analyze both the impact and feasibility of the three types of attacks. Moreover, since network availability is the top priority in the security objectives for the smart grid, we use experiments to quantitatively evaluate the impact of denial-of-service (DoS) attacks on a power substation network. Our work provides initial experimental data of DoS attacks against a power network and shows that the network performance degrades dramatically only when the DoS attack intensity approaches to the maximum.
Wireless sensor networks (WSNs) have many applications that handle sensitive information such as surveillance, reconnaissance, and target tracking. Therefore, a WSN deployed in a hostile region should be resilient to attacks. The current approach to defending against malicious threats is to develop and deploy a specific defense mechanism for a specific attack. However, the problem with this traditional approach to defending sensor networks is that the solution for the jamming attack does not defend against other attacks (e.g., sybil, selective forwarding, and wormhole attacks). In reality, one cannot know a priori what type of attack an adversary will launch. Also, given the resource constraints of sensor nodes, the current defense mechanisms cannot be simply combined on the node to provide a complete solution. This work addresses the challenges with the traditional approach to securing sensor networks and presents a collaborative framework (the Distributed Security Framework -DSF) that can defend against all known attacks. The framework is extensible, therefore, as new attacks are discovered they can also be defended against. The DSF leverages existing defense mechanisms created by researchers. These defense mechanisms are distributed in such a way that they can, collectively, provide comprehensive defense to the network. The efficacy of the DSF is determined using simulations for scenarios consisting of multiple stationary and multiple mobile attackers. The simulation results show that though the DSF consumes more energy than single defense schemes, it can significantly enhance the network security even when the network is under multiple types of attacks.
CyberCIEGE is a sophisticated network security simulation packaged as a video game and used by educators around the world to enhance information assurance education and training at universities, community colleges, within the DoD, and in other government agencies. The CyberCIEGE game engine was recently expanded to include Public Key Infrastructure (PKI) features including certification authorities, selection of installed roots and cross certification. CyberCIEGE Virtual Private Network (VPN) gateways, VPN clients and email clients were then extended to incorporate the new PKI features. CyberCIEGE PKI abstractions are described in terms of player configuration choices and the consequences of these choices on network management and vulnerabilities. The CyberCIEGE game engine modifications include modeling of chains of trust and risks of cross certification schemes. The benefits of these enhancements include coherent integration of identity management technologies, ranging from the human interface through to the supporting distributed infrastructure, into scenarios. Benefits also include support for recent new scenarios focused on the PKI infrastructure, identity management, or both; and the ability to tie both identity management and PKI to concepts of identification, authentication, provenance, and access control.
Cooperative caching in MANETS and forwarding data items through mobile nodes in Delay Tolerant Networks are two important methods for improving performance and providing connectivity in mobile tactical networks. In this paper we present the challenges of securing cooperative caching the DTN systems. Our security solution is based on MML-IPSec. The main challenge of this system is key distribution and its impact on performance. We propose leveraging military structure, knowledge of missions, and the roles of personnel to enable secure systems to maintain high performance.
We describe a distributed reasoning system called Otto-Mate that is used to detect, reason about, and respond to incidents on a computing network. Events for monitoring computing networks occur at different system levels. Some information might relate to data, some might be operating system specific, some application or service related, some could be network related, and from each there will be compound events that describe incident effects and information about the situation context. All together there can be thousands of events per second. Today's approaches to monitoring networks are typically centralized, sending events over the network to a single engine for analysis. Centralized monitoring ultimately cannot scale to address the volume of events that one would ideally like to be able to monitor, so techniques of today often make severe compromises relating to the events that they ingest. Centralized monitoring creates a single point of failure and also generates significant network load. To overcome these deficiencies we have developed a more distributed, approach: our reasoner agents can (in theory) be installed on every monitored resources and the reasoner language (used for programming the reasoners) enables knowledge in a reasoner's working memory to be synchronized over multiple reasoners enabling them to implement parallel distributed reasoning algorithms that are able detect event patterns irrespective of whether the events are local or remote. Distributing the reasoning makes the system extremely resilient. Additionally, since the knowledge shared between the reasoning agents represents summary information, and because many on-line event correlation algorithms often suppress reporting once an incident has been reported, the amount of network load needed to support the distributed monitoring can actually be reduced. To demonstrate our approach we describe its application to the monitoring of a computing network that has been instrumented to protect it -
This paper presents performance metrics to be used for evaluation of cyber dynamic defense solutions. Currently, there are no standard, industry-defined metrics or benchmarks for evaluating cyber security architectures and systems for dynamic defense. These systems have relied instead on a layered, defense in depth approach, where the only measurement made is the number of defenses. In order to characterize the performance of cyber defense solutions, a variety of metrics need to be defined and captured based on observable effects on both cyber attacks and defenses. By establishing these metrics, the benefit each individual layer provides to an overall defensive solution can be determined, allowing system designers to select the most effective suite of defensive techniques. The metrics presented were formulated using a discrete event simulation of dynamic defense solutions. The focus of this simulation was to assess the increased cost to cyber attackers, where cost is based in time. Metrics are captured from the perspective of the defender, as well as that of the attacker. Collection of data supporting computation of the metrics will be shown in a discrete event simulation environment, as well as recommendations for network locations and equipment that can be used to collect these metrics in a notional network architecture.
Content Distribution Network (CDN) architectures face a wide range of security threats. In this paper, we compare the cost of achieving low and high security for different CDN architectures. We reviewed the existing and emerging systems, identified the threats that they face, defined the general security requirements and considered the mechanisms available to meet the requirements. To assess the security cost, we first defined the process for selecting security mechanisms, and then defined the process for ranking the mechanisms for each architecture. The security comparison result clearly shows that the more the cost of providing service is pushed to the end points, the higher the security cost. To the best of our knowledge, this study is the first effort to assess a security cost comparison of different CDN architectures. Our work is orthogonal to other studies that try to find ways of reducing the content distribution service cost, rather than quantifying the cost to provide service security.
The following topics are dealt with: communication software; computer security; network security; computer communications; computer networks; data mining; database applications; information retrieval; pattern recognition; and performance evaluation.
Network traffic classification is essential in several fields such as network security and design, or providing adequate QoS. Early detection of network applications is an important feature, which enables recognition of the application online, without the need to capture and examine whole network flow. In this paper we propose a method for classification of Internet applications in the first phases of connection using sizes of first several packets only. For this purpose we used 2 types of neural network (multilayer perceptron and radial basis function network). We examine the performance of this approach for various number of packets from flow and map packet sizes to bins to decrease computational complexity.
Firewalls are among the most important mechanisms used to enforce network security policies. However, It has been observed that most firewall policies on the Internet are poorly designed. A firewall error may allow the spread of malicious traffic or block legitimate one causing serious damages. A major source of firewall misconfigurations stem from the logically entangled nature of firewall filtering rules. Moreover, updating filtering rules could induce to faults and in turn could lead to irreparable consequences. Despite of the importance of automatic correction of firewall configurations, this problem has not been explored in previous work. In this paper, we propose a formal and fully automatic approach for correcting a firewall during execution. We prove that our method is both correct and safe. To a better efficiency, we also propose a rule-based optimization approach. Finally, our methods have been implemented in a prototype. The first results are very promising.
Typically, security solutions are defined to meet the requirements of security policies, and are configured to implement some of their rules. Approaches proposed so far in the literature to validate security solutions have merely taken interest to the need of: a) describing the security policy used to define and configure these solutions b) generating executable description of attack scenarios targeting the secured system and c) verifying whether the secured systems react as expected. In this paper we develop a logic-based approach for the modeling of security policies and solutions based on the concept of observations, and the generation of executable scenarios of attacks. This approach provides a unified formalism for the specification of security policies, security solutions, library of legitimate actions and attacks, and correctness rules in the form of predicates over executions. We propose a modeling of two types of security solutions, namely passive and active solutions. We develop a Model Checker to generate executable scenarios of attacks, verify the security state of the system, and test whether the solutions react as expected to security attacks. A case study is proposed to illustrate the proposal.
Synchronous e-training is emerging as an alternative for developing human resources training plans in large organizations. However, synchronous e-training platforms must face many issues affecting their resilience. Self-recovering and self-protection software techniques should be implemented to enable the continuity of e-training activities faced with network failures or entities of the platform going down. In this paper, the resilience issues in synchronous e-training are identified, and those presented in a real e-training platform are analyzed. The platform includes a self-optimizing technique to make efficient use of network resources as participants join and leave. Furthermore, various self-recovering and a self-protection techniques are proposed, making possible the continuity of ongoing e-training activities with reduced functionality until the automatic full system recovery. Most of the proposed techniques are applicable to other real-time communication systems, since the e-training platform is built using standard technologies such as SIP and RTP, commonly used in voice over IP systems.
The following topics are dealt with: theory of network computing; dependable network computing; theoretical aspect of network computing; network security; mobile ad hoc networks; cloud computing; machine learning techniques for predicting anomalies; performance modelling and QoS; workshop: interconnectioin networks for multicore chips; overlay networks/peer to peer systems; self-configuring networks; dependable networks and systems; routing mechanisms; practical experiments and prototypes; and sensor networks.
To solve current problems of identity authentication in CSCW, an arbitrated digital signature scheme based on a hybrid cryptosystem is proposed in this paper. This paper mainly discusses issues on reasons of putting forward this scheme and implementation steps of this scheme, including the selection of AES and ECC, and methods of key distribution. Finally, this paper presents a model based on this scheme and sums up its advantages.
As the mobile network migrates to an all-IP network with increasing speed, many new security requirements emerge. We analyze security threats and requirements of 3GPP/4G mobile networks and discuss various kinds of existing security measures. We argue that merely improving security mechanisms and protocols to protect the security of air interface is insufficient. The mobile network security must rely on the secure terminal environment. We also analyze the weaknesses of terminal protection by virus scanning, and propose a novel protection scheme for mobile networks based on security services and trusted terminals. By building a trusted computing environment at the mobile terminal, the proposed scheme combines verifying validity of software with access control, and checks the validity and the integrity of software in the security service provider. Under our scheme, the security ability supplied to the terminals and the whole network is much better.
Network coding is widely used in many fields such as Data sharing, reliable transmission, network security.ets. The recently studies said that the network coding can improve reliable transmission by reducing the packet retransmissions number in the reliability network transmission. After that, more research has quantitative analysis the reliability benefit of network coding for reliable multicasting for the tree topology network and calculate the expected number of packets transmissions to send a packet from the source node to K receivers using link-by-link ARQ and the network coding respectively. Since the tree topology is too simple, we try to apply this result to more complicated topology.
With rapid development of network technology, networks bring people convenience; but network security problems are also increasingly serious, especially in LAN security, which is a widely used network connection mode. According to statistics, in terms of the economic losses caused by internal staff is much higher than the invasion of outsiders. Therefore, how to prevent from insider misuse is related to one of the key issues of network security. LAN security solutions in the past are mostly the superposition of several network security technologies; there is not to solve the fundamental problem. This paper presents dynamic LAN security solutions based on ordinary PC, in cooperation with the controller and switches, real-time monitoring the operation actions of the internal personnel, dynamically changing the user's security level to protect confidential information from disclosure. Experimental results show that this method can not only prevent a security breach within the staff, the economic costs without increasing too much, but also can solve the problem of LAN security leaks.
Electricity industries are being transformed worldwide, driven by the need for more energy, urbanization, scarcity of natural resources and global warming. Building a safe, reliable and highly stable power grid system has become an inevitable trend. The smart grid came into being therefore. Automatic control is an advanced modern communication technology. Automatic control system can improve network security and stability effectively, what refers to in the paper is a typical automatic control mode mentioned, the PI control, through theory and experiment we demonstrate the feasibility of improving the motor's stability by PI control.
In order to improve the reliability of network security evaluation, the application of analytical hierarchy process method on the network security assessment was studied in depth. Firstly, the characters of network security were introduced. Secondly, the basic theory of analytic hierarchy process was analyzed, and then the index system of integrated evaluation of network security was established. Thirdly, the analysis procession of AHP was carried out. And finally the evaluation was carried out, and the results showed that this method had high precious and would applied in network security assessment effectively.
In order to improve the security of network, the multi-sensor data fusion technology was studied in depth. Firstly, the relating technology was introduced, and then the basic theory of multi-sensor data fusion technology was analyzed, and then the theory model of multi-sensor data fusion technology was constructed, and then the main application of Data mining model multi-senor data fusion technology on network security was studied, and then comparison analysis of all kinds of data fusion method was carried out, and finally the simulation analysis was carried out, and the results showed that this method had better reliability.
The trend of spreading malicious web contents through legitimate but compromised websites is not very rare to find. Websites bearing the trust of millions of clients are chosen to exploit the trust of its users by installing unwanted and malevolent contents on client machines through drive by download. This stealthier mechanism is aimed to convert a client machine into a botnet and to gain access on its resources like stored passwords, sensitive information and fingerprinting of running softwares. To deal with it, there are different solutions like honeypots and system state observers but all these antidotes are client resident. In this paper we have devised a heuristic based solution which resides on web servers and circumvents the movement of malicious contents toward client machines while keeping the server repute trusted and its availability 24/7 even after the compromise. Instead of blocking the complete website or any of its page, only malevolent contents are sanitized which adds novelty to the proposed system.
This work is proposed to design shielding for interference mitigation and network security within the buildings in the unlicensed 2.4GHz ISM band and to provide maximum transparency at broadcast frequencies. Thus, mobile phones, radio and television signals in buildings will not be affected. Frequency Selective Surface (FSS) is attached onto existing common construction material to transform the standard material into a band stop frequency selective wall. To obtain the band response of FSS having a desired frequency interval, a new FSS periodic element geometry is introduced. The obtained results show that desired frequency response is achieved. HFSS software is used for simulation purposes.
Byzantine agreement problem has been mostly studied for networks with fallible processors. However, link failure is a common problem in distributed system when the messages are relayed through individual processors and delivered to the receiver. There are few works on locating malicious faulty links using byzantine framework. The proposed protocol can detect/locate maximum number of malicious faulty links and tolerate maximum number of allowable faulty links to make each fault-free processor reaching a common agreement.
Nowadays, wireless network security has a considerable attention. However, wireless communication faces several security threats. Consequently, several security efforts have been exerted to combat the wireless attacks, but unfortunately complete attack prevention is not realistically attainable. Thus, the emphasis on detecting intrusions through a second line of defense, in the form of Intrusion Detection System (IDS), is increasing. Selecting an effective and appropriate IDS system should take its functionality and performance evaluation into account. Unbiased and reliable evaluation necessarily depends on a well-engineered evaluation methodology. Dealing with this challenge, this paper proposed a holistic methodology for IDSs evaluation in wireless networks. Our methodology includes all necessary and sufficient tasks for IDSs evaluation. Also, we present holistic taxonomies of wireless IDSs and wireless security attacks from the perspective of the IDS evaluator.
There are the two common means for propagating worms: scanning vulnerable computers in the network and sending out malicious email attachments. Modeling the propagation of worms can help us understand how worms spread and devise effective defence strategies. Most traditional models simulate the overall scale of infected network in each time tick, making them invalid for examining deep inside the propagation procedure among individual nodes. For this reason, this paper proposes a novel probability matrix to model the propagation mechanism of the two main classes of worms (scanning and email worms) by concentrating on the propagation probability. The objective of this paper is to access the spreading and work out an effective scheme against the worms. In order to evaluate the effects of each major component in our probability model, we implement a series of experiments for both worms. From the results, the network administrators can make decision on how to reduce the number of vulnerable nodes to a certain threshold for scanning worms, and how to immunize the highly-connected node for preventing worm's propagation for email worms.
Security software usability has been an ongoing issue for end-users. Whilst manufactures have focused on making computers and operating systems more usable, the same cannot be said for security software. Whilst the number of threats continues to escalate, end-users are left attempting to implement a security solution on their own. Previous research has shown that users are unaware of Internet threats and do not know where to start with mitigation. This paper demonstrates that in 2011, Internet Security Software is gradually becoming more usable, although there are key elements which still require improvements. This paper shows the strengths and weaknesses of current security software, and proposes a series of solutions that software vendors should consider in future releases.
Distributed Denial-of-Service (DDoS) attacks continue to be one of the most pernicious threats to the delivery of services over the Internet. Not only are DDoS attacks present in many guises, they are also continuously evolving as new vulnerabilities are exploited. Hence accurate detection of these attacks still remains a challenging problem and a necessity for ensuring high-end network security. An intrinsic challenge in addressing this problem is to effectively distinguish these Denial-of-Service attacks from similar looking Flash Events (FEs) created by legitimate clients. A considerable overlap between the general characteristics of FEs and DDoS attacks makes it difficult to precisely separate these two classes of Internet activity. In this paper we propose parameters which can be used to explicitly distinguish FEs from DDoS attacks and analyse two real-world publicly available datasets to validate our proposal. Our analysis shows that even though FEs appear very similar to DDoS attacks, there are several subtle dissimilarities which can be exploited to separate these two classes of events.
Large-scale network security issue has been troubled by the biggest problems network managers, this paper, data flow detection and deep packet inspection technology, proposed a compound detection system and security events to be realized, given a new solutions to large-scale network security solution.
Mobile Internet is steadily gaining momentum allowing users to browse the World Wide Web from their mobile phones, downloading at the same time applications and games. Drawing results from a study we realized, in a sample of 7172 students in 17 Universities of 10 Eastern and Southern Europe countries, we are initially providing useful insight into the state of mobile downloading penetration in the student population. Furthermore, we are examining the effect it has to security awareness, feeling and practices of students discovering that there is indeed a statistically significant connection. Namely, users actively downloading are only 37%of the student population and are mostly concentrated in older student ages. In addition, their actual behavior in regards to security practices is generally better than that of other users, especially for those that mainly download applications. As it was found, students with lower monthly bills do not know if their mobile phone has the ability to download software. Thus, by enhancing their knowledge, operators will profit by increased mobile downloading while users will enjoy better services.
The cloud has become an attractive platform for enterprises to deploy and execute their business services for B2B collaborations. Naturally, some of the confidential B2B collaborations require secure tunnels to secure the messaging between the services that are deployed within the same cloud or different clouds. This article examines this issue by reviewing existing network security technologies and presenting an electronic contract based solution that provides a secure Connectivity as a Service (CaaS) for intra-cloud and inter-cloud communications with little or no configuration effort.
Due to the explosive proliferation of network security breaches and threats, major study and analysis should be implemented and performed to secure the confidentiality and privacy of data transmission against intrusions and attacks. The protection of data transmission is a crucial condition, which requires secure and robust detection system to ensure data transfers without being exposed or intruded by a third party. More specifically, intrusion into Smartphone communication and transaction is significantly affected the reliability of data transfer and security. However, the Smartphone security insignificantly has not received many attentions, therefore, in this paper; we thoroughly study the performance analysis of the intrusion detection and prevention system in Smartphone communications and transactions. We also propose a system that significantly ensures the security of Smartphone's users. Moreover, we will present the implications of the study to enhance the current intrusion detection and prevention systems for more secure and reliable data transmission as well as efficient group communications and transactions.
Web services are increasingly becoming an integral part of next-generation web applications. A Web service is defined as a software system designed to support interoperable machine-to-machine interaction over a network based on a set of XML standards. This new architecture and set of protocols brings new vulnerabilities that can be exploited by attackers. To prevent and detect such attacks, several security techniques are available like authentication and encryption mechanisms, firewalls and intrusion detection systems (IDS). Nevertheless these security methods encounter some problems, especially when dealing with new attacks. Relying on additional security principles seems to be important to well protect Web services. In this paper, we propose using honeypots to detect and study attacks against Web services. Honeypots are used to learn new techniques, tools and motivations of hackers to better protect the production systems against attacks. Our solution (WS Honeypot) is to deploy a honeypot as a web service application. This honeypot captures all request messages and analyses them by using machine learning techniques in order to detect and study attacks.
Nowadays, all commercial and personal activities rely almost exclusively on digital information that is constantly accessed, exchanged and archived over the Internet. These facts attracted the interest of the hacker community, driven by the will to obtain profits by exploiting many existing vulnerabilities. Consequently, the number of reported attacks increased dramatically, together with the financial losses associated to them. Botnets have become the cornerstone of on-line criminal activities and can be considered the most serious threat to the Internet. Current detection and prevention methodologies are not able to assure a complete protection as the complexity and subtlety of security attacks and generated illicit traffic grow: these include the encapsulation of illicit traffic in legitimate communications or the replication of normal communications profiles in order to bypass the various network defense mechanisms. Consequently, novel identification and prevention approaches must be proposed and studied in order to address all these issues. In this paper, we present a novel detection methodology that, by building high-level traffic profiles and modeling their embedded multi-scaling dynamics, can accurately identify the components created by illicit applications. The analysis of captured traffic samples over sliding time-windows allows the identification of illicit traffic components that are hidden in legitimate communications. The proposed methodology is also able to cope with the most stringent confidentially restrictions that typically prevent the use of other detection tools.
A simple, low-power DC offset signalling-based optical OFDM symbol synchronisation technique is proposed and experimentally demonstrated, for the first time, in realtime end-to-end 11.25Gb/s 25km SMF IMDD systems using directly modulated DFB lasers. The technique is bandwidth overhead-free and suitable for high line rates. It can also support asynchronous receiver clocking, add extra network security at the physical layer, and more importantly, provide live symbol synchronisation in both point-to-point and point-to-multipoint
Application layer protocol identification problem is the premise and foundation of network security monitoring, intrusion detection and other network control system. With the gradual development of network applications, new application layer protocol will produce, therefore bringing the difficulty to identify and update protocols. Although current supervised-based learning method can overcome low accuracy and high time complexity issue of traditional method, yet it is limited by the number of labeled data and also cannot adapt to the rapid updating of the application layer protocols, and it is restricted in further promotion. Accordingly, we propose a semi-supervised learning method to solve the above two issues. Firstly, we adopt Affinity Propagation (AP)clustering algorithm to cluster the mixed data which contains a small labeled data and large unlabeled data. Secondly, we use the labeled data to map the clustering result to specific network application. Evaluation shows that the proposed method is effective in both specific network applications and new protocol identification.
For problems that cannot be modeled and solved efficiently using a centralized approach, distributed algorithms are a necessity. Self-organizing systems are systems constructed from a network of autonomous communicating agents whereby from simple individual behaviors emerges a global system behavior that is complex, efficient, adaptable, and robust. For the right behavior to emerge, the components must have the correct incentives when they select among their next action. In this paper, we explore the problem of self organization in the context of a mobile sensor network tracking an event. We select two different paradigms, a newtonian force-based approach and a potential energy approach. We test the resulting algorithms in a simulation environment.
One of the very important tasks, a computer network (or security) administrator has to fulfill under constructing a (distributed) firewall security policy, is to guarantee the absence of inconsistencies (or anomalies) and possibility to implement the policy in the given network configuration. The paper outlines an approach to verification of filtering rules of firewalls. The approach is intended for detection and resolution of filtering anomalies in the specification of security policy of computer networks. It is based on Model Checking technique. The paper proposes the models of computer networks, the models of firewalls and filtering anomalies, as well as an algorithm of detection of such anomalies. We suggest also a method for verification of filtering rules based on the mentioned models. The main peculiarities of the approach consist in using Model Checking exactly to detect the anomalies of filtering rules and in ability to specify temporal parameters in filtering rules.
Intrusion Detection Systems (IDSs) are important components in the strategy of defense in depth for network security. Whether an IDS detects intrusions correctly or not depends on the correctness of both its design and the implementation. A survey of the literatures over the last decade reveals various methodologies for the evaluation and testing of network intrusion detection algorithms. However most of these efforts focus on signature based and anomaly based network IDSs. There is another approach of detection known as protocol based detection or protocol anomaly detection. In this paper, we propose an approach for testing the correctness of implementations of protocol based intrusion detection systems. We demonstrate that by adapting protocol conformance testing methodologies we can effectively test the correctness of the implementations.
The main idea in this paper is to design and analyze a symmetric key generation algorithm whereby we can strengthen wireless network security. Our main goal is generating a sequence of highly secure secret keys based on an ARQ based transmission mechanism that relies on the statistical independence of channel errors between the attacker and legitimate users. This leads to some information loss for the adversary which allows us to constantly extract keys by using universal Hashing techniques from communication process, about which we can make sure that adversarys knowledge remains negligible. More specifically, the key generation algorithm is analyzed and designed in a way that targeted security as well as the required throughput and synchronization goals for the transmission are achieved. Simulation results show that the designed algorithm achieves the desired requirements for both system security and throughput.
Traffic graph analysis has become an increasingly useful tool in network security. By summarizing the aggregate activity of a particular service or network using graph based representations, it is possible to model normal activity using a variety of different attributes which are not easily identified or exploited by attackers. In this paper, we discuss several examples of analysis using traffic graphs and demonstrate its potential for scan detection, identifying hitlist attackers, and identifying spammers.
With the development of national economy, the construction of highway network makes astounding advances. The province highway network is evolving after some years' construction. With the formation of the road network, the security issue of highway toll network is becoming prominent. In this paper, the status and the network security issue of the current highway toll network charge, combined with Zhejiang Expressway, are discussed and a security model for the highway toll network is established, and the level of network security prevention system is proposed.
With the development of computer network technology, people gradually pay more attention to the network information security. More and more advanced technologies are being used to solve this issue. In this paper, the real-time protected management system of broadband network information safety in unified controlling platform is presented. This system possesses the ability of intelligent detection. It contains such subsystems as unified controlling center, network invasion detection, host invasion detection, vulnerability scan, and net honey and so on. This system can dynamically detect the abnormalities and then promptly give an alarm to firewall, router and so on. It can make up the shortage of other static defense tools.
According to the former counterterrorism czar, Richard A. Clarke (2010), our national infrastructure could be severely damaged in 15 minutes by a cyber attack. A worm attack on an Internet Protocol (IP) network is one type of attack that is possible. Such an attack would result in a non-stationary arrival process of packets on a link in the network. In this paper we present an initial use of our Optimal Splitting Technique for Rare Events (OSTRE) to simulate the congestion imposed by the worm on the link. This initial application is oriented to testing the technique in this dynamic environment and report on its use as compared with conventional simulations.
Cyber security analysis tools are necessary to evaluate the security, reliability, and resilience of networked information systems against cyber attack. It is common practice in modern cyber security analysis to separately utilize real systems computers, routers, switches, firewalls, computer emulations (e.g., virtual machines) and simulation models to analyze the interplay between cyber threats and safeguards. In contrast, Sandia National Laboratories has developed new methods to combine these evaluation platforms into a cyber Live, Virtual, and Constructive (LVC) testbed. The combination of real, emulated, and simulated components enables the analysis of security features and components of a networked information system. When performing cyber security analysis on a target system, it is critical to represent realistically the subject security components in high fidelity. In some experiments, the security component may be the actual hardware and software with all the surrounding components represented in simulation or with surrogate devices. Sandia National Laboratories has developed a cyber LVC testbed that combines modeling and simulation capabilities with virtual machines and real devices to represent, in varying fidelity, secure networked information system architectures and devices. Using this capability, secure networked information system architectures can be represented in our testbed on a single computing platform. This provides an experiment-in-a-box capability. The result is rapidly produced, large scale, relatively low-cost, multi-fidelity representations of networked information systems. These representations enable analysts to quickly investigate cyber threats and test protection approaches and configurations.
The smart grid, generally referred to as the next-generation power electric system, relies on robust communication networks to provide efficient, secure, and reliable information delivery. Thus, the network security is of critical importance in the smart grid. In this paper, we aim at classifying and evaluating the security threats on the communication networks in the smart grid. Based on a top-down analysis, we categorize the goals of potential attacks against the smart grid communication networks into three types: network availability, data integrity and information privacy. We then qualitatively analyze both the impact and feasibility of the three types of attacks. Moreover, since network availability is the top priority in the security objectives for the smart grid, we use experiments to quantitatively evaluate the impact of denial-of-service (DoS) attacks on a power substation network. Our work provides initial experimental data of DoS attacks against a power network and shows that the network performance degrades dramatically only when the DoS attack intensity approaches to the maximum.
Wireless sensor networks (WSNs) have many applications that handle sensitive information such as surveillance, reconnaissance, and target tracking. Therefore, a WSN deployed in a hostile region should be resilient to attacks. The current approach to defending against malicious threats is to develop and deploy a specific defense mechanism for a specific attack. However, the problem with this traditional approach to defending sensor networks is that the solution for the jamming attack does not defend against other attacks (e.g., sybil, selective forwarding, and wormhole attacks). In reality, one cannot know a priori what type of attack an adversary will launch. Also, given the resource constraints of sensor nodes, the current defense mechanisms cannot be simply combined on the node to provide a complete solution. This work addresses the challenges with the traditional approach to securing sensor networks and presents a collaborative framework (the Distributed Security Framework -DSF) that can defend against all known attacks. The framework is extensible, therefore, as new attacks are discovered they can also be defended against. The DSF leverages existing defense mechanisms created by researchers. These defense mechanisms are distributed in such a way that they can, collectively, provide comprehensive defense to the network. The efficacy of the DSF is determined using simulations for scenarios consisting of multiple stationary and multiple mobile attackers. The simulation results show that though the DSF consumes more energy than single defense schemes, it can significantly enhance the network security even when the network is under multiple types of attacks.
CyberCIEGE is a sophisticated network security simulation packaged as a video game and used by educators around the world to enhance information assurance education and training at universities, community colleges, within the DoD, and in other government agencies. The CyberCIEGE game engine was recently expanded to include Public Key Infrastructure (PKI) features including certification authorities, selection of installed roots and cross certification. CyberCIEGE Virtual Private Network (VPN) gateways, VPN clients and email clients were then extended to incorporate the new PKI features. CyberCIEGE PKI abstractions are described in terms of player configuration choices and the consequences of these choices on network management and vulnerabilities. The CyberCIEGE game engine modifications include modeling of chains of trust and risks of cross certification schemes. The benefits of these enhancements include coherent integration of identity management technologies, ranging from the human interface through to the supporting distributed infrastructure, into scenarios. Benefits also include support for recent new scenarios focused on the PKI infrastructure, identity management, or both; and the ability to tie both identity management and PKI to concepts of identification, authentication, provenance, and access control.
Cooperative caching in MANETS and forwarding data items through mobile nodes in Delay Tolerant Networks are two important methods for improving performance and providing connectivity in mobile tactical networks. In this paper we present the challenges of securing cooperative caching the DTN systems. Our security solution is based on MML-IPSec. The main challenge of this system is key distribution and its impact on performance. We propose leveraging military structure, knowledge of missions, and the roles of personnel to enable secure systems to maintain high performance.
We describe a distributed reasoning system called Otto-Mate that is used to detect, reason about, and respond to incidents on a computing network. Events for monitoring computing networks occur at different system levels. Some information might relate to data, some might be operating system specific, some application or service related, some could be network related, and from each there will be compound events that describe incident effects and information about the situation context. All together there can be thousands of events per second. Today's approaches to monitoring networks are typically centralized, sending events over the network to a single engine for analysis. Centralized monitoring ultimately cannot scale to address the volume of events that one would ideally like to be able to monitor, so techniques of today often make severe compromises relating to the events that they ingest. Centralized monitoring creates a single point of failure and also generates significant network load. To overcome these deficiencies we have developed a more distributed, approach: our reasoner agents can (in theory) be installed on every monitored resources and the reasoner language (used for programming the reasoners) enables knowledge in a reasoner's working memory to be synchronized over multiple reasoners enabling them to implement parallel distributed reasoning algorithms that are able detect event patterns irrespective of whether the events are local or remote. Distributing the reasoning makes the system extremely resilient. Additionally, since the knowledge shared between the reasoning agents represents summary information, and because many on-line event correlation algorithms often suppress reporting once an incident has been reported, the amount of network load needed to support the distributed monitoring can actually be reduced. To demonstrate our approach we describe its application to the monitoring of a computing network that has been instrumented to protect it -
This paper presents performance metrics to be used for evaluation of cyber dynamic defense solutions. Currently, there are no standard, industry-defined metrics or benchmarks for evaluating cyber security architectures and systems for dynamic defense. These systems have relied instead on a layered, defense in depth approach, where the only measurement made is the number of defenses. In order to characterize the performance of cyber defense solutions, a variety of metrics need to be defined and captured based on observable effects on both cyber attacks and defenses. By establishing these metrics, the benefit each individual layer provides to an overall defensive solution can be determined, allowing system designers to select the most effective suite of defensive techniques. The metrics presented were formulated using a discrete event simulation of dynamic defense solutions. The focus of this simulation was to assess the increased cost to cyber attackers, where cost is based in time. Metrics are captured from the perspective of the defender, as well as that of the attacker. Collection of data supporting computation of the metrics will be shown in a discrete event simulation environment, as well as recommendations for network locations and equipment that can be used to collect these metrics in a notional network architecture.
DHT based routing protocols are very exciting innovative aspects in the area of P2P overlay networks; this kind of networks are appropriate and accepted especially in file sharing like transactions. The reason of the Distributed Hash Table is giving the way to search the resources (mostly files) inside the P2P network. A DHT protocol typically provides a single function to the P2P application: given a key, find the node (s) responsible for that key. All other functions (such as actually retrieving the resource or storing the resource on the node responsible for it) are provided by higher layers of the P2P application. In this paper the main concentration is moving towards to find the security vulnerabilities and determine them on presented routing protocols of such networks. The Chord protocol is preferred as the routing protocol for the variety of motives; those motives are covered in this paper.
A rapid increase of wireless networks and mobile computing applications has changed the landscape of network security. A MANET is more susceptible to the attacks than wired network. As a result, attacks with malicious intent have been and will be devised to take advantage of these vulnerabilities and to cripple the MANET operation. Hence we need to search for new architecture and mechanisms to protect the wireless networks and mobile computing applications. In this paper, we examine the nodes that come under the vicinity of base node and members of the network and communication is provided to genuine nodes only. It is found that the proposed algorithm is a effective algorithm for security in MANETs.
With the growing diversity of malware, researchers must be able to quickly collect many representative samples for study. This is commonly achieved by harvesting the malware from honeypots: Insecure systems presenting a wide attack surface to the public Internet, aiming to attract attackers. However, software-based honeypots have both performance issues in light of 10+ Gb/s networks, as well as difficulties in preventing the compromise of the honeypot system itself. We present an architecture for a honeypot using dedicated hardware instead of a general-purpose processor. Our system is fast enough to keep up with high-speed networks and more resilient against subversion attempts than existing software solutions. It consists of a high-speed implementation of the Internet protocol stack attached to hardware-based emulations of vulnerable applications. A specialized implementation of the TCP protocol, capable of managing hundreds of thousands of simultaneous connections, allows the system to span large honeynets. The practical feasibility of the approach has been demonstrated on a real FPGA platform connected to a 10 Gb/s network interface.
Rogue wireless access points (RWAPs) bypass physical endpoint security of local area networks and present significant security threats by creating network attack vectors behind firewalls, exposing confidential information, and allowing unauthorized utilization of network resources. A family of more promising methods detects RWAPs indirectly by identifying unauthorized wireless hosts through using temporal TCP/IP characteristics of SYN, FIN, and ACK local round trip times (LRTT). Thus any unauthorized wireless hosts found indicate the presence of a RWAP. With these session-based temporal characteristics, traffic from wireless and wired nodes can be differentiated by exploiting the fundamental differences between Ethernet and 802.11b/g/n. In this work, we empirically analyzed extensive LRTT data and designed a light system - RAPiD with several algorithms for effective wireless hosts detection. Ultimately, SYN, FIN, and ACK LRTTs can be compared against each other to discover wireless hosts regardless of network speeds. The results show first time how merging 802.11n wireless technology can still be accurately separated from Ethernet hosts, even as it continues to improve.
Based on the general characteristics of trust between social networking and P2P network, a trust model and an approach of evaluation trust in P2P network under the trusted environment are proposed. An algorithm for computing the trust values is put forward based on the trust relationship among hosts, nodes and pseudonyms. The security analyses prove that the approach can ensure anonymous of host in trusted environments and defend P2P network against the Pseudo spoofing and ID stealth attacks.
Facing the growing on-demand service and security issues in current Internet, this paper proposed a novel idea compounding of OpenFlow, Autonomic and Locator/ID separation technologies to support on-demand service and network security. In addition, we have implemented a DDoS defender to realize autonomic self-defense concept based on OpenFlow-enabled switch.
Aiming at the security problems of the important disclosed files on Intranet, an intelligent transparent encryption model is proposed. This model can unify policies of rank management security for all hosts on Intranet. Combined with the security assessment process, evaluating secret level of documents in the host, this model decides whether to file for dynamic encryption. Through the practical application of the model, the security is enhanced greatly on the Intranet.
This paper analyses the characteristics of the Web Accessing DoS attacks, then proposes an active defense model. Based on the differences of data and time between the Web Accessing DoS attacks and the normal users' browsing behavior, the active defense model will divide the web accessing traffic into three types: the normal browsing traffic, the actual attacking traffic, the dubitable attacking traffic. The policies for accessing traffic are different: the normal browsing traffic is permitted to access the web site; the actual attacking traffic is forbidden to access the web site; the dubitable attacking traffic will be led into the deception web site, then the active defense model will determine whether to permit the traffic to access the web site or not according to the observing result. The experimental results show that the model is effective in detecting and preventing the Web Accessing DoS attacks.
An entropy gain based overall trust degree (OTD) computation model is proposed for large scale trusted networks. To overcome subjectivity in trust decision, attributes' entropies and their gains are adopted to set attribute weights in the OTD fusion model. Attribute weights are dynamically adjusted by refreshing sample set and attribute entropy gains, to adapt to variations of network entities. Under the proposed method, attribute weights maintain relative stable but keep responses to network uncertainties.
A method of design and implementation of watermarking copyright protection program based on MPEG-4 bit stream is proposed in this paper. The selected DCT coefficients are changed little to meet the specific relationship and represent one bit information. The watermark information is embedded in IVOP color quantized DCT DC coefficient. Not only video spectrum distribution is obtain without the additional change, but also the watermark is not impacted from DCT coefficients quantify. Experiments show that the watermark algorithm in MPEG-4 video achieved good results in the visibility and robustness through a variety of attack treatment.
With the demands for network security, some heterogeneous security equipments such as firewalls, intrusion detection systems, and anti-virus gateways are widely deployed in network, and produce massive security events which need to be merged and analyzed. Therefore, a distributed and multi-protocol supported network security monitoring system is proposed. The paper describes the architecture of the network security monitoring system. Focusing on the system acquisition layer, two methods are designed for monitoring data collection: syslog-based collection and real-time traffic-based collection. The ActiveMQ which based on the JMS specification was adopted for data transmission...
Radio Frequency Identification (RFID) systems based on EPC (Electronic Product Code) Network Environment automatically identify tagged objects, using RF signals without direct contact. Its initial success in offering strategic advantages for businesses, by efficient tracking of inventory in the supply chain, has left this technology wide open to many applications that are only limited by people's imagination. In this paper, we explain mobile RFID network based on EPC and analyze threats of the mobile RFID system. Our architecture is security and efficiency. The proposed idea is simple but important to create a secure IOT architecture.
In home network environment, security is one of the most important issues that need to be addressed because the services provided by all kinds of household appliances are likely related with the privacy information of the residents. Although several access control mechanisms have been proposed in the last few years, these existing models always preserve all the access control policies in the home gateway or the distributed household appliances, and all of them have conspicuous defects. In this paper, we divide access control policies into general policies and privacy policies according to different security requirements of the residents. With the help of SPKI certificates theory, we demonstrate that our model is an effective access control mechanism by providing two types of access control policies with different security schemes.
Malicious web pages that launch client-side attacks on web browsers have became a severe threat in today's Internet. High-interaction client honeypots are security devices that detect these malicious web pages on a network. However, high-interaction client honeypots are not good enough for detecting malicious web pages, especially for web pages carrying rootkit which is used to hide the presence of a malicious object (process, file, registry key, network port). To this deficiency, this paper brings forward a detecting technique of kernal integrity which is based on System Services Descriptor Table(SSDT) in High-interaction honeypots client side. The experimental results indicate that the correct ratio in detecting malicious servers raise obviously.
Although the new type of network security incidents continue to occur, most security incidents are similar, the response methods have in common, so CBR (Case Based Reasoning) technology can be used to describe the successful experience of the past incident response. Based on past examples of how to develop rapid response strategy is the key to incident responses. Automated planning method can greatly improve the efficiency and level of decision making. According to the characteristics of incident responses, combined with automatic planning method, CBR technology and ontology technology, a novel approach of getting incident response methods is presented.
Recently, Wang et al. showed that Das et al.'s dynamic ID-based remote user authentication scheme is vulnerable to an impersonation attack and can not achieve mutual authentication. Consequently, a more efficient and secure dynamic ID-based remote user authentication scheme was proposed and claimed that it was now secure and of practical value. However, in this paper, we will show that Wang et al.'s scheme is still vulnerable to off-line password guessing attacks, where the adversary can off-line guess a legal user's password from eavesdropping. Moreover, the dynamic ID feature of their scheme can not be achieved and the adversary is able to determine who was communicating with the remote server. Finally, the process of Wang et al.'s scheme is inconvenient for the user Ui due to any user who picks up Ui's smart card can easily change the original password and Ui has no choice for choosing his/her password in the registration phase.
Recently, Khan et al. showed that Wang et al.'s dynamic ID-based remote user authentication scheme is not feasible for real-life implementations such as without preserving anonymity of a user during authentication, user cannot choose the password he/she wants, no provision for revocation of lost or stolen smart card, and can not provide session key agreement. Consequently, an improved version of dynamic ID-based remote user authentication scheme was proposed and claimed that it was now secure and of practical value. However, in this paper, we will show that user anonymity of Khan et al.'s scheme is not preserved and a registered user Uj can identify the login person Ui trying to login into the server. Furthermore, Khan et al.'s scheme suffers from insider attacks and the malicious insider can impersonate legal users to login into remote server.
Intrusion Detection System (IDS) is an important and necessary component in ensuring network security and protecting network resources and infrastructures. In this paper, we effectively introduced intrusion detection system by using Principal Component Analysis (PCA) with Support Vector Machines (SVMs) as an approach to select the optimum feature subset. We verify the effectiveness and the feasibility of the proposed IDS system by several experiments on NSL-KDD dataset. A reduction process has been used to reduce the number of features in order to decrease the complexity of the system. The experimental results show that the proposed system is able to speed up the process of intrusion detection and to minimize the memory space and CPU time cost.
More complicated computational tasks are posed to the network equipments, such as Deep packet inspection (DPI) for network security check and network coding to achieve efficient multicast, etc. These complicated applications need processors to process the whole packet payload, potentially causing low throughput and long latency due to the large access delay to external memories. The behind hint lies that we can get the packet-processor/thread pair binding information in advance from the front-end dispatching component before the packet will be actually processed by cores. This interesting observation enables us design a new architecture of memory access for packet processors instead of the traditional model. In this paper we explore to apply push model to packet processors. The push model makes the data being pushed into the local memory/on-chip L1 cache in an on-demand and fine granularity manner ahead of being asked by running instructions, making a core always feels getting its data from the local memory/L1 cache instead of fetching them from the external memory in pull model. In order to verify the effectiveness, we design and implement the push model with the Intel IXP2850, and then conduct experiments to show the performance of push model in the IXP2850 simulator compared with the pull model. Simulation results indicate that applying push model to packet processors could improve the system throughput and reduce the packet processing latency and reducing required number of hardware threads.
Two key elements in disaster recovery are backing up data at remote sites and reliability of services by virtualization. To support operational services and to speed up recovery process, resources should be distributed fairly and efficiently. Thus, we discuss resource allocation algorithms to support remote data storage and live virtual machines (VMs) migration. We identify two opposing forces: on the one hand, backup data should be stored as close as possible to the original site to guarantee high access speed and to minimize network load. On the other hand, upon a site failure, VM migration should minimize the impact of resuming VMs on other sites, to protect application performance and to reduce VM restoring time. We present optimal algorithms trading-off these two contrasting goals, and we compare their performance for different network topologies and resource distributions among sites.
We propose and analyze a class of trust management protocols for encounter-based routing in delay tolerant networks (DTNs). The underlying idea is to incorporate trust evaluation in the routing protocol, considering not only quality-of-service (QoS) trust properties (connectivity) but also social trust properties (honesty and unselfishness) to evaluate other nodes encountered. Two versions of trust management protocols are considered: an equal-weight QoS and social trust management protocol (called trust-based routing) and a QoS only trust management protocol (called connectivity-based routing). By utilizing a stochastic Petri net model describing a DTN behavior, we analyze the performance characteristics of these two routing protocols in terms of message delivery ratio, latency, and message overhead. We also perform a comparative performance analysis with epidemic routing for a DTN consisting of heterogeneous mobile nodes with vastly different social and networking behaviors. The results indicate that trust-based routing approaches the ideal performance of epidemic routing in delivery ratio, while connectivity-based routing approaches the ideal performance in message delay of epidemic routing, especially as the percentage of selfish and malicious nodes present in the DTN system increases. By properly selecting weights associated with QoS and social trust metrics for trust evaluation, our trust management protocols can approximate the ideal performance obtainable by epidemic routing in delivery ratio and message delay without incurring high message overhead.
Along with recent Internet security threats, different security measures have emerged. Whilst these security schemes ensure a level of protection against such threats, they sometimes have significant impact on perceived Quality of Service (QoS). There is thus need to retrieve ways for an efficient integration of security requirements with their QoS counterparts. In this paper, we devise a Quality of Protection framework that tunes between security requirements and QoS using a multi-attribute decision making model. The performance of the proposed approach is evaluated and verified via a use case study using computer simulations.
We propose a PHY-authentication protocol to detect spoofing attacks in wireless networks, exploiting the rapid-decorrelation property of radio channels with distance. In this protocol, a PHY-authentication scheme that exploits channel estimations that already exist in most wireless systems, cooperates with any existing-either simple or advanced-higher-layer process, such as IEEE 802.11i. With little additional system overhead, our scheme reduces the workload of the higher-layer process, or provides some degree of spoofing protection for ``naked" wireless systems, such as some sensor networks. We describe the performance of our approach as a function of the spoofing pattern and the snapshot performance that can be easily measured through field tests. We discuss the implementation issues of the authentication protocol on 802.11 testbeds and verify its performance via field tests in a typical office building.
Mobile Ad hoc Networks (MANET) have been highly vulnerable to attacks due to the dynamic nature of its network infrastructure. Among these attacks, routing attacks have received considerable attention since it could cause the most devastating damage to MANET. Even though there exist several intrusion response techniques to mitigate such critical attacks, existing solutions typically attempt to isolate malicious nodes based on binary or naive fuzzy response decisions. However, binary responses may result in the unexpected network partition, causing additional damages to the network infrastructure, and naive fuzzy responses could lead to uncertainty in countering routing attacks in MANET. In this paper, we propose a risk-aware response mechanism to systematically cope with the identified routing attacks. Our risk-aware approach is based on an extended Dempster-Shafer mathematical theory of evidence introducing a notion of importance factor. In addition, our experiments demonstrate the effectiveness of our approach with the consideration of the packet delivery ratio and routing cost.
Successful "cracking" of bit-level security compromises network integrity and physical layer augmentation is being investigated to improve overall security. Intra-cellular security is addressed here using device-specific RF "Distinct Native Attribute" (RF-DNA) fingerprints in a localized regional air monitor, with targeted applications including cellular networks such as the Global System for Mobile (GSM) Communications and last mile Worldwide Interoperability for Microwave Access (WiMAX) systems. Previous work demonstrated GSM inter-manufacturer classification (manufacturer discrimination) using RF-DNA fingerprinting and achieved accuracies of 92%at SNR = 6 dB. These results are extended here for intra-manufacturer classification (serial number discrimination). Historically, intra-manufacturer discrimination has posed the greatest challenge and RF-DNA fingerprinting has been effective with both Orthogonal Frequency Division Multiplexed (OFDM) and Direct Sequence Spread Spectrum (DSSS) network signals. Intra-manufacturer GSM results are provided here based on identical signal collection, fingerprint generation, and MDA/ML classification processes used for previous inter manufacturer assessment. When comparing performance, the trend for GSM intra-manufacturer classification is consistent with previous work for other network-based signals and device classification is much more challenging. For classification accuracies of 80%or better, intra manufacturer fingerprinting requires an increase of 20-25 dB in SNR to achieve inter-manufacturer performance.
Most of current ring-based Ethernet protection schemes work on a basis of filtering database (FDB) flush operation to guarantee data forwarding on protection switching. However, when a link or node failure occurs in a congested ring network, delayed data frames that have lower priorities than control frames can cause the FDB inconsistency problem that ring nodes build incorrect FDB information. Under this circumstance, some of ring nodes forward data frames in the wrong directions, so that the ring network shows undesirable and unstable performance. In order to resolve this problem, this paper introduces the following three techniques: flush delay timer, purge triggering, and priority setting. Especially, for the priority setting scheme we develop an absorbing Markov chain model for the Ethernet Ring Protection (ERP) protocol defined by G.8032. Then, we theoretically estimate the expected protection time on protection switching underlying the M/M/1/k queue system and also suggest an enhanced priority setting method to guarantee reliable protection switching even in a congested network. Along with our theoretical approach, we additionally evaluate the protection performance with a network simulator to verify our mathematical model. Simulation results show that our analytic model is indeed capable of estimating the expected protection switching time.
Node behavior profiling is a promising tool in many aspects of network security, especially in malware detection. In this paper, based on node behavior profiles proposed in the literature, we propose a fast anomaly detection scheme using SPRT (Sequential Probability Ratio Test) for malware/worm detection. The key idea of this paper is, instead of checking most of the nodes in a network, only a small number of sample nodes are required for detection with the help of SPRT. In our initial studies, we evaluate the fast detection scheme using real enterprise data (LBNL traces). The results show that the fast detection scheme achieves good performances in terms of low false positive and high detection rates.
Wireless mesh networks (WMNs) have emerged as support for applications on various domains, such as military,financial and healthcare. Those applications claim for high level of both end-to-end performance and security. However, WMNs are naturally susceptible to security issues that can compromise network performance. This paper presents a cross-layer and adaptive scheme for balancing performance and security on data routing when the network experiences malicious actions from damaged nodes. Simulation results show that our approach based on a Multiple Criteria Decision Making method improves the tradeoff network performance and security even under malicious activity.
Network firewalls act as the first line of defense against unwanted and malicious traffic targeting private networks connected to the Internet. Predicting the overall firewall performance, especially under attack, becomes crucial to network security engineers and designers in assessing how affective and tolerable a network firewall is, thereby be able to sustain the availability of network services. In this paper, we present an analytical queueing model based on the embedded Markov chain to study and analyze the performance of rule-based firewalls when subjected to normal and DoS attacks. We derive equations for key features and performance measures of engineering and design significance. In addition, we validate our analytical model against real experimental measurements.
Multi-string matching is a key technique for implementing network security applications like Network Intrusion Detection Systems (NIDS). Existing DFA-based approaches always tradeoff between memory and throughput, and fail to has the best of both worlds. This paper extends the classic longest prefix principle from single-character to multi-character string matching and proposes a multi-string matching acceleration scheme named Independent Parallel Compact Finite Automata (PC-FA). In the scheme, DFA is divided into k PC-FAs, each of which can process one character from the input stream, achieving a speedup up to k with reduced memory occupation. Theoretical proof is given for the equivalency between traditional DFA and PC-FA approach. Experimental evaluations show that seven times of speedup can be practically achieved with a reduced memory size than up-to-date DFA-based compression approaches.
Previous distributed anomaly detection efforts have operated on summary statistics gathered from each node. This has the advantage that the audit trail is limited in size since event sets can be succinctly represented. While this minimizes the bandwidth consumed and helps scale the detection to a large number of nodes, it limits the infrastructure's ability to identify the source of anomalies. We describe three optimizations that together allow fine-grained tracking of the sources of anomalous activity in a Grid, thereby facilitating precise responses. We demonstrate the scheme's scalability in terms of storage and network bandwidth overhead with an implementation on nodes running BOINC. The results generalize to other types of Grids as well.
The number of users of mobile devices has increased enormously over the past five years. Along with this, the public concern about how to make sure personal information remains private and safe has also grown, and it has become a high priority standard for mobile device producers to fulfill. Programming languages, by themselves, offer a limited safely, unfortunately. A common example is the use of credit cards, where the credit limit can never be exceed since the programming language cannot guarantee. The authors identify the need to structure the knowledge involved during the process of mobile devices safety development as a crucial step to guarantee trust between users and producers. The proposal project consists on creating a representative model between users and mobile device producers called Onto Device Trust. This model is based on ontologies where knowledge and information are structured on classes and concepts. The ontologies involved in this representative model are: Languages, Tools, Safeties, Industries, Smart Cards, Mobile Devices, Applications, Certificates, Evaluations, Clients, Analyzers and Testers. The evaluation of the ontology content in the Onto Device Trust model is carried out through a technical judgment of the ontology and its associated conceptualization.
The ability to measure metric information from videos is an essential functionality in security software packages that populate databases with human morphometric and gait descriptors. The configuration of such security systems often involves calibrating the cameras in the surveillance network. For this purpose, markers are placed on the ground in the vicinity of each camera location in order to compute a critical image-to-ground homography. This paper shows how the homography computation can be avoided by describing the key components of a system designed for view-independent video metrology. Experiments on measuring the height of subjects and ground distances from different camera views demonstrate the viability of the approach. In addition, it is shown that the calibration model of the proposed method yields more accurate measurements than the frequently used square pixel camera model.
"merge" It is gradual progress of network of next generation and inexorable trend of development, realize the network merge and merge with the business, it is basic prerequisite and basic key element that the data are merged. " unified user's data " As inevitable development trend that network merge, build up about of the next generation network at present, network merge with business merging research focusing of field. This paper introduces the current research on unified user's data, analyze it in network security threat under the current merging network, provides the safe problem that should pay close attention and study of the unified user's data.
Many social networks being analyzed today are generated from sources with privacy concerns. A number of network centrality measures have been introduced to better quantify various social dynamics of interest to social scientists. In this paper, we propose an approximation of a social network that allows for certain centrality measures to be calculated while hiding information about the full network. Our approximation is not a perturbed graph, but rather a generalize trie structure containing a network hop expansion set for each node in the graph. We show that a network with certain topological structures, naturally hides nodes and increases the number of candidate nodes in each equivalence class. The storage of our graph approximation naturally clusters nodes of the network with similar graph expansion structure and therefore, can also be used as the basis for identifying 'like' nodes in terms of similar structural position in the network. For branches of the trie that are not private enough, we introduce heuristics that locally merges segments of the trie to enforce k-node anonymity.
Packet filtering in firewall either accepts or denies network packets based upon a set of pre-defined filters called firewall policy. Firewall policy is designed under the instruction of security policy. A network security policy is a generic document that outlines the needs for computer network access permissions. And it determines how firewall filters are designed. If inconsistencies, such as redundant filters, insufficient filters or contradict filters, exist between security policy and firewall policy, firewall policy could not filter packets exactly, and the network protected by the firewall will be affected. To resolve this problem, we propose an inconsistency detection system to detect the inconsistencies between the security policy and firewall policy. When the administrator could not get host IP addresses, port number and other specific values, according to the network configurations, our proposed system could transform the network security policy and firewall policy to the same range value, represent and analyze their spatial relationships to detect their inconsistencies. The proposed system has been successfully implemented in a prototype system. We have been confirmed the effectiveness of the proposed system.
Self-healing key distribution enables users in a large and dynamic group to establish session keys for secure group communication over an unreliable network. The main property of self-healing key distribution is that the users are capable of recovering lost session keys on their own, without requesting additional transmissions from the group manager. In this paper, we propose a self-healing key distribution scheme, called Secure Self-healing Group Keying, for wireless sensor networks. Under an appropriately defined security model, we show that our scheme outperforms previous research work in terms of storage and communication overhead.
We performed statistical analysis on the total PTR resource record (RR) based DNS query packet traffic from a university campus network to the top domain DNS server through March 14th, 2009, when the network servers in the campus network were under inbound SSH dictionary attack. The interesting results are obtained, as follows: (1) the network servers, especially, they have a function of SSH services, generated the significant PTR RR based DNS query request packet traffic through 07:30-08:30 in March 14th, 2009, (2) we calculated sample variance for the DNS query request packet traffic, and (3) the variance can change in a sharp manner through 07:30-08:30. From these results, it is clearly concluded that we can detect the inbound SSH dictionary attack to the network server by only observing the variance of the total PTR RR based DNS query request packet traffic from the network servers in the campus network.
Transmission Control Protocol (TCP) Synchronized (SYN) Flood has become a problem to the network management to defend the network server from being attacked by the malicious attackers. The malicious attackers can easily exploit the TCP three-way handshake by making the server exhausted and unavailable with spoofed Internet Protocol (IP) address. The main problem in this paper is how to detect TCP SYN flood through network. This paper used anomaly detection to detect TCP SYN flood attack based on payload and unusable area in Hypertext Transfer Protocol (HTTP). The results show that the proposed detection method can detect TCP SYN Flood in the network through the payload.
Malicious software has become a major threat to computer users on the Internet today. To combat it, security researchers need to gather and analyze many samples to develop proper defense mechanisms. The setting of honeypots, which emulate vulnerable applications, is one method of gathering attack code. In contrast to the conventional software-based honeypots, we have proposed a dedicated hardware architecture for honeypots to both allow high-speed operation at rates of 10+ Gb/s as well as a higher resilience against attacks on the honey pot infrastructure itself. We now improve the flexibility of our prior solution by using partial dynamic reconfiguration to update the functionality of the honey pot during operation.
Jamming is a serious security threat to a wireless sensor network since the network relies on open wireless radio channel. A jamming attacker launches jamming attacks easily by transmitting high-power signals and all legitimate sensor nodes interfered by jamming signals suffer corrupted packet transmissions. More importantly, the jammer is typically strategic and chooses its jamming strategy in response to the possible defense strategy taken by the sensor network. In this paper we model the interaction between the sensor network and the attacker as a non-cooperative non-zero-sum static game. In such a game, the sensor network has a set of strategies of controlling its probability of accessing the wireless channel and the attacker manipulates its jamming by controlling its jamming probability after sensing a transmission activity. We propose an efficient algorithm for computing the optimal strategies for jamming attack and network defense. A critical issue is that there may exist a number of possible strategy profiles of Nash equilibria. To address this issue, we further propose to choose realistic Nash equilibria by applying Pareto-dominance and risk dominance. Our numerical results demonstrate that the strategies chosen by Pareto-dominance and risk dominance achieve the expected performance. Our results presented in the paper provides valuable defense guidance for wireless sensor networks against jamming attacks.
There are many approaches proposed to provide anonymous peer-to-peer communications. Data sent via peer-to-peer communications is vulnerable to traffic analysis. Traffic analysis is the process of intercepting and analysing messages in order to compromise information from patterns in communications. An intruder can get information about the content of the data, the requester's and provider's identities. Anonymous approaches are designed with the following three goals: to protect the identity of provider, to protect the identity of requester and to protect the contents of transferred data between them. This article presents a new peer-to-peer approach to achieve anonymity between a requester and a provider in a hybrid peer-to-peer information-sharing environment with trusted servers called supper node so that the provider will not be able to identify the requester and no other peers can identify the two communicating parties with certainty. This article shows that the proposed approaches improved reliability and has more security. This approach, based on onion routing and randomization, protects transferring data against traffic analysis attack. The ultimate goal of this anonymous communications approach is to allow a requester to communicate with a provider in such a manner that nobody can determine the requester's identity and the content of transferred data.
Identity authentication is an important component of network security to guarantee the security of the information and service on open network. In this paper, we propose a secure strong two-factor identity authentication which stores digital certificate in smart card, and then protects the PIN of smart card with fuzzy fingerprint vault. Only the legal user can release the securely stored PIN to open the smart card and acquire the stored digital certificate with its private key by inputting his/her fingerprint. This scheme further perfects the safe authentication of PKI, and can be applied in the identity authentication of high-end user or the user with special safety requirement.
Wireless sensor networks are a new type of networked systems, characterized by severely constrained computational and energy resources, and an ad hoc operational environment. When wireless sensor networks are deployed in a hostile terrain, security becomes extremely important, as they are prone to different types of malicious attacks. Due to the inherent resource limitations of sensor nodes, existing network security methods, including those developed for Mobile Ad-Hoc Networks, are not well suitable for wireless sensor networks. As a crucial issue security in wireless sensor networks has attracted a lot of attention in the recent year. This paper made a thorough analysis of the major security issue and presented the ongoing aspect of further development to designers in their struggle to implement the most cost effective and appropriate method of securing their network.
The Internet is huge and largely unregulated place. There are a lot of dangers in putting up a web page on Internet. These vary from invasions of privacy to hackers cracking our system. One of the most important issues in traditional and modern network architecture is security. Web security is a vast area encompassing computer system security, network security, authentication services, message validation, personal privacy issues and cryptography. Cryptography is proven to be a better solution for implementing web security. Cryptography is not only about encrypting and decrypting messages; it is also about solving real world problems that require information security.
Wireless Sensor Network (WSN) is an emerging technology that shows great promise for various futuristic applications both for mass public and military. The sensing technology combined with processing power and wireless communication makes it lucrative for being exploited in abundance in future. The inclusion of wireless communication technology also incurs various types of security threats. Although the content of sensor messages describing events of interest may be encrypted to provide confidentiality, the context surrounding these events may also be sensitive and therefore should be protected from eavesdroppers. An adversary armed with knowledge of the network deployment, routing algorithms, and the base-station (data sink) location can infer the temporal patterns of interesting events by merely monitoring the arrival of packets at the sink, thereby allowing the adversary to remotely track the spatio-temporal evolution of a sensed event. One of the most notable challenges threatening the successful deployment of sensor systems is privacy. Although many privacy-related issues can be addressed by security mechanisms, one sensor network privacy issue that cannot be adequately addressed by network security is source-location privacy.
This paper discusses an application of multilayer Perceptron in wireless sensor network security. The wireless sensor network (WSN) is a network of distributed autonomous devices that supervise physical or environmental conditions corporately. The multilayer perceptron (MLP) based media access control protocol (MAC) to secure CSMA-based wireless sensor networks against denial-of-service attacks launched by adversaries. The MLP boost security of a WSN by consistently monitoring the parameter that reveals unusual variation in case of an attack the MLP alerts the MAC layer and the physical layer of the sensor mode when the suspicion factor, the output of the MLP exceeds a preset threshold level. Back propagation and Radial Basis Function Network (RBFN) algorithms are used for training the MLP. Simulation results show that the MLP helps in extending the life time of the WSN.
In the past few years government and military organizations have widely adopted LAN, WAN and Internet to take advantage of advancement in technology. Computers are integral part of everyday operations. Organizations depend on them. A computer failure will have a critical impact on the organization. The term Security brings to mind all shorts of issues that refer to data protection and prevention of unauthorised access. The common motives for computer crimes could be lure for money, revenge, terrorism, fun, recognition or curiosity. Information systems can be attacked by outsiders who may penetrate a computer system or by insiders who are authorised to use the resources but misuse their authorization. An attacker may disrupt the information system of an organization (active attack) or gain access to its sensitive information (passive attack). Although no direct damage is done in a passive attack, any leak in information could have drastic repercussions for the organization. In simple words, security can be defined as: protecting information system from intended access. The goal of network management is to provide users with a quality of service. To meet this goal, network services plan involve strategic and technical planning of engineering, operations and maintenance of the network. The field of network security and management is constantly undergoing changes in technology, applications and hence the need for continually changing skills set on the part of academics. We will try to cover the various points with respect to organizational structure, policies and techniques of network security and management. Impact of various factors including management has been discussed on network security.
A mobile ad hoc network (MANET) is a collection of mobile nodes that communicate with each other by forming a multi-hop radio network. Security remains a major challenge for these networks due to their features of open medium, dynamically changing topologies, reliance on cooperative algorithms, absence of centralized monitoring points, and lack of clear lines of defense. Design of an efficient and reliable node authentication protocol for such networks is a particularly challenging task since the nodes are battery-driven and resource constrained. This paper presents a robust and efficient key exchange protocol for nodes authentication in a MANET based on multi-path communication. Simulation results demonstrate that the protocol is effective even in presence of large fraction of malicious nodes in the network. Moreover, it has a minimal computation and communication overhead that makes it ideally suitable for MANETs.
During the last few years, need for protection of computer networks from the outside world is evident. Connectivity of the computer networks to worked wide internet exposes them to many kinds of cyber crimes. Currently Intrusion detection systems have grown to be an ordinary component of network security infrastructure. With mounting global network connectivity, the issue of intrusion has achieved importance, promoting active research on efficient Intrusion Detection Systems (IDS). Artificial Immune System (AIS) is a new bio-inspired model, which is applied for solving various problems in the field of information security. The unique features AIS encourage the researchers to employ this techniques in variety of applications and especially in intrusion detection systems. This paper presents a survey on current Artificial Immune System based Intrusion Detections.
Computer networks are the core of the cyberinfrastucture and are endangered by malicious activities and natural disasters. These threats stress the need for a resilient computer networks to ensure business operational continuity and optimum performance. Several aspects affect computer network resilience level such as network topology, routing protocols, and others. This paper demonstrates Redundancy, Fault Tolerance and Resilience concepts within the computer network domain to illustrate the computer networks resilience goals. It also proposes a set of parameters to evaluate routing protocols' resilience. Results showed that the compounded use of throughput rate, convergence time, and routing protocol table size can be used as to evaluate the routing protocols behavior towards the computer networks resilience.
In recent years phishing attacks have become one of the most important problems of online security. Aza Raskin, the creative lead of Mozilla Firefox team, proposed a new type of phishing attack, tabnabbing attack as he names it. The attack is different from classical phishing attacks; while classical attacks rely on deception of users with a similar URL and/or content in appearance to the original site, this attack uses our memory weakness and false perception that browser tabs are immutable i.e., do not change while inactive. We develop a Firefox add-on to protect users against this attack. Our method is based on the fact that a phishing web site should change its layout radically to look like the original site. This add-on watches the open tabs and indicates whether one changes its layout, favicon and/or title to become like another site.
The following topics are dealt with: intelligent sensor; wireless sensor networks; sensor fusion; sensor network security; and sensor network for health care.
Today P2P networks are responsible for a large amount of traffic on the Internet, as many Internet users employ such networks for content distribution. At the same time, P2P networks are vulnerable to security threats such as Internet worms and facilitate their propagation. Internet worms and more generally malware are a major concern to the network security community. There are many different type of worms in the wild, mostly categorized based on how they find and infect their new victims (i.e. active, passive, etc.). In this paper, we investigate a new approach for detecting passive worms and malware in P2P networks based on the popularity of files in the network. As part of our investigation, we crawl the Gnutella P2P network over a 12 day period collecting file names and file popularity statistics. We are then able to extract the highly popular files and identify worm/malware files within them with high accuracy.
A Routing Algorithm base on Trustworthy Core Tree for WSN (RATCT) is proposed in this paper aims to prolong network lifetime as well as increase network security in a hierarchical-cluster sensor network. Cluster heads with higher residual energy and trust level are elected from underlying sensor nodes. A minimum energy consumption spanning tree algorithm is used to organize all cluster heads into a trustworthy core tree with sink node as tree root. The Trustworthy core tree is expanded to cover all nodes so that each node report to sink node along a unique route. A trust model is integrated in RATCT to evaluate node's trust level and detect evil nodes. Simulation results testify to the effectiveness of the algorithm in producing a longer network lifetime and a safer network.
In recent years, there has been a growing interest in information protection and security for large organizations. This has led to a growing demand for more aggressive forms of security to complement the existing techniques. One of these security methods involves the use of distributed honey nets. Honey nets are network systems deployed for the sole purpose of being compromised, in order to assess adversaries. In this paper, we conduct a comprehensive survey for implementing distributed honey nets and explain their architecture and design. We also present our experience in simulating a virtual distributed honey net environment at King Fahd University of Petroleum and Minerals (KFUPM) using Honey wall CDROM, Snort and Sebek tools. Our experience shows that Honey wall CDROM proved to be a solid tool that is capable of capturing great deal of information and assisting in analyzing traffic on the distributed honey pots. The honey net designer, nevertheless, needs to consider few issues related to scalability and resource utilization.
Wireless networks have made their mark in higher educational institutes worldwide. The authors have conducted a web-based survey to investigate various aspects of wireless networks in higher education institutes around the world. This paper discusses the findings of the survey results in higher education institutes worldwide. Thirty-six higher education institutes participated in this survey. Three of the institutes filled the survey partially and therefore survey results for thirty-three institutes only were considered. There were seventeen questions in the survey covering topics related to wireless networks only in education institutes. This paper presents the survey results pertaining to wireless network standards, security and future of this technology in education institutes. Most of the earlier studies that have been done prior to this, have covered wireless usage in their own country. This research has attempted to survey countries across the world. For a better understanding of the survey results this paper presents a brief review of different wireless technologies in relation to wireless network security and standards available in the market. Subsequently survey results of wireless network standards, security techniques used by the higher education institutes are depicted and analysed.
Covert channels via the widely used TCP/IP protocols have become a new challenge issue for network security. In this paper, we propose an effective method to detect the existence of hidden information in TCP ISNs (Initial Sequence Numbers), which are known as the most difficult covert channels to be detected. Our method uses phase space reconstruction to characterize dynamic nature of ISNs. A statistical model is then proposed. Based on this proposed model, the classification algorithm is developed to identify the existence of information hidden in ISNs. Simulation results have demonstrated that our proposed detection method outperforms the-state-of-the-art in terms of high detecting accuracy and greatly reduced computational complexity. Instead of off-line processing as the-state-of-the-art does, our new scheme can be used for on-line detection.
With VoIP being deployed on large scale, forensic analysis of captured VoIP traffic is of major practical interest. In this paper, we present a new fingerprinting approach that identifies the types of devices (name, version, brand, series) in captured VoIP traffic. We focus only on the signaling plane and discard voice related data. Although we consider only one signaling protocol for the illustration, our tool relies on structural information trees and can easily be adapted to any protocol of that has a known syntax. We have integrated our tool within the well known tshark application in order to provide an easy to use support for forensic analysts.
Routing security is an important and well known issue in Ad hoc network applications and development. Various kinds of solutions have been proposed but they are impractical to be fully applied. In this paper, firstly the wormhole attack topology is analyzed, then cryptography and trust mechanism are combined to design a new multipath trust-based secure routing protocol(MTSR). MTSR based on AODV and SAODV, is distributed and can resist almost all available routing attacks such as discarding, Sybil, spoofing, jamming, flooding, rushing, and especially wormhole attack. Its trust value computation follows the principle of slowly increasing but sharply decreasing, and it does not require any additional equipment, strict assumptions, node location and precise time information.
Currently Intrusion detection systems have grown to be an ordinary component of network security infrastructure. With mounting global network connectivity, the issue of intrusion has achieved importance, promoting active research on efficient Intrusion Detection Systems (IDS). Artificial Immune System (AIS) is a new bio-inspired model which is applied for solving various problems in the field of information security. The unique features AIS encourage the researchers to employ this techniques in variety of applications and especially in intrusion detection systems. Proper IDS design is essential to improve the performance of the IDS. The centralized design of this IDS has disadvantage of central processing for massive processes for each packets passing trough network. In this paper we proposed a distributed multi-layerd framework to enhance the detection performance and efficiency of this IDS. In our design the genetic algorithm is used for enhancing the secondary immune response. The fundamental design of our proposed AIS based IDS consists of 2 main components: IDS central engine and detection sensors. Each of these components is composed of some agents which correlate with each other in order to detect the anomalies and intrusions. Our design goal is to decrease the detection time for each connection by distributing the detectors to each host.
VPN technology is an effective method and easy to realize on WLAN. Several of VPN technologies have already been widely used nowadays. Each of them has its advantages and disadvantages. this paper introduces the working principle of SSL VPN and IPSec VPN, then analyzes these two kinds of VPN scheme of the advantages and disadvantages, finally several aspects such as the field of application the security strategy Installation were Analysis and compared.
Network attack and defense experiments have always been the important practices in courses of network security. As the Internet is not a target of network attack and defense in experiments, a virtual network environment for attack and defense is needed. In this paper, we propose a network attack and defense experimental platform based on the virtual honey net. Network attacker is formed by a number of hosts. Network defender is formed by the system of a virtual honey net. Using data capture and data control functions of the honey net, we can analyze the tools, methods, and techniques used by intruders.
With the growing diverse demands for Internet applications, network security issues become more acute. To address the appropriate network security from network intrusion detection event has become an important research in network security. In this paper, user behavior features are extracted to create the model for the user transmission behavior. The demands for anomaly detection and the specific characteristics of audit data are studied. An intrusion detection algorithm and based on statistics variance method in user transmission behavior (IDSV) and the implementation framework are provided. And then, the IDSV algorithm is applied into ARP spoofing detection. The simulation results show that IDSV algorithm does well in detection rate of intrusion detection and has good detection performance of different application features. The IDSV algorithm can detect intrusion effectively under user behavior in different applications.
Multi-pattern matching with wildcards is to find all the occurrences of a set of patterns with wildcards in a text. This problem arises in various fields, such as computational biology and network security. But the problem is not extensively studied as the single pattern case and there is no efficient algorithm for this problem. In this paper, we present efficient algorithms based on fast Fourier transforms. Let $P={p_1,ldots,p_k}$ be a set of patterns with wildcards where the total length of patterns is $|P|$, and a text $t$ of length $n$ over alphabet $a_1,ldots,a_{sigma}$. We present two algorithms for this problem where patterns are matched simultaneously. The first algorithm finds the matches of a small set of patterns in the text in $O(nlog |P|+nk)$ time. The words used in the algorithm are of size $klceil2lgsigmarceil+sum_{i=1}^k lceillg|p_i|rceil$ bits. The second one finds the matchings of patterns in the text in time $O(nlog |P|logsigma+nk)$ by computing the Hamming distance between the patterns and the text. The algorithm uses the words with $sum_{i=1}^k lceillg|p_i|rceil$ bits. We also demonstrate an FFT implementation based on the modular arithmetic for machines with word size of 64 bits. Finally, we show that both algorithms can be easily parallelized and the parallelized algorithms are given as well.
A server needs strong security systems. For this goal, a new perspective to network security is won by using data mining paradigms like outlier detection, clustering and classification. This study uses K-Nearest Neighbor (KNN) algorithm for clustering and classification. KNN algorithm needs data warehouse which impersonates user profiles to cluster. Therefore, requested time intervals and requested IPs with text mining are used for user profiles. Users in the network are clustered by calculating optimum k and threshold parameters of KNN algorithm. Finally, over these clusters, new requests are separated as outlier or normal by different threshold values with different priority weight values and average similarities with different priority weight values.
As an important means of information security, data backup technology has attained increasing attention. This paper proposes a data backup method based on file system filter driver. It monitors various operated events of application program through file system filter driver and do incremental backup by recorded events. The related experiment verified that the method reduces the file backup redundancy, improves the backup efficiency, and to the right to monitor the file attributes, name changes and other events. The method has broad application prospect in actual data backup system.
In contemporary communication system, Peer-to-Peer Session Initiation Protocol (P2PSIP) is the forthcoming migration from the traditional client-server based SIP system. Traditional centralized server based SIP system is vulnerable to several problems like performance bottleneck, single point of failure. So, integration of Peer-to-Peer system (P2P) with Session Initiation Protocol (SIP) will improve the performance of a conventional SIP system because a P2P system is highly scalable, robust, and fault tolerant because of its decentralized manner and self-organization of the network. However, P2PSIP architecture faces several challenges including trustworthiness of peers, resource lookup delay, Network Address Translation (NAT) traversal, etc. This paper focuses on understanding the needs of integration of P2P and SIP. It reviews the existing approaches to identify how they have integrated P2P and SIP and solved the challenges introduced by P2PSIP.
Network anomaly detection is an important issue in network security. Detecting network anomaly is a challenging because of the multivariate property of the collected data, the diversity of the causes and the complexity of the existing algorithms. However, traditional methods have some shortcomings, such as low real-time capability, great resource consumption, high false positive rate (FPR) and high false negative rate (FNR). Projection pursuit regression (PPR) is a widely used multivariate analysis method and can be exploited to mining structures in multivariate data set. Based on PPR, we propose a novel network anomaly detection approach, which combines regression learning of genetic algorithm (GA) and Projection Pursuit to eventually evaluate the anomaly results comprehensively. We verify the proposed approach on the 1999 DARPA data set, and network anomaly can be detected with higher detection rate (DR) and lower false positive rate, compared to the Phad method. Furthermore, our approach achieves good detection rate even for some specific kind of anomaly which is difficult to detect previously.
Current Internet has confronted quite a few problems in terms of network security, scalability, performance, etc., mainly due to the rapid increase of the number of end-users and various new service demands. Therefore, revolutionary Future Internet researches come up with resolving the fundamental weakness of Internet. The importance of Future Internet testbed is also growing faster for researchers to experiment and verify newly proposed Future Internet technologies on the experimental networks. Furthermore, the federation and management of distributed Future Internet testbeds is required to support reliable end-to-end researches over inter-domain networks, countries, and even continents. In this paper, we introduce the design and experiment of federated network operations and management system over global Future Internet testbeds based on DvNOC (Distributed Virtual Network Operations Center) and GMOC (GENI Meta Operations Center). We also take several considerations into account for more reliable and scalable federated network operations on Future Internet.
In this work, we unveil new privacy threats against Voice-over-IP (VoIP) communications. Although prior work has shown that the interaction of variable bit-rate codecs and length-preserving stream ciphers leaks information, we show that the threat is more serious than previously thought. In particular, we derive approximate transcripts of encrypted VoIP conversations by segmenting an observed packet stream into subsequences representing individual phonemes and classifying those subsequences by the phonemes they encode. Drawing on insights from the computational linguistics and speech recognition communities, we apply novel techniques for unmasking parts of the conversation. We believe our ability to do so underscores the importance of designing secure (yet efficient) ways to protect the confidentiality of VoIP conversations.
We are currently moving from the Internet society to a mobile society where more and more access to information is done by previously dumb phones. For example, the number of mobile phones using a full blown OS has risen to nearly 200%from Q3/2009 to Q3/2010. As a result, mobile security is no longer immanent, but imperative. This survey paper provides a concise overview of mobile network security, attack vectors using the back end system and the web browser, but also the hardware layer and the user as attack enabler. We show differences and similarities between "normal" security and mobile security, and draw conclusions for further research opportunities in this area.
Network and web security measures are now having utmost attention and so the use of firewall, which is vigilant to scrutinize incoming-outgoing packets between private network and Internet. The decision is usually relied upon the source/destination address or port number. Even high end technologies and higher layer protocols like DNS, SMTP, POP3 etc. are also vulnerable and susceptible to be exploited. This urges to redefine the role and scope of firewalls. This paper defines the functions of existing firewalls and various filtration techniques along with their limitations and shortcomings. Further the new schema of MLF (Multi-Layer Filtering) for enhanced network security and to obstruct obnoxious sites is proposed. The security from intruders and attackers is not only based on their IP addresses or their content but through an adoptable method for which a new architecture is proposed.
On the computer and network technology as its symbol of information trend changed the world, including accounting, various industries caused profound effect, this trend not only change the habit of people thinking, has also changed the way of people work. The traditional book accounting system has been increasingly replaced by computer, database and the network accounting system, make efficiency and lower cost. But at the same time, a new network accounting system also brought a safety problem, particularly, as the continued computer network security technology development; this problem is becoming more and more outstanding. The issues of accounting information system is a system problems, only by the security technology and safety product cannot guarantee the system safety, only from the view of the system, and set up the comprehensive security system, so can better cope with the serious security challenges.
The physical layer of an optical network is vulnerable to a variety of attacks, including jamming, physical infrastructure attacks, eavesdropping, and interception. As the demand for network capacity grows dramatically, the issue of securing the physical layer of optical network cannot be overlooked. In this survey paper, we discuss the security threats in an optical network as well as present several existing optical techniques to improve the security. In the first part of this paper, we discuss various types of security threats that could appear in the optical layer of an optical network, including jamming, physical infrastructure attacks, eavesdropping, and interception. Intensive research has focused on improving optical network security, in the above specific areas. Real-time processing of the optical signal is essential in order to integrate security functionality at the physical layer while not undermining the true value of optical communications, which is its speed. Optical layer security benefits from the unique properties of optical processinginstantaneous response, broadband operation, electromagnetic immunity, compactness, and low latency. In the second part of this paper, various defenses against the security threats outlined in this paper are discussed, including optical encryption, optical code-division multiple access (CDMA) confidentiality, self-healing survivable optical rings, anti-jamming, and optical steganography.
Application identification is paid much attention by network operators to manage application based traffic control in the Internet. However, encryption is one of the factors to make application identification difficult, because it is so hard to infer the original (unencrypted) packets from encrypted packets. Therefore the accuracy of application identification is getting worse as the increase of encrypted traffic. In this paper, the changes in traffic features due to encryption are analyzed, and two methods are developed that can be used with an existing method for identifying applications from encrypted traffic. Experimental results show that these methods improve identification accuracy up to 28.5%for encrypted traffic compared to existing methods. Moreover, identification using the best combination of flow features enables high accuracy with less computation due to the elimination of features that do not flow a Gaussian distribution and thus degrade accuracy.
Recently, broadband optical access networks were being deployed worldwide to deliver large bandwidth to end users. Currently, the operators have been mainly deploying time-division multiplexing (TDM) passive optical networks (PON) in their networks. As bandwidth demand continues to increase, the industry further selected several next generation optical access (NGA) options to upgrade from current TDM-PON architectures. Besides increasing bandwidth requirement, current TDM-PONs suffers from limited reach and split-ratio. To further reduce the capital and operational expenditure requirements of current TDM-PON, NGA architectures must consolidate the number of central office sites. Long-reach optical access (LROA) architectures are promising solutions that employ optical amplifiers to increase optical budget to support longer reach and higher split-ratio in the distribution network. As more end users are supported over shared physical medium in LROA, access network security is of greater importance. The first part of this article reviews important LROA architectures and their enabling technologies. Moreover, few works in current literature address security weaknesses in LROA. The second part of this article reviews these security weaknesses as well as monitoring and control techniques employed in current TDM-PONs. In addition, a novel class of quasi-passive and reconfigurable devices is presented that addresses aforementioned security weaknesses and is suitable for LROA.
In current network, as we all know, packets delivered by routers only rely on destination-address-directed forwarding, but their source addresses are not checked. Consequently, this incurs many serious network security breach events which are hard to trackback. Under this situation, a switch (we call it SAVI switch) followed SAVI (Source Address Validation Improvement) framework proposed by IETF was invented which dedicates to resolving this problem in user local subnet. SAVI switch is a direct and very effective anti-spoofing device, but because it just steps into a phase of industrialization and for economical and incremental deployment reasons, these switches are not fully covered in domain. This results in two issues at the same time: 1)how to filter out and abandon those packets whose source IP addresses belong to SAVI switches coverage, but actually not, otherwise, this will severely compromise the SAVI switch access users' motivation and SAVI's promotion. 2) how to traceback those packets' source router-the first hop routers of spoofed packets. In this paper, we present SAVT, a practical and smart scheme for source address validation and traceback in campus network for all outbound packets, it just need less 25%routers as filter router can resolve those two questions in most condition. Experiments illustrate our proposal keeps the promise of practicality, stability and efficiency.
Preserving an open Internet is a challenge given potential harms that might come to Internet users. This article suggests some directions and actions that might lead to preservation of Internet users' freedoms to speak and hear while also offering them protection from harm.
Preserving an open Internet is a challenge given potential harms that might come to Internet users. This article suggests some directions and actions that might lead to preservation of Internet users' freedoms to speak and hear while also offering them protection from harm.
When we look at the organizations involved in maintaining utility system security-vendors, integrators, end users-it's fair to say that security is "everybody's business." To the extent these groups cooperate with one another throughout the system lifecycle, security will be enhanced. At the same time, perhaps the most important aspect of security for the various players to keep in mind is that it is a journey and not a destination. There will always be new threats. Likewise, there will be new methods and technologies for meeting those threats. Vigilance, cooperation and technical expertise, when applied in unison, offer the best defence.
Intrusion Detection Systems are the major technology used for protecting information systems. However, they do not directly detect intrusion, but they only monitor the attack symptoms. Therefore, no assumption can be made on the outcome of the attack, no assurance can be assumed once the system is compromised. The intrusion tolerance techniques focus on providing minimal level of services, even when the system has been partially compromised. This paper presents an intrusion tolerant approach for Denial of Service attacks to Web Services. It focuses on the detection of attack symptoms as well as the diagnosis of intrusion effects in order to perform a proper reaction only if the attack succeeds. In particular, this work focuses on a specific Denial of Service attack, called Deeply-Nested XML. Preliminary experimental results show that the proposed approach results in a better performance of the Intrusion Detection Systems, in terms of increasing diagnosis capacity as well as reducing the service unavailability during an intrusion.
The Precision Time Protocol (PTP) specified by IEEE 1588 contains a security extension to properly protect the exchanged information. The purpose of this extension is to detect manipulation and illegitimate replay of PTP messages and consequently prevent malicious attacks. To gain confidence in certain implementations this article presents a basic test suite to validate and verify the IEEE 1588 security extension. Based on an analysis of the security functionality a test setup and basic test cases are presented that allow to test Targets Of Evaluation (TOE) - ordinary as well as transparent clocks - and being able to give evidence about the correct security functionality. Outcome is a set of 13 test cases that cover all aspects of PTP security, namely message protection, mutual authentication and management of security associations.
In the analysis of the characteristics of network traffic we proposed a method for web site network security. We separate efficiently users by cookie, analyse user's activity by Hidden Semi-Markov Model, caculate the normal degree and allocate the bandwidth for the normal users. So we can defense DDoS attacks and control abnormal network traffic flow.
With commonness of utilizing wireless sensor networks in sensitive applications like military and civil domains, the necessity for network security and reliability has become a critical concern. In this paper, we proposed IFRP (an Intrusion/Fault tolerant Routing Protocol). IFRP uses both single and multipath to increase reliability and resiliency while considering energy efficiency. Routing has been switch from single path to multipath in the face of malicious behavior by local warden technique. In addition, IFRP detects and isolates intruder or faulty nodes by utilizing of local warden technique and centralized decision. Our simulations show that IFRP can increase reliability and effectiveness against nodes, which dropped packets, with acceptable energy consumption.
Currently, the main threats of network security are coming from hacker intrusion, deny of service (DoS), computer virus, worm, spam, malicious code and sniffer because there are many weaknesses in the original design of IPv4. One is that attackers could send IP spoofing packets which is he wants to attack. That is the attackers modify the IP from the true one to another on the IP field. However, it is hard to trace the source of attacks from victims. In this paper, we will propose a method that reduces the storage of the router and trace the attack source by a single packet. According to the experimental results, we can prove that it works well.
In recent years, attackers have started to use web pages to deliver their malicious code to users. Web-based malware overcomes signature-based detection by modification of the code or using zero-day exploits. We propose a malicious activity detection method using Hidden Markov Models (HMM) alongside a client honeypot system. Our algorithm is able to detect the potential malicious behaviour of a web server based on current and past interactions between the web client and the server and can also predict possible future behaviours. The prediction algorithm learns from previously scanned behaviours recorded by a client honeypot system. We group such behaviours in order to enable common characteristics to be investigated across these groups.
This paper describes a novel approach for the network security based on the combination of biological intrusion detection and self-healing concepts. The presented approach integrates an artificial immune intrusion detection system for network security inspired by idiotypic networks. The IDS detects and analyzes the malicious activities in the network and their effects to trigger the self-healing system. It means the detection of the damage caused by malicious activities is used to start the self - healing mechanism. It is an automated system and enhances the fault repair and system recovery.
This paper presents a wireless risk assessment method to help an administrator manage wireless network security. The assessment method consists of a risk model and an assessment measure. The risk model is in charge of modeling the wireless network risk. Security requirements, wireless attacks, and system configurations are considered in the model. The assessment measure is an algorithm which determines the risk value of the wireless network according to the risk model. Our risk model is developed upon an extended analytic hierarchy process, which contains the 4 layers: the risk layer, the requirement layer, the attack layer, and the configuration layer. The separate layers of the risk model are helpful in dealing with the dynamics of a wireless network because only the related layers are introduced to the assessment measure when changes of the network are detected. Based on the risk model per device, our assessment measure evaluates the wireless network risk in consideration of the relations between devices, attacks, and configurations. Hence, our risk assessment method, composed of the risk model and the assessment measure, can determine the wireless network risk efficiently while considering the dependencies in the wireless network. Two examples are introduced in this paper to examine the feasibility of our method. In the first example, we demonstrate that the risk values derived by our method meet the ground truth by performing practical experiments. The second example shows that our method can evaluate the risk of a changing wireless network with efficiency, and can distinguish disparities in different wireless networks.
We can mitigate the threat of mass malware by understanding the techniques, tactics, and procedures unique to this threat. An analysis of empirical attacker data indicates that basic, generic defenses, such as minor reductions of the attack surface and the use of available platform memory protection, are effective against mass malware.
With increase in densities of semiconductor chips and decrease in cost, it is feasible to store huge amount of data in main memory. Main Memory Database Management System (MMDBMS) uses main memory as primary storage for huge amount of data and provides very high speed access to it. This makes MMDBMS suitable option for implementation of real-time network security systems. This paper presents light-weight mechanism for detection and prevention of SYN flood attack using MMDBMS. Experimental results are provided to support proposed mechanism.
There are a number of studies on using directional antennas in wireless networks. Many of them concentrate on analyzing the theoretical capacity improvement by using directional antennas. Other studies focus on designing proper Medium Access Control (MAC) protocols to improve the practical network throughput. There are few works on the security improvement using directional antennas. In this paper, we explore the benefits of directional antennas in security improvements on both single-hop and multi-hop wireless networks. In particular, we found that using directional antennas in wireless networks can significantly reduce the eavesdropping probabilities of both single-hop transmissions as well as multi-hop transmissions and consequently improve the network security.
Mixing information is a key technique in network security. In mental poker, Internet lottery, and Mix-Net for electronic voting, we use shuffling to maintain anonymity, privacy, and fairness. In this paper, we propose a method to confirm whether a mix is done well by finding a fixed point in the mix system while keeping how the shuffle was done a secret.
The main idea in this paper is to design and analyze a symmetric key generation algorithm whereby we can strengthen wireless network security. Our main goal is generating a sequence of highly secure secret keys based on an ARQ based transmission mechanism that relies on the statistical independence of channel errors between the attacker and legitimate users. This leads to some information loss for the adversary which allows us to constantly extract keys by using universal Hashing techniques from communication process, about which we can make sure that adversarys knowledge remains negligible. More specifically, the key generation algorithm is analyzed and designed in a way that targeted security as well as the required throughput and synchronization goals for the transmission are achieved. Simulation results show that the designed algorithm achieves the desired requirements for both system security and throughput.
Traffic graph analysis has become an increasingly useful tool in network security. By summarizing the aggregate activity of a particular service or network using graph based representations, it is possible to model normal activity using a variety of different attributes which are not easily identified or exploited by attackers. In this paper, we discuss several examples of analysis using traffic graphs and demonstrate its potential for scan detection, identifying hitlist attackers, and identifying spammers.
In order to restrain botnet security issues arising from operation of the network, in the dissect of botnets based on the principle of a real botnet tracking, detection methods, effectively restrain botnet network security threats. Through the understanding of the concept of botnets and botnet generated on the principle of the internal working mechanism of development, type and risk of such a comprehensive study, gives the tracking, detection and prevention of specific methods of different botnets. Experiments show that this method is effective to inhibit the breeding of botnets in the slow extension of the network, defense and control of active botnets, strengthen the operation of the network user data safety and security has a very important significance.
A new method for intrusion recognition and tracing under attack-defense confront environment is proposed in this paper. In order to do that antagonizing status and state transforming is deeply studied between attacker and defender, and all kinds of security information and knowledge are analyzed and formally described. Then based on obtained information, a safety model for attack-defense confronting is presented, from the model a safety state transition graph can be produced. The model is given a formalized description based on Expanded Finite-State Automata (EFSA), which visually describes both intruding process and defending process. The model is used to thoroughly analyze attack and defense activities and predict the subsequent safety state transitions, and also intuitively illustrates all possible routes and states during attacker's reaching specific target. So the model can be used to trace intruding process and deduce attack intention and target, further to predict follow-up attack, which can provide a useful evidence and guidance for attack response and security decision. Finally this method is demonstrated and validated in an example network environment.
Scan test is the standard method, practiced by industry, that has consistently provided high fault coverage due to high controllability and high observability. The scan chain allows to control and observe the internal signals of a chip. However, this property also facilitates hackers to use scan architecture as a means to breach chip security. This paper addresses this issue by proposing a new method called Secure and testable Scan design through Test Key Randomization(SSTKR). SSTKR is a key based method to prevent hackers from stealing secret information. Linear Feedback Shift Register (LFSR) is used to generate authentication keys to be embedded in test vectors. Unique key is used for every test vector which prevents scan based side channel attacks effectively. Any attempt to steal secret information will lead to a randomized response. SSTKR has very low area and test time overhead without performance penalty. Our approach also facilitates in-field test of the chip.
Honey net technologies play an increasingly important role in network security. However, conventional honey net has some disadvantages like low hardware utilization, complexity configuration and difficultly management increasingly highlighted. Towards solving these problems that honey net currently facing, in this paper, we propose the necessity of using virtual technology to construct honey net. Through the studying on core functions of honey net, the paper analyzed and provided the workflows of core functions in virtual system. Based on that, we designed and implemented a virtual honey net proposal. The results show that the established virtual honey net works well and solves the shortcomings of conventional honey net in a certain extent.
Application-level protocol specifications are helpful for network security management, including intrusion detection and intrusion prevention which rely on monitoring technologies such as deep packet inspection. Moreover, detailed knowledge of protocol specifications is also an effective way of detecting malicious code. However, current methods for obtaining unknown and proprietary protocol message formats (i.e., no publicly available protocol specification), especially binary protocols, highly rely on manual operations, such as reverse engineering which is time-consuming and laborious. In this paper, we propose Biprominer, a tool that can automatically extract binary protocol message formats of an application from its real-world network trace. In addition, we present a transition probability model for a better description of the protocol. The chief feature of Biprominer is that it does not need to have any priori knowledge of protocol formats, because Biprominer is based on the statistical nature of the protocol format. We evaluate the efficacy of Biprominer over three binary protocols, with an average precision more than 99%and a recall better than 96.7%.
This essay proposes a new cross heterogeneous domain authentication model mainly based on PKI, and designs the details of authentication processes in different situations. The model achieves cross domain authentication between PKI domain and Kerberos domain effectively, and supports mutual authentications. Theoretical analysis shows that the proposed scheme has good compatibility, expansibility and reliability. Therefore, this model is suitable for using in large-scale network environment mainly based on PKI.
In this paper, we present a new authentication mechanism for wireless network security based on certificate less-based signature technology and fingerprint recongnition technology. We introduce the principle of certificate less signature, illustrate the process of certificate less signature authentication, analyze the advantages and disadvantages of the certificate less-based signature technology. We show that the introduction of fingerprint identification into certificate less signature can enhance the security and efficiency of wireless network security authentication.
With rapid development of computer network technology, computer networks are applied extensively in commercial banks and are increasingly confronted with various attacks. A strategy for solving network security of commercial banks is proposed in this paper, based on analysis of insecurity threats, particularity of network security, and trends of network security technology. A network security system is constructed on the strategy.
This paper begins with network shopping of university students. Then in a comprehensive analysis by knowing the confidence level of site quality, supplier quality, the confidence level of network security, gender ratio of network shopping, the actual experience of network shopping and the current stage impact of factors that influence network shopping of university students. Finally the paper gives the current phase of individual factors affecting network shopping of university students.
Researchers in the network security field lack quantitative methods to analysis anti-denial of service (DoS)attacks ability. In this paper, we describe an anti-denial of service testing method based on mobile agent taking on the attacker. The goal of the testing was to evaluate the system and its defense mechanism's anti-denial of service attack ability. The process of testing anti-denial of service attacks can be broadly divided into three phases: DoS attack scenario, information gathering and performance analysis. We describe how the testing process was designed and customized to the defenses, how the performance was measuring, how the tests were run, and what we learned from them about defenses and about testing security system.
A configurable IPSec processor for a high performance in-line network security processor that integrates two embedded 32-bit CPU cores, and an IPSec protocol processor on a SoC is presented. The IPSec processor can implement the transport/tunnel mode AH and ESP protocol of the IPSec, and support AES-128/192/256, HMAC-SHA-1 algorithm. The number of AH, ESP, AES, HMAC-SHA-1 IP-cores in the design can be configured for different use such as 10 Gigabit Ethernet and Gigabit Ethernet, even for the next generation 40/100G Network. Low power is also considered in the design. In the IPSec processor, crossbar switch architecture for multi-core data transfer is adopted. With four parallel AH, ESP, AES, HMAC-SHA-1 IP-cores separately connected to an 8x8 crossbar switch in the IPSec processor, a throughput of 1.5Gbps at 200MHz is achieved and hardware verification is implemented by FPGA. By simulation, the IPSec protocol operation can achieve 10Gbps wire speed with 32 IPSec protocol IP-cores and cryptographic IP-cores configured in the IPSec processor.
On the basis of analysis of research status on IMS network security, this paper categorizes and analyzes possible abnormal events in IMS network. Combined with mature detection theories, we present corresponding strategy of detection from application layer to provide theoretical and decisive support for warning and further network process measurements to reduce the loss brought by abnormal events and improve the security of the whole IMS network.
This paper proposes a secure data aggregation scheme for clustered wireless sensor networks. In our design, the encrypted sensor readings were transmitted to the cluster head with MAC and the cluster head process the encrypted data aggregation without decryption. For the readings have the same value and come from different nodes, the cluster head remains the node's identifiers in data aggregation processing to provide the information of global data distribution. Compared with related works, except providing data privacy protection, our scheme had better performances in resilient against active attack, node compromise attack and DoS attack.
With the development of application based on Internet, network security highlights its place increasing. Firewall and IDS are the equipment often used in Internet, but both of them can not run automatically. If we can reconfigure the firewall using the result of IDS, the security must be enhanced to a high level. In this paper, we designed an intrusion prevent system (IPS) based on Snort and Net filter by researching kernel codes of Snort and Net filter. The policy control module of the system was written in Multi-thread technologies. Meanwhile, the Algorithm of IDS and rule set of firewall was optimized to improve system efficiency. The system can block the attack source by dynamically modify firewall rules according to IDS.
Intrusion Detection System has become the important part of the computer and network security because it can effectively compensate for the lack of network security measures. However, the intrusion detection technology relies heavily on pattern matching algorithms, as the choice of pattern matching algorithms directly affect the detection rate, so it is more important. First the paper is detailed analysis the characteristics of the four kinds of single pattern matching which based on prefix search, and then it tests by the number of different pattern strings. The experimental results show that the KMP algorithm can improve effectively the speed of pattern matching. So the application of KMP algorithm can improve the efficiency of intrusion detection.
As the flooding of malicious codes such as worms, how to analyze large number of malicious samples quickly and effectively becomes a great issue for researchers in network security. This paper proposed an analysis algorithm for worm network behavior based on event sequence, which uses the data flow recombination and compression methods to process the pure malicious data. With this algorithm, one can quickly extract the network behavior profile and the signature of the worm. The application of this algorithm will greatly improve the efficiency of analyzing the worm network behavior, which will be significant for the deployment of firewalls and network invasion detection systems.
The scope of information security is becoming wider everyday due to new and emerging dimensions in the evolution of computer applications with the intention to lessen manual operations. These have necessitated the needs to manufacture portable and smart devices that can be interconnected to the Internet for home and industrial usages and for environmental surveillance. Consequently, global and national regulatory laws and statutes aimed at addressing illegal activities committed across computer and sensor networks are also increasing. Nevertheless, there are incessant cases of abuse through information leakage, masquerading, telephone hacking and password cracking that are perpetrated by some users of these technologies. Thus, this paper critically describes and analyses emerging security challenges in the investigation of smart devices that interface with computer systems. The review will be functionally useful to researchers, vendors, security professionals and IT end users in general.
User identification is an important access control mechanism for client-server networking architectures. The concept of single sign-on can allow legal users to use the unitary token to access different service providers in distributed computer networks. Recently, some user identification schemes have been proposed for distributed computer networks. Unfortunately, most existing schemes cannot preserve user anonymity when possible attacks occur. Also, the additional time-synchronized mechanisms they use may cause extensive overhead costs. To overcome these drawbacks, we propose a secure single sign-on mechanism that is efficient, secure, and suitable for mobile devices in distributed computer networks.
As the growing popularity of wireless terminals remote access, the security of wireless remote access is getting increasing attention. It is an effective solution to establish a virtual private channel on the public Internet with the use of Secure Sockets Layer (SSL) to protect data security. The background is that we will establish and maintain a secure and trusted private SSL VPN tunnel based on the SSL protocol in the public network. First of all, I will make a brief introduction about SSL / TLS protocol, and then elaborate a kind of terminal architecture design and implementation of secure access system based on SSL / TLS.
Now that Distributed Denial-of-Service (DDoS) attack is one of the most serious network security threats, it becomes particularly important to detect it. In this paper, the author presents a method of mathematical analysis SYN Flooding attack detection. We can use this method to detect SYN Flooding attacks. With it, we can detect these attacks more timely.
This paper describes that after we used of information technology tools for information security survey data, special investigation and examination evaluation, we proposed based on Rough Set Theory to approach information security risk warning. This method first gets the information of Information Security Event occurred, and dynamically analyzes the institution for information safety construction case which related to security events. We makes analysis based on Rough Set Theory, and List the organizations whose information security event will happen more probably. These organizations quickly take some strengthen risk prevention measures, in order to reduce the probability of occurrence of similar information security events.
The Universal Postal Union is motivated to develop a new platform: .post, to offer digital services secured from SPAM and cybercriminal activities such as phishing and identity theft. Assuming that strong authentication, network security and QoS must be offered on the international .post platform, the use of national authentication tokens and IMS (IP Multimedia Subsystem) networks is proposed and discussed. A prototype architecture for the .post platform is proposed and the development of an IMS/SIP client to enable the exchange of secured and authenticated mail is described as a sample service to demonstrate the feasibility of the concept.
With the complexity of the network environment, there are many problems with using network devices, the policies of network devices are vitally important to control the transmission of the network information. It is necessary for network environment optimization and network security improvement to detect the policy anomaly of the network devices. We put forward a policy anomaly detection algorithm based on the rule of MapReduce in order to meet the requirement of real-time and large quantity of data. In order to get the high efficiency of the policy, we construct a policy tree, build a pleasant operation environment provided by Hadoop and use distribute computing to calculate the tree.
In this paper we propose a solution to strengthen the security of Domain Name System (DNS) servers associated with one or more Top Level Domains (TLD). In this way we intend to be able to reduce the security risk when using major internet services, based on DNS. The proposed solution has been developed and tested at FCCN, the TLD manager for the. PT domain. Through the implementation of network sensors that monitor the network in real-time, we are capable to dynamically prevent, detect or limit the scope of attempted intrusions or other types of occurrences to the DNS service. The platform relies heavily on cross-correlation allowing data from a particular sensor to be shared with the others. Administration tasks such as setting up alarms or performing statistical analysis are made through a web-based interface.
Modern day networks consist of a mix and match of technologies with varying capabilities. Securing such networks is a tedious task and demands a lot of contribution from security professionals; however, failing to conduct an in depth and standardized analysis may result in an imperfect network security design. ITU-T provides recommendation for end-to-end network security in its standard X.805 which precisely lays down the foundation for the assessment of network security. Keeping X.805 requirements and the current network in view, the next thing that security professionals face is the correlation of both and its practical implementation. This paper contributes to answer the same and gives a comprehensive methodology for practical implementation of X.805 architecture. Also this paper takes into account the current trend of Network Access Control solution and its quantitative contribution towards achieving the desired goals set by X.805 standard.
Intrusion detection system (IDS) is one of the basic components of network security. IDS can use signatures or abnormal behaviors to detect attacks. When IDS uses signatures, in order to detect more attacks, we have to implement more attack signatures on IDS. However, in resource-constrained wireless sensor networks, it is not efficient to implement all of attack signatures. And it is hard to maintain attack signature database because it should be updated frequently when new kind of attacks appear. In this paper, we propose a distributed IDS mechanism which reduces the usage of memory and overhead. Analysis of our proposal shows that our proposal requires less memory.
Internal information systems play an important role in keeping the enterprises running well. To detect system anomalies, previous research achieved good results with system symptoms; however, the presented results are primarily performed on a relatively small scale and within a short time period. To understand the system's long-term profiles, we collected four common symptom data including CPU usage, memory loading, disk I/O, and network I/O from more than 100 online internal systems that includes 300 servers for 9 months. We randomly selected 50 servers from these servers and analyze their data in order to understand each symptom's long-term features. Based on our findings in network I/O, we propose a new approach combining a density-based clustering and wavelet methods to detect system anomalies. We also select 44 other servers to evaluate the false positive rate and simulate three types of system anomalies to evaluate the detection rate. The experiment results show that our approach has a great improvement on both the false positive rate and the detection rate compared to another wavelet-based network anomaly detection approach.
Automatically learned filter table is used in many network security mechanisms to validate packets. Building filter item for each IP address in access networks can prevent IP spoofing at fine granularity but may consume large amount of filter table which is limited due to the expensive storage which is usually TCAM for high speed access. It is an urgent problem to use filter table effectively and keep network available. We analyze the change of filter table size and find that setting proper lifetime for filter item can significantly improve the utilization of filter table and avoid denial of service. In this paper, we take SAVI (source address validation improvement) switch as an example, and propose a dynamic adjustment method. It has two phases. Firstly it calculates out an optimal lifetime value for each switch based on one week user online logs, and then adjusts it dynamically to capture the bursts of filter table size. We deploy our prototype in a real campus network which has about 1000 SAVI switches providing network accessing service for nearly 20000 users. Based on the analysis of one month user online logs, we verify that our algorithm can reduce 92%of the duplicate confirming processes and guarantee the availability of network.
Green computing is one of the emerging computing technologies to provide Green environment. It is mainly used to protect environment, optimize energy consumption and keeps green environment. Green computing also refers to environmentally energy sustainable computing. Modern world needs Secured and reliable data in unsecured communications. Energy efficient usage, Optimum resource allocation and dynamic routing help for green computing. The intelligent machines collaborative decisions and information sharing without direct human intervention is one Machine point to another machine point communication. In this paper we discussed secured and reliable point to point communications towards green computing.
The lookup mechanism used to locate services in Peer-to-Peer systems can be attacked with little effort due to its decentralized and self-organizing nature. Security mechanisms aiming at rendering the lookup mechanism more robust mostly require a high amount of network resources. These mechanisms cannot be applied without adaptations when network resources are limited. In this paper, we introduce a novel approach to increase lookup robustness in mobile Peer-to-Peer networks. Here, network resources are limited by the mobile ad hoc network that is used as communication substrate. Our approach harnesses cross-layer information provided from the mobile ad hoc underlay to the Peer-to-Peer overlay. We derive analytical models to compare our approach to existing security mechanisms and validate our results by means of simulation. Our core findings how that our approach consumes less resources than existing mechanisms while the robustness remains at a comparable level.
Many network activities can benefit from accurate traffic classification and categorization, such as QOS control, network security monitoring, and traffic accounting. In this paper, a new approach based on feed-forward neural network is proposed for accurate traffic classification, which eliminates the disadvantages of port-based or payload-based classification methods. Extensive experimentation and comparison have been carried out to explore this new approach; it has been found out that, combined with a fast correlation-based feature selection filter, better performance and more accurate classification results can be obtained using neural network method compared to other techniques. For its good performance and elimination of accessing the contents of the packets, the proposed technique is expected to have a promising application prospect in internet traffic classification.
Due the significant need for real-time anonymization we propose Real-time Netshuffle [1]; a complete graph distortion technique designed to mitigate risk to inference attacks in traffic anonymization. Real-time Netshuffle provides an additional layer of security, in concert with other on-line traffic anonymization techniques, while imposing only minimal damage to the empirical value of the data.
Network reachability is one of the key factors for capturing end-to-end network behavior and detecting the violation of security policies. While quantifying network reachability within one administrative domain is already difficult, quantifying network reachability across multiple administrative domains is more difficult because the privacy of security policies becomes a serious concern and needs to be protected through this process. In this paper, we propose the first cross-domain privacy-preserving protocol for quantifying network reachability. Our protocol constructs equivalent representations of the Access Control List (ACL) rules and determines network reachability while preserving the privacy of the individual ACLs. This protocol can accurately determine the network reachability along a network path through different administrative domains. We have implemented and evaluated our protocol on both real and synthetic ACLs. The experimental results show that the online processing time of an ACL with thousands of rules is less than 25 seconds, the comparison time of two ACLs is less than 6 seconds, and the communication cost between two ACLs with thousands of rules is less than 2100 KB.
Network forensics is widely used in tracking down criminals and detecting network anomalies, and data capture is the basis of network forensics. Compared to traditional networks, data capture faces significant challenges in cognitive radio networks. In traditional wireless networks, one monitor is usually assigned to one channel to capture traffic, which incurs very high cost in a cognitive radio network because the latter typically has a large number of channels. Furthermore, due to the uncertainty of the primary user's activity, cognitive radio devices change their operating channels randomly, which makes data capturing more difficult. In this paper, we propose a systematic method to capture data in cognitive radio networks with a small number of monitors. We utilize incremental support vector regression to predict packet arrival time and intelligently switch monitors between channels. In addition, a protocol is proposed to schedule multiple monitors to perform channel scan and packet capturing in an efficient manner. The real-world experiments and simulations show that our method is able to achieve the packet capture rate above 70%using a small number of monitors, which outperforms the random scheme by 200%-300%.
Filtering IP packets with spoofed source addresses not only improves network security, but also helps with network diagnosis and management. Compared with filtering spoofing packets at the edge of network which involves high deployment and maintenance cost, filtering at autonomous system (AS) borders is more cost-effective. Inter-AS anti-spoofing, as its name suggests, is implemented on AS border routers to filter spoofing packets before their entering or leaving an AS. Existing inter-AS anti-spoofing approaches focus on filtering efficiency, but lacks of deployability. In this paper we first introduce three properties of a deployable inter-AS anti-spoofing approach, incremental deployability, high deployment incentives and low deployment cost. Then we propose DIA, the first inter-AS anti-spoofing approach meeting the three properties. We present the design of DIA and evaluate its deployability with real Internet data. The evaluation results show that DIA provides high deployment incentives for Internet Service Providers by significantly mitigating spoofing based denial of service attacks. Our implementation proves that DIA can be easily implemented in commodity routers and minimize the deployment cost.
Has the behavior of spammers changed over the last few years? To answer this question, we conduct a study from three recent data sources. Specifically, we focus on the following broad questions: (a) how are email addresses harvested, (b) where is spam coming from, and (c) how does spam evolve over time. First, we discuss whether spammers still use email harvesting: 34%of the honeypot accounts we publicised received spam after 72 days on average. Interestingly, we find that simple email address obfuscation is quite effective against harvesting. Second, we identify significant skew in the spatial distribution of the origin of spam in both the IP-level and AS-level of granularity. We find that 20%of the active IPs are responsible for 80%of the total volume of spam and that 10%of the spamming ASes are responsible for the 90%of the volume. Finally, we study the temporal characteristics of the spamming IPs and find that spam activity has spread to new /8 subnetworks since 2006. Considering these spatio-temporal trends, the future of anti-spam is mixed: the current skewed spatial distribution of spam sources could be helpful in filtering spam, but the fact that spam sources are spreading in the IP space is a worrisome sign.
As the mobile Internet increases with the popularization of smartphones, the conventional services are gradually being expanded to the wired and wireless integrated environment. These changes in the network environment require a new paradigm for the pursuit of safe and stable communication. In this paper, we propose a Seamless and Secure service Framework (SSF) that can maintain a sustainable secure relation between the user terminal and the sensitive service system by using both personal and network information. Typical examples of the SSF are the secure payment of bills and authentication. The SSF can block at the root the illegal use of the service (by a third party) even if personal information is leaked, as long as the user terminal is not lost. In addition, the SSF can provide a seamless service environment even if the access network is changed by movement of a terminal with the multiple network interfaces within the heterogeneous network environment.
Guaranteeing the safety of computers connected to the Internet is a challenging task. Despite the efforts of contemporary security software, the threat remains due to the incapability of existing software to predict and prevent the variance of attacks. According to recent studies, new computer malware appears at an almost constant rate, making their confrontation a rather difficult task and therefore creating a need for a different approach that will increase the effectiveness of security software. This paper introduces forecasting models and techniques from the financial world. Some possible approaches are investigated, such as the correlation between computer malware incidents and extracted data from electronic social networks, that could possibly lead to effective forecasting, in an attempt to come up with new ways for preventing imminent computer epidemics.
The Laboratory of Cryptography and System Security (CrySyS) is dedicated to conduct research in the field of computer security and user privacy. This paper shows a research roadmap of the CrySyS Lab from its inception in 2003 until today. We will present the major achievements in the past with a particular emphasis on the research and teaching curriculum in security and privacy. We will discuss network- and wireless system security, the core competences of CrySyS. Building on the research foundation and competences in these areas, we will lead the laboratory into new territories of security and privacy in wireless embedded computing systems and the future Internet.
The modern cyber crime activities largely rely on malware-based infrastructure, i.e. botnets and backdoors in popular services for collecting private financial data, distributed denial of service and etc. A significant effort to develop better methods and tools for accurate malware detection and prevention is mounted both by the industry and academic community. With this paper we present current research roadmap for two adjacent fields: line-speed malware detection in modern network channels and privilege escalation prevention at host level by means of run-time monitoring of the networking applications normal behavior.
In the context of the SysSec Network of Excellence call for consolidating the European (and international) systems security research community, this position paper aims at summarizing the current research activities in the Communication Systems Group (CSG) of ETH Zurich relating to network security. Our research is aligned along three projects: 1) identifying, validating, and characterizing computer infections from intrusion alerts, 2) building privacy-preserving collaborative network security mechanisms based on efficient secure multi-party computation (MPC) primitives, and 3) dissecting Internet background radiation towards live networks with one-way flow classification. In addition, we highlight important directions for future research.
The paper presents the current and future research in the area of network and computer security at NASK, focusing on activities of CERT Polska and NISM teams. A large part of the activities are driven by the operational needs of CERT Polska, but independent research themes are also pursued. Most of the work concentrates either on threat detection methods (server and client honey pots) and threat intelligence or on distribution of security-related information. Other research areas are briefly described as well.
The systems and network security, and the secure and reliable systems groups carry out research in computer and network dependability at the Vrije Universiteit Amsterdam. The former group, led by Prof. Herbert Bos, has a strong and historical background in attack detection, dynamic analysis, and reverse engineering of software. The secure and reliable systems group, led by Prof. Andrews. Tanenbaum, is instead traditionally rooted on studying and guaranteeing dependability properties of systems. The two groups interact in a natural way and blend their knowledge and expertise to contribute on the security of networks and systems.
The Horst Go&#776;rtz Institute (HGI) located at Ruhr-University Bo chum, Germany, is one of Europe's largest university-based institution for interdisciplinary research in the field of IT security. Ruhr-University Bo chum offers both Bachelor and Master degree programs in IT security and more than 500 students are enrolled in these study courses. The research at RUB focusses on cryptography, embedded security, and network security. Recently, the research group "Embedded Malware" was established, which focusses on systems security. In this position paper, we provide a brief overview of this group and discuss previous research and future research topics.
Dynamic state estimation in power systems plays a central role in controlling the system operations. State estimation, however, is vulnerable to malicious cyber attacks that contaminate the controller's observation through injecting false data into the system. It is, therefore, of paramount significance not only to detect the attacks, but also to recover the system state from the contaminated observation when an attack is deemed to be present. This paper offers a distributed framework for an optimal such joint attack detection and system recovery. This framework has two main features: 1) it provides the network operator with the freedom of striking any desired balance between attack detection and state recovery accuracies, and 2) it is distributed in the sense that different controlling agents distributed across the network carry out the attack detection and system recovery tasks through iterative local processing and message passing.
Coordinated cyberattacks of power meter readings can be arranged to be undetectable by any bad data detection algorithm in the power system state estimation process. These unobservable attacks present a potentially serious threat to grid operations. Of particular interest are sparse attacks that involve the compromise of a modest number of meter readings. An efficient algorithm to find all unobservable attacks [under standard DC load flow approximations] involving the compromise of exactly two power injection meters and an arbitrary number of line power meters is presented. This requires O(n<sup>2</sup>m) flops for a power system with n buses and m line meters. If all lines are metered, there exist canonical forms that characterize all 3, 4, and 5-sparse unobservable attacks. These can be quickly detected in power systems using standard graph algorithms. Known-secure phasor measurement units [PMUs] can be used as countermeasures against an arbitrary collection of cyberattacks. Finding the minimum number of necessary PMUs is NP-hard. It is shown that p + 1 PMUs at carefully chosen buses are sufficient to neutralize a collection of p cyberattacks.
In smart grid, the strong coupling between cyber and physical operations makes power systems vulnerable to cyber attacks. In this paper, stealth false data attacks are investigated where the attackers without prior knowledge of the power grid topology, try to make inferences through phasor observations. We show that when the system dynamics are small and can be approximated linearly, linear independent component analysis (ICA) can be applied to estimate the Jacobian matrix multiplied by the eigenvectors of the covariance matrix of the state variables. The inferred structural information can then be used to launch unobservable attacks. As demonstrated by the simulation results using data generated by MATPOWER, the proposed scheme can indeed inject false data with low detectability.
At present, incidents of computer networks attack are significantly increasing therefore the effective Intrusion Detection Systems (IDS) are essential for information systems security. In order for the IDS to be effective, they have the capability of detecting new arrival attacks. Additionally, the correct detection rate should also be at high level whereas the low false positive detection rate is preferred. This paper proposes the new intrusion detection technique by using hybrid methods of unsupervised/supervised learning scheme. The proposed technique integrates the Principal Component Analysis (PCA) with the Support Vector Machine (SVM). The PCA is applied to reduce high dimensional data vectors and distance between vectors including its projection onto the subspace. SVM is then used to classify different groups of data, Normal and Anomalous. The results show that the proposed technique can improve the performance of anomaly intrusion detection, the intrusion detection rate and generate fewer false alarms.
Changes that are operated by autonomic networks and systems may generate vulnerabilities and increase the exposure to security attacks. We present in this paper a new approach for increasing vulnerability awareness in such self-managed environments. Our objective is to enable autonomic networks to take advantage of the knowledge provided by vulnerability descriptions in order to maintain safe configurations. In that context, we propose a modeling and an architecture for automatically translating these descriptions into policy rules that are interpretable by an autonomic configuration system. We also describe an implementation prototype and evaluate its performance through an extensive set of experiments.
For the VAST 2011 Network Security Mini-Challenge, we adopted geovisual analytic methods and applied them in the field of network security. We used the GeoViz Toolkit [1] to represent cyber security events, by fabricating a simple geography of several sets of blocks (one for the workstations, one for the servers, and one for the Internet) using ArcGIS 10 (by ESRI - Environmental System Research Institute). Security data was tabulated using Perl scripts to parse the logs in order to create representations of event frequency and where they occurred on the network. The tabulated security data was then added as attributes of the geography. Exploration of the data and subsequent analysis of the meaning and impact of the cyber security events was made possible using the GeoViz Toolkit.
To visualize security trends for the data set provided by the VAST 2011 Mini Challenge #2 a custom tool has been developed. Open source tools [1,2], web programming languages [4,7] and an open source database [3] has been used to work with the data and create a visualization for security log files containing network security trends. In this paper, the tools and methods used for the analysis are described. The methods include the log synchronization with different timezone and the development of heat maps and parallel coordinates charts. To develop the visualization, Processing and Canvas [4,7] was used.
The task of the VAST 2011 Grand Challenge was to investigate potential terrorist activities and their relation to the spread of an epidemic. Three different data sets were provided as part of three Mini Challenges (MCs). MC 1 was about analyzing geo-tagged microblogging (Twitter) messages to characterize the spread of an epidemic. MC 2 required analyzing threats to a computer network using a situational awareness approach. In MC 3 possible criminal and terrorist activities were to be analyzed based on a collection of news articles. To solve the Grand Challenge, insight from each of the individual MCs had to be integrated appropriately.
We provide an overview of optical techniques that have been developed specifically to provide communications security at the physical layer and discuss recent advances in the field. We also highlight open areas of research.
Modern network security research has demonstrated a clear necessity for open sharing of traffic datasets between organizations - a need that has so far been superseded by the challenges of removing sensitive content from the data beforehand. Network Data Anonymization is an emerging field dedicated to solving this problem, with a main focus on removal of identifiable artifacts that might pierce privacy, such as usernames and IP addresses. However, recent research has demonstrated that more subtle statistical artifacts may yield fingerprints that are just as differentiable as the former. This result highlights certain shortcomings in current anonymization frameworks; particularly, ignoring the behavioral idiosyncrasies of network protocols, applications, and users. Network traffic synthesis (or simulation) is a closely related complimentary approach which, while more difficult to execute accurately, has the potential for far greater flexibility. This paper leverages the statistical-idiosyncrasies of network behavior to augment anonymization and traffic-synthesis techniques through machine-learning models specifically designed to capture host-level behavior. We present the design of a system that can automatically learn models for network host behavior across time, then use these models to replicate the original behavior, to interpolate across gaps in the original traffic, and demonstrate how to generate new diverse behaviors. Further, we measure the similarity of the synthesized data to the original, providing us with a quantifiable estimate of data fidelity.
Despite more than a decade of significant government investment in network defense research and technology development, there have been relatively few successful transitions across the chasm between research and operational use. Prior work describes approaches to crossing the valley of death from the perspective of the government sponsor or independent tester. The researcher and developer's perspective offered in this paper adds to our understanding of the challenges faced and solutions applied to deployment of advanced technologies into operational environments. The paper describes lessons learned from recent transitions of two information assurance technologies - the VIAssist&#174; netflow visualization tool and the MeerCAT&#174; wireless vulnerability analysis tool - into operational use by the Department of Homeland Security (DHS) and the Department of Defense (DoD).
As protection against the current privacy weaknesses of StateLess Address AutoConfiguration (SLAAC) in the Internet Protocol version 6 (IPv6), network administrators may choose to deploy the new Dynamic Host Configuration Protocol for IPv6 (DHCPv6). Similar to the Dynamic Host Configuration Protocol (DHCP) for the Internet Protocol version 4 (IPv4), DHCPv6 uses a client-server model to manage addresses in networks, providing stateful address assignment. While DHCPv6 can be configured to assign randomly distributed addresses to clients, the DHCP Unique Identifier (DUID) was designed to identify uniquely identify clients to servers and remains static to clients as they move between different subnets and networks. Since the DUID is globally unique and exposed in the clear, attackers can geotemporally track clients by sniffing DHCPv6 messages on the local network or by using unauthenticated protocol-valid queries that request systems' DUIDs or leased addresses. DUIDs can also be formed with system-specific information, further compromising the privacy and security of the host. To combat the threat of the static DUID, a dynamic DUID was implemented and analyzed for its effect on privacy and security as well as its computational overhead. The privacy implications of DHCPv6 must be addressed before large-scale IPv6 deployment.
The progress of telematics (=Telecommunication + Automation + Informatics) methods promisesin tele-medicine applications increasing quality of treatment at reduced costs. At the two example cases of COPD (Chronic Obstructive Pulmonary Disease) and of Dialysis a related sensor equipment to characterize the health status, as well as equipment for continuous and safe data transmission are presented. Data processing at a tele-service center has to filter information,such that only the critical cases are confronted to the doctors. Challenges, system architecture and first experiences of this tele-medicine approach for remote monitoring of patients are reported.
Denial of Service (DoS) attacks on a computer system or network cause loss of service to users typically by flooding a victim with many requests or by disrupting the connections between two machines. Although significant amount of work has been done on DoS attacks, DoS attacks on streaming video servers were not investigated in detail. In this paper, we investigate DoS resilience of Real Time Streaming Protocol (RTSP). We show that by using a simple command line tool that opens a large number of RTSP connections, we can launch DoS attacks on the server and the proxy. We discuss in detail how the CPU and the memory resources are affected by the attacks. We observe that, with the DoS attack we launch, clients can also keep the connections alive for a long period and maintain the resources allocated at the server. We propose a lightweight dynamic detection framework for the RTSP based DoS attacks.
In this paper we consider fluid controlled models of networks. We proposed an new approach to analyzing network work using fluid models and conflict control theory domain. We obtain analytic solution for certain class of networks and proved time optimality. In the network game we found Nash equilibrium. We consider special case of network activity - denial of service attack and found dependency on download time depending of attack direction and traffic volume. The theoretical results were tested in an simulation environment OMNET++.
In the recent years, more attention is given to firewalls as they are considered the corner stone in Cyber defense perimeters. The ability to measure the quality of protection of a firewall policy is a key step to assess the defense level for any network. To accomplish this task, it is important to define objective metrics that are formally provable and practically useful. In this work, we propose a set of metrics that can objectively evaluate and compare the hardness and similarities of access policies of single firewalls based on rules tightness, the distribution of the allowed traffic, and security requirements. In order to analyze firewall polices based on the policy semantic, we used a canonical representation of firewall rules using Binary Decision Diagrams (BDDs) regardless of the rules format and representation. The contribution of this work comes in measuring and comparing firewall security deterministically in term of security compliance and weakness in order to optimize security policy and engineering.
Network security policy enforcement consists in configuring heterogeneous security mechanisms (IPsec gateways, ACLs on routers, stateful firewalls, proxies, etc) that are available in a given network environment. The complexity of this task resides in the number, the nature, and the interdependence of the mechanisms. We propose in this paper a formal data flow model focused on detecting multi-layer inconsistencies between security mechanisms. This model is independent from specific security mechanisms to admit the security technology diversity and evolution.
Management of a switch fabric security configuration, a core component of Storage Area Networks, is complex and error prone. As a consequence, misconfiguration of and/or a poor understanding of a switch fabric may unnecessarily expose an enterprise to known threats. A formal model of a switch security configuration is presented. This model is reasoned over to help manage complex switch fabric security configurations.
Computing systems today have large number of security configuration settings that are designed to offer flexible and robust services. However, incorrect configuration increases the potential of vulnerability and attacks. Security Content Automation Protocol provides a unified mean to automate the process of checking the desktop system compliance using standard interfaces. However, misconfiguration can be identified only if global checking that includes network and desktop configuration is performed, as many of these configurations are highly interdependent. In this work we present a SCAP-based tool that integrates host and network configuration compliance checking in one model and allows for executing comprehensive analysis queries in order to verify security and risk requirements across the end-to-end network as a single system. Our proposed tool translates XCCDF reports generated from SCAP tools into logical objects that can be further composed to create global logical analysis using more advanced security analytic tools such as ConfigChecker and PROLOG-based tools. This project also shows the value of building on the effort of standard tools to improve the state of the art.
Many algorithms have been proposed in the last decade to detect traffic anomalies in enterprise networks. However, most of these algorithms cannot detect anomalies that occur beyond enterprise boundaries. Anomaly monitoring and detection on end-to-end Internet paths, although important for network operations, is challenging due to lack of access and control over intermediate network devices. In this paper, we propose an algorithm that detects anomalies or significant events on an end-to-end Internet path by monitoring the path's available bandwidth. We first evaluate existing algorithms on a comprehensive dataset of more than a million bandwidth measurements spanning three years. We show that existing algorithms do not incorporate the typical behavior of a path in the anomaly detection process and consequently incur accuracy degradations. We therefore propose to filter noisy bandwidth measurements to extract a typical or baseline statistical distribution of a path's bandwidth. This baseline model is in turn leveraged in a generic decision-theoretic framework to provide timely detection of significant path events. We show that the proposed detector provides highly accurate performance and easily surpasses the accuracy of existing techniques.
The use of digital library services within the network and the external aspects, analysis of university library network security risks faced by the hardware, software, security risk, technical risk, management risk and the risk of external attacks and other major security risk. Combination of network security technologies, were put forward their own coping strategies. Focus on analysis of hacker attacks and network viruses control methods, including the use of anti-virus protection software, the use of firewall technology, encryption and authentication technology, intrusion detection, VLAN technology, content inspection and monitoring technology, and virtual honey pot technologies. Library network systems only to achieve security of the nature of the equipment, Network technology, professional and management standardization, in order to effectively avoid the risk of a variety of network security and ensure the normal operation of network systems, research on prevention and control of universities Digital library network security risks with a reference and guide.
With the rapid development of computer networks, network security has become a hot topic of concern to people, not just a simple network security threat network needs normal people, but also the pursuit of networking technology in innovative source of power. To address this issue, this article on network security solution to the problem definition and further discussion, and propose how to establish network security management.
Crowd sourced crisis mapping technologies aggregate information from individuals, trusted networks of informants, and publicly available media sources for use in informing decision making and response. Deploying this technology in conflict zones introduces a new set of vulnerabilities that can be exploited by hostile actors who have developed or adopted network surveillance and attack capabilities. Successful infiltration or compromise of these deployments can have a significant, negative impact on the lives of those reporting to and operating crisis maps. This paper identifies several fundamental vulnerabilities present in crisis mapping deployments and defines a set of best practices to defend those vulnerabilities from successful attack. The recommendations are adapted from established principles in computer and network security, combined with the authors' experience operating crisis mapping deployments in Afghanistan, Sudan, and Libya.
Ad Hoc network is a new type of wireless mobile networks. It does not rely on any fixed infrastructure, no center nodes, and its computing resources are limited. According to identity-based cryptography, threshold sharing scheme and mapping point algorithm, this paper presents a new Ad Hoc network security model, and verifies its safety. Safe mode without third party KGC includes sharing scheme, communication threshold key process of decrypted and signature verification.
Network security visualization is a highlighted topic of network security research in recent years, The existing research situation of network security visualization is analyzed. For the technical issues that the index of security situation is not accurate, and visual effects is not straightforward, the paper designed and implemented the security situation visualization prototype system based on geographic information systems, network topology graph, attack paths. The security situation data show in multiple views, multi-angle, multi-level display to the user by visualization technology, therefore the performance of the security situation will be more accurate and vivid, assessment of network security situation become timely and accurate, laying the foundation for rapid decision-making.
Due to the vulnerability of computer system, Internet worm is still greatly threatenning network security. It is found that the spread of worm regularly change because of the different space distributions of hosts and this phenomenon is defined as diurnal pattern. Based on diurnal pattern, two worm propagation models, namely simple diurnal forced model and complicated diurnal forced one, are constructed to describe and predict worm propagation. To verify the accuracy of them, Conficker dataset is adopted to obtain the practical curves of worm propagation. Through comparisons of numerical curves from models and practical curves, it is proved that simple model is good in describing the initial stage of worm propagation and complicated model can accurately predict the whole worm propagation.
Based on EIR's MS (mobile station) legality control ability, discuss on how to improved mobile communication network security. Innovative ideas are proposed to enhance network security. Indicate the role of EIR deployment in communication network security.
For the status quo of the safety of E-government and the major security threats in China, some basic precautions are introduced in this paper, and some countermeasures are discussed to guarantee the safety of E-government information, from the two perspectives of management and the corresponding technical measures that E-government has for its special security needs.
While many protocols for sensor network security provide confidentiality for the content of messages, contextual information usually remains exposed. Such contextual information can be exploited by an adversary to derive sensitive information such as the locations of monitored objects and data sinks in the field. Attacks on these components can significantly undermine any network application. Existing techniques defend the leakage of location information from a limited adversary who can only observe network traffic in a small region. However, a stronger adversary, the global eavesdropper, is realistic and can defeat these existing techniques. This paper first formalizes the location privacy issues in sensor networks under this strong adversary model and computes a lower bound on the communication overhead needed for achieving a given level of location privacy. The paper then proposes two techniques to provide location privacy to monitored objects (source-location privacy)periodic collection and source simulationand two techniques to provide location privacy to data sinks (sink-location privacy)sink simulation and backbone flooding. These techniques provide trade-offs between privacy, communication cost, and latency. Through analysis and simulation, we demonstrate that the proposed techniques are efficient and effective for source and sink-location privacy in sensor networks.
Firewalls are the most widely adopted technology for protecting private networks. However, most firewalls in Internet have been plagued with policy errors. An important source of errors stem from the lack of automatic tools ensuring a correct deployment of a network security policy expressed in a high level language, into firewall configurations. In this paper, we propose a formal and automatic method for deploying a security policy, written in an expressive language into both centralized and distributed firewall configurations. Further-more, our method verifies that no in coherences exist within the security policy. When inconsistencies are detected, the usual feedback returned permits us to propose a discrepancy resolution approach. Moreover, we propose an approach for optimizing the security policy. The correctness of our method is proved. Finally, it has been implemented in a prototype. The first results are very promising.
Mobile Peer-to-Peer (P2P) malware has emerged as one of the major challenges in mobile network security in recent years. Around four hundred mobile viruses, worms, trojans and spy ware, together with approximately one thousand of their variants have been discovered to-date. So far no classification of such mobile P2P security threats exists. There is no well known simulation environment to model mobile P2P network characteristics and provide a platform for the analysis of the propagation of different types of mobile malware. Therefore, our research provides a classification of mobile malware based on the behaviour of a node during infection and develops a platform to analyse malware propagation. It proposes and evaluates a novel behaviour-based approach, using AI, for the detection of various malware families. Unlike existing approaches, our approach focuses on identifying and classifying malware families rather than detecting individual malware and their variants. Adaptive detection of currently known and previously unknown mobile malware on designated mobile nodes through a deployed detection framework aided by AI classifiers enables successful detection. Although we have classified around 30%of the existing mobile P2P malware into 13 distinct malware families based on their behaviour during infection, this paper focuses on two, Cabir & Commwarrior, in order to analyse the proposed detection framework.
More and more effort is being spent on security improvements in today's computer networking environments. However, due to the nature of computer security there is still a lack of good quantitative assessment methods. Inventing and developing new ways of measuring security are therefore needed in order to more exact describe, assess, and improve security of computer environments. One existing quantitative security measure is guesswork. Guesswork gives the average number of guesses in a brute force attack when breaking an encrypted message. In the current definition of guesswork it is assumed that the attacker uses a single processor when breaking an encrypted message. However, an intelligent and motivated attacker will likely use several processors that can work in parallel to break an encrypted message. This paper formally investigates how guesswork changes over time in multi-processor attacks. The result is applied on three probability distributions, the English alphabet, the geometric, and the truncated geometric to illustrate some behaviors.
The scanning method of computer worms is an important modeling parameter. Active worms use different methods to scan the network for selecting their victim hosts. Topology-aware active worms scan the network by using the information of the network topology. This property, makes P2P network a suitable place for propagation of this kind of worms. Regarding the instability of network topology, especially in P2P networks, investigating the effect of the join and leave of hosts is completely necessary. In this paper, we study the propagation of topology-aware active worms based on SIR epidemiological model considering the join and leave of hosts. To have a better look at the effect of the join and leave of hosts on the propagation of active worms, the size of epidemic and infectious ratio parameters have been used. The results of this modeling show that the join and leave of hosts have considerable impact on the size of epidemic and the propagation performance of topology-aware active worms.
This paper investigates backbone communications in smart grid based on the application of wide area measurement system. The selection of communication protocols are most affected by the requirements of timing and load profile in the system, and this paper makes a comparison between different communication backbone mediums and propose an optimized protocol to achieve real-time data accessing and retrieving as well as preserving network security and data privacy. Extensive computer simulation has been implemented with Opnet to work out the traffic performance under varying load conditions.
With tremendous attacks in the Internet, there is a high demand for network analysts to know about the situations of network security effectively. Traditional network security tools lack the capability of analyzing and assessing network security situations comprehensively. In this paper, we introduce a novel network situation awareness tool - CNSSA (Comprehensive Network Security Situation Awareness) - to perceive network security situations comprehensively. Based on the fusion of network information, CNSSA makes a quantitative assessment on the situations of network security. It visualizes the situations of network security in its multiple and various views, so that network analysts can know about the situations of network security easily and comprehensively. The case studies demonstrate how CNSSA can be deployed into a real network and how CNSSA can effectively comprehend the situation changes of network security in real time.
TCG's trusted network connect (TNC) architecture improves network security through remote attestation. However, because of the deficiencies of existing binary attestation and property attestation, current TNC is not flexible and privacy-friendly enough to be used in a large scale network environment such as Internet. Aiming at these problems, this paper firstly analyzes the relations among system properties in the context of TCG-based remote attestation and proposes a new property relation model. Then a layered property attestation framework is proposed based on this model. Finally these ideas are used in the design of a real trusted network connect system. It is shown that the verifier need only obtain and verify the specific integrity measurement that he is interested in and the privacy of the attester's configuration is protected reasonably.
With the rapid growth of network technologies, many new web services have been developed to provide various applications and computing functions. These services rely deeply on the internet. Therefore, packet classification is an important issue of network security that typically adopts a flexible packet filtering system to classify each processed packet. Traditional packet classification requires hung computing time to process large amount of internet packets. Hence, we propose a GPGPU-based parallel packet classification method to decrease the computational cost. We also evaluate the performance of the proposed method with implementation on various memory architectures of CUDA device. The experiment results demonstrate that the proposed method can achieve significant speed up over the sequential packet classification algorithms on single CPU.
This paper is concerned with the presentation of a perspective on robustness in e-voting systems. It is argued that the effective design of an e-voting system and its viability can be enhanced by a two-pronged approach to robustness. First, it requires a clear distinction between two forms of robustness: robustness at protocol level and robustness at system level. Second, selected technologies should be integrated into an appropriate architecture in order to address robustness simultaneously at the two levels. The proposed approach is illustrated by the design and implementation of an e-voting system based on the FOO92 protocol. A service-oriented architecture supported by onion routing forms the basis of the system. It facilitates the distribution of tasks and state, the dynamic path configuration and the just-in-time (JIT) composition of the election system. The system conforms to most e-voting requirements.
With ever increasing network speed, scalable and reliable detection of network port scans has become a major challenge. In this paper, we present a scalable and flexible architecture and a novel algorithm, to detect and block port scans in real time. The proposed architecture detects fast scanners as well as stealth scanners having large inter-probe periods. FPGA implementation of the proposed system gives an average throughput of 2 Gbps with a system clock frequency of 100 MHz on Xilinx Virtex-II Pro FPGA. Experimental results on real network trace show the effectiveness of the proposed system in detecting and blocking network scans with very low false positives and false negatives.
As information the level of the society continues rising, the network has been used as office tools in our daily work, influencing all the aspects of human's life. As the characteristics of the computer network itself, especially its unique characteristic of openness and shareware, it has brought unprecedented challenges to network security. It has already become the most important problem faced by network workers how a large network can run normally, safely, efficiently and smoothly. On this basis, people began to develop the technology of intrusion detection, which is essentially different from security technologies existed in the past. It is a proactive detection instead of the passive ones of network security technology, and it is a reasonable complement for traditional security technologies. Intrusion detection technology will greatly enhance the network security capabilities.
The Naval Research Laboratory (NRL) Network Pump, or Pump, is a standard for mitigating covert channels that arise in a multilevel secure (MLS) system when a high user (HU) sends acknowledgements to a low user (LU). The issue here is that HU can encode information in the timings of the acknowledgements. The Pump aims at mitigating the covert timing channel by introducing buffering between HU and LU, as well as adding noise to the acknowledgment timings. We model the working of the Pump in certain situations, as a communication system with feedback and use then this perspective to derive an upper bound on the capacity of the covert channel between HU and LU in the Pump. This upper bound is presented in terms of a directed information flow over the dynamics of the system. We also present an achievable scheme that can transmit information over this channel. When the support of the noise added by Pump to acknowledgment timings is finite, the achievable rate is nonzero, i.e., infinite number of bits can be reliably communicated. If the support of the noise is infinite, the achievable rate is zero and hence a finite number of bits can be communicated.
Today Internet security has become a serious issue for anyone connected to the Internet. The internet is a great tool for many things, but unfortunately it can also pose a security risk for personal information and privacy. Hidden Markov Model (HMM) based applications are common in various areas, but the incorporation of HMM's for anomaly detection is still in its infancy. There are many approaches for building the IDS; here we are chosen Hidden Markov Model approach for building Anomaly based Intrusion Detection System (ABIDS) as a network security tool. This model has two phases, in the first phase the model is trained and in the second phase the model is tested. In both phases we have used a standard masquerade dataset. This dataset contains 50 users and each user has 15000 records. The first 5000 records have been used to train the model and the remaining 10000 records have been used for evaluation (testing) of the model. This model works for even high dimensional data streams with high performance detection rate and robust to noise.
Fast development of computer and especially Internet caused many issues for its users as well as its benefits. Nowadays, cyber criminals are utilizing Botnets to reach their goals. They have noticed that centralized structure is detected quickly. Hence the Peer to Peer Botnets are the most recent kind of Botnets that, they are applying encryption as well as rootkit capabilities to not being detected. In addition they mimic the performance of P2P software such as BitTorrent to make hard distinguishing the healthy packet from malicious packet in a large dataset. The proposed method is based on the correlation of Process Name besides the Ports as well as the Network Traffic. In the existing Operating Systems, every process is assigned a number that is called Port. By using this unique port and the process name, our experimental results show an acceptable rate of detection.
Spanning tree protocol (STP) is a link layer protocol used for link management, prevention of loop formation etc. in the network. Although STP is widely used, it is still prone to many kinds of attacks that exploit the lack of security features both in basic working process and STP packet format. By exploiting STP control packet an attacker can pretend to be the new root in STP domain and perform unauthorized activities that lead to root take-over attack, STP control packet flooding, traffic redirection and so on. In this paper, a coverage based distributed intrusion detection system (DIDS) has been introduced, for the detection of attacks on STP. The proposed scheme computes a set of switches in the network that can cover the STP network completely; where every switch belongs to that set is installed with a small module of IDS. This set of IDSs logically divides the STP network into a set of local zones. All the switches in a zone is directly connected to one switch installed with IDS and thus covered by at least one IDS in STP domain. Each IDS can detect and verify any exploit inside its local zone. Additionally IDSs communicate with each other so that any exploit outside the local zone of a particular IDS can also be detected and verified. The results show that the proposed DIDS approach can detect all the STP based attacks.
Botnets have become the top threat to Internet security. Botnets generate and transmit huge amounts of malicious traffic for various purposes. In this paper, we propose a dynamic programming based algorithm to calculate a set of appropriate routers for placing early filters of malicious traffic in the network in order to maximize the benefits of early malicious traffic filtering. Moreover, we discuss how to combine our early filtering approach and the QoS routing of traffic so that the bandwidth saved by early filtering of malicious traffic can be efficiently utilized by legitimate traffic.
Cyber security for smart grid communication systems is one of the most critical requirements need to be assured before smart grid can be operationally ready for the market. Privacy is one of very important security consideration. The customer information privacy in smart grid need to be protected. Smart grid data privacy encompasses confidentiality and anonymity of the information extracted from smart device metering transmission in smart grid communication system. In this paper, we consider a home area network as a basic reading data aggregation and dispatch unit in smart grid systems. Then, we propose a secure in-network data aggregation and dispatch scheme to keep the confidentiality and anonymity for collecting power usage information of home smart devices to the household smart meter and the reverse control message distributing procedure. Specifically, we introduce an orthogonal chip code to spread reading-data of different home smart devices into spread code, followed by a circuit shifting operation to coupling neighboring smart devices tightly. We adopt an in-network mechanism to further mask it with its spread data with its forwarding data. Finally, we analyze the cyber security protection levels using an information theoretic quantity, residual uncertainty. Simulation studies are conducted to test the performance on different metering datasets for the proposed scheme. This paper sets the ground for further research on optimizing of home power management systems with regarding to the privacy of customer power usage behaviors.
In this paper we consider the problem of routing traffic between k source-destination pairs. Using game theoretic modeling we provide randomized strategies to minimize the threat of attacks on links by an adversary. The adversary is assumed to have a choice of c edges for attack. We propose iterative methods to find the Nash Equilibrium of the zero-sum game. The proposed schemes have been implemented using existing network models (GEANT in Europe and the AT&T network in US) and show marked reduction in the gain of the attacker. As the gain of the attacker is related to the congestion on the edges, our schemes also reduce congestion.
Collaboration of defensive network components of multiple operators is a promising approach for increasing anomaly detection accuracy. This concept involves sharing of possibly sensitive data, hence privacy preservation has to be taken into account. In this paper, we argue that common approaches for sharing traffic information often impede proper analysis due to privacy-preserving mangling operations, and ignore the opportunity to exploit additional knowledge of the originating network operator for interpreting the monitored data. We propose COMINDIS, a lightweight framework for sharing notions of suspiciousness among network operators, and show how to exploit different detection systems for deriving a better understanding of Internet hosts' activities. We evaluate the system both by using a network emulator and by experimenting with a real traffic trace.
This paper presents a routing framework that embeds location and communication privacy into the routing mechanisms. It conceals endpoint identification by introducing waypoints, through encrypted routing hints, where each waypoint has knoweldge of the next hop, assuring network privacy over several waypoints. Based on IPv6 extension headers and Onion Routing techniques, the network waypoints comply with normal routing procedures, avoiding explicit tunneling or full packet encryption. By focusing on the network as a cooperative entity for privacy preservation, we propose a lightweight approach that can be easily deployed, establishing a good compromise between privacy and optimal routing.
Mobile social network (MSN) is a promising networking and communication platform for users having similar interests (or attributes) to connect and interact with one another. For many recently introduced secure MSN data communication schemes, attribute-based encryption is often adopted to preserve user privacy and prevent outside attackers from eavesdropping. In this paper, we propose an efficient and secure user revocation scheme to address inside attacks based on an attribute-based encryption technique. The proposed scheme enables a trusted authority (TA) to flexibly control the data decryption capability of mobile social users. It disables malicious users from decrypting any data packet. As a result, proper user behavior is encouraged, inside attacks are reduced, and network security is enhanced. Through the analysis, we demonstrate that the proposed user revocation scheme is able to resist attribute collusion attacks and revoke collusion attacks. Extensive simulation results further confirm that the proposed scheme has much smaller communication overhead and much shorter delay than the existing solution [1].
Intentional attack incurs fatal threats on modern networks by paralyzing a small fraction of nodes with highest degrees to disrupt the network. To enhance the network robustness, in this paper we propose a fusion-based defense mechanism where each node performs local detection and feedbacks minimum (one-bit) information to the fusion center in order to infer the presence of attack. By formulating the attack and defense strategy as a zero-sum game, we leverage the expected payoff as a benchmark to evaluate the effectiveness of the fusion-based defense mechanism. Both analytic results and empirical data support that fusion-based defense is able to prevent the network from disruption, even under poor detection capability and fragile nature of complex networks.
To facilitate reliable communications, efficacious signaling is of crucial importance for information dissemination control, especially in dynamic and infrastructureless networking paradigms such as mobile ad hoc networks (MANETs). Since data transportation much resembles the spread of epidemics, two signal distribution schemes, self healing and vaccine spreading, are proposed to analyze the information dynamics via epidemic modeling. We consider a more realistic scenario where the effect of signaling is proportional to the distribution time. Intuitively, early distribution leads to slight cure while late distribution fails to prevent the epidemic from outbreak. Optimal control theory and early-stage analysis are exploited to determine the optimal distribution time for timely control. Moreover, we demonstrate how signal distribution aids to minimize buffer occupancy in epidemic routing. This research therefore paves novel avenues to network defense and information dissemination.
In this paper we analyze the security architecture of ProtoGENI. ProtoGENI is a prototype control framework implementation of GENI (Global Environment for Network Innovations). We perform a variety of experiments in an effort to identify potential vulnerabilities presented in the current implementation. We classify our attacks into three types: data plane to data plane, data plane to control plane, and data plane to Internet. Our results indicate the potential for a breach of confidentiality and availability internally within ProtoGENI, as well as risks to external Internet. We make suggestions outlining possible defense strategies to improve ProtoGENI security and aid in future development.
We present a real-time neural network-based network analyzer system for hotspot area. There are many applications that available in the market today for provide us the network graph of our hotspot areas. These graphs will be analyzed by a network administrator. Because a hotspot area often runs 24 hours, an administrator has a difficulty to monitor the traffic all the time. Therefore we proposed the automatic system to help a network administrator in monitoring the network. This system will replace human skill in interpreting the graph with an Artificial Neural Network System. To minimize the number of input vector we use mean value of axis, so the micro-computer e.g. notebook, laptop, PDA, and other gadgets can handle this system. Testing result showed this system could classify between normal, high and un-normal traffic of network graph periodically.
In Computer Security area, Intrusion Detection System (IDS) plays important role in detecting any kinds of network attacks. Denial of Service (DoS) and Probing attacks are common detectable intrusions that are frightened by most network users since the final result of these attacks is collapsing the network. Our previous research has proposed a robust statistical method, the BACON-MVV method, that provides 100%accuracy in detecting patterns of DoS and Probing attacks, inspite of the training sets used contains suspicious packet traffic called outliers. One problem not yet being addressed by previous research was the processing time taken as the packet traffics to be analysed for detecting any intrusion grows bigger. In this paper, we propose a Parallel BACON-MVV method based on Data Decomposition to be implemented in IDS. Experiment using our own generated simulation datasets shows that this proposed method runs significantly faster than its serial version.
In order to provide a low-cost and simple user attestation method, in our previous work, we proposed a method with strong authentication by using digital camera of cellular phone. But, this method used 2D color code, thus there are some process costs such as create and decode 2D color code. In this paper, we present an improved attestation system using a cellular phone and 2D color code. We compare the performance of the encryption methods and the efficiency of the proposed attestation method is confirmed by an experimental prototype system.
We discuss countermeasure against insider threats in network security aspect. In the context of countermeasure against insider threats, there is no perimeter for access control in a network. A traditional access control process by using a firewall on a perimeter is not suitable. We show a mechanism of countermeasure against insider threats in network security for countermeasure technically and legally.
In recent years, there have been to studied to construct a new network infrastructure for accepting evolved network technology. There is a research of controlling the network by "Flow". Flow is used a combination of layers of L4 from L2, to identify the communication network traffic. High Performance Packet Processing Engine (HPPPE), that FUJITSU developed, is one of the next generation packet processing engine. This system can process data from low-layer up to high-layer according to the format of the data being used per service. HPPPE provides service functions as add-on. In Saga University, user authentication system for using network named Open gate has been developed and used widely from 2001. In this study, we have developed network user authentication gateway system like Open gate using "HPPPE". This system determines using the network and ending the network based on "Flow".
Recently, several severe cyber attacks to commercial networks and their systems have been found more frequently compared to the last half decade. According to the analysis results from the experts, most of the attacks had similar approach to reach their goal: compromising the web server and bridging to the storage server instead of direct penetrating through the firewalls within the intranet. Because web servers are usually easier to compromise than other insiders in the server farm, this unwanted approach would be common for hackers. This means that the servers in the secured server farm became unsafe and the storage server has no exception. Because of the reason, this paper introduces the concept of the secondary backup and proposes a consequent framework.
This paper presents a new algorithm aimed at the vulnerability assessment of web applications following a black-box approach. The objective is to improve the detection efficiency of existing vulnerability scanners and to move a step forward toward the automation of this process. Our approach covers various types of vulnerabilities but this paper mainly focuses on SQL injections. The proposed algorithm is based on the automatic classification of the responses returned by the web servers using data clustering techniques and provides especially crafted inputs that lead to successful attacks when vulnerabilities are present. Experimental results on several vulnerable applications and comparative analysis with some existing tools confirm the effectiveness of our approach.
Mobile Ad hoc Networks (MANET) have been highly vulnerable to attacks due to the dynamic nature of its network infrastructure. Among these attacks, routing attacks have received considerable attention since it could cause the most devastating damage to MANET. Even though there exist several intrusion response techniques to mitigate such critical attacks, existing solutions typically attempt to isolate malicious nodes based on binary or na&#239;ve fuzzy response decisions. However, binary responses may result in the unexpected network partition, causing additional damages to the network infrastructure, and na&#239;ve fuzzy responses could lead to uncertainty in countering routing attacks in MANET. In this paper, we propose a risk-aware response mechanism to systematically cope with the identified routing attacks. Our risk-aware approach is based on an extended Dempster-Shafer mathematical theory of evidence introducing a notion of importance factors. In addition, our experiments demonstrate the effectiveness of our approach with the consideration of several performance metrics.
As the computer networks continue to increase in size, complexity and importance, the network security issue becomes more and more important. In this paper, we propose a real time anomaly detection system based on relative entropy. The proposed system captures the network traffic packets and then uses relative entropy and adaptive filter to dynamically determine the traffic changes and to examine whether the traffic change is normal or contains anomaly. Our experimental results show that the proposed system is efficient for on-line anomaly detection, using traffic trace collected in high-speed links.
In competitive electricity markets, one of the important tasks of the system operator is to manage congestion of transmission lines as it leads to a violation of network security limits. This paper presents an effective approach for system operator to identify the most sensitive zone to relieve congestion in transmission lines. In this approach sensitivity factors are utilized to perform the task of defining the system into no. of zones. Once the zones are formed then the task of SO becomes much easier as they have to concentrate on most sensitive zone rather on the entire system. Also concept of desired generation scheduling is proposed for rescheduling generation in order to relieve congestion. By this method the contribution of generators to the congested line is to be found out which are thereafter utilized for rescheduling purpose of relieving congestion. Here our task is managing congestion of most sensitive zone with the help of rescheduling of generators in that particular zone only. Thus we are trying to make the system congestion free without disturbing the generators of the other zones of the system that is only by means of the generators of its own zone. Results obtained for network overload alleviation of IEEE 30 Bus System & IEEE 39 Bus New England System are presented for illustration purposes. Here in this approach only real power flow violations of lines has been considered.
Attack graph increasingly becomes a key technique for network security analysis, however, the prevalent Attacker's Ability Monotonic Assumption (AAMA) constraint for attack graph generation could not make full use of the direction of network attack and the hierarchy of defence. As a result, using AAMA is not efficient enough in the process of attack graph generation, especially for large-scale complicated network. With the aim of improving the efficiency of attack graph generation and reducing attack graph's complexity, we proposed the concept of Network Security Gradient (NSG) to reflect the hierarchy of network defence, and the Gradient Attack Assumption (GAA) based on NSG to constraint the process of attack graph generation. To make our theory of NSG more sound and reasonable, we proposed two NSG marking algorithms, respectively from static analysis of network topology and dynamic analysis of network access flow, to rank network nodes automatically. Experiment results showed that both of the two algorithms can mark NSG for network correctly and rationally.
Computer networks are inevitably attacked as a result of their openness, while network attack usually actualized by exploiting vulnerability existing in network environment. Attack graph, consisted of lots of related atomic attacks, can fully display the exploitation and dependence relations among all of the vulnerabilities existed in network. Thus, it is a very useful tool for network vulnerability analysis and network security evaluation. However, the prevalent Attacker's Ability Monotonic Assumption (AAMA) constraint for attack graph generation could not make full use of the direction of network attack and the hierarchy of defense. As a result, using AAMA to constraint the process of attack graph generation is not only inefficient but also couldn't reduce the complexity of attack graph, especially for large-scale complicated network. According to lots of experiment and theoretical analysis, we found that it is mainly the existence of Circuitous Attack Paths (CAP) in attack graph lead to it complexity and the low efficiency of generation. To address this problem, we proposed the concept of Network Security Gradient (NSG) to reflect the direction of the network attack and the hierarchy of defense, and the Gradient Attack Assumption (GAA) to constraint the process of attack graph generation for the purpose of avoiding CAPs. Testified by a case study, using the GAA to constraint the process of attack graph generation can destruct those circuitous attack paths, therefore, is an effective way to improve the efficiency of attack graph generation and reduce the complexity of attack graph, and make it more useful for vulnerability analysis and network security evaluation.
Firewalls play an important role in the security of communication systems. They are widely adopted for protecting private networks by filtering out undesired network traffic in and out of the secured network. The verification of firewalls is a great challenge because of the dynamic characteristics of their operation, their configuration is highly error prone, and finally, they are considered the first defense to secure networks against attacks and unauthorized access. In this paper, we propose a new approach for modeling and verification of firewall configuration rules using domain restriction method. Our approach is implemented in Event-B formal techniques, where we model firewall configuration rules, and then use invariant checking to verify the consistency of firewall configurations in Event-B theorem proving framework.
The rapid burst of Internet usage and the corresponding growth of security risks and online attacks for the everyday user or enterprise employee lead to the concepts of Awareness Creation and Information Security Culture. Nevertheless, security education has remained an academic issue mainly. Teaching system security or network security on the basis of practical experience inherits a great challenge for the teaching environment, which is traditionally solved using a computer laboratory at a university campus. The Tele-Lab project offers a system for hands-on IT security training in a remote virtual lab environment on the web, accessible by everyone. The Tele-Lab platform provides individual learning environments for each student, that may consist of up to three virtual machines per learning environment. Besides in explorative learning, where students use the laboratory whenever they like, the Tele-Lab is used in a blended learning approach: a lecturer introduces a security topic in class using e.g. Powerpoint slides. Subsequently, the students perform a supervised practical exercise in the virtual laboratory. A typically sized course with 15 students can in consequence request up to 45 virtual machines from the Tele-Lab server. The paper at hand briefly presents usage, management and operation of Tele-Lab as well as its architecture. Furthermore, this work introduces an architecture for clustering of the virtual lab on application level and the necessary prerequisites for the implementation. The paper also presents an existing distributed usage scenario.
RFID is one of the enabling technologies of the Internet of Things. RFID has the potential to enable machines to identify objects, understand their status, and communicate and take action if necessary, to create "real time awareness." The pervasiveness of RFID technology has given rise to a number of serious issues including security and privacy concerns. This paper will discuss current RFID usage issues and conduct a threat analysis of the RFID system components then identify issues/risks and elucidate how these issues can be resolved or risks can be mitigated.
Trust model has become the focus of network security. The description of trust is given newly with fuzzy set theory for pervasive computing environment. In course of trust synthesis evaluation, the model of subjective trust-rank evaluation based on the interval-valued fuzzy set is proposed in this paper so that the evaluation of users trust-rank becomes more flexibility and more reliable. According to the dynamic character of trust, a novel model and method of trust-rank's calculation based on interval-valued fuzzy set theory is provided. The application instance shows that proposed fuzzy trust evaluation provides a new valuable way for pervasive computing.
Propagating malwares have drawn significant attention as a result of their serious impact on the internet and the network security. Although the development of stochastic models of worm propagation has been on, but the real internet prevention mechanism have not been deployed in the network simulations. The real worm propagation cycle identifies the ways and means the worms exploit to spread themselves, it provides for real parameter modeling based on the campus environment topology. The paper considers some common practices and typical worm propagation and tries to analytically provide mathematical model based on improved SIR Two-Factor model for worm propagation on the campus network environment that will help in predicting and forecasting the trend.
Research on networks is an area that we should watch more closely than any other. Perhaps more important than the borrowing of techniques, however, is paying close attention to the ferment over whether new network designs with security in mind are worth the societal price and effort to actually implement in a world that already has a considerable sunk investment in structure. How networks build themselves does have considerable influence on our field of practice.
In this paper an efficient computation scheme for analyzing the security of power transmission networks is presented. In order to strategically allocate protection devices in the network, the problem of finding the sparsest stealthy false data attack to the state estimator is studied. While the attack search problem is traditionally solved as a generic constrained cardinality minimization problem, this paper exploits the problem structure intrinsic to the power network application to obtain a polynomial time approximate algorithm based on a minimum cut relaxation. Experiment results from realistic test cases are presented to demonstrate the efficiency and accuracy of the proposed algorithm.
Cross-domain Internet-scale collaborative security is affected by a native dichotomy. On one side, sharing of monitoring data across domains may significantly help in detecting large scale threats and attacks; on the other side, data sharing conflicts with the need to protect network customers' privacy and confidentiality of business and operational information. The approach first proposed in this paper enables what we call conditional data sharing, i.e., permit cross-domain sharing of fine-grained organized subsets of network security data (called monitoring data feeds), only when a threshold number of domains are ready to reveal their data for the same feed. The proposed approach revolves on a careful combination of distributed threshold based cryptography with identity-based encryption. It appears scalable and easy to deploy, not requiring neither a-priori monitoring data feeds identification, nor explicit coordination among domains. Protection is accomplished by simply using different cryptographic keys per feed, and automatically permitting per-feed key reconstruction upon the occurrence of independent and asynchronous per-domain/per-feed alerts.
The power grid is under pressure to maintain highly reliable supply under constrained expansion budgets and environmental policies. This can be achieved through realization of smart grid technologies and methodological advancements that would allow further improvement of asset utilization, economic operation and network security. This paper introduces a method for evaluating potential benefits as well as the technical limitations of employing dynamic thermal rating (DTR) on overhead lines (OHL) in a stressed network environment. The paper, initially models system-wide network performance under actual thermal ratings to investigate the benefits of DTR under specific operating scenarios as well as over static thermal rating (STR) on OHLs in a given network. Secondly, it investigates the benefit of implementing several additional long-term emergency rating-duration times for secure and adequate operation through a smarter ICT rule-setting program that improves network performance without compromising its reliability under contingent scenarios. The proposed methodology is employed on the IEEE 24-bus network test system suggesting a cost benefit model that balances the interests of both network operators and asset managers.
Anti-virus Scanning is one of the key technologies in today's network security field. Multi-pattern matching algorithm plays an important role in the anti-virus applications. This paper analyzed some commonly used pattern matching algorithm. On the basis of AC-BNFA algorithm, this paper proposed an algorithm handling viruses with more than one signatures quickly. Experiments show that this algorithm is much faster than both exsited automaton-based algorithms and BM algorithm without consuming much memory.
As the rapid development of the Mobile Internet (MI) applications, the Mobile Internet Services (MIS) are becoming the focus of development in the future. This paper focuses on the security issues of MIS, and puts forward the security framework for MIS. This framework is based on the existing network security standards, and describes the threats and requirements based on the features of the MIS. It provides a method and basis for security issues from multiple planes and levels.
Cloud is a relatively new concept and so it is unsurprising that the information assurance, data protection, network security and privacy concerns have yet to be fully addressed. The cloud allows users to avoid upfront hardware and software investments, gain flexibility, collaborate with others, and take advantage of the sophisticated services. However, security is a huge issue for cloud users especially access control, user profile management and accessing services offered by the private cloud environment. A privacy enhancement system on Academic-based private cloud system using Eucalyptus open source cloud infrastructure has been proposed in this paper. This system provides the cloud users to improve the privacy and security of the private personal data. Two approaches (Role-based Access Control and Attribute-based Access Control model) are combined as a new approach (ARBAC). This means that they are applied to improve the privacy which supports both mandatory and discretionary access control needs on the target private cloud system.
Network firewalls act as the first line of defense against unwanted and malicious traffic targeting Internet servers. Predicting the overall firewall performance is crucial to network security engineers and designers in assessing the effectiveness and resiliency of network firewalls against DDoS (Distributed Denial of Service) attacks as those commonly launched by today's Botnets. In this paper, we present an analytical queueing model based on the embedded Markov chain to study and analyze the performance of rule-based firewalls when subjected to normal traffic flows as well as DoS attack flows targeting different rule positions. We derive equations for key features and performance measures of engineering and design significance. These features and measures include throughput, packet loss, packet delay, and firewall's CPU utilization. In addition, we verify and validate our analytical model using simulation and real experimental measurements.
Firewalls are the mainstay of enterprise security and the most widely adopted technology for protecting private networks. As the quality of protection provided by a firewall directly depends on the quality of its policy (i.e., configuration), ensuring the correctness of firewall policies is important and yet difficult. To help ensure the correctness, we propose a systematic structural testing approach for firewall policies. We define structural coverage (based on coverage criteria of rules, predicates, and clauses) on the firewall policy under test. To achieve high structural coverage effectively, we have developed four automated packet generation techniques: the random packet generation, the one based on local constraint solving (considering individual rules locally in a policy), the one based on global constraint solving (considering multiple rules globally in a policy), and the one based on boundary values. We have conducted an experiment on a set of real policies and a set of faulty policies to detect faults with generated packet sets. Generally, our experimental results show that a packet set with higher structural coverage has higher fault-detection capability (i.e., detecting more injected faults). Our experimental results show that a reduced packet set (maintaining the same level of structural coverage with the corresponding original packet set) maintains similar fault-detection capability with the original set.
Because the Internet has been widely applied in various fields, more and more network security issues emerge and catch people's attention. However, adversaries often hide themselves by spoofing their own IP addresses and then launch attacks. For this reason, researchers have proposed a lot of traceback schemes to trace the source of these attacks. Some use only one packet in their packet logging schemes to achieve IP tracking. Others combine packet marking with packet logging and therefore create hybrid IP traceback schemes demanding less storage but requiring a longer search. In this paper, we propose a new hybrid IP traceback scheme with efficient packet logging aiming to have a fixed storage requirement for each router (under 320 KB, according to CAIDA's skitter data set) in packet logging without the need to refresh the logged tracking information and to achieve zero false positive and false negative rates in attack-path reconstruction. In addition, we use a packet's marking field to censor attack traffic on its upstream routers. Lastly, we simulate and analyze our scheme, in comparison with other related research, in the following aspects: storage requirement, computation, and accuracy.
Contemporary network security applications generally require the ability to perform powerful pattern matching to protect against attacks such as viruses and spam. Traditional hardware solutions are intended for firewall routers. However, the solutions in the literature for firewalls are not scalable, and they do not address the difficulty of an antivirus with an ever-larger pattern set. The goal of this work is to provide a systematic virus detection hardware solution for network security for embedded systems. Instead of placing entire matching patterns on a chip, our solution is a two-phase dictionary-based antivirus processor that works by condensing as much of the important filtering information as possible onto a chip and infrequently accessing off-chip data to make the matching mechanism scalable to large pattern sets. In the first stage, the filtering engine can filter out more than 93.1%of data as safe, using a merged shift table. Only 6.9%or less of potentially unsafe data must be precisely checked in the second stage by the exact-matching engine from off-chip memory. To reduce the impact of the memory gap, we also propose three enhancement algorithms to improve performance: 1) a skipping algorithm; 2) a cache method; and 3) a prefetching mechanism.
Welcome to the ninth installment of the Design and Implementation (D&I) Series. The D&I Series was created with a specific goal to facilitate knowledge transfer between scientists and industry-oriented engineers. In contrast to the classic peer-reviewed, academically oriented articles on communication technologies, D&I articles focus primarily on the practical Lessons Learned information that is gained while designing, implementing, and deploying new communications products, services, and solutions.
Capturing and replaying real flows are important for testing network security products. However, capturing real flows demands a high storage cost and runs a risk of capture loss, which makes the replay inaccurate. Replaying real flows should be accurate and stateful to adapt to the reaction of the device under test. It should also efficiently reproduce a defect and help developers identify the flows triggering defects. Therefore, this work first presents the (N, M, P) capture scheme which begins with, for each connection, capturing at most N bytes of application payload and then at most M bytes of application payload for at most each of the subsequent P packets in the same connection. This scheme reduces 87 percent of storage cost while retaining 99.74 percent of original events. This work develops a tool named SocketReplay with the mechanisms of loss recovery, stateful replay, and selective replay. Loss recovery tracks TCP sequence numbers to identify capture loss and recovers incomplete flows with dummy data. Stateful replay maintains the states in the TCP/IP stack to replay real flows. Selective replay incrementally selects flows to replay. The results show that SocketReplay can accurately and efficiently reproduce product events and significantly decrease the volume of replayed packet traces.
Firewalls are the most widely adopted technology for protecting private networks. However, most firewalls in Internet have been plagued with policy errors. An important source of errors stem from the lack of automatic tools ensuring a correct deployment of a network security policy expressed in a high level language, into firewall configurations. In this paper, we propose a formal and automatic method for deploying a security policy, written in an expressive language into both centralized and distributed firewall configurations. Further-more, our method verifies that no in coherences exist within the security policy. When inconsistencies are detected, the usual feedback returned permits us to propose a discrepancy resolution approach. Moreover, we propose an approach for optimizing the security policy. The correctness of our method is proved. Finally, it has been implemented in a prototype. The first results are very promising.
Mobile Peer-to-Peer (P2P) malware has emerged as one of the major challenges in mobile network security in recent years. Around four hundred mobile viruses, worms, trojans and spy ware, together with approximately one thousand of their variants have been discovered to-date. So far no classification of such mobile P2P security threats exists. There is no well known simulation environment to model mobile P2P network characteristics and provide a platform for the analysis of the propagation of different types of mobile malware. Therefore, our research provides a classification of mobile malware based on the behaviour of a node during infection and develops a platform to analyse malware propagation. It proposes and evaluates a novel behaviour-based approach, using AI, for the detection of various malware families. Unlike existing approaches, our approach focuses on identifying and classifying malware families rather than detecting individual malware and their variants. Adaptive detection of currently known and previously unknown mobile malware on designated mobile nodes through a deployed detection framework aided by AI classifiers enables successful detection. Although we have classified around 30%of the existing mobile P2P malware into 13 distinct malware families based on their behaviour during infection, this paper focuses on two, Cabir & Commwarrior, in order to analyse the proposed detection framework.
Application-level protocol specifications are helpful for network security management, including intrusion detection and intrusion prevention which rely on monitoring technologies such as deep packet inspection. Moreover, detailed knowledge of protocol specifications is also an effective way of detecting malicious code. However, current methods for obtaining unknown and proprietary protocol message formats (i.e., no publicly available protocol specification), especially binary protocols, highly rely on manual operations, such as reverse engineering which is time-consuming and laborious. In this paper, we propose Biprominer, a tool that can automatically extract binary protocol message formats of an application from its real-world network trace. In addition, we present a transition probability model for a better description of the protocol. The chief feature of Biprominer is that it does not need to have any priori knowledge of protocol formats, because Biprominer is based on the statistical nature of the protocol format. We evaluate the efficacy of Biprominer over three binary protocols, with an average precision more than 99%and a recall better than 96.7%.
Computer Trojan has been threatening the information security of computer systems since its birth. While many companies offer many solutions to deal with computer Trojan, users can also buy a wide range of anti-virus and anti-Trojan software on the market so far, the information security software, facing to Trojan horse's attack, has been in the embarrassing position where they had to defend it passively. In this paper, in the Windows X86 platforms, based on the DLL Preemptive Injection, we introduce boldly a method against computer Trojans, and implement a down loader Trojan detection model.
With tremendous attacks in the Internet, there is a high demand for network analysts to know about the situations of network security effectively. Traditional network security tools lack the capability of analyzing and assessing network security situations comprehensively. In this paper, we introduce a novel network situation awareness tool CNSSA (Comprehensive Network Security Situation Awareness) to perceive network security situations comprehensively. Based on the fusion of network information, CNSSA makes a quantitative assessment on the situations of network security. It visualizes the situations of network security in its multiple and various views, so that network analysts can know about the situations of network security easily and comprehensively. The case studies demonstrate how CNSSA can be deployed into a real network and how CNSSA can effectively comprehend the situation changes of network security in real time.
TCG's trusted network connect (TNC) architecture improves network security through remote attestation. However, because of the deficiencies of existing binary attestation and property attestation, current TNC is not flexible and privacy- friendly enough to be used in a large scale network environment such as Internet. Aiming at these problems, this paper firstly analyzes the relations among system properties in the context of TCG-based remote attestation and proposes a new property relation model. Then a layered property attestation framework is proposed based on this model. Finally these ideas are used in the design of a real trusted network connect system. It is shown that the verifier need only obtain and verify the specific integrity measurement that he is interested in and the privacy of the attester's configuration is protected reasonably.
With the rapid growth of network technologies, many new web services have been developed to provide various applications and computing functions. These services rely deeply on the internet. Therefore, packet classification is an important issue of network security that typically adopts a flexible packet filtering system to classify each processed packet. Traditional packet classification requires hung computing time to process large amount of internet packets. Hence, we propose a GPGPU-based parallel packet classification method to decrease the computational cost. We also evaluate the performance of the proposed method with implementation on various memory architectures of CUDA device. The experiment results demonstrate that the proposed method can achieve significant speed up over the sequential packet classification algorithms on single CPU.
This paper is concerned with the presentation of a perspective on robustness in e-voting systems. It is argued that the effective design of an e-voting system and its viability can be enhanced by a two-pronged approach to robustness. First, it requires a clear distinction between two forms of robustness: robustness at protocol level and robustness at system level. Second, selected technologies should be integrated into an appropriate architecture in order to address robustness simultaneously at the two levels. The proposed approach is illustrated by the design and implementation of an e-voting system based on the FOO92 protocol. A service-oriented architecture supported by onion routing forms the basis of the system. It facilitates the distribution of tasks and state, the dynamic path configuration and the just-in-time (JIT) composition of the election system. The system conforms to most e-voting requirements.
This paper investigates backbone communications in smart grid based on the application of wide area measurement system. The selection of communication protocols are most affected by the requirements of timing and load profile in the system, and this paper makes a comparison between different communication backbone mediums and propose an optimized protocol to achieve real-time data accessing and retrieving as well as preserving network security and data privacy. Extensive computer simulation has been implemented with Opnet to work out the traffic performance under varying load conditions.
The main idea in this paper is to design and analyze a symmetric key generation algorithm whereby we can strengthen wireless network security. Our main goal is generating a sequence of highly secure secret keys based on an ARQ based transmission mechanism that relies on the statistical independence of channel errors between the attacker and legitimate users. This leads to some information loss for the adversary which allows us to constantly extract keys by using universal Hashing techniques from communication process, about which we can make sure that adversarys knowledge remains negligible. More specifically, the key generation algorithm is analyzed and designed in a way that targeted security as well as the required throughput and synchronization goals for the transmission are achieved. Simulation results show that the designed algorithm achieves the desired requirements for both system security and throughput.
Traffic graph analysis has become an increasingly useful tool in network security. By summarizing the aggregate activity of a particular service or network using graph based representations, it is possible to model normal activity using a variety of different attributes which are not easily identified or exploited by attackers. In this paper, we discuss several examples of analysis using traffic graphs and demonstrate its potential for scan detection, identifying hitlist attackers, and identifying spammers.
In this paper, we present a new authentication mechanism for wireless network security based on certificate less-based signature technology and fingerprint recongnition technology. We introduce the principle of certificate less signature, illustrate the process of certificate less signature authentication, analyze the advantages and disadvantages of the certificate less-based signature technology. We show that the introduction of fingerprint identification into certificate less signature can enhance the security and efficiency of wireless network security authentication.
With rapid development of computer network technology, computer networks are applied extensively in commercial banks and are increasingly confronted with various attacks. A strategy for solving network security of commercial banks is proposed in this paper, based on analysis of insecurity threats, particularity of network security, and trends of network security technology. A network security system is constructed on the strategy.
This paper begins with network shopping of university students. Then in a comprehensive analysis by knowing the confidence level of site quality, supplier quality, the confidence level of network security, gender ratio of network shopping, the actual experience of network shopping and the current stage impact of factors that influence network shopping of university students. Finally the paper gives the current phase of individual factors affecting network shopping of university students.
Researchers in the network security field lack quantitative methods to analysis anti-denial of service (DoS)attacks ability. In this paper, we describe an anti-denial of service testing method based on mobile agent taking on the attacker. The goal of the testing was to evaluate the system and its defense mechanism's anti-denial of service attack ability. The process of testing anti-denial of service attacks can be broadly divided into three phases: DoS attack scenario, information gathering and performance analysis. We describe how the testing process was designed and customized to the defenses, how the performance was measuring, how the tests were run, and what we learned from them about defenses and about testing security system.
A configurable IPSec processor for a high performance in-line network security processor that integrates two embedded 32-bit CPU cores, and an IPSec protocol processor on a SoC is presented. The IPSec processor can implement the transport/tunnel mode AH and ESP protocol of the IPSec, and support AES-128/192/256, HMAC-SHA-1 algorithm. The number of AH, ESP, AES, HMAC-SHA-1 IP-cores in the design can be configured for different use such as 10 Gigabit Ethernet and Gigabit Ethernet, even for the next generation 40/100G Network. Low power is also considered in the design. In the IPSec processor, crossbar switch architecture for multi-core data transfer is adopted. With four parallel AH, ESP, AES, HMAC-SHA-1 IP-cores separately connected to an 8x8 crossbar switch in the IPSec processor, a throughput of 1.5Gbps at 200MHz is achieved and hardware verification is implemented by FPGA. By simulation, the IPSec protocol operation can achieve 10Gbps wire speed with 32 IPSec protocol IP-cores and cryptographic IP-cores configured in the IPSec processor.
On the basis of analysis of research status on IMS network security, this paper categorizes and analyzes possible abnormal events in IMS network. Combined with mature detection theories, we present corresponding strategy of detection from application layer to provide theoretical and decisive support for warning and further network process measurements to reduce the loss brought by abnormal events and improve the security of the whole IMS network.
With the development of application based on Internet, network security highlights its place increasing. Firewall and IDS are the equipment often used in Internet, but both of them can not run automatically. If we can reconfigure the firewall using the result of IDS, the security must be enhanced to a high level. In this paper, we designed an intrusion prevent system (IPS) based on Snort and Net filter by researching kernel codes of Snort and Net filter. The policy control module of the system was written in Multi-thread technologies. Meanwhile, the Algorithm of IDS and rule set of firewall was optimized to improve system efficiency. The system can block the attack source by dynamically modify firewall rules according to IDS.
Intrusion Detection System has become the important part of the computer and network security because it can effectively compensate for the lack of network security measures. However, the intrusion detection technology relies heavily on pattern matching algorithms, as the choice of pattern matching algorithms directly affect the detection rate, so it is more important. First the paper is detailed analysis the characteristics of the four kinds of single pattern matching which based on prefix search, and then it tests by the number of different pattern strings. The experimental results show that the KMP algorithm can improve effectively the speed of pattern matching. So the application of KMP algorithm can improve the efficiency of intrusion detection.
As the flooding of malicious codes such as worms, how to analyze large number of malicious samples quickly and effectively becomes a great issue for researchers in network security. This paper proposed an analysis algorithm for worm network behavior based on event sequence, which uses the data flow recombination and compression methods to process the pure malicious data. With this algorithm, one can quickly extract the network behavior profile and the signature of the worm. The application of this algorithm will greatly improve the efficiency of analyzing the worm network behavior, which will be significant for the deployment of firewalls and network invasion detection systems.
Mixing information is a key technique in network security. In mental poker, Internet lottery, and Mix-Net for electronic voting, we use shuffling to maintain anonymity, privacy, and fairness. In this paper, we propose a method to confirm whether a mix is done well by finding a fixed point in the mix system while keeping how the shuffle was done a secret.
The network technology, multimedia technology and communication technology are widely used in all kinds of education areas. The network-teaching is a novel application result for higher education in the information age. However, it is an increasingly outstanding problem for the management and security in the resource library of the network teaching and education. By the characteristics of chaotic theory and encryption, in this paper, a kind of secure communication system based on spatiotemporal chaos is designed and exploited for solving the security protection problem of network-teaching video resource. By using Visual C++ platform, an illustrative example is realized and its experimental results demonstrate that the online video-teaching resource is protected very well during the encrypted transmission based on the spatiotemporal chaotic technology.
We discuss countermeasure against insider threats in network security aspect. In the context of countermeasure against insider threats, there is no perimeter for access control in a network. A traditional access control process by using a firewall on a perimeter is not suitable. We show a mechanism of countermeasure against insider threats in network security for countermeasure technically and legally.
This paper presents a new algorithm aimed at the vulnerability assessment of web applications following a black-box approach. The objective is to improve the detection efficiency of existing vulnerability scanners and to move a step forward toward the automation of this process. Our approach covers various types of vulnerabilities but this paper mainly focuses on SQL injections. The proposed algorithm is based on the automatic classification of the responses returned by the web servers using data clustering techniques and provides especially crafted inputs that lead to successful attacks when vulnerabilities are present. Experimental results on several vulnerable applications and comparative analysis with some existing tools confirm the effectiveness of our approach.
Mobile social network (MSN) is a promising networking and communication platform for users having similar interests (or attributes) to connect and interact with one another. For many recently introduced secure MSN data communication schemes, attribute-based encryption is often adopted to preserve user privacy and prevent outside attackers from eavesdropping. In this paper, we propose an efficient and secure user revocation scheme to address inside attacks based on an attribute-based encryption technique. The proposed scheme enables a trusted authority (TA) to flexibly control the data decryption capability of mobile social users. It disables malicious users from decrypting any data packet. As a result, proper user behavior is encouraged, inside attacks are reduced, and network security is enhanced. Through the analysis, we demonstrate that the proposed user revocation scheme is able to resist attribute collusion attacks and revoke collusion attacks. Extensive simulation results further confirm that the proposed scheme has much smaller communication overhead and much shorter delay than the existing solution [1].
To facilitate reliable communications, efficacious signaling is of crucial importance for information dissemination control, especially in dynamic and infrastructureless networking paradigms such as mobile ad hoc networks (MANETs). Since data transportation much resembles the spread of epidemics, two signal distribution schemes, self healing and vaccine spreading, are proposed to analyze the information dynamics via epidemic modeling. We consider a more realistic scenario where the effect of signaling is proportional to the distribution time. Intuitively, early distribution leads to slight cure while late distribution fails to prevent the epidemic from outbreak. Optimal control theory and early-stage analysis are exploited to determine the optimal distribution time for timely control. Moreover, we demonstrate how signal distribution aids to minimize buffer occupancy in epidemic routing. This research therefore paves novel avenues to network defense and information dissemination.
This paper designs a novel sampling data preprocessing subsystem in integrated protection based on IEC61850. This subsystem will be used to solve the problems of data redundancy processing and bad value processing. A framework structure of integrated protection software system based on the concept of module distributed is firstly presented, and then the details of preprocessing subsystem are given. This subsystem is the most important foundation of integrated protection. It can provide a platform to analyze and validate the protection arithmetic. Meanwhile, the subsystem will reach the requirement for security, real-time and reliability.
As information the level of the society continues rising, the network has been used as office tools in our daily work, influencing all the aspects of human's life. As the characteristics of the computer network itself, especially its unique characteristic of openness and shareware, it has brought unprecedented challenges to network security. It has already become the most important problem faced by network workers how a large network can run normally, safely, efficiently and smoothly. On this basis, people began to develop the technology of intrusion detection, which is essentially different from security technologies existed in the past. It is a proactive detection instead of the passive ones of network security technology, and it is a reasonable complement for traditional security technologies. Intrusion detection technology will greatly enhance the network security capabilities.
Today Internet security has become a serious issue for anyone connected to the Internet. The internet is a great tool for many things, but unfortunately it can also pose a security risk for personal information and privacy. Hidden Markov Model (HMM) based applications are common in various areas, but the incorporation of HMM's for anomaly detection is still in its infancy. There are many approaches for building the IDS; here we are chosen Hidden Markov Model approach for building Anomaly based Intrusion Detection System (ABIDS) as a network security tool. This model has two phases, in the first phase the model is trained and in the second phase the model is tested. In both phases we have used a standard masquerade dataset. This dataset contains 50 users and each user has 15000 records. The first 5000 records have been used to train the model and the remaining 10000 records have been used for evaluation (testing) of the model. This model works for even high dimensional data streams with high performance detection rate and robust to noise.
Trust model has become the focus of network security. The description of trust is given newly with fuzzy set theory for pervasive computing environment. In course of trust synthesis evaluation, the model of subjective trust-rank evaluation based on the interval-valued fuzzy set is proposed in this paper so that the evaluation of users trust-rank becomes more flexibility and more reliable. According to the dynamic character of trust, a novel model and method of trust-rank's calculation based on interval-valued fuzzy set theory is provided. The application instance shows that proposed fuzzy trust evaluation provides a new valuable way for pervasive computing.
Propagating malwares have drawn significant attention as a result of their serious impact on the internet and the network security. Although the development of stochastic models of worm propagation has been on, but the real internet prevention mechanism have not been deployed in the network simulations. The real worm propagation cycle identifies the ways and means the worms exploit to spread themselves, it provides for real parameter modeling based on the campus environment topology. The paper considers some common practices and typical worm propagation and tries to analytically provide mathematical model based on improved SIR Two-Factor model for worm propagation on the campus network environment that will help in predicting and forecasting the trend.
Ubiquitous cyber systems and their supporting infrastructure impact productivity and quality of life immensely. Their penetration in our daily life increases the need for their enhanced resilience and for means to secure and protect them. One major threat is the software monoculture. Latest research work illustrated the danger of software monoculture and introduced diversity to reduce the attack surface. In this paper, we propose a biologically-inspired defense system, ChameleonSoft, that employs multidimensional software diversity to, in effect, induce spatiotemporal software behavior encryption and a moving target defense. The key principles are decoupling functional roles and runtime role players; devising intrinsically-resilient composable online programmable building blocks; separating logic, state and physical resources; and employing functionally-equivalent, behaviorally-different code variants. Given, our construction, ChameleonSoft is also equipped with an autonomic failure recovery mechanism for enhanced resilience. Nodes employing ChameleonSoft autonomously and cooperatively change their recovery and encryption policy both proactively and reactively according to the continual change in context and environment. In order to test the applicability of the proposed approach, we present a prototype of the ChameleonSoft Behavior Encryption (CBE) and recovery mechanisms. Further, using analysis and simulation, we study the performance and security aspects of the proposed system. This study aims to evaluate the provisioned level of security by measuring the level of induced confusion and diffusion to quantify the strength of the CBE mechanism. Further, we compute the computational cost of security provisioning and enhancing system resilience. A brief attack scenario is also included to illustrate the complexity of attacking ChameleonSoft.
Modernizing our critical infrastructure often involves upgrades with Cyber-Physical Systems (CPS) to enhance efficiency, safety and reliability. New security and resilience requirements and challenges arise given the mission- and time-critical nature of CPS applications. These applications are always targeted by sophisticated persistent attacks exploiting potential cyber-physical integration vulnerabilities. In this paper, we present CyPhyCARD (Cooperative Autonomous Resilient Defense &#x201C;CARD&#x201D; platform for Cyber Physical Systems) as a resilient and secure defense cloud. The foundation of CyPhyCARD is our Cell-Oriented Architecture (COA) that enables distributed, dynamically configurable, and runtime-programmable platforms. COA comprises composable intrinsically resilient, active components termed &#x201C;Cells&#x201D; that dynamically manage heterogeneous resources and executable software code variants to execute CyPhyCARD defense missions. CyPhyCARD uses our generic Evolutionary Sensory system (EvoSense) to circulate context-driven, functionally customizable sensors and effectors through the target of defense. EvoSense provides cooperative autonomous control and sharing amongst interconnected defense service providers (CyPhyCARD) and/or their target of defense to enhance attack detection and deterrence. Further, CyPhyCARD uses our ChameleonSoft system to secure its infrastructure of cells. ChameleonSoft is a multidimensional software diversity system that autonomously induces runtime confusion and diffusion thereby, in effect, encrypting the spatiotemporal software behavior and realizing a moving target defense. Both EvoSense and ChameleonSoft are built using the COA. CyPhyCARD is designed to increase the cost for the attacker at all times through persistently asymmetric operations achieved, in part, using a moving target defense construction and automated recovery provided by ChameleonSoft; rabid global attack detection and mitigation through EvoS- nse; and Operation resilience in presence of attacks using attack containment and honeypots defense missions. We demonstrate, using an attack scenario, how our proposed solution reacts to threats targeting CyPhyCARD and/or its target of defense systems.
Attack graphs are a novel way of examining how safe a network is from attacks and analysing the shortcomings of these networks. The analysis of the attack graph may help in assessing network security. However, an attack graph can be very large in size - containing a million nodes and a million edges. Thus analysing such a large graph becomes problematic and time consuming. We have proposed an indexing scheme for fast data retrieval. Using this indexing scheme we can identify the vulnerable machines in a network corresponding to an attack pattern.
Attack graph increasingly becomes a key technique for network security analysis, however, the prevalent Attacker's Ability Monotonic Assumption (AAMA) constraint for attack graph generation could not make full use of the direction of network attack and the hierarchy of defence. As a result, using AAMA is not efficient enough in the process of attack graph generation, especially for large-scale complicated network. With the aim of improving the efficiency of attack graph generation and reducing attack graph's complexity, we proposed the concept of Network Security Gradient (NSG) to reflect the hierarchy of network defence, and the Gradient Attack Assumption (GAA) based on NSG to constraint the process of attack graph generation. To make our theory of NSG more sound and reasonable, we proposed two NSG marking algorithms, respectively from static analysis of network topology and dynamic analysis of network access flow, to rank network nodes automatically. Experiment results showed that both of the two algorithms can mark NSG for network correctly and rationally.
Computer networks are inevitably attacked as a result of their openness, while network attack usually actualized by exploiting vulnerability existing in network environment. Attack graph, consisted of lots of related atomic attacks, can fully display the exploitation and dependence relations among all of the vulnerabilities existed in network. Thus, it is a very useful tool for network vulnerability analysis and network security evaluation. However, the prevalent Attacker's Ability Monotonic Assumption (AAMA) constraint for attack graph generation could not make full use of the direction of network attack and the hierarchy of defense. As a result, using AAMA to constraint the process of attack graph generation is not only inefficient but also couldn't reduce the complexity of attack graph, especially for large-scale complicated network. According to lots of experiment and theoretical analysis, we found that it is mainly the existence of Circuitous Attack Paths (CAP) in attack graph lead to it complexity and the low efficiency of generation. To address this problem, we proposed the concept of Network Security Gradient (NSG) to reflect the direction of the network attack and the hierarchy of defense, and the Gradient Attack Assumption (GAA) to constraint the process of attack graph generation for the purpose of avoiding CAPs. Testified by a case study, using the GAA to constraint the process of attack graph generation can destruct those circuitous attack paths, therefore, is an effective way to improve the efficiency of attack graph generation and reduce the complexity of attack graph, and make it more useful for vulnerability analysis and network security evaluation.
The rapid burst of Internet usage and the corresponding growth of security risks and online attacks for the everyday user or enterprise employee lead to the concepts of Awareness Creation and Information Security Culture. Nevertheless, security education has remained an academic issue mainly. Teaching system security or network security on the basis of practical experience inherits a great challenge for the teaching environment, which is traditionally solved using a computer laboratory at a university campus. The Tele-Lab project offers a system for hands-on IT security training in a remote virtual lab environment on the web, accessible by everyone. The Tele-Lab platform provides individual learning environments for each student, that may consist of up to three virtual machines per learning environment. Besides in explorative learning, where students use the laboratory whenever they like, the Tele-Lab is used in a blended learning approach: a lecturer introduces a security topic in class using e.g. Powerpoint slides. Subsequently, the students perform a supervised practical exercise in the virtual laboratory. A typically sized course with 15 students can in consequence request up to 45 virtual machines from the Tele-Lab server. The paper at hand briefly presents usage, management and operation of Tele-Lab as well as its architecture. Furthermore, this work introduces an architecture for clustering of the virtual lab on application level and the necessary prerequisites for the implementation. The paper also presents an existing distributed usage scenario.
This ongoing research and development activity addresses aspects of a potential capability to detect credential misuse and a suggested alerting approach based on known attack conditions to support automated mitigation techniques. This research is based on the assumption that the audit data and human-computer activity characteristics extracted from networked components contain the footprint(s) of those trying to breach network security. It takes advantage of the combination of near-real-time suspicious activity detection with biometric behavior profiling to reduce profiling false positives and network access controls that enable faster and more focused responses to detected suspicious activities.
The market for security software has witnessed an unprecedented growth in recent years. A closer examination of this market reveals certain idiosyncrasies that are not observed in a traditional software market. For example, it is a highly competitive market involving many vendors, often with a very aggressive pricing strategy adopted by new entrants. Yet, the market coverage seems to be quite low. Prior research has not attempted to explain what aspects of security software make this market deviate from the traditional ones. In this paper, we develop a quantitative model to study this market. Our model identifies a possible reason behind this behavior--that of a negative network effect, which pulls the market in exactly the opposite direction when compared to the positive network effect observed in traditional software markets. Overall, our results highlight the unique nature of the security software market, furnish rigorous explanation for several counter-intuitive observations in the real world, and provide managerial insights for vendors on market competition and product development strategies.
Wireless network security is a daunting challenge as researchers analyze the vulnerabilities of wireless medium access control schemes such as IEEE 802.11, Bluetooth, and IEEE 802.16. While these wireless protocols employ traditional contention and non-contention approaches, recent work has identified the delay and throughput performance advantages of a flow-specific medium access hybrid solution for wireless networks such as the traffic-adaptive Cooperative Wireless Sensor Medium Access Control (CWS-MAC) protocol. Accordingly, this paper addresses the security of wireless networks employing these hybrid, flow-specific schemes by analyzing the vulnerabilities in traffic-adaptive CWS-MAC, many of which are applicable to hybrid medium access schemes in general. Critical vulnerabilities are identified that expose the protocol to denial of service, energy exhaustion and "greedy" attacks. The effect of attacks exploiting these vulnerabilities are shown to include decreased throughput and increased delay in both the contention and non-contention modes as well as increased per node energy consumption.
As the computer networks continue to increase in size, complexity and importance, the network security issue becomes more and more important. In this paper, we propose a real time anomaly detection system based on relative entropy. The proposed system captures the network traffic packets and then uses relative entropy and adaptive filter to dynamically determine the traffic changes and to examine whether the traffic change is normal or contains anomaly. Our experimental results show that the proposed system is efficient for on-line anomaly detection, using traffic trace collected in high-speed links.
A suspect (apriori known) for a given malafide intention has to be lured into Context honeypot using lure messages. The message is generated in a privacy enforced environment in such a way that it can entice him/her into the Context honeypot. Earlier work advocates the formation of message with genuine and synthetic information. The choice of synthetic information should in no way destroy the opaque characteristic of the system. This paper tries to address this need. Here we (a) propose a framework to generate privacy enforced lure message and (b) conduct experiments in a particular domain with set of test users in choosing the synthetic information. The lure message having the synthetic information is used as bait to lure the test user. The response from the user helps the framework in regulating the synthetic information. The best message thus generated is taken as bait to induce the suspected user. The outcome of the result corroborates usefulness/success of the proposed framework.
In competitive electricity markets, one of the important tasks of the system operator is to manage congestion of transmission lines as it leads to a violation of network security limits. This paper presents an effective approach for system operator to identify the most sensitive zone to relieve congestion in transmission lines. In this approach sensitivity factors are utilized to perform the task of defining the system into no. of zones. Once the zones are formed then the task of SO becomes much easier as they have to concentrate on most sensitive zone rather on the entire system. Also concept of desired generation scheduling is proposed for rescheduling generation in order to relieve congestion. By this method the contribution of generators to the congested line is to be found out which are thereafter utilized for rescheduling purpose of relieving congestion. Here our task is managing congestion of most sensitive zone with the help of rescheduling of generators in that particular zone only. Thus we are trying to make the system congestion free without disturbing the generators of the other zones of the system that is only by means of the generators of its own zone. Results obtained for network overload alleviation of IEEE 30 Bus System & IEEE 39 Bus New England System are presented for illustration purposes. Here in this approach only real power flow violations of lines has been considered.
Managing the network security events data stream, which is very large and real-time, and mining useful information from the data stream to analyse and forecast security situation, are very difficult. In this paper, we present an architecture, hybrid data stream cube, and its incremental updating algorithm. Experimental results proved that this architecture could facilitate on-line, multi-dimension, multi-level analysis of network security events data stream and shorten the response time.
This paper briefly introduces the network boundaries can be effectively protected using network defense in depth system and the shortcomings of its technical means. Then introduced the data mining technology applications in the firewall policy optimization, traffic identification, intrusion detection technology areas at different levels of defense in depth, and lists a variety of specific application modes, which can effectively enhance the dynamic adaptation and automated optimization capabilities of defense in depth system to a wide range of network attacks and traffic flow changes.
CAPTCHA is a new kind of network security mechanism. Studying the recognition of CAPTCHA can help to discover its hidden defects and thus make it more secure. For closely-connected CAPTCHAs that can hardly be recognized by methods of state of art, this paper proposed a new recognition algorithm based on holistic verification. During the process of this algorithm, Recurrent Neural Network (RNN) was first used to recognize unknown CAPTCHAs. Then, recognition results were verified by SVM rejection, synthetic data generation and Extreme Learning Machine (ELM). Experiments results show that this algorithm can not only recognize closely-connected CAPTCHAs but also effectively boost the recognition rate of RNN.
In order to set up universal criteria for measuring and evaluating network security and survivability, this paper presents a novel index system. This index system was built according to the Protection-Detection-Response (PDR) security model and the Resistance-Recognition-Recovery (R3) survivability principle of network in the presence of attacks, failures, or accidents. And all the 24 metrics in the index system are used to cover the characters of network security and survivability. The index system is organized with three-layer structure like a tree. The first layer, i.e. root node, is called network security and survivability. The second layer includes three indexes, i.e. resistance metric, awareness metric, and recovery metric. The third layer includes 20 indexes abstracted from different network layers, survivability characters, and survivability methods and so on. This novel index system is quantifiable, universal, consistent, compatible, and fine-scale, which is suitable for different network systems. At last, a case for how to using these index system to measure network security and survivability is also given.
A distributed framework for network forensics is presented in this paper, which tries to capture and store the digital evidence of the information leaking through the network. The architecture of the frame work is composed of the distributed data agents and the forensic center. The former can extract and compress the text of the content of all target network transmission, and the latter can locate the address of the host which illegally transmitted classified or improper information in the network, based on the evidence data gathered from the data agents. The time of the data being stored is longer than a year with the high compression ratio of the text, so the user can confirm the events of the information leaking that happened fairly long time ago.
Anti-virus Scanning is one of the key technologies in today's network security field. Multi-pattern matching algorithm plays an important role in the anti-virus applications. This paper analyzed some commonly used pattern matching algorithm. On the basis of AC-BNFA algorithm, this paper proposed an algorithm handling viruses with more than one signatures quickly. Experiments show that this algorithm is much faster than both exsited automaton-based algorithms and BM algorithm without consuming much memory.
As the rapid development of the Mobile Internet (MI) applications, the Mobile Internet Services (MIS) are becoming the focus of development in the future. This paper focuses on the security issues of MIS, and puts forward the security framework for MIS. This framework is based on the existing network security standards, and describes the threats and requirements based on the features of the MIS. It provides a method and basis for security issues from multiple planes and levels.
Cloud is a relatively new concept and so it is unsurprising that the information assurance, data protection, network security and privacy concerns have yet to be fully addressed. The cloud allows users to avoid upfront hardware and software investments, gain flexibility, collaborate with others, and take advantage of the sophisticated services. However, security is a huge issue for cloud users especially access control, user profile management and accessing services offered by the private cloud environment. A privacy enhancement system on Academic-based private cloud system using Eucalyptus open source cloud infrastructure has been proposed in this paper. This system provides the cloud users to improve the privacy and security of the private personal data. Two approaches (Role-based Access Control and Attribute-based Access Control model) are combined as a new approach (ARBAC). This means that they are applied to improve the privacy which supports both mandatory and discretionary access control needs on the target private cloud system.
Cross-domain Internet-scale collaborative security is affected by a native dichotomy. On one side, sharing of monitoring data across domains may significantly help in detecting large scale threats and attacks; on the other side, data sharing conflicts with the need to protect network customers' privacy and confidentiality of business and operational information. The approach first proposed in this paper enables what we call &#x201C;conditional data sharing&#x201D;, i.e., permit cross-domain sharing of fine-grained organized subsets of network security data (called monitoring data feeds), only when a threshold number of domains are ready to reveal their data for the same feed. The proposed approach revolves on a careful combination of distributed threshold based cryptography with identity-based encryption. It appears scalable and easy to deploy, not requiring neither a-priori monitoring data feeds identification, nor explicit coordination among domains. Protection is accomplished by &#x201C;simply&#x201D; using different cryptographic keys per feed, and automatically permitting per-feed key reconstruction upon the occurrence of independent and asynchronous per-domain/per-feed alerts.
In this paper an efficient computation scheme for analyzing the security of power transmission networks is presented. In order to strategically allocate protection devices in the network, the problem of finding the sparsest stealthy false data attack to the state estimator is studied. While the attack search problem is traditionally solved as a generic constrained cardinality minimization problem, this paper exploits the problem structure intrinsic to the power network application to obtain a polynomial time approximate algorithm based on a minimum cut relaxation. Experiment results from realistic test cases are presented to demonstrate the efficiency and accuracy of the proposed algorithm.
The power grid is under pressure to maintain highly reliable supply under constrained expansion budgets and environmental policies. This can be achieved through realization of smart grid technologies and methodological advancements that would allow further improvement of asset utilization, economic operation and network security. This paper introduces a method for evaluating potential benefits as well as the technical limitations of employing dynamic thermal rating (DTR) on overhead lines (OHL) in a stressed network environment. The paper, initially models system-wide network performance under actual thermal ratings to investigate the benefits of DTR under specific operating scenarios as well as over static thermal rating (STR) on OHLs in a given network. Secondly, it investigates the benefit of implementing several additional long-term emergency rating-duration times for secure and adequate operation through a smarter ICT rule-setting program that improves network performance without compromising its reliability under contingent scenarios. The proposed methodology is employed on the IEEE 24-bus network test system suggesting a cost benefit model that balances the interests of both network operators and asset managers.
Time series arise frequently in many sciences and engineering application, including finance, digital audio, motion capture, network security, and transportation. In this work, we propose a technique for discovering anomalies in time series that takes advantages of the Symbolic Aggregate approXimation (SAX) technique and inspiration from a motif discovery algorithm. We use SAX to reduce the dimension of the time series and apply the idea of motif discovery to detect anomalies. We consider recessive sequences instead of frequent sequences similar to motif finding. We evaluate the algorithm on several real-world data from different areas, such as the car speed data, the motion capture data, and the weather data. Experiments demonstrate the effectiveness of the proposed algorithm to discover anomalies in real-world time series.
One of the most important and challenging area in the smart grid context is security and privacy section. Smart grid is a vulnerable system and can be attacked even from aboard, attacks that may cause different level of issues and harms on the devices and society. So, research community has paid attention to this topic and the reasons of required security and privacy for the smart grid. The first step of designing and implementing security for any system such as a smart grid is an authentication scheme followed by a key management protocol. Other security aspects like integrity, authorization and confidentiality can be implemented as long as a strong key management protocol has already been designed and addressed. In this paper we provide a new scheme for the mutual authentication between the smart grid utility network and Home Area Network smart meters. Our proposed mechanism is capable of preventing different attacks like Brute-force, Replay, Man-In- The-Middle and Denial-of-Service attacks. Also, we provide a novel key management protocol for data communication between the utility server and customers smart meters. Our proposed protocol improves the network overhead caused by security key management controlling packets, and at the same time it is enough secured in order to prevent above mentioned attacks. In fact, by generating and broadcasting only one function periodically by the server that is in charge of the network security, our protocol simply refreshes entire nodes public key and private key as well as multicast required security keys if any.
Real-time traffic classification is becoming increasingly critical for network management, traffic engineering, and network security. Current software-based solutions, however, have difficulties dealing with a great number of flows in today's high-speed networks. This paper proposes RocketTC, a scalable FPGA-based architecture, to accelerate traffic classification while maintaining high accuracy. It combines two significant elements: (1) an efficient flow management scheme using on-chip BRAMs for storing the flow table, and (2) a parallel and pipelined classification engine array with partial dynamic reconfiguration (PDR) on FPGA. We have implemented and evaluated RocketTC on Xilinx Virtex-5 FPGA based platform. Our results show a sustained throughput of over 20 Gbps for minimum packet size of 40 bytes, and high accuracy above 97%for classifying nearly a hundred popular applications. Additionally, it is easy for RocketTC to update more application types.
Vulnerability analysis is the basis of satellite network construction. This paper summarized the popular vulnerability Analysis Methods in the field of computer network security. This paper also described and compared these methods. Then this paper introduced the Vulnerability analysis methods for communication network, command & control network, mobile ad hoc network and satellite network. At last, the shortage of current research on satellite network vulnerability was analyzed and the next research idea was proposed.
Wireless sensor networks (WSNs) are usually deployed in unattended adversary or even hostile environments. Great difficulties and challenges of bootstrapping network security stem from the particular nature of WSNs. Key management mechanism is a basic key problem of them. To the case of manually deployed WSNs, we introduce a key distribution scheme based on dual directional hash chains and honeycomb-like network topology. The analysis and evaluation show that our scheme can effectively weaken the threat of node capture, be resilience against node replication or node forgery. Besides of good security properties, the scheme promises good node addition or network extension ability.
This paper talks about how to select plant varieties and the mix of shrub and grass protection program of the slope protection and how to determine the ratio of grass slope based on the silty soil embankment slope and the local climate and hydrological characteristics And it also tells how to follow the three-dimensional mat grass slope processes and the method of hydraulic seeding, which has achieved good results in slope protection.
With the heterogeneous threats of network ongoing, the problems of network security become more and more intractable, besides, the compatibility issues of security protection-equipments are emerging in endlessly. Thus, the unified treat management has been a research hotpot currently. In this paper, based on analyzing the status quo of UTM (unified treat management), as well as the performance detects of related equipments, we present the NAC-UTM, a holistic protection scheme of intranet, which combines UTM with technology of network access control. The emphasis of our research is to illustrate the operating principle of UTM-NAC scheme, and then we verify the superiority of this scheme with a simplified experiment. The test result indicates that our solution could greatly promote performance of UTM equipments.
Real network traffic is an important asset for research and development in the field of network security, but due to privacy concerns of leaking personal and host information, acquiring real traffic is quite restricted in practice. Over the past years, researchers have developed anonymization techniques to specify sensitive application fields or patterns to be hidden, but the specification will take great effort to investigate the sensitivity of a large number of application fields beforehand, and can be inaccurate due to the lack of patterns for some sensitive information. This work presents an innovative method towards automatically inferring where sensitive information is in the application messages. The method can leverage existing application protocol parsers to locate the application fields or optionally infer the fields by clustering and aligning similar application messages. After that, this method can infer the degree of sensitivity of application fields with the C4.5 decision tree algorithm based on the three measures: entropy, diversity, and one-to-one mapping. The experimental results demonstrate that the inference of sensitivity is effective with low false-negative and acceptable false-positive rates.
Massive trojan and virus spreading via web vulnerabilities has become a severe threat to today's cyber-society. To traditional honeypots, which is able to collect malware with automated manner, becomes one of the most severe and common detection methods nowadays. Compared with honeypots, malicious links detection system based on script text analysis has several advantages due to its lower cost and easier deployment. Firstly, according to the current analysis of trojan web links and existing problems, we summarize the requirement for malicious links detection system, and describe the system design based on requirement. After that we describe the algorithm with mathematical meathod. Finally, we analyze and summarize the experimental results, and verify the reliability and rationality of malicious links by this detection system.
With the rapid development of computer network, there are more and more network security incidents becuase of the defaults of network protocols. ARP spoofing is a terrible one, that makes use of the defaults of ARP protocol to attack network. It destroys the communication between hosts by sending the wrong IP/MAC address. Some security measures can prevent ARP spoofing attacks, but have some constrains. The text designs a method to defend ARP spoofing, which is based on NDIS intermediate driver.
Against technical defects of traditional network access control system, detail UAC and TNC two kinds of new network security access technology, and analyze and compare them.
With the explosive growth of network applications and complexity, the threat of Internet worms against network security becomes increasingly serious. In this paper, s the structure and characteristic of complex networks are first analyzed, then the exploration function component and execution mechanism of Internet worms are presented, the strategies for worm scans are analyzed and evaluated, some critical techniques of internet worm prevention are given, meanwhile the advantage and disadvantage of these methods are analyzed, finally find how to improve the capability of worm detection system.
Against technical defects of traditional network access control system, detail NAC and NAP two kinds of new network security access technology, and analyze and compare them.
Real-Time anomaly detection is currently a hot topic in the area of network security research. In this paper, we firstly introduce the average length of data message as the measurement of abnormal behavior, and then advance a model of sampling measurement, in which the stratified sampling algorithm based on content trigger is utilized to select the bits in the IP packet identification field as the sampling and mask's length and contents. The comparison between statistic characters of total messages traffic and the samples in a large-scale network decides whether the sampIes are precise and efficient. Based on statistic characters of the samples and examination theory of hypothesis, real-time anomaly detection model is built. Lastly, average length of network data packets is defined to be the measurement of network behavior, and then we successfully realize the real time detection of distributed denial of service attack of network. Methods and ideas in this paper could provide some meaningful advice for other network security detection researches.
To solve the network security response problem, a novel network response decision-making method based on the two-matrix game model was present. We analyzed the attackers and system administrators' different strategies, and then constructed the dual-benefit matrix of the attackers and defenders. On the basis of this, the two-matrix network security game model was proposed. At last, the optimal defense strategy decision-making algorithm was proposed. Using this algorithm, we could analyze benefits of both offensive and defensive, and solved the Nash Equilibrium Point which balanced the both and gave the optimal response decision-making strategy. The proposed method could make accurate response decision and had a high efficiency. Also, it occupied a lower memory which could reduce the interference to normal user when administrators manage the network.
With the rapid development of mobile ad hoc network, security and trust are important to the network, especially for military ad hoc network. Trust routing scheme is the main issue in military ad hoc network. Trust routing is related to many factors such as routing hop number, node information integrity degree and node behavior. To use multi-source information, multi-agent collaboration method is used to design trust routing scheme. The definition of trust routing is proposed in the paper. Ad hoc On-Demand Distance Vector (AODV) routing protocol is used to validate trust routing scheme. NS-2 simulator is used to simulate different network scenarios and node behavior. Simulation results show that network security and efficiency can be improved in trust routing scheme.
With the popularity of different wireless access technologies for Internet access, the integration of heterogeneous wireless networks is foreseen in the near future. Solutions for secured Internet access in such scenario become an urgent requirement. In this paper, we present SHAWK, a platform for secure integration of heterogeneous advanced wireless networks, including cellular networks, wireless local area networks (WLANs) and wireless mesh networks (WMNs) for ubiquitous Internet access. We have developed effective mechanisms to strike the balance between security and performance requirements in such heterogeneous networks from media access layer to application layer, including a customized context-aware media independent handoff (MIH) framework, a unified access testbed for 3G and Wi-Fi integration, a secure fast handoff scheme between different wireless networks, a practical secure routing solution to defend wormhole attack and an novel application for secure application mobility. Through comprehensive experiments, we show that our mechanisms can ensure the network security, with satisfactory system performance.
Traditional low-latency anonymization techniques apply non-standardized, complex, and often even proprietary protocols. Apart from poor performance, the high development effort leads to the existence of at most one single implementation. This in turn increases the risk of creating so-called software monocultures, where failures in the single implementation can compromise the overall security. In this paper we introduce SOR -- a novel approach for anonymization that is completely based on standardized, well tested, and performance-tuned protocols. It utilizes out-of-the-box nested SSH connections to achieve an exhaustive state-of-the-art anonymization system based on onion routing. Our approach supports both sender and receiver anonymity. Besides of many audited implementations, the SSH protocol itself is mature and has been thoroughly analyzed with respect to security issues. The results of our evaluation show that our approach outperforms existing alternatives by a factor of up to nine without sacrificing the degree of anonymity. Moreover, SOR can be easily implemented which increases the chances of having many alternative clients available.
Computer network and communication technology have already been widely used in all aspects of our daily life, but the network security problem is getting more and more prominent, while the network wiretap is one of the biggest challenges of the network security. With the security problem caused by network wiretap becoming more serious, how to monitor the network wiretaps has been the primary problem most researchers have been faced with. By using the ARP cheats to monitor network is a kind of quick and convenient method, which is the topic of this thesis as well.
With the development of network security technical, the pivotal factor is not technique but the administrator and management mechanism. In the domain of information security, social engineering can get into information system by using people' s weakness, just not using the computer's leak. This method can be used in many attacking way and turn up a new important project for its study and protection. In this paper we discuss the character of social engineering, focus on the threats of information security brought by it, and give an elementary discuss on the test item in the Information Security Grade Test.
With the development of internet and information technology network security has become the hot spots of computer science. Effective monitoring operational status of every equipment is very important in the Local Area Network (LAN). This paper studies key techniques for monitoring LAN, such as process automatic loading and hidden, network covert communications, network data secure transmission, to solve the main problems in network monitoring which are operation obviously in monitoring client and information exchange with plaintext. First, the monitoring model is designed, and then the system structure is introduced. According to the testing results the system played the key role in network monitoring and could monitor operation status of network terminal effectively.
The exhaustion of IPv4 addresses on November 2011 has made the future of the internet in the IPv6 and raised new challenges in the network security research. This paper proposed a dual stack ipv4/ipv6 network testbed for dealing with the designation and implementation of an intelligent approach for malware detection in IPv6 networks. All the equipments, tools and network are configured based on real implementation of a dual stack ip4/ipv6 network. With fully functional operation for handling basic transition between IPv6 clients over IPv4 networks, the dual stack IPV4/IPv6 testbed is suitable for investigating the malware detection in real ipv6 networks. The experimental results from the testing phase show the efficiency and the functionality of the dual stack IPv4/IPv6 testbed.
This paper proposes one kind of active network security technique which combined artificial immune intrusion detection system (IDS) and firewall, it can omni-directional carry on protection to the computers and networks.
Malicious softwares or malwares for short have become a major security threat. While originating in criminal behavior, their impact are also influenced by the decisions of legitimate end users. Getting agents in the Internet, and in networks in general, to invest in and deploy security features and protocols is a challenge, in particular because of economic reasons arising from the presence of network externalities. An unexplored direction of this challenge consists in under- standing how to align the incentives of the agents of a large network towards a better security. This paper addresses this new line of research. We start with an economic model for a single agent, that determines the optimal amount to invest in protection. The model takes into account the vulnerability of the agent to a security breach and the potential loss if a security breach occurs. We derive conditions on the quality of the protection to ensure that the optimal amount spent on security is an increasing function of the agent's vulnerability and potential loss. We also show that for a large class of risks, only a small fraction of the expected loss should be invested. Building on these results, we study a network of interconnected agents subject to epidemic risks. We derive conditions to ensure that the incentives of all agents are aligned towards a better security. When agents are strategic, we show that security investments are always socially inefficient due to the network externalities. Moreover if our conditions are not satisfied, incentives can be aligned towards a lower security leading to an equilibrium with a very high price of anarchy.
Line current differential (87L) protection relies on communications for the exchange of current values and, if applied over asymmetrical channels, on external time sources for current alignment. Proper engineering of 87L schemes calls for a backup strategy that considers the loss of communications and/or the loss of external time sources. This paper reviews various channel and time backup strategies for 87L protection schemes and considers utility practices and regulatory constraints related to line protection redundancy, forced line outages, preferred balance between protection dependability and security, ability to provide adequate protection coverage with distance or overcurrent elements, and availability of an independent pilot channel for directional comparison backup schemes.
Intrusion detection is a challenging and critical problem in network security. Extensive research activities have been aimed at network-based intrusion detection systems, but most of them are proved unsatisfactory. This paper presents an effective intrusion detection classification model based on projection pursuit. With maximizing a projection index, Projection Pursuit uses Genetic Algorithm to search for the optimal projection direction, projects network connections records from high-dimensional space into 1-dimensional space, and uses the projection values to analyze the data structure and classify the type of intrusion. This intrusion detection classification model not only cuts down the computing complexity in the process of network connection records, but also opens out non-linear structure not like in latent semantics analysis only discovering linear structure, and the results of classification can also be visualized. The results of the experiments on KDD CUP 1999 data show that this model not only has good performance, but also reduces the number of false alarms effectively.
For WIFI networks in terms of user access and data transmission security, this paper analysis major security threat, explain the popular security technology. Propose comprehensive measures to resolve WIFI network security. Put forward WIFI network basic security configuration Program, and intermediate security configuration Program, and advanced security configuration Program. Aimed at helping WIFI network user to establish a secure network application platform.
Distributed Denial of Service (DDoS) attacks is very recent and popular devastating attack in the field of cyber society. Flooding DDoS attacks produce adverse effects for critical infrastructure availability, integrity and confidentiality. Current defense approaches cannot efficiently detect and filter out the attack traffic in real time. Online analysis of real time attack traffic and their impact and degradation of host and network based performance metrics becomes very essential. So, online measurement of these network performance metrics itself acts as an Intrusion detection system. The anomalies are the inference for network security analyst to suspect whether the network is under attack or not. Based on the assumption that the attacker flows are very aggressive than the legitimate users the proposed work provides sufficient bandwidth to genuine users during flooding DDoS attack. The Interface Based Rate Limiting (IBRL) algorithm proposed in this paper is used to mitigate the identified DDoS attacks. The implementation is carried out on an experimental testbed build up on Linux machines and Virtual routers. The experimental results show that there is considerable increase in the host and network based performance metrics for legitimate users even under DoS and DDoS attacks.
In conventional network security simply relies on mathematical algorithms and low counter measures to taken to prevent intrusion detection system, although most of this approaches in terms of theoretically challenged to implement. Therefore, a variety of algorithms have been committed to this challenge. Instead of generating large number of rules the evolution optimization techniques like Genetic Network Programming (GNP) can be used. The GNP is based on directed graph, In this paper the security issues related to deploy a data mining-based IDS in a real time environment is focused upon. We generalize the problem of GNP with association rule mining and propose a fuzzy weighted association rule mining with GNP framework suitable for both continuous and discrete attributes. Our proposal follows an Apriori algorithm based fuzzy WAR and GNP and avoids pre and post processing thus eliminating the extra steps during rules generation. This method can sufficient to evaluate misuse and anomaly detection. Experiments on KDD99Cup and DARPA98 data show the high detection rate and accuracy compared with other conventional method.
In this paper, we present our efforts at incorporating aspects of systems security and software security into the two-course senior capstone project sequence (Software Engineering and Senior Project courses) in the undergraduate Computer Science curriculum at Jackson State University. In this regard, we discuss: (i) the development of course modules and lab projects on security that were respectively taught and assigned to the students in the two capstone project course sequence, and (ii) the incorporation of aspects of systems and software security in the capstone projects that were conducted by our students during the academic year 2010-11. To further assist students on their capstone projects, we developed and offered two new elective courses on Systems and Software Security and Advanced Information Security in conjunction with the two capstone project courses during the Fall and Spring semesters as well as incorporated a network security module, lab projects and reading assignment to a regular Computer Networks course. We disseminate all of our course modules, lecture materials and lab project descriptions through an actively maintained website. Our approach could be adopted as a model for incorporating security in the software systems design and development courses in a computer science or software engineering curriculum.
Generally to secure a network from denial-of-service to Smurf attacks, hackers that perpetrate exploits, it is necessary to perform the tasks like Searching for multiple strings in packet payloads, approximate string matching, IP traceback via probabilistic marking, IP traceback via logging, detecting worms, etc. To execute the tasks there are many algorithms used. The search algorithms like Breadth First Search (BFS), Depth First Search (DFS), Depth First Iterative Deepening (DFID) etc are used to traverse any network like graph or a tree. The performance of one search algorithm may be better than the performance of any other search algorithms for search a particular node of a network. The algorithms, like BFS, DFS and DFID can be used to traverse any tree like network entirely. But it is necessary to determine a best suitable algorithm to search the goal node instead of using any one of them randomly. Then the identified algorithm can be used to traverse the network to reach the goal node.
Network Intrusion Detection is the most happening field of the network security research. It is a new kind of defense technology of the network security, used as a countermeasure to preserve data integrity and system availability during an intrusion. An ideal IDS system should be capable of evolving itself to identify not only known attacks but also unknown attacks. Algorithms based on Genetic Engineering and Immune Systems are known to evolve and learn from small examples. In this paper it is proposed to investigate the efficacy of genetic search methods for feature selection and Immune system to classify threats and non threats.
Computer Forensic process consists of Preparation, Acquisition, Preservation, Examination and Analysis, and Reporting. With the booming of the virtualization technology and the popularity of virtual machines for end users to deal with daily works, the probability of using virtual machines for malicious purposes keeps increasing. In this paper we propose a methodology by using virtual forensics for malware analysis and network forensics. Traditional forensics is done by using physical data. When company has large storage data and virtual environment, it creates a problem for traditional forensic while acquiring data. This paper proposes challenges, tools to be used, forensic techniques to be used and how to acquire data from cloud.
This paper presents the simulation results relevant to the 15.2.7 Working Package of the European SESAR Project<sup>1</sup>. The main goal was to conduct a risk assessment of network security for the AeroMACS airport network. The risk analysis is based on a new approach for network security assessment that measures quantitatively the network risk level. Critical aspects such as the impact of a successful attack on a node and the risk propagation of that attack within an aeronautical wireless airport communication network have been taken into account. We specifically focus on the access network vulnerabilities, and a first network risk study is conducted for a predefined scenario. Some security guideline are provided to enhance the security policies and to improve the end-to-end security using some additional mechanisms such as certificate-based authentication.
E-commerce has made great strides in providing a convenient, fast and secure shopping experience for consumers. However, there is still a significant portion of shoppers whose security fears impact how they spend their money online. Because of this, security issues associated with e-commerce and customer sites must be constantly reviewed and updated with appropriate countermeasures. As web security threats detrimentally affect the success of electronic consumerism, it is imperative to educate both consumers and businesses on the issues and how to eliminate or minimize the risks of security breaching in an e-commerce environment. This paper presents a survey and analysis on e-commerce related security issues, the impact to E-commerce success, and the available integrated security strategies. We attempt to offer a simple guide how to properly deal with the security threats that detrimentally affect e-commerce. In addition, this paper provides an analysis on the barriers that prevent many developing countries from adopting e-commerce. Some recommendations on how to overcome these problems will also be provided.
The 21st century is the era of the Internet, featured by rapid development and application of computer network technology. Security issues also present themselves accompanying the emergence of computer networks. To tackle these issues, we must come up with countermeasures. In this article, the author firstly analyzes the security and reliability issues existing in computer networks, and then proposes some opinions on how to deal with the current computer network security issues, aiming to discuss this together with extensive customers in order to enhance the awareness of computer network security protection.
The purpose of this paper is to overview the honeypot and honeynet technologies, based on thorough analysis of the deficiency of obtain employment information network's security their concealment and security, a design of high concealed and high safe honeypot system has to be implemented successful. In the end, this paper validates the systematic design by performing the experiments.
VoIP (Voice Over IP) was ranked third among the top 11 technologies of the decade in 2011. It is one of the most popular networking services. As it is readily adopted, the VoIP traffic is increasing steadily. The large amount of data transported by VoIP makes it ideal for creating covert channels. Attacks based on covert channels becomes a new challenge for network security. In this paper, possible covert channels via VoIP are analyzed, and an effective countermeasure to detect hidden messages in both SEQ (Sequence Number) and SSRC (Source Identifier) fields in the RTP protocol during conversation phase is proposed. This proposed method creates a new processing space, in which, normal traffic is analyzed and characterized by a proposed statistical model. This model is used in detecting hidden information in SSRCs and SEQs. Simulation results show that 100%detection rate can be realized. As the proposed model requires only a small amount of training data and no illegal traffic is used in the training, the computational complexity is small and can be used for on-line covert channel detection.
Network security has become a growing challenge in academic institutions. Higher education institutions are beginning to look at areas to improve the protection of their information technology structure due to the increase of viruses, spam, hackers, and identity theft. Educational institutions have the highest vulnerability to network security threats. Open access admission to college allows for open labs, limited staff, and resources which hinder security measures for those institutions.
The rapid proliferation of wireless ad-hoc networks and mobile computing applications has changed the landscape of network security. In this paper, we use hybrid approach that have both misuse and anomaly intrusion detection system using cross feature analysis for obtaining the trained data and test data from the trace file and to model the intrusion detection pattern. Apart from detection based on trace data, we proposed an innovative technique which operates through the implementation of battery consumption based detection on wireless ad-hoc networks by correlating attacks with their impact on device power consumption on the fly. The proposed system monitors power behavior to detect potential intrusions by noting irregularities of power consumption. This proposed and implemented work of the IDS using Network Simulator (NS-2) has achieved high detection rate and low false positive rate.
The paper deals with the problem of probabilistic error analysis of an encrypted transmission used within safety-related control system for applications with increasing safety integrity level (SIL). The requirements to cryptographic block code in safety-related communications for railway application are describe. The main part is oriented to description of mathematical apparatus for the error probability of the cryptography block code for the communication system on the end-to-end with GSM-R communication channel. The practical results are related with the quantitative evaluation of an average error probability of the cryptography code for several lengths of safety - related message which are expanded with determination of the cryptography degradation with using GMSK modulation scheme.
Static code attributes such as lines of code and cyclomatic complexity have been shown to be useful indicators of defects in software modules. As web applications adopt input sanitization routines to prevent web security risks, static code attributes that represent the characteristics of these routines may be useful for predicting web application vulnerabilities. In this paper, we classify various input sanitization methods into different types and propose a set of static code attributes that represent these types. Then we use data mining methods to predict SQL injection and cross site scripting vulnerabilities in web applications. Preliminary experiments show that our proposed attributes are important indicators of such vulnerabilities.
Users' mental models of security, though possibly incorrect, embody patterns of reasoning about security that lead to systematic behaviors across tasks and may be shared across populations of users. Researchers have identified widely held mental models of security, usually with the purpose of improving communications and warnings about vulnerabilities. Here, we implement previously identified models in order to explore their use for predicting user behavior. We describe a general approach for implementing the models in agents that simulate human behavior within a network security test bed, and show that the implementations produce behaviors similar to those of users who hold them. The approach is relatively simple for researchers to implement new models within the agent platform to experiment with their effects in a multi-agent setting.
The development of healthcare websites shows the era of tableware has been coming. These websites are not only the media of searching a general medical and health information but also the way of providing for measuring physiological signals of the patients. Healthcare websites are desirable for many enterprises, organizations, and health apparatus. However, the current healthcare services provided by the websites are similar, so repeating the same developments will waste manpower and money. Moreover, the individual websites without proper integration will result in the hazards of data exchange and the integration of all functions. In this paper, the authors propose the approach of an empirical study of blood pressure of U-healthcare portal websites. Based on premise of low-cost, we construct the U-healthcare portal websites can be ease to use, rapid to develop, and ease to maintain by the developers with information or related background, or caretakers. More important, such a site will be required to be with highly network security. Furthermore, the website is with the flexibility as well as the expansibility. to provide the merits of integration and circulation of the information. Thus, the proposed approach in this paper is aimed at the distributed U-healthcare websites to provide a feasible solution. For more practical applications, in the future research, one may cascade all the related U-healthcare websites for extending as the omnibus U-healthcare system.
IPsec as a very popular security protocol solves the increasing problems of network security. The IPsec-gateway cluster as a solution of large-scale IPsec implement improves the availability of IPsec-gateway. The traditional IPsec-gateway cluster needs to use a large number of hardware resources to keep availability of IPsec-gateway. However, the low utilization rate of resources restricts the scalability of IPsec-gateway cluster. In this paper, we propose a new IPsec-gateway cluster mechanism by improving and extending IKEv2 protocol. Meanwhile, we design a standby IPsec-gateway Selection Algorithm (GWSA), a distributed and switch SA policy (DSAP), ESP packets synchronous and retransmission policy. This mechanism can deploy IPsec-gateways in different network segments and prevent ESP packets loss when IPsec-gateway performs switching. Through simulation, we show that the above mechanism can improve the availability and scalability of IPsec-gateway cluster.
Nowadays, network intrusion detection systems (NIDSs) have become an essential part for the network security infrastructure. However, the large number of false alarms is a big problem for these detection systems which greatly reduces their effectiveness and efficiency. To mitigate this problem, we have developed an intelligent false alarm filter to help filter out false alarms by adaptively and periodically selecting the most appropriate machine learning algorithms (e.g., support vector machine, decision tree, k-nearest neighbor) that conduct the best single-algorithm performance. Therefore, our intelligent false alarm filter can keep reducing the number of false alarms at a high and stable level. In this paper, we aim to conduct a case study in exploring the performance of our developed false alarm filter by implementing a fuzzy classifier based on if-then rules. By comparing with other algorithms that have been implemented in our false alarm filter, the experimental results show that the if-then rules based fuzzy algorithm performs a bit better than the baseline algorithm and can be improved by selecting an appropriate fuzzy partition.
Cloud model is an effective tool for qualitative and quantitative transform, a specific structure generator is formed by normal cloud model through expectation, entropy and hyper entropy. This particular structure makes the normal cloud model has more general applicability and simply and straightforward completed conversion process between qualitative and quantitative. Since The frequency of network security incidents is nonlinear, traditional prediction methods network security incidents. which stands for the number such as ARMA and Gray systems are difficult to dea1 with of incidents per unit time. The cloud model used in the network security evaluation systems and the evaluation methods in the network security domain presented in this paper.
The popularity and adoption of smart phones has greatly stimulated the spread of mobile malware, especially on the popular platforms such as Android. In light of their rapid growth, there is a pressing need to develop effective solutions. However, our defense capability is largely constrained by the limited understanding of these emerging mobile malware and the lack of timely access to related samples. In this paper, we focus on the Android platform and aim to systematize or characterize existing Android malware. Particularly, with more than one year effort, we have managed to collect more than 1,200 malware samples that cover the majority of existing Android malware families, ranging from their debut in August 2010 to recent ones in October 2011. In addition, we systematically characterize them from various aspects, including their installation methods, activation mechanisms as well as the nature of carried malicious payloads. The characterization and a subsequent evolution-based study of representative families reveal that they are evolving rapidly to circumvent the detection from existing mobile anti-virus software. Based on the evaluation with four representative mobile security software, our experiments show that the best case detects 79.6&#x025; of them while the worst case detects only 20.2&#x025; in our dataset. These results clearly call for the need to better develop next-generation anti-mobile-malware solutions.
While many protocols for sensor network security provide confidentiality for the content of messages, contextual information usually remains exposed. Such contextual information can be exploited by an adversary to derive sensitive information such as the locations of monitored objects and data sinks in the field. Attacks on these components can significantly undermine any network application. Existing techniques defend the leakage of location information from a limited adversary who can only observe network traffic in a small region. However, a stronger adversary, the global eavesdropper, is realistic and can defeat these existing techniques. This paper first formalizes the location privacy issues in sensor networks under this strong adversary model and computes a lower bound on the communication overhead needed for achieving a given level of location privacy. The paper then proposes two techniques to provide location privacy to monitored objects (source-location privacy)periodic collection and source simulationand two techniques to provide location privacy to data sinks (sink-location privacy)sink simulation and backbone flooding. These techniques provide trade-offs between privacy, communication cost, and latency. Through analysis and simulation, we demonstrate that the proposed techniques are efficient and effective for source and sink-location privacy in sensor networks.
Firewalls are the mainstay of enterprise security and the most widely adopted technology for protecting private networks. As the quality of protection provided by a firewall directly depends on the quality of its policy (i.e., configuration), ensuring the correctness of firewall policies is important and yet difficult. To help ensure the correctness, we propose a systematic structural testing approach for firewall policies. We define structural coverage (based on coverage criteria of rules, predicates, and clauses) on the firewall policy under test. To achieve high structural coverage effectively, we have developed four automated packet generation techniques: the random packet generation, the one based on local constraint solving (considering individual rules locally in a policy), the one based on global constraint solving (considering multiple rules globally in a policy), and the one based on boundary values. We have conducted an experiment on a set of real policies and a set of faulty policies to detect faults with generated packet sets. Generally, our experimental results show that a packet set with higher structural coverage has higher fault-detection capability (i.e., detecting more injected faults). Our experimental results show that a reduced packet set (maintaining the same level of structural coverage with the corresponding original packet set) maintains similar fault-detection capability with the original set.
Network firewalls act as the first line of defense against unwanted and malicious traffic targeting Internet servers. Predicting the overall firewall performance is crucial to network security engineers and designers in assessing the effectiveness and resiliency of network firewalls against DDoS (Distributed Denial of Service) attacks as those commonly launched by today's Botnets. In this paper, we present an analytical queueing model based on the embedded Markov chain to study and analyze the performance of rule-based firewalls when subjected to normal traffic flows as well as DoS attack flows targeting different rule positions. We derive equations for key features and performance measures of engineering and design significance. These features and measures include throughput, packet loss, packet delay, and firewall's CPU utilization. In addition, we verify and validate our analytical model using simulation and real experimental measurements.
As a tool for network operators to recover network investment costs from network users as well as to provide forward-looking economic signals, distribution network pricing models are also expected to identify and recover investment costs related to maintaining network security. The existing models reflect network security by determining the maximum allowed contingency flow along each component through implementing deterministic contingency analysis. They fail to consider two reliability cost drivers: 1) reliability levels of network components, and 2) interruption tolerance levels at different nodes.
Research on networks is an area that we should watch more closely than any other. Perhaps more important than the borrowing of techniques, however, is paying close attention to the ferment over whether new network designs with security in mind are worth the societal price and effort to actually implement in a world that already has a considerable sunk investment in structure. How networks build themselves does have considerable influence on our field of practice.
Modern datacenter and enterprise networks require application identification to enable granular traffic control that either improves data transfer rates or ensures network security. Providing application visibility as a core network function is challenging due to its performance requirements, including high throughput, low memory usage, and high identification accuracy. This paper presents a payload-based application identification method using a signature matching engine utilizing characteristics of the application identification. The solution uses two-stage matching and pre-classification to simultaneously improve the throughput and reduce the memory. Compared to a state-of-the-art common regular expression engine, this matching engine achieves 380/0 memory use reduction and triples the throughput. In addition, the solution is orthogonal to most existing optimization techniques for regular expression matching, which means it can be leveraged to further increase the performance of other matching algorithms.
Contemporary network security applications generally require the ability to perform powerful pattern matching to protect against attacks such as viruses and spam. Traditional hardware solutions are intended for firewall routers. However, the solutions in the literature for firewalls are not scalable, and they do not address the difficulty of an antivirus with an ever-larger pattern set. The goal of this work is to provide a systematic virus detection hardware solution for network security for embedded systems. Instead of placing entire matching patterns on a chip, our solution is a two-phase dictionary-based antivirus processor that works by condensing as much of the important filtering information as possible onto a chip and infrequently accessing off-chip data to make the matching mechanism scalable to large pattern sets. In the first stage, the filtering engine can filter out more than 93.1%of data as safe, using a merged shift table. Only 6.9%or less of potentially unsafe data must be precisely checked in the second stage by the exact-matching engine from off-chip memory. To reduce the impact of the memory gap, we also propose three enhancement algorithms to improve performance: 1) a skipping algorithm; 2) a cache method; and 3) a prefetching mechanism.
Security Visualization is a very young term. It expresses the idea that common visualization techniques have been designed for use cases that are not supportive of security-related data, demanding novel techniques fine tuned for the purpose of thorough analysis. Significant amount of work has been published in this area, but little work has been done to study this emerging visualization discipline. We offer a comprehensive review of network security visualization and provide a taxonomy in the form of five use-case classes encompassing nearly all recent works in this area. We outline the incorporated visualization techniques and data sources and provide an informative table to display our findings. From the analysis of these systems, we examine issues and concerns regarding network security visualization and provide guidelines and directions for future researchers and visual system developers.
Aspirations for a boundless communication paradigm for future generation networks have changed the conventional way of looking at network security. With such a vision, security techniques should not only be securing local end-users but also be protecting entire networks from malicious adversaries. Unfortunately, providing security protection for such gigantic networks is a very challenging task. Based on the analogous relationships of various cell interactions in a metabolic network and a complex heterogeneous network, we explore the possibilities of human immune system (HIS) inspired anomaly detection for protecting interworked heterogeneous networks. In light of this, the applicability of two key schools of thought on how the HIS detects anomalies; namely, the Negative Selection (NS) and the Danger Theory (DT) are discussed in this article. While the NS theory is the traditional understanding of anomaly detection in the HIS, the DT is a radical new concept that challenges the main fundamentals of the NS theory, which is currently being hotly debated among immunologists. Based on this, the article highlights the applicability and the limitations of these two theories in detecting malicious anomalies in heterogeneous networks. From our analysis, we establish a doctrine that the DT outperforms its counterpart (the NS theory) in detecting anomalies in a complex heterogeneous network. Our analysis also indicates that the DT inspired anomaly detection is efficient in detecting malicious network adversaries and updating network segments that are under attack, thereby increasing the survivability of heterogeneous networks.
Welcome to the ninth installment of the Design and Implementation (D&I) Series. The D&I Series was created with a specific goal to facilitate knowledge transfer between scientists and industry-oriented engineers. In contrast to the classic peer-reviewed, academically oriented articles on communication technologies, D&I articles focus primarily on the practical Lessons Learned information that is gained while designing, implementing, and deploying new communications products, services, and solutions.
Capturing and replaying real flows are important for testing network security products. However, capturing real flows demands a high storage cost and runs a risk of capture loss, which makes the replay inaccurate. Replaying real flows should be accurate and stateful to adapt to the reaction of the device under test. It should also efficiently reproduce a defect and help developers identify the flows triggering defects. Therefore, this work first presents the (N, M, P) capture scheme which begins with, for each connection, capturing at most N bytes of application payload and then at most M bytes of application payload for at most each of the subsequent P packets in the same connection. This scheme reduces 87 percent of storage cost while retaining 99.74 percent of original events. This work develops a tool named SocketReplay with the mechanisms of loss recovery, stateful replay, and selective replay. Loss recovery tracks TCP sequence numbers to identify capture loss and recovers incomplete flows with dummy data. Stateful replay maintains the states in the TCP/IP stack to replay real flows. Selective replay incrementally selects flows to replay. The results show that SocketReplay can accurately and efficiently reproduce product events and significantly decrease the volume of replayed packet traces.
Mobile Ad hoc Networks (MANET) have been highly vulnerable to attacks due to the dynamic nature of its network infrastructure. Among these attacks, routing attacks have received considerable attention since it could cause the most devastating damage to MANET. Even though there exist several intrusion response techniques to mitigate such critical attacks, existing solutions typically attempt to isolate malicious nodes based on binary or na&#239;ve fuzzy response decisions. However, binary responses may result in the unexpected network partition, causing additional damages to the network infrastructure, and na&#239;ve fuzzy responses could lead to uncertainty in countering routing attacks in MANET. In this paper, we propose a risk-aware response mechanism to systematically cope with the identified routing attacks. Our risk-aware approach is based on an extended Dempster-Shafer mathematical theory of evidence introducing a notion of importance factors. In addition, our experiments demonstrate the effectiveness of our approach with the consideration of several performance metrics.
User identification is an important access control mechanism for client-server networking architectures. The concept of single sign-on can allow legal users to use the unitary token to access different service providers in distributed computer networks. Recently, some user identification schemes have been proposed for distributed computer networks. Unfortunately, most existing schemes cannot preserve user anonymity when possible attacks occur. Also, the additional time-synchronized mechanisms they use may cause extensive overhead costs. To overcome these drawbacks, we propose a secure single sign-on mechanism that is efficient, secure, and suitable for mobile devices in distributed computer networks.
Because the Internet has been widely applied in various fields, more and more network security issues emerge and catch people's attention. However, adversaries often hide themselves by spoofing their own IP addresses and then launch attacks. For this reason, researchers have proposed a lot of traceback schemes to trace the source of these attacks. Some use only one packet in their packet logging schemes to achieve IP tracking. Others combine packet marking with packet logging and therefore create hybrid IP traceback schemes demanding less storage but requiring a longer search. In this paper, we propose a new hybrid IP traceback scheme with efficient packet logging aiming to have a fixed storage requirement for each router (under 320 KB, according to CAIDA's skitter data set) in packet logging without the need to refresh the logged tracking information and to achieve zero false positive and false negative rates in attack-path reconstruction. In addition, we use a packet's marking field to censor attack traffic on its upstream routers. Lastly, we simulate and analyze our scheme, in comparison with other related research, in the following aspects: storage requirement, computation, and accuracy.
In recent years, there has been an increasing interest in the authentication process due to the key role that it has in the network security. Port Knocking (PKn) is an authentication method in which data transmits through the closed ports. This method is prone to attacks when attackers sniff the network. This paper proposes a new method which is called &#x201C;Secure Port Knock-Tunneling&#x201D; to eliminate both DOS-Knocking and NATK-nocking attacks. The possibility of implementation of this method is investigated on the Mikrotik devices.
Forensic research and tools for consumer-grade routers is a relatively unexplored area. This is surprising considering the amount of consumer-grade routers currently in use around the world. This paper describes a consumer-grade router and network forensics tool called WiFi Stakeout. This tool is in the early design stages but will be capable of being used by forensic investigators to extract router and device network information from a consumer-grade router or network.
Mobile agents are used with an increasing trend in network security, QoS, routing, network monitoring and management applications. The platforms such as IBM Aglets, JADE, Mobile-C are commonly used in mobile agent applications. These platforms give support for only IPv4 networks. However, they don't support IPv6 networks. In this study, Mobile-C platform which was developed for IPv4 network structure is modified to support IPv6 networks. Sending an agent, agent movement and data collection tests with Mobile-C platform has been carried out successfully in a real IPv6 network.
This paper provides an introduction and overview of power system real-time data encryption under network environment, describes the encryption requirements of power system real-time data communication, and briefly introduces a new type of a security-enhanced power system data encryption scheme. The QKD-based encryptions can delivering end-to-end network security via high-speed quantum key distribution, and against sophisticated eavesdropping attacks. QKD will be a most promising way of real-time data encryption in power system in the future.
Diffie-Hellman (DH) symmetric encryption plays an important role in the network security, and the security proof for many protocols relies on the DH assumption. The Logic of Local Sessions (LLS) is a practical security protocol logic. With its automatic tool Security Protocol Verifier (SPV), LLS can verify many interesting properties for complex security protocols. However, It does not discuss the DH key exchange and the related properties. In this paper, we extend the original LLS with DH exponentiation. After revising some primary concepts and definitions, a new theorem called DH-Secrecy Properties is proposed and proved. Avoiding influencing the proof of the former axioms, our extension enlarges the range of applicability of LLS.
As the HVDC power transmission project in China increases gradually, the network security of HVDC control and protection system has become more and more important. This article provides an analysis on threats and requirements for the network security of the control and protection system, and combining the character of the network structure of the control and protection system, give the corresponding system security policy and information security policy.
Information security metrics are very important to guide the direction for measuring the effectiveness of security controls in compliance with the information security standards. However, lack of method to guide organization in choosing the technical security metrics may cause technical security control objectives and capabilities failed. This research proposes a model of technical security metrics to measure the effectiveness of network security management, such as network security controls and services such as firewall and Intrusion Detection Prevention System (IDPS) in the protection of Supervisory and Data Acquisition (SCADA) systems. The methodology used is Plan-Do-Check-Act process model. The proposed technical security metric provides guidance for SCADA owners in complying with requirements of ISO/IEC 27001 Information Security Management System (ISMS) standard. The proposed model should be able to provide a comprehensive measurement and prove the effectiveness of ISO/IEC 27004 ISMS Measurement standard.
Intrusion Detection System (IDS) have become increasingly popular over the past years as an important network security technology to detect cyber attacks in a wide variety of network communication. IDS monitors' network or host system activities by collecting network information, and analyze this information for malicious activities. Cloud computing, with the concept of Software as a Service (SaaS) presents an exciting benefit when it enables providers to rent their services to users in perform complex tasks over the Internet. In addition, Cloud based services reduce a cost in investing new infrastructure, training new personnel, or licensing new software. In this paper, we introduce a novel framework based on Cloud computing called Cloud-based Intrusion Detection Service (CBIDS). This model enables the identification of malicious activities from different points of network and overcome the deficiency of classical intrusion detection. CBIDS can be implemented to detect variety of attacks in private and public Clouds.
In the recent years one of the most focused topics in the field of network security and more specifically intrusion detection systems was to find a solution to reduce the overwhelming alerts generated by IDSs in the network. Inspired by human defence system and danger theory we propose a complementary subsystem for IDS which can be integrated into any existing IDS models to aggregate the alerts in order to reduce them, and subsequently reduce false alarms among the alerts. After evaluation using different datasets and attack scenarios, our model managed to aggregate the alerts by the average rate of 97.5 percent.
Robotic systems have inherently nonlinear phenomena as joints undergo sliding and/or rotating. This in turn requires that the system running states be predicted correctly. This paper makes a full analysis of the robot states by applying observer-based adaptive wavelet neural network (OBAWNN) tracking control scheme to tackle these phenomena such as system uncertainties, multiple time-delayed state uncertainties, and external disturbances such that the closed loop system signals must obey uniform ultimate boundedness and achieve H<sup>&#x221E;</sup> tracking performance. The recurrent adaptive wavelet neural network model is used to approximate the dynamics of the robotic system, while an observer-based adaptive control scheme is to stabilize the system. The advantage of employing adaptive wavelet neural dynamics is that we can utilize the neuron information by activation functions to on-line tune the hidden-to-output weights, and the adaptation parameters to estimate the robot parameters and the bounds of the gains of delay states directly using linear analytical results. It is shown that the stability of the closed-loop system is guaranteed by some sufficient conditions derived from Lyapunov criterion and Riccati-inequality. Finally, a numerical example of a three-links rolling cart is given to illustrate the effectiveness of the proposed control scheme.
The rapid development of the blooming city requires to expand the government healthcare services for the public because of increasing inhabitants. Additional temporary government healthcare service is set up within a university having an operating hospital in order to solve the urgent requests of the mass amount of the inhabitant. This has resulting resources (medical equipment and IT services) sharing by these healthcare entities but independent operations should be maintained. In order to keep the privacy and security of the digital data and patient data, this paper proposes a method of creating a new subnet as common network area for sharing the resources, while maintaining their independencies at the same time. We have added the network security appliances (firewalls) connecting to the healthcare entities. Through the features of network address translation (NAT), network policy and access control list, this common subnet with hybrid connection (university hospital network and the government hospital network) can accessed by these entities via the specified IP addresses. As a result, the existing IP addresses for each healthcare entity network can be retained, and providing the secured method to grant the access right to the common area to share the digitalized medical resources. In order to protect the security of the cooperated hospital's internal networks, unauthorized traffic into and out of the subnet is blocked or restricted by firewalls as per policies configured.
With the rapid development and wide application of information technologies, enterprise network security is particularly important. IEEE802.1x standard is now widely used in the industry for a secure access technologies, this article on 802.1X agreed principles and certification process for analysis and presentations, and combined with access to Symantec's security technology, fast, secure and reliable Internet access environment.
In this paper, we analyzed requests for consultation about the Internet troubles given to a regional network security center called Net An shin Center(NAC), discussing how to rescue victims involved in the troubles and what kind of educational and enlightenment activities about information moral not to become such victims should be performed.
Intrusion detection technology is an active defensive security technology. It is an important part of the network security system. It can make up for the lack of traditional security technology. In this paper, we has all-round systematically elaborate on the intrusion detection technology in the multiple aspects at the concept, function, method, principle. As the result, the research of the intrusion detection technology will become the hot spot in the field of network security. In this paper, it provides a technical and theoretical basis for the further research of intrusion detection technologies.
In order to combine the advantage of quantitative and qualitative analysis, a scheme is put forward to assess network security situation by using Honeynet. Firstly defines various network security assessment concepts, and make a quantitative model for network security situation assessment using virtual honeynet. With the relationships among honeynet active, host active of computer networks and IP active when network intrusion occurs, a binary linear regression prediction model is proposed. In the end, a prototype system is designed and performed experiments, and the validity of the scheme is proved.
The paper deals with the need to enhance security of the VoIP infrastructure. There are several ways to achieve an enhancement in security. We opted for the honeypot which can provide us information about attacker's behaviour. We will examine in particular a VoIP honeypot referred to as Artemisa. It is one of many existing honeypots tailored for IP telephony. The paper describes its function and application within a real IP telephony infrastructure. The aim of another tested honeypot is to gather data about the attacks while simulating a SSH server. The gathered information could be crucial for further improvements of the existing security mechanism in our VoIP network.
Hundreds of millions of webusers access to or use a variety of network services via various digital identity credentials every day. Network security issues such as Identity authentication, privacy and information sharing have become focuses of IT industry. To strengthen network security management and risk prevention, some of China's large-scale sites have launched real name registration policy, indicating that digital identity gradually stepping into normalization. This paper intends to explore ways of encouraging the establishment of digital identity authentication system.
This paper shows a intrusion detection model that combines the BP networks and the expert system aiming at the bad effect of a single detection model .This model uses the improvement the BP neural network, simultaneously through the similar expert solves the actual problem inference mechanism, creat a neural network expert system model. The experiment simulation show that this model has less iteration times, quicker convergence rate?higher detection rate and sufficient availability, at present mainly applied in the mine network security.
Academic researchers in digital forensics often lack backgrounds in related laws. This ignorance could make their research and development legally invalid, or with less relevance in practice. To better assist academic researchers, we discuss related laws that regulate the government's investigation and summarize different requirements of acquiring data and evidence in different crime scene investigations. We show that certain strategies (including attacks against security systems) would violate relevant laws, and so law enforcement cannot use them to collect data. We recommend that researchers focus on crime scene investigations that do not need Warrant/Court Order/Subpoena for trace back related network forensics. This would help make their research and development accepted more easily by law enforcement with a larger impact.
The need for network traces has always been a critical element for the success of network, and network security, research. However, the plethora of privacy, legal and policy issues has often prevented access to collected traces. This has created the need for developing anonymization methods and tools to protect the privacy of the released traces while preserving utility in the data. A key dilemma in anonymizing network traces is whether to preserve IP pseudonym consistency, i.e., whether the same IP address is replaced by the same pseudo IP. On one hand, globally-consistent prefix-preserving IP address anonymization is subject to various privacy attacks. On the other hand, many usages of the trace data require some levels of consistency. We solve this dilemma by observing that a better privacy-utility tradeoff can be obtained by maintaining temporal pseudonym consistency. That is, we divide flows into buckets based on temporal closeness, and anonymize the flows within each bucket separately such that pseudonym consistency is maintained within each bucket, but broken across buckets. We present a new anonymization method based on these insights. Furthermore, our experimental results show that our method provides the needed privacy protections with little adverse effects on the utility of the trace.
Recently Network traffic anomaly detection has become a popular research tendency, as it can detect new attack types in real time. The real-time network traffic anomaly detection is still an unsolved problem of network security. The network traffic appears as a complex dynamic system, precipitated by many network factors. Although various schemes have been proposed to detect anomalies, they are mostly based on traditional statistical physics. In these methods, all factors are integrated to analyze the variation of the network traffic. But in fact, the changing trend of network traffic at some moment is only determined by a few primary factors. In this paper, we present a non-statistical network traffic anomaly detection method based on the synergetic neural networks. For our method, a synergetic dynamic equation based on the order parameters is used to describe the complex behavior of the network traffic system. When the synergetic dynamic equation is evolved, only the order parameter determined by the primary factors can converge to 1. Therefore, the network traffic anomaly can be detected by referring to the primary factors. We evaluate our approach using the intrusion evaluation data set of the network traffic provided by the defense advanced research projects agency (DARPA). Experiment results show that our approach can effectively detect the network anomaly and achieve high detection probability and low false alarms rate.
Motivated by the increasing need for developing automated decision-support tools for cyber-physical networks subject to uncertainties, we have been pursuing development of a new control-theoretic framework for network security and vulnerability. In this paper, we build on the proposed framework to put forth concrete definitions for security and (dually) discoverability, for a class of models that can represent dynamics of numerous cyber-physical networks of interest: namely, dynamical network spread models. These security and discoverability definitions capture whether or not, and to what extent, a stakeholder can infer the temporal dynamics of the spread from localized and noisy measurements. We then equivalence these security and security-level definitions to the control-theoretic notions of observability and optimal estimation, and so obtain explicit algebraic and spectral conditions for security and analyses of the security level. Further drawing on graph-theory constructs, a series of graphical conditions for security, as well as characterizations of security levels, are derived. A case study on zoonotic disease spread is also included, to illustrate concrete application of the analyses in management of cyber-physical infrastructure networks.
The smart grid is characterized by the twoway flow of electric power and information. For the information flow implementation and support, several wireless communication technologies and standards are being considered. Although there is no doubt that using wireless communications offers significant benefits over wired connections, the wireless technology introduces additional vulnerability in terms of network security. This work addresses physical layer security, a topic that has been hardly investigated in the smart grid domain. To understand new types of threats, we review fundamentals of wireless communication and examine physical attack models in depth. As a promising solution to physical security, we describe a random spread-spectrum based wireless communication scheme that can achieve both fast and robust data transmission. We expect that the work presented here will advance the research on wireless smart grid security.
Most networks today employ static network defenses. The problem with static defenses is that adversaries have unlimited time to circumvent them. This article proposes a moving-target defense based on the Internet Protocol version 6 (IPv6) that dynamically obscures network-layer and transport-layer addresses. This technique can be thought of as "frequency hopping" in the Internet Protocol space. By constantly moving the logical location of a host on a network, this technique prevents targeted attacks, host tracking, and eavesdropping. The authors demonstrate the design's feasibility and functionality using prototypes deployed on Virginia Tech's campuswide IPv6 network.
Recent advances in microelectromechanical system (MEMS) technology have boosted the deployment of wireless sensor networks (WSNs). Limited by the energy storage capability of sensor nodes, it is crucial to jointly consider security and energy efficiency in data collection of WSNs. The disjoint multipath routing scheme with secret sharing is widely recognized as one of the effective routing strategies to ensure the safety of information. This kind of scheme transforms each packet into several shares to enhance the security of transmission. However, in many-to-one WSNs, shares have high probability to traverse through the same link and to be intercepted by adversaries. In this paper, we formulate the secret-sharing-based multipath routing problem as an optimization problem. Our objective aims at maximizing both network security and lifetime, subject to the energy constraints. To this end, a three-phase disjoint routing scheme called the Security and Energy-efficient Disjoint Route (SEDR) is proposed. Based on the secret-sharing algorithm, the SEDR scheme dispersively and randomly delivers shares all over the network in the first two phases and then transmits these shares to the sink node. Both theoretical and simulation results demonstrate that our proposed scheme has significant improvement in network security under both scenarios of single and multiple black holes without reducing the network lifetime.
The network technology, multimedia technology and communication technology are widely used in all kinds of education areas. The network-teaching is a novel application result for higher education in the information age. However, it is an increasingly outstanding problem for the management and security in the resource library of the network teaching and education. By the characteristics of chaotic theory and encryption, in this paper, a kind of secure communication system based on spatiotemporal chaos is designed and exploited for solving the security protection problem of network-teaching video resource. By using Visual C++ platform, an illustrative example is realized and its experimental results demonstrate that the online video-teaching resource is protected very well during the encrypted transmission based on the spatiotemporal chaotic technology.
This paper designs a novel sampling data preprocessing subsystem in integrated protection based on IEC61850. This subsystem will be used to solve the problems of data redundancy processing and bad value processing. A framework structure of integrated protection software system based on the concept of module distributed is firstly presented, and then the details of preprocessing subsystem are given. This subsystem is the most important foundation of integrated protection. It can provide a platform to analyze and validate the protection arithmetic. Meanwhile, the subsystem will reach the requirement for security, real-time and reliability.
A microprocessor-based relay protection unit has strong self-test capability that allows the device to monitor operation conditions and diagnose problems by itself. Based on the characteristics of microprocessor-based protection software and hardware architecture, this paper proposed an online health monitoring methodology using the self-test information of protection units. In accordance with expression modes and acquisition methods, self-test Information of protection equipment can be divided into following four categories, state self-test information, analog self-test information, description self-test information and statistics self-test information. Considering all factors affecting the health status of protection units, this paper established a generic evaluation model of health status. The master station for condition monitoring collects self-test information and evaluates the health status of protection unit online, and provides guidance for condition-based maintenance.
As an essential and significant part of network security, the security of web applications has received more and more attentions at present. In this paper, we review the security of current web applications, and enumerate the most common attacks on them such as injection, cross site scripting, and insecure direct object references. Then by taking injection attack as an example, we explain the principles of injection attack and analyze the reasons for the vulnerability. Finally, to prevent these attacks, we provide several valuable suggestions.
This paper surveys the field of security separation technology from a network security perspective. The basic idea of security separation is to use physical or logical measures to protect computers with sensitive information from external or internal threats. The paper first discusses two basic concepts of security separation, i.e., end device separation and network separation. After that, it introduces four different techniques and products to implement the separation idea, and presents the primary characteristics of them. Finally, it makes a conclusion and provides some suggestions for future work.
Computer network and communication technology have already been widely used in all aspects of our daily life, but the network security problem is getting more and more prominent, while the network wiretap is one of the biggest challenges of the network security. With the security problem caused by network wiretap becoming more serious, how to monitor the network wiretaps has been the primary problem most researchers have been faced with. By using the ARP cheats to monitor network is a kind of quick and convenient method, which is the topic of this thesis as well.
With the development of network security technical, the pivotal factor is not technique but the administrator and management mechanism. In the domain of information security, social engineering can get into information system by using people' s weakness, just not using the computer's leak. This method can be used in many attacking way and turn up a new important project for its study and protection. In this paper we discuss the character of social engineering, focus on the threats of information security brought by it, and give an elementary discuss on the test item in the Information Security Grade Test.
With the development of internet and information technology network security has become the hot spots of computer science. Effective monitoring operational status of every equipment is very important in the Local Area Network (LAN). This paper studies key techniques for monitoring LAN, such as process automatic loading and hidden, network covert communications, network data secure transmission, to solve the main problems in network monitoring which are operation obviously in monitoring client and information exchange with plaintext. First, the monitoring model is designed, and then the system structure is introduced. According to the testing results the system played the key role in network monitoring and could monitor operation status of network terminal effectively.
Contemporary network services don't have any statistical ranking mechanism for proactive security management. Since the emerging threats are actively exploiting the vulnerabilities in network services to compromise the system, not much attention has been paid to rank these services based on their vulnerability history. We argue in this paper that a reliable mechanism could be used to rank these services based on their vulnerability history. Such ranking will be significantly helpful for proactive network security management to partition services and deploy security countermeasures. We propose a framework using stochastic order alternatives to statistically rank network services based on time intervals between exploits as reported by National Vulnerability Database (NVD). We show that Statistical techniques can be used to rank these services by modeling the related metrics. We validated our technique using products of known ranking, and presented some case studies to confirm our result on real network services.
Low-interaction honeypots can provide a cost-effective security mechanism for a wide range of computer systems. A central challenge in the development of low-interaction honeypots is the development of emulation programs that mimic the action of server applications on the target platform. The emulation programs must be of high enough fidelity to fool attackers. However, the manual development of these emulations is extremely time-consuming. This paper describes an agent-based optimization system that can automate the generation of emulation programs for honeypots. The system is evaluated in its ability to emulate a mail server. In this evaluation, the system produced correct responses to more than 99%of test data queries.
This paper describes a process for analyzing web server requests and responses for use in developing emulation programs for low interaction honeypot responses. In order to manage the complexity of this task, this approach first identifies that static and dynamic resources made available from a web server. Then, for each dynamic resource a fuzzing approach was used to generate and analyze the range of responses for a range of requests, including ones with the types of malformed input that can appear in computer security attacks.
Intrusion Detection is an important and classical research area in network security. It is observed that existing intrusion detection methods usually research all data in the network as a whole. However, in reality, data in the network can be categorized into two types: upward IP data and downward IP data. These two types of IP data may play different roles in intrusion detection process. Based on this observation, a novel intrusion detection method called Duplex Traffic Joint Analyzing(DTJA) method is proposed so as to consider both upward and downward IP data more specifically. With this method, intrusion clues can be found more effectively and efficiently. Experiment results indicate this method is feasible.
The increasing number of intrusions and data thefts on online systems is one of the triggers of the growing concern about security inside organizations. Nowadays, dynamic and extensible detection tools are required and critical to detect and diagnose vulnerabilities in Web systems. In this paper we present the development and evaluation of a vulnerability scanner for online systems. Unlike most existing tools, it is free and open source, available at SourceForge, and has a modular and extensible architecture. The achieved results show that the proposed tool, called Uniscan, is able to better detect and diagnose vulnerabilities such as LFI, RFI and RCE.
This paper addresses the problem of indoor location for devices that are connected to WLAN. Nowdays most of the smart terminals: mobile phones, portable computers and tablets are WLAN enabled. Location of these devices is of interest to network security issues as well as employee and company security and reliability. Most of the algorithms on the market use triangulation as a basic rule for location of the devices, when distance between acces point and mobile user is estimated. Using advanced signal processsing tehniques of the baseband OFDM signal, we propose a solution that gives reliable estimates of the distance at reasonable computational cost.
The latest trends in the development of high interaction honeypots show that adaptive honeypots, which lure attackers by changing their behavior, are a feasible solution for gathering of as much information as possible about them. Adaptive Honeypot systems based on Game theory are in a development stage and the systems created until now are focused mostly on applying game-theoretic concepts for the configuration and reciprocal actions of high-interaction honeypots [1]. The paper presents a tested proof of concept system that integrates dynamic taint analysis with an existing adaptive honeypot in order to detect the rootkit malware that the attacker installs.
The onion routing network Tor is undoubtedly the most widely employed technology for anonymous web access. Although the underlying onion routing (OR) protocol appears satisfactory, a comprehensive analysis of its security guarantees is still lacking. This has also resulted in a significant gap between research work on OR protocols and existing OR anonymity analyses. In this work, we address both issues with onion routing by defining a provably secure OR protocol, which is practical for deployment in the next generation Tor network. We start off by presenting a security definition (an ideal functionality) for the OR methodology in the universal compos ability (UC) framework. We then determine the exact security properties required for OR cryptographic primitives (onion construction and processing algorithms, and a key exchange protocol) to achieve a provably secure OR protocol. We show that the currently deployed onion algorithms with slightly strengthened integrity properties can be used in a provably secure OR construction. In the process, we identify the concept of predictably malleable symmetric encryptions, which might be of independent interest. On the other hand, we find the currently deployed key exchange protocol to be inefficient and difficult to analyze and instead show that a recent, significantly more efficient, key exchange protocol can be used in a provably secure OR construction. In addition, our definition greatly simplifies the process of analyzing OR anonymity metrics. We define and prove forward secrecy for the OR protocol, and realize our (white-box) OR definition from an OR black-box model assumed in a recent anonymity analysis. This realization not only makes the analysis formally applicable to the OR protocol but also identifies the exact adversary and network assumptions made by the black box model.
Security countermeasures help ensure information security: confidentiality, integrity and availability(CIA), by mitigating possible risks associated with the security event. Due to the fact, that it is often difficult to measure such an impact quantitatively, it is also difficult to deploy appropriate security countermeasures. In this paper, we demonstrate a model of quantitative risk analysis, where an optimisation routine is developed to help a human decision maker to determine the preferred trade-off between investment cost and resulting risk. An offline optimisation routine deploys a genetic algorithm to search for the best countermeasure combination, while multiple risk factors are considered. We conduct an experimentation with real world data, taken from the PTA(Practical Threat Analysis) case study to show that our method is capable of delivering solutions for real world problem data sets. The results show that the multi-objective genetic algorithm (MOGA) approach provides high quality solutions, resulting in better knowledge for decision making.
Internet phishing attack is increasing and causing enormous economic loss in recent years. The network security literacy learning for educating students to defense phishing attack becomes more important. However, with the growing of attacking tricks, the anti-phishing knowledge is continuously increasing. The traditional learning platform with antiphishing technical documents is not sufficient for students to prevent the phishing attacks. To transform the continuously increasing anti-phishing knowledge to learning materials, the Bloom's taxonomy of cognitive domain was used to classify anti-phishing knowledge into different levels of learning skills. Since traditional anti-phishing documents are difficult to read for students, the Anti-phishing Education Game was developed to provide an interactive narrative learning environment for learning by doing. Thus, the game missions can be adaptively provided to students based on their learning achievements and engage students' attention. The experimental result shows that students have significant progress in identifying phishing page and high learning motivation by using our approach.
Honeypot is a closely monitored computer resource that emulates behaviors of production host within a network in order to lure and attract the attackers. The workability and effectiveness of a deployed honeypot depends on its technical configuration. Since honeypot is a resource that is intentionally made attractive to the attackers, it is crucial to make it intelligent and self-manageable. This research reviews at artificial intelligence techniques such as expert system and case-based reasoning, in order to build an intelligent honeypot.
With the Internet spread and deepening of the application. Internet and e-commerce business development has become an inevitable trend. Network security is more and more attention by people. Therefore, the use of an effective network security information filtering systems, real-time monitoring of network information is very necessary. We discuss Network Information Filtering System mainly, filtering methods, key technology.
Evaluation of campus network security is a complicated systematic project. The paper analyzes the reasons of the questions, obtains the solving method, and at last builds the M(1,2,3) model without the interference of redundant data, which is different from )M(g+) and is a nonlinear model. The paper will apply the new algorithm in the fuzzy evaluation on campus network security.
We introduce one Double-Reduct approach to get key rules of data set in this paper. Adopting rule usefulness as a measure of key rules, our approach greatly reduces the rule numbers comparing to the traditional method. Through an experimental study of computer information security in prison, this approach is proved to be feasible and effective.
As the network technology and applications continue to evolve, computer networks become more and more important. However, network users can attack the network infrastructure (such as domain name service and routing services, etc.). The networks can not provide the minimum required quality of service for control. Network situation can not be aware in a timely manner. And network maintaining and upgrading are not easy. We argue that one root cause of these problems is that control, management and forwarding function are intertwined tightly. We advocate a complete loosing of the functionality and propose an extreme design point that we call "3N", after the architecture's three separated networks: forwarding network, control network and management network. Accordingly, we introduce four network entities: forwarder, controller, manager and separators. In the 3N architecture, the forwarding network mainly forwards packets at the behest of the control network and the management network, the control network mainly perform route computation for the data network, and the management network mainly learn about the situation of the data network and distribute policies and configurations, and the three networks working together to consist a efficient network system. In this paper we presented a high level overview of 3N architecture and some research considerations in its realization. We think the 3N architecture is helpful to improve network security, availability, manageability, scalability and so on.
We reviewed the confidentiality and authentication for multi-party applications. A conceptual model of a secure multicast network has been presented by adding a quantum key distribution (QKD) network to its architecture. The model is divided into three phases, the first phase, QKD network generate secure keys to encrypt not only data but also authentication process. The second phase, authentication making sure that no one except the legitimate user receive the message and making sure that nobody except the legitimate sender writes or modifies it. In the last phase, Multicast network encrypt the data by secure key and send it to receivers. The proposed scheme amid to provide a secure end to end communication for multicast network.
In this paper we introduce the architecture and system platform of a new regular expression processor for next generation security platforms for content awareness and network security processing. The paper first outlines the feature requirements of state-of-the-art network and security systems, then presents the proposed content processing system and processor architecture.
This paper provides an introduction and overview of power system real-time data encryption under network environment, describes the encryption requirements of power system real-time data communication, and briefly introduces a new type of a security-enhanced power system data encryption scheme. The QKD-based encryptions can delivering end-to-end network security via high-speed quantum key distribution, and against sophisticated eavesdropping attacks. QKD will be a most promising way of real-time data encryption in power system in the future.
Diffie-Hellman (DH) symmetric encryption plays an important role in the network security, and the security proof for many protocols relies on the DH assumption. The Logic of Local Sessions (LLS) is a practical security protocol logic. With its automatic tool Security Protocol Verifier (SPV), LLS can verify many interesting properties for complex security protocols. However, It does not discuss the DH key exchange and the related properties. In this paper, we extend the original LLS with DH exponentiation. After revising some primary concepts and definitions, a new theorem called DH-Secrecy Properties is proposed and proved. Avoiding influencing the proof of the former axioms, our extension enlarges the range of applicability of LLS.
As the HVDC power transmission project in China increases gradually, the network security of HVDC control and protection system has become more and more important. This article provides an analysis on threats and requirements for the network security of the control and protection system, and combining the character of the network structure of the control and protection system, give the corresponding system security policy and information security policy.
We propose a method on safe data transmission for the embedded system, which is convenient and of high computational efficiency. This method not only removes the redundancy by re-encoding the not frequently changed information between two adjacent data, but also encrypts the frame by Rivest-Shamir-Adleman (RSA) algorithm after scrambling it with a random matrix. The experimental results show a good security of data transmission and a certain compression effect. Due to the slow variance in the collected data of embedded system and the existence of large historical data redundancy, the method mentioned above is more suitable for the embedded system than the conventional methods.
Nowadays the security issues of Campus Network become more sharp and urgent, in order to improve the initiative of Campus Network security protection and the validity and pertinency of attacking information collection, this paper presents a new proactive security technology named honeypot to expand the network topology space and confuse the attacker, delay attacking and distract target, exhaust resources, tempt and trap hackers, and track, record and analyze hacker's actions comprehensively, and then get acquainted with the serious intrinsic internal and external threats with which the Campus Network is being confronted currently and the common attack tools, methods and rules, so as to amend the network security architecture according to specific situations, to revised security management principles of all levels, to adjust the firewall configuration to enhance the holistic security of Campus Network.
Although system administrators are frequently urged to protect the machines in their network, the fact remains that the decision to mandate protection is far from universal. To better understand this decision, we formulate a model of interdependent network security where there is a system administrator responsible for a network of size n against autonomous attackers attempting to penetrate the network and infect the network machines with viruses or other exploits. We introduce the concept of a loss profile, which encapsulates the idea of variable loss due to infection. Through the application of a simple loss profile to this interdependent network security scenario, we conclude that the decision is dependent upon a number of factors including external and internal vulnerabilities, the types and likelihoods of different amounts of loss, and the interaction of all of these effects. Through this analysis, we form a model for decision-making that is simple to understand and applicable to many other interdependent security scenarios.

The shift towards cyber operations represents a shift not only for the defense establishments worldwide but also cyber security research and education. Traditionally cyber security research and education has been founded on information assurance, expressed in underlying subfields such as forensics, network security, and penetration testing. Cyber security research and education is connected to the homeland security agencies and defense through funding, mutual interest in the outcome of the research, and the potential job market for graduates. The future of cyber security is both defensive information assurance measures and active defense driven information operations that jointly and coordinately are launched, in the pursuit of a cohesive and decisive execution of the national cyber defense strategy. The cohesive cyber defense requires universities to optimize their campus wide resources to fuse knowledge, intellectual capacity, and practical skills in an unprecedented way in cyber security. The future will require cyber defense research teams to address not only computer science, electrical engineering, software and hardware security, but also political theory, institutional theory, behavioral science, deterrence theory, ethics, international law, international relations, and additional social sciences. This paper is the result of an ocular survey of the U.S. 48 academic CAE-R research centers, evaluating the collective group of research centers' ability to adapt to the shift towards cyber operations, and the challenges therein.
A surge of applications of p2p technology brings in problems such as the bandwidth shortage, internet piracy, and network security and so on. Accurate Identification of P2P traffic grows more important in effective network management. In this paper, a new P2P identification method based on compound characteristics is proposed. The accuracy of traditional identification methods will vary with different P2P protocols. Against the defect of the P2P identification, three statistical characteristics are adopted through analyzing the various characteristics of different flows. With this method, the payload information has not to be checked, and Identification results will not be affected by data encryption. Finally, we construct a p2p identification system using the compound characteristics. Performance of the system proved that the proposed method can not only identify much more P2P traffics, but also have higher recognition rate, lower false positives and failure rate.
Security audit plays an important role in information system, but currently there are various deficiencies in the security audit systems. This paper elaborates the concept of compliance audit, and then introduces the system architecture and components of a log-based network security audit system for compliance. In the end, the detailed design of key components and technologies used are emphasized. Application results indicate that this system achieves the compliance requirements and can effectively enhance the security of the system being audited.
You are the host of many electrical contractors, and their electrical/non-electrical subcontractors who work at your sites on a daily basis. During your large planned outages it is necessary to bring in additional electrical/non-electrical contractors to perform maintenance & repairs on your sites electrical equipment. You are subject to OSHA's Multi-Employer Worksite Policy CPL 02-00-124 and have a completely different set of responsibilities and risks to mitigate. You need to start asking questions and obtaining data from each contractor to protect your company and yourself. You must know what your site hazards are, as well as how your contractor plans to mitigate those hazards. This paper will provide an overview of OSHA's Multi-Employer Worksite Policy CPL 02-00-124. In addition details will be provided on how to prevent regulatory multi-employer worksite issues with contractors that routinely work at your site, as well as those contractors who periodically work on site.
In response of the fact that traditional intrusion detection systems are not able to fulfill the requirements for specific network security, such as fast processing speed, stronger defense capability, and higher real-time performance, a model of network security defense is built on the integration of data stream mining and intrusion detection; and, a data stream clustering algorithm is designed for mining in the model. Through analysis and simulation, the model turns out to be higher in detection rate and lower in false-alarming or false negative rate, thus achieving a better result.
Network security requirements are generally regarded once network topology is implemented. In particular, once firewalls are emplaced to filter network traffic between different Local Area Networks (LANs). This commun approach may lead to critical situations: First, machines that should not communicate could belong to a same LAN where the network traffics do not pass through the firewall for being filtered. Often overwhelmed by the complexity of security requirements and the growth of networks, network administrators are struggling to resolve such design faults while ensuring not to cause further vulnerabilities. Second, according to network security policy, the required number of LANs, and therefore the number, range and thus, the cost required for both network and security equipments, can be much more reduced than that originally proposed by the network administrator. In this paper, we present an automatic approach that consists on proposing a network topology which is both safe and optimal by taking into account the network security policy, given in a high-level language. The safety property ensures that every prohibited traffic has to cross the firewall to be filtered. The optimal property allows to deduce the necessary and sufficient resources (Sub networks, network switches, firewalls range) to be used. To our best knowledge, such problematic has not been explored in previous works, despite the importance of these challenges. Our method has been implemented using Graph Coloring Theory. The first results are very promising. Experiment conducted on large-scale networks demonstrate the efficiency and the scalability of our approach.
With the more emphasis paid on network security and privacy, the research on anonymous communication system has developed quickly. In the past, researchers mainly focused on how to improve the anonymity, but few focused on how to encourage users to contribute for whole system. This paper presents an incentive mechanism and designs an Incentive-based Low-latency anonymous communication framework (ILACF). In this framework, firstly all nodes in anonymous communication system are divided into four types according to their contribution, and then router selection mechanism is presented. The traffic performance of ILACF is analyzed. The theoretic analysis and simulation experiment indicate that the incentive mechanism is very useful and ILACF can gain better communication performance than Tor and ALHACF.
One of the most critical attacks against web applications is data manipulation classified in logical attacks. They are not identified by automated vulnerability scanners, so they need human surveillance in many cases. This paper provides a server-side validation mechanism which leverages from user's access level to prevent a kind of manipulation of data exchanged between client and server and guarantees originality of data. This manipulation executes in order to modify vital parameters' value in a way, in which user often shouldn't be able to. The results of case study show the feasibility and applicability of the proposed method in almost all cases. Furthermore, it can be applied in ready applications with few changes in target application.
Web Applications form an integral part of our day to day life. The number of attacks on websites and the compromise of many individuals' secure data are increasing at an alarming rate. With the advent of social networking and e-commerce, Web security attacks such as phishing and spamming have become quite common. The consequences of these attacks are ruthless. Hence, providing increased amount of security for the users and their data becomes essential. Most important vulnerability as described in top 10 web security issues by Open Web Application Security Project is SQL Injection Attack (SQLIA) [3]. This paper focuses on how the advantages of randomization can be employed to prevent SQL injection attacks in web based applications. SQL injection can be used for unauthorized access to a database to penetrate the application illegally, modify the database or even remove it. For a hacker to modify a database, details such as field and table names are required. So we try to propose a solution to the above problem by preventing it using an encryption algorithm based on randomization. It has better performance and provides increased security in comparison to the existing solutions. Also the time to crack the database takes more time when techniques such as dictionary and brute force attack are deployed. Our main aim is to provide increased security by developing a tool which prevents illegal access to the database.
Computer network security is a fashionable and fast-moving field. In the last decade many methodologies and tools have been developed for improving the security of networks and their hosts, but the resources used to deal with the problem often do not yield results commensurate with costs. In the last period the adoption of Network Intrusion Prevention Systems promises to represent an effective line of defense against a variety of attacks that could compromise the security and proper functioning of an enterprise information system. This paper introduces a Network Intrusion Prevention System based on Ontological and Slow Intelligence approach. By the use of Ontology the proposed Network Intrusion Prevention System will analyze the input semantically while will improve over the time sharing knowledge among other similar systems or experts according to the Slow Intelligence approach. A first prototype of the environment has been developed and first experimental results have been showed.
Situation element extraction is a significant step of network security situation awareness. This paper presents a network security situation element extraction model based on projection pursuit regression. A nonlinear optimization problem is proposed in the model, in which the projection direction and the polynomial coefficients are optimization variables, and projective index function is target function. Due to the high precision and fast convergence speed, the particle swarm optimization is introduced to solve the optimization problem. The simulation experimental results show that this model is efficient.
The core of future network security protection will be active defence, where security situation prediction plays an important part. In order to solve the problem of uncertainty conversion between qualitative concept and quantitative values in situational awareness, and to improve the degree of matching between data distribution and actual situation, this paper presents a posture prediction algorithm based on multidimensional cloud model. Several cloud models are constructed according to training historical data, and obtain the corresponding integrated cloud of key attributes by soft-or calculation. At last we get network future situation value in the inverse operations of membership function. Experiments show that the method has good performance of detection accuracy predict trends stability.
State of the art security research in the field of wireless sensor networks has focused on providing security in a coarse-grained, full-fledged and static fashion. This implies providing confidentiality, data authentication, data integrity and freshness to the entire spectrum of communication between the participating nodes in a network. In this paper however we advocate that as a result of a number of factors relating wireless sensor networks, providing security in similar fashion for the entire communication set isn't a pragmatic approach and does not precisely reflect the application level security requirements. We therefore propose DiFiSec, a dynamic, fine-grained and adaptable security framework that supports various levels of plug gable security for distinct data communication sets depending on the context, environment and criticality of the data. These plug gable security levels can be enacted at the levels of component wirings and receptacles, hence empowering application users to select only the most appropriate security respecting the resource-constrained nature of WSNs. Furthermore, to support system evolution and changing application requirements DiFiSec offers runtime adaptability. A prototype of this system has been implemented on SunSPOT sensor nodes where we have evaluated our approach in comparison with other network security variations.
This paper describes a data collection distributed platform composed of various high interaction honeypots deployed in different locations, along with our first analyses of this data. This deployment follows a previous experiment conducted with the same honeypot that was deployed at a single location (cf. [1]). The objective is to check if the results of our first experiment can be generalized.
Protocol state machine is very essential in network security and implementation fields, however, improper management of software evolution, compounded by changing and imprecise requirements, along with "short time to market" phenomenon, often leads to a lack of up-to-date specifications and they are often characterized by bugs, anomalies and even threads. How to mine the accurate protocol state machine under investigation is still an open problem. We address this problem by using an interactive grammar inference technique as it could generate queries to the protocol implementation in learning process. This paper describes: (1) a flexible method to construct and parse real packet according to the packet format, (2) how to generate packet queries to explore protocol state machine space, (3) applies the QSM technology to mine protocol state machine from network traces. To access the usefulness of our approach, several experiments for different protocols are performed and we could get more objective and accurate results compared with other protocol specification mining methods.
According to the specific demand of the construction of the information network confrontation defense system, this paper proposes a new network defense in-depth model of APR-WPDRRC based on closed-loop control mechanism. Since the model integrates variety of network defense in-depth new technology, it can achieve cooperation linkage and closed-loop control of quick pre-warning, active protection, dynamic detection, real-time response, disaster recovery and precision counterattack. The model has a good network defense agility of adaptability, intrusion tolerance attack and strong survivability especially when it suffers in large-scale, distributed, instantaneous changing network attacks.
The overall objective of the network security situation assessment is to establish security situation assessment system. This system can unify the large-scale network. First, it provides a security policy unified the whole network. It also provides the techniques for wide-area situation assessment. At the same time this system provides situational awareness and decision support tools for the implementation of network security command. Firstly, the information model of security situation was studied based on XML. Then the security situation value of system including three levels: service, host and network was analyzed and calculated. The-support vector regression machine was chosen for predicting the network security situation and determined the parameter values. Finally, the effectiveness of the method was verified through experimental analysis.
With the automation level of the computer networks and grid rising, the dependence of the electric power enterprise on information systems is growing, while the security problems faced by become stronger, so using traditional security technologies only to maintain the system security can no longer meet the enterprise requirements. In this paper, against the information network characteristics of the power enterprise, combined with intrusion tolerance theory, we put forward the security model based on pre-response intrusion tolerance technology used by power enterprise information systems. The model inherits the intrusion tolerance capacity and intrusion tolerance performance of the pre-response intrusion tolerance technology, and can provide a more comprehensive fault tolerance and intrusion tolerance capabilities to ensure the reliability and the uninterruptedness of the information system services.
There are two main methods that identify host information Banner and Fingerprint. Through research, analysis and combined with the two methods, we complete and realize passive host characteristic information identification. For other network security research, particularly with regard to improvement of network intrusion detection system, we prepare data, and broaden ideas.
The popularity of the Internet is rising day by day, hence the security is becoming the main focused point with the advent and rising popularity of the Internet. In this paper we are proposing an integrated framework of malware collection and analysis using both of the technologies called server honeypots and client honeypots. As the server honeypots enable us to provide the deep understanding of the server side attacks whereas client honeypots enable us to provide the deep understandings of client side attacks. During our research on honeypot technologies, our main goal was to do the analysis of collected malwares from honeypots and for this we need the malwares samples from both the honeypots known as client and server honeypots. By using this integrated framework we are able to collect both types of attacks vectors. Here we are presenting malware collection and detection using both of the honeypot technologies known as client and server Honeypots. We introduce the design and implement of this system and give the results.
We investigated client honeypots for detecting and circumstantially analyzing drive-by download attacks. A client honeypot requires both improved inspection performance and in-depth analysis for inspecting and discovering malicious websites. However, OS overhead in recent client honeypot operation cannot be ignored for improving honeypot multiplication performance. We propose a client honeypot client system that uses our proposed multi-OS and multi-process honeypot multiplication approaches and implemented this system to evaluate its performance. Our process sandbox mechanism, a security measure for our multi-process approach, creates a virtually isolated environment for each web browser. In a field trial, we confirmed that the use of our multi-process approach was three or more times faster than that of a single process and [our multi-OS approach lineally improved system performance according to the number of honeypot instances. Thus, our proposed multiplication approaches improve performance efficiency and enables in-depth analysis on high interaction systems.
A casual network security monitoring system is proposed in this paper. The system is easy to deploy without reconfiguring the central network infrastructure, the firewall, and the intrusion detector system (IDS) of an organization. A virus-infected host, which is hidden by the network address translator (NAT) of a sub LAN, can be identified easily by using this monitoring system with the IDS. This monitoring system consists of a portable sensor device and a web site with wiki software. The portable sensor device, which is located on a target LAN that may have virus-infected hosts, is remote-controlled by a network manager's commands. The commands and the results are written on a wiki page.
A casual network security monitoring system is proposed in this paper. The system is easy to deploy without reconfiguring the central network infrastructure, the firewall, and the intrusion detector system (IDS) of an organization. A virus-infected host, which is hidden by the network address translator (NAT) of a sub LAN, can be identified easily by using this monitoring system with the IDS. This monitoring system consists of a portable sensor device and a web site with wiki software. The portable sensor device, which is located on a target LAN that may have virus-infected hosts, is remote-controlled by a network manager's commands. The commands and the results are written on a wiki page.
Deep Packet Inspection is a vital task in network security applications such as Firewalls and Intrusion Detection Systems (IDS). Patterns based detectors used in Packet Inspection implement multi-pattern matching algorithms to check whether the packet payload have a specified patterns in a patterns set. Computational cost is one of the major concerns of the commercial Intrusion Detection Systems (IDSs). Although these systems are proven to be promising in detecting network abnormalities, they need to check all the patterns to identify a suspicious abnormal in the worst case. This is time consuming. This paper proposes an efficient two-level IDS, which applies a statistical patterns approach and a Sequential Differentiate Method (SeqDM) for the detection of unauthorized packets. The two-level system converts high-faceted character space into a low-faceted character space. It is able to reduce the computational cost and integrates groups of patterns into an identical patterns. The integration of patterns reduces the cost involved for valid packet identification. The final decision is made on the integrated low-faceted character space. Finally, the proposed two-level system is evaluated using DARPA 1999 IDS dataset for the detection of unauthorized packets.
Power grid vulnerability, as the deepening and developing of network security issues, shows its great research significance. Based on the energy function theory, branch static energy function model was constructed comprehensively. Multi physical quantity was mapped to the amount of branch energy; The branch vulnerability can be reflected more accurately under different operation states; And then, the Vulnerable Sensitivity index established in the form of energy was founded to assess vulnerability and the Branch Vulnerability index was proposed to evaluate branch vulnerability degree; The Vulnerable Sensitivity with the system load and structure changing was analysed to confirm the key vulnerable branch . At last, the simulations of this method in IEEE-30 bus system demonstrate its validity and feasibility. The proposed approach is promising to be applied in on line vulnerability assessment.
With the rapid development of computer network technology, the security of computer network becomes increasingly important. Three main threats facing computer network security include: hackers, computer virus and denial of service attack. Things leading to the safety of the network are mainly: resources sharing, data communication, computer virus and TCP/IP protocol security flaws. A safety network system should include at least three kinds of measures: legal measures, technical measures and review and management measures. The paper analyzes the main threat facing computer network security, discusses network security technology and advances some effective countermeasures in view of the hidden danger of current common network security.
Now, Intrusion Detection System (IDS) and Intrusion Prevention System (IPS) are very important technology for network security. They can discover and report unauthorized or anomalies to detect the actions violating safety strategy, through the rational deployment and configuration in computer network. The working principle of IDS & IPS, and the feasibility of collaboration between them are discussed in this paper. Intrusion Management System (IMS) is a trend of technology application. The core of intrusion management system is the collaborative intrusion detection technology. The current application problems of the collaborative intrusion detection technology are summed up, then the corresponding solutions for the problems are proposed, based on a brief analysis of the intrusion management system.
At present, network security attacks are numerous. Traditional single defense equipment and testing equipment are unable to meet the requirements of network security under the new circumstances. Therefore, the research on network security situation has become a hot topic in the field of network security. To enhance the accuracy and time effectiveness of the network security situation forecast, a fuzzy prediction method of network security situation based on Markov is proposed in this paper. The method is based on the Markov state transition matrix that depicts the correlation of network security and predicts the security status. By introducing the vulnerability information to build the membership degree of fuzzy security situation for the security status and integrating improved Zadeh formula, the prediction value of the network security situation is obtained. Finally, the effectiveness of the method is shown by the experiment results on KDD CUP99 data and DARPA2000 data.
Fraudulent use of network security tokens is a serious concern for any system that contains data that must be secured against illicit access, duplication, or manipulation. Anomaly-based techniques to classify logins as fraudulent or legitimate have been proposed and used successfully, however the lack of clear mathematical structure in the space of IP addresses means that many of these methods require significant supplemental information such as payload, failed token usages, or user activity upon the secured network in order to achieve accurate detection rates. When this additional information is not available, such as in network-based intrusion detection systems, many systems to detect fraudulent security token usage require a series of usages before a classification can be made. We present an anomaly detection system based upon IP addresses, a mapping of geographic location as inferred from IP address, and usage timestamps that is capable of identifying fraudulent token usage with as little as a single instance of fraudulent usage while overcoming the often significant limitations in geographic IP address mappings.
Anomaly detection is essential in network security. It has been researched for decades. Many anomaly detection methods have been proposed. Because of the simplicity of principles, statistical and Markovian methods dominate these approaches. However, their effectiveness is constrained by specific preconditions, which make them work for only appropriate data sets which satisfy their premises. Other than statistical and Markovian model, information theory provides a different perspective about anomaly detection. However, the computation of information theoretic measures is still based on statistics. In this paper, we present a novel, information theoretic anomaly detection framework. Instead of statistics, it employs lossless compression for measuring the information quantity, and detects outliers according to compression result. We also discuss the selection of underlying compression algorithm, and choose a grammar compression for utilizing the structure of data. With grammar compression, our method overcomes the shortcomings of statistical and Markovian methods. In addition, the implementation and operation of our method is even simpler than traditional approaches. We test our method on four data sets about text analyzing, host intrusion detection and bug detection. Experimental results show that, even traditional methods fail in some situations, our simple method works well in all cases.
It is well known that network coding can improve the secrecy of a network that is represented by a graph with edges tapped by an eavesdropper. However, the application of such secure network coding to emerging problems in information-theoretic secrecy in wireless networks has been limited; in particular, the graph-based wiretap network approach does not readily extend to the physical wireless network. In this paper, we consider the impact of such an extension on scaling laws for wireless networks. We first employ simple examples to illustrate how an extension of the secure network coding approach can address difficult problems that have plagued wireless network security; for example, relaxing the known eavesdropper location assumption or avoiding the use of artificial noise generation. Based on this understanding, we then add secure network coding approaches to recent constructions for secrecy scaling in large wireless networks, most notably achieving secure pernode throughput in large wireless networks in the presence of an arbitrary number of non-collaborating eavesdroppers.
Growing in cyber attacks has driven a growth in demand for network security. Cryptology is the science of secret communication systems including cryptography and cryptanalysis. Different heuristic and meta heuristic algorithms has been used for cryptanalysis and certainly one of the most important one is GA. Usually N-gram and Dictionary attack are used to have a suitable fitness and best chromosomes which are time consuming and poor performance methods. In this paper a new method is proposed that not only improve performance of analysis but also it needs time not much as mentioned methods. NSGA-II (one of the most important multi-objective GAs) is used for better key evaluation and increasing the accuracy and performance. This analysis has been employed on a substitution cipher and some good results achieved.
Network interdiction problems consist of games between an attacker and an intelligent network, where the attacker seeks to degrade network operations while the network adapts its operations to counteract the effects of the attacker. This problem has received significant attention in recent years due to its relevance to military problems and network security. When the attacker's actions achieve uncertain effects, the resulting problems become stochastic network interdiction problems, and allow the players to adapt to new information collected during the game. In this paper, we study stochastic network interdiction games where the attacker has one or two stages to attack the network, and can collect information on the outcomes of previous attacks. For the single stage problem, we develop a new solution algorithm, based on parsimonious integration of branch and bound techniques with increasingly accurate lower bounds, that obtains solutions significantly faster than previous approaches in the literature. We extend the single stage formulation to a two stage formulation, and develop a new set of performance bounds for this problem. We integrate these bounds into a modified branch and bound procedure that extends the single stage approach to two stages. The efficacy of the new algorithms is shown using simulated experiments with networks studied in previous papers.
Honeypot have many advantages in the fight against the unknown attacks, using Honeypot technology on the mining enterprise network can greatly improve the security of the system . In this paper ,a modified algorithm is proposed, and a high degree of interactivity level of the virtual honeypot network is developed on this basis, It can give good reference to the network maintenance personnel.
Intrusion detection is a very important topic in dependable computing. Intrusion detection system has become a vital part in network security systems with wide spread use of computer networks. It has been the recent research focus and trend to apply various kinds of data mining techniques in IDS for discovering new types of attacks efficiently, but it is still in its infancy. The most difficult part is their poor performance and accuracy. This paper presents an innovative model, called MID, that counts maximal frequent patterns for detecting intrusions, needless to count all association rules, can significantly improve the accuracy and performance of an IDS. The experimental results show that MID is efficient and accurate for the attacks that occur intensively in a short period of time.
Based on the characteristics of urban water supply system, the urban pipe network security assessment system was established, and then AHP method combined with fuzzy evaluation was used to carry out a comprehensive security assessment for the leakage of the urban pipe network. By calculating a real case and analyzing various leakage causes and their impact, the assessment model was verified reasonable.
This article introduces the honeynet network structure and working mechanism from the perspective of network security technology, makes an in-depth analysis of honeynet technologies, and brings in suspicious behavior directed security strategies. It Introduces honeynet systems based on genetic algorithm log detection, and makes a rather further study of data mining using genetic algorithms.
Conventional network security control and protective measures based on identity authentication and content authentication can't meet the security requirement of current cloud computing. To address this problem, a behavior authentication mechanism is proposed, which includes establishment of behavior authentication set and behavior authentication strategy. A stochastic Petri net model for this mechanism is constructed. We solve the model by using top-down decomposition and bottom-up hierarchical analysis techniques to decrease the complexity of the model and underling Markov process. This paper further conducts performance evaluation of the system. The research results provide a reference for the design and application of a practical behavior authentication mechanism.
The traditional solution to database security has a drawback that it can not deal with malicious attacks by persons with legal identity, and that, it is in general not cost-effective to users who have different security requirements for it only offers fixed security level. By adopting multi-layer security model, namely "user +OS +DBMS +transaction-level intrusion tolerance", it integrates redundancy and variety technology; by adopting integral security strategy and server-oriented intrusion tolerance technology, it realizes the survivability and availability of database, and the confidentiality and integrity of sensitive data. In this way, it can effectively resist malicious attacks by persons with legal identity and reduce the cost of security.
Small-scale, medium-term deployments are an important tool when planning to install a wireless sensor network (WSN) monitoring system. It is vital to assess the software and hardware reliabilty of the desired solution in a well specified scenario. Also, it allows the system designer to evaluate beforehand the radio environment, a critical task in crowded environments such as buildings. The multitude of walls and large metal objects can produce unwanted effects such as fading and multi-path reflections while high powered radio devices produce interference when operating in the same band as the sensor nodes. This leads to a high packet loss rate and affect battery life through increased number of packet retransmissions. In this paper we report our results from a week long continous monitoring of an indoor space. Our focus is on sampling data for temperature, humidity, barometric pressure and ambient light. We analyze the results and provide experience regarding future real systems aimed at high spatial resolution measurements of environmental parameters indoors. Our study concerns sensor data, energy consumption of sensor nodes and radio channel characterization. A network sniffer is also employed to provide insight into the behaviour of the wireless mesh networking protocol. We consider that the relevance of our work can be highlighted in saved costs associated with maintenance operations and man-hours after a final, large-scale deployment of WSN instrumentation has already taken place.
Network security is a complex systematic project. The existing safety assessment methods have some shortcomings such as feasibility, smaller application scope, man-made interference and so on. The simulation results show that, using the established evaluation model for network security evaluation is simple, eliminate the interference of human factors, and can quickly get the correct results of the evaluation. This article provides new ideas and methods to work for a comprehensive evaluation of computer network security situation, especially with a certain reference value to predict and control of network security issues in the future.
Network protocol vulnerability detection is paramount to network security. Formalization provides an important way for vulnerability detection. In this paper, we apply TLA, which is a powerful tool for formal analysis, to analyze network protocols. An approach is proposed that aims to detect vulnerabilities of a protocol effectively with the TLA, with the Kerberos protocol being taken as an example. Firstly, roles for the protocol, especially those related to intruders, are created. Then actions of the roles are specified. Sessions among the roles are built. And environment parameters are set. A prototype program is developed to implement the approach, which covers the model and the detection properties of the protocol. Experiments show that our approach is effective and powerful in specifying and checking a protocol, and it is better than SPIN and SMV.
In recent years, lots of researches have explored how to extend the lifetime of wireless sensor networks. Many of them focus on how to defend against Denial of Service (DoS) [1] attacks, and focus on how to arrange the locations of sensor nodes in different wireless sensor network architectures to extend the lifetime of sensor networks and to improve network security. This paper will use artificial bee colony algorithm and Or-opt algorithm to plan a transmission route of the detect features. Patrol nodes are also exploited to reduce power depletion and to extend the network lifetime. Patrol nodes will collect sensing data of nearby sensor nodes, detecting patterns in the collection data to confirm the collection data whether is normal or not. The experiments proof our method can reduce the time consume for dada transformation between patrol nodes.
This research presents an intrusion detection system t (IDS) with honeypot over distributed networks. The main objective is compare measuring the effectiveness of attacks between IDS and IDS with honeypot by unicast IP address attack and multicast IP address attack. These attack forms generating by NESSUS is a attack program via wire network and wireless network consists of Snort is a program that is used to detect intrusion and Honeyd to simulate a honeypot computer, which is installed on the system practical for Linux with a number of more 2 points (Sensor) to detect each program will be sent to the primary database for the analysis of experimental results. From this project will allow administrators to monitor and protect their networks more efficient organization.
Security Visualization is a very young term. It expresses the idea that common visualization techniques have been designed for use cases that are not supportive of security-related data, demanding novel techniques fine tuned for the purpose of thorough analysis. Significant amount of work has been published in this area, but little work has been done to study this emerging visualization discipline. We offer a comprehensive review of network security visualization and provide a taxonomy in the form of five use-case classes encompassing nearly all recent works in this area. We outline the incorporated visualization techniques and data sources and provide an informative table to display our findings. From the analysis of these systems, we examine issues and concerns regarding network security visualization and provide guidelines and directions for future researchers and visual system developers.
Many applications in several domains such as telecommunications, network security, large-scale sensor networks, require online processing of continuous data flows. They produce very high loads that requires aggregating the processing capacity of many nodes. Current Stream Processing Engines do not scale with the input load due to single-node bottlenecks. Additionally, they are based on static configurations that lead to either under or overprovisioning. In this paper, we present StreamCloud, a scalable and elastic stream processing engine for processing large data stream volumes. StreamCloud uses a novel parallelization technique that splits queries into subqueries that are allocated to independent sets of nodes in a way that minimizes the distribution overhead. Its elastic protocols exhibit low intrusiveness, enabling effective adjustment of resources to the incoming load. Elasticity is combined with dynamic load balancing to minimize the computational resources used. The paper presents the system design, implementation, and a thorough evaluation of the scalability and elasticity of the fully implemented system.
As a tool for network operators to recover network investment costs from network users as well as to provide forward-looking economic signals, distribution network pricing models are also expected to identify and recover investment costs related to maintaining network security. The existing models reflect network security by determining the maximum allowed contingency flow along each component through implementing deterministic contingency analysis. They fail to consider two reliability cost drivers: 1) reliability levels of network components, and 2) interruption tolerance levels at different nodes.
Aspirations for a boundless communication paradigm for future generation networks have changed the conventional way of looking at network security. With such a vision, security techniques should not only be securing local end-users but also be protecting entire networks from malicious adversaries. Unfortunately, providing security protection for such gigantic networks is a very challenging task. Based on the analogous relationships of various cell interactions in a metabolic network and a complex heterogeneous network, we explore the possibilities of human immune system (HIS) inspired anomaly detection for protecting interworked heterogeneous networks. In light of this, the applicability of two key schools of thought on how the HIS detects anomalies; namely, the Negative Selection (NS) and the Danger Theory (DT) are discussed in this article. While the NS theory is the traditional understanding of anomaly detection in the HIS, the DT is a radical new concept that challenges the main fundamentals of the NS theory, which is currently being hotly debated among immunologists. Based on this, the article highlights the applicability and the limitations of these two theories in detecting malicious anomalies in heterogeneous networks. From our analysis, we establish a doctrine that the DT outperforms its counterpart (the NS theory) in detecting anomalies in a complex heterogeneous network. Our analysis also indicates that the DT inspired anomaly detection is efficient in detecting malicious network adversaries and updating network segments that are under attack, thereby increasing the survivability of heterogeneous networks.
Protective relay settings for wind turbine generators often depend on short-circuit calculations performed using sequence network circuits in rms-based protection software. Traditional assumptions made in deriving the sequence network representation of induction machines result in some error in short-circuit calculations, which could potentially cause incorrect relay settings. A more accurate sequence network model of induction machines is derived in this paper to aid in short-circuit calculations in induction-generator-based wind plants. A rigorous mathematical approach to defining the stator short-circuit current equations under general faults is described and validated with transient simulations. The sequence network model of the induction machine is derived from the closed-form mathematical solutions. An example short-circuit calculation is performed on a simple network in this paper, and found to have a marked improvement in accuracy from traditional sequence network calculation methods for unbalanced faults. Resonant effects of power factor correction capacitors are also described.
Approaches in security visualization have made significant progress in addressing challenges in the ever changing landscape of network security. However, many approaches are limited in both scope and scale, especially when we consider the complexity of the complete security analysis process. In this article, we review several notable recent systems in security visualization, examining their relative strengths and limitations. We then show that recent research in general network visualization, which often deals with domains other than security, provides new visual metaphors and interaction techniques that will help address limitations in security visualization systems. We examine several of these network visualization approaches in detail, and discuss how they can be applied to meet the challenges of the next generation of security visualization systems.
Intrusion detection systems, or IDSs, are network security tools that generate huge quantities of information which are challenging to analyze. Information visualization is essential for efficiently parsing these data to discover the underlying causes of computer security breaches. AlertWheel is a user interface featuring a novel radial overview visualization, as well as filtering, drilling down, and saving and annotating subsets of data, to support the workflow of real network defense analysts. In designing AlertWheel, we identified new ways of displaying bipartite graphs (i.e., network diagrams showing links between two sets of nodes). The links in AlertWheel??s visualizations are positioned and shaped to avoid occlusion of data, and three different edge bundling techniques are used to reduce clutter.
In this paper, we study a network security configuration problem. More specifically, we consider distributed intrusion detection systems in a network subject to possible simultaneous attacks launched by a number of attackers. We formulate an N + M-person
The functions and features to be provided in substation intelligent electronic devices (lEDs) to accommodate critical infrastructure protection programs are defined in this standard. Security regarding the access, operation, configuration, firmware revision, and data retrieval from an IED is addressed in this standard. Communications for the purpose of power system protection (teleprotection) is not addressed. Encryption for the secure transmission of data both within and external to the substation, including supervisory control and data acquisition, is not part of this standard as this is addressed in other efforts.
With the development of computer network technology, people gradually pay more attention to the network information security. More and more advanced technologies are being used to solve this issue. In this paper, the real-time protected management system of broadband network information safety in unified controlling platform is presented. This system possesses the ability of intelligent detection. It contains such subsystems as unified controlling center, network invasion detection, host invasion detection, vulnerability scan, and net honey and so on. This system can dynamically detect the abnormalities and then promptly give an alarm to firewall, router and so on. It can make up the shortage of other static defense tools.
With the development of national economy, the construction of highway network makes astounding advances. The province highway network is evolving after some years' construction. With the formation of the road network, the security issue of highway toll network is becoming prominent. In this paper, the status and the network security issue of the current highway toll network charge, combined with Zhejiang Expressway, are discussed and a security model for the highway toll network is established, and the level of network security prevention system is proposed.
As an important means of information security, data backup technology has attained increasing attention. This paper proposes a data backup method based on file system filter driver. It monitors various operated events of application program through file system filter driver and do incremental backup by recorded events. The related experiment verified that the method reduces the file backup redundancy, improves the backup efficiency, and to the right to monitor the file attributes, name changes and other events. The method has broad application prospect in actual data backup system.
"merge" It is gradual progress of network of next generation and inexorable trend of development, realize the network merge and merge with the business, it is basic prerequisite and basic key element that the data are merged. " unified user's data " As inevitable development trend that network merge, build up about of the next generation network at present, network merge with business merging research focusing of field. This paper introduces the current research on unified user's data, analyze it in network security threat under the current merging network, provides the safe problem that should pay close attention and study of the unified user's data.
A server needs strong security systems. For this goal, a new perspective to network security is won by using data mining paradigms like outlier detection, clustering and classification. This study uses K-Nearest Neighbor (KNN) algorithm for clustering and classification. KNN algorithm needs data warehouse which impersonates user profiles to cluster. Therefore, requested time intervals and requested IPs with text mining are used for user profiles. Users in the network are clustered by calculating optimum k and threshold parameters of KNN algorithm. Finally, over these clusters, new requests are separated as outlier or normal by different threshold values with different priority weight values and average similarities with different priority weight values.
Management honeypot is a new theory we have proposed. This paper first point out the necessity of conducting a study on the incentives of management honeypot and then examines the characteristics of the incentives of management honeypot. Meanwhile, it explores the three categories of management honeypot incentives - the system-lacking incentive, the managers' psychological incentives and the employees' psychological incentives, and analyses the relationships between these categories. Based on the case analysis of Foxconn, this paper reaches the conclusion that researching the management honeypot incentives can help discover the actual management problems, improve the management performance and prevent future problems.
Intrusion Detection System plays a reasonable supplementary role for the firewall in the network security. It can help protect computers from network attacks and improve the security and reliability of the computer. At present intrusion detection system analysis module uses the pattern matching technology. In this article an optimized algorithm was proposed through analyzing the advantages and disadvantages of the main pattern matching algorithm of current Intrusion Detection System. And it also proved that the optimized algorithm has better matching efficiency than the original algorithm through simulation experiments. So the performance of the system can be improved if this algorithm is applied to the intrusion detection system.
Network security is one of the most important research fields among information security techniques. Most problems of network security are often caused by network attacks. In order to ensure network secure running it is very necessary to analyze and research the procedure of network attacks. In this paper, the formal method is researched and automaton theory is used to describe the procedure of network attacks. The state transform diagrams of some typical network attacks are given. Different state transform diagrams are used to describe models for various network attacks and these models can be combined flexibly so as to describe complicated attack behaviors. This formal method for describing network attacks provides an effective approach for researching the mechanism of network attacks.
Network intrusion detection is an important part of network security research. Stackelberg game theory is applied to study attack packets sampling problem in network security, and network attack packets sampling model is proposed in this paper. The network security system is regarded as the follower in this game, adjusting the strategies to maximize its benefit function according to the behavior of network attacker. We proved the existence and uniqueness of Stackelberg equilibrium in this game model. Packets sampling algorithm PSA-SM and its equivalent algorithm SPSA were designed based on linear programming. Attack packets injecting algorithm IPIA-GT is also designed. The experiment results showed that network security system can use SPSA to increase the sampling probability of attack packets, and network attacker can also use IPIA-GT to decrease the sampling probability of attack packets. When both of them used optimal strategies, the game got its equilibrium.
Notice of Violation of IEEE Publication Principles



"A Petri-net Model of Network Security Testing"

by Jun-long Yan, Ming-xin He, Tie-yuan Li

in the 2011 IEEE International Conference on Computer Science and Automation Engineering (CSAE), 2011, pp. 188 &#x2013; 192



After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.



This paper contains significant portions of original text from the paper cited below. The original text was copied with insufficient attribution (including appropriate references to the original author(s) and/or paper title) and without permission.



Due to the nature of this violation, reasonable effort should be made to remove all past references to this paper, and future references should be made to the following article:



"Attack Net Penetration Testing"

by J.P. McDermott

in the Proceedings of the 2000 Workshop on New Security Paradigms (NSPW 2000), 2001, pp. 15 &#x2013; 21



Given the increasing dependence of our societies on networked information systems, the overall security of these systems should be measured and improved. Existing security metrics have generally focused on measuring individual vulnerabilities without considering their combined effects. In this paper, we proposed a Petri-net based model. It retains key advantages of the flaw hypothesis and attack trees approaches while providing some new benefits. This novel model provides a theoretical foundation and a practical framework for continuously measuring network security in a dynamic environment.
On the basis of the Hadoop distributed file system (HDFS), this paper presents the design and implementation of QDFS, a distributed file storage service system that employs a backup policy based on recovery volumes and a quality-aware data distribution strategy. The experimental results show that QDFS increases the storage utilization ratio and improves the storage performance by being aware of the quality of service of the DataNodes. The improvements of QDFS compared with HDFS make the Hadoop distributed computing model more suitable to unstable wide area network environments.
With increasing denial of service attacks on network infrastructure, there is an urgent need to develop technique to assess the threat of attacks on network security online. A novel model of security threat assessment relying on several predefined metrics of network performance is proposed to measure the impact of denial of service attacks on service availability in real time. This model applies the technique of D-S evidence reasoning to fuse three metrics of network performance, which are designed carefully to reflect the reliability of service availability in three perspectives. Our approach includes three steps: determining performance parameters, calculating threat index and characterizing the threat state of service availability. Compared with other methods, this method avoids the unilateral result obtained from single sensor, helps administrators to determine security threat state, and provides threat evolution of service availability over time. Testing in a real network environment shows that this method greatly improves the accuracy of threat assessment, demonstrates the impact of denial of service attacks on network security is different from the beginning to the end of DoS attacks, and provides administrators with threat evolution picture macroscopically. Moreover, it lays the foundation for administrators to adopt security response policies in real time for reliable and robust network.
Key management is a major challenge to achieve security in wireless sensor networks. Previous research on sensor network security mainly considers homogeneous sensor networks, where all sensor nodes have the same capabilities. The recent research has shown that the survivability of the homogeneous sensor network can be improved if sensor nodes are grouped in clusters in which a powerful cluster head assigned. In this paper, we adopt a Heterogeneous Sensor Network (HSN) model for better performance and security. We proposed a novel key management scheme, which utilizes Elliptic Curve Cryptography and the t-degree trivariate symmetric polynomial in the design of an efficient key management scheme for sensor nodes. In addition, we also design a dynamic key update technique based on one-way hash chain and Time Slice mechanism which significantly reduced the communication overhead in key agreement and update phase. The performance evaluation and security analysis show that the proposed key management scheme can provide perfect resilience against node capture and scalability with significant saving on communication overhead and storage space.
Today, the issue of network security has been one of the most important issues more and more pressing during informatization construction. Given the fact that IPv6 technology has been defined by IETF as the standard of next generation for IPv4, which is an indispensable link of future network development, this paper bases on the status quo and issues of network security, comprehensively analyzes the fundamental reason for network security, and carries out analysis from basic principle, significant characters, security mechanism, security issues introduced from IPv6 and other aspects for network based on IPv6 architecture, studies and discusses on the security issues of network based on IPv6.
Intrusion detection is a critical component of network security; detection schemes fundamentally use the observed characteristics of network packets as a basis for such determinations. In this study, a cluster center distance method is applied to classify packet type. The cluster center is determined using characteristics of a portion of selected packet data samples prior to detecting. Meanwhile, a well-known back-propagation neural network combined with the roulette wheel selection method and pseudo-random rule are combined with back propagation network (BPN) to determine the intrusion packet type. KDDCUP99 data sets were used as the evaluation packet samples of this experiment. Simulation results demonstrate that roulette wheel selection combined with BPN scheme provides higher detection rate for DoS and R2Lattack packets; BPN with pseudo-random rule can yield higher detection rate for U2R attack packets.
Data safety is one of key technologies in the EIMSs. According to the characteristics of the EIMSs, we propose an innovative architecture of EIMSs with emphasis on several important issues. The architecture is deigned based on several policy models including data-safety defense policy authentication policy, data-access-control policy and data-security encryption policy.
To solve the security issues of network resources in file sharing, a reputation evaluation system was build based on resource trust degree. A set of reputation evaluation formula and method was developed to build trust model using the reputation evaluation method based on resource credibility and recommended reliability, and the reputation evaluation system was applied to network security. Through the trust mechanism, users can get the historical experiences of target nodes, thus the network resource security and the integrity of download resources are ensured. Users can use the model to select safer network resource service objects, and an incentive mechanism can play a part in selecting network resources.
This article describes the design and implementation of the sharing management information system based on WEBGIS wireless base station, regarding the information sharing of the wireless base station as the main object. It designs information management through the structural design, technical system selection, database selection, WEB application server, operating system selection, network security architecture and planning. The system uses the popular WEBGIS technology, the component-based GIS development mode to improve the efficiency of development, making the interface beautiful and friendly with scalability. With the establishment of the system, it can effectively strengthen the supervision of resource consolidation and performance evaluation, reduce the duplication of similar wireless base station projects, improve the operating factor of wireless base station, play a positive role in further standardizing the management level of sharing resources and creating a new situation in the professional work of co-construction and sharing.
Mobile code is an important programming paradigm for our increasingly networked world. However the benefits offered have not been sufficient to simulate their wide spread deployment, the main reason is their inherent security. This paper introduces a protocol based on publicly verified signature chain, execution tracing and centralized Trusted Third Party (TTP) scheme. The protocol allows detection of attacks against code, state and execution flow of mobile agents.
For more serious network security threat, security tools, also developed rapidly, but a fundamental limitation of traditional host-based anti-malware systems is that they run inside the very hosts they are protecting, making them vulnerable to counter-detection and subversion by malware, so VMM-based anti-malware systems have recently become a hot research field. Based on the analysis of existing malware detection technique using virtual machine, this article analysis and research on the different detection methods deeply, and point out possible research topics in the next step.
With the network security issues being more prominent, the safety of system and network resources become more and more important problem. Intrude detecting (ID) has become a top research topic nowadays. Considering the strong generalization ability, high sorting precision and such advantages the support vector machine (SVM) shows in practices involves small sample, high dimension, we will mainly focus on studying and consummating the SVM methods in intrude detecting. ID always generates huge data sets; such raw data sets are incapable of being training due to its large scale and high dimension and redundancy. Intrusion detection system always has the disadvantages such as over-loaded, occupying too much resource, an extension of training and forecasting time... therefore, the simplification of practical information becomes such a necessity. Recursive support vector machine (R-SVM) and Rough set were used for exacting main features of raw data, and many kinds of classification algorithms were used here and it has been tested by KDDCUP1999 date set. The result shows that, the SVM classification based on R-SVM runs excellent, its accuracy is as good as the SVM classification based on the whole features and considerably reduces the training and testing time.
Pattern matching is a commonly used operation in many applications including image processing, computer and network security, bioinformatics, among many others. Aho-Corasick (AC) algorithm is one of the well-known pattern matching techniques and it is intensively used in computer and network security. In order to meet the real-time performance requirements imposed on these security applications, developing a high-speed parallelization technique is essential for the AC algorithm. In this paper, we present a new memory efficient parallelization technique which efficiently places and caches the input text data and the reference data in the on-chip shared memories and texture caches of the Graphic Processing Unit (GPU). Furthermore, the new approach efficiently schedules memory accesses in order to minimize the overhead in loading data to the on-chip shared memories. The approach cuts down the effective memory access latencies and leads to significant performance improvements. Experimental results on Nvidia GeForce 9500GT GPU shows up to 15-times speedup compared with a serial version on 2.2Ghz Core2Duo Intel processor, and 15Gbps throughput performance.
Importance of intrusion detection system (IDS) for network security management is widely accepted. Efficiency of IDS is mainly affected by algorithms used for feature identification and classification. Data mining can be very fruitful for feature selection and intrusion detection. In this paper, we have presented J48 classification algorithm for intrusion detection. To evaluate the performance of the algorithm correctly classified instances, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Root relative squared error and kappa statistics measures are applied.
In order to improve the survivability power of embedded real-time systems (ERTS), according to the character of it, strengthened design method was used to realize survivability design. Then through chose analysis of embedded mobile video-on-demand system, the intrusion tolerant technology was introduced in whole survivability design.
Network Intrusion Detection Systems (NIDS) monitor internet traffic to detect malicious activities. Unfortunately, the amount of data that must be analyzed by NIDS is too large. Several feature selection and feature extraction techniques have been proposed to reduce the size of data. Few are focused on finding exactly by how much the dataset should be reduced. The purpose of this paper is to contribute to the finding of that finite amount of data required for successful intrusion detection. A new hybrid algorithm MID-PCA combining PCA (Principal Component Analysis) and mRMR (minimum Redundancy Maximum Relevance - MID evaluation criteria) is proposed. PCA is first applied to the original dataset. Then, mRMR-MID is applied to the intermediary output to further reduce redundancy and maximize relevancy. An exhaustive evaluation of the MID-PCA algorithm is conducted with the KDD Cup'99, a used widely dataset in the network security community. MID-PCA performance was compared to that of PCA and mRMR using two classifiers namely J48 (C4.5) and BayesNet. Experimental results assert the effectiveness of the newly proposed algorithm MID-PCA for NIDS feature extraction compared with PCA and Mutual Information. The newly proposed MID-PCA shows better performance and classification accuracies with reduced datasets of only 4 dimensions for BayesNet (99.77%) and 6 dimensions for J48 (99.94%). This is an improvement over PCA which achieves similar classification accuracy with 12 principal components (twelve dimensions). An extension of this paper will conduct broader experiments using other datasets, then compare results to that of several well known feature reduction algorithms to confirm the superiority of MID-PCA.
Contingency analysis is one of the major components in power system security analysis. Hydro-Quebec has a long interest in on-line real-time security based monitoring system. As early as 1982, the first operational tool to provide dynamic transfer limits was put in real-time operation in the control center. Over several decades, more sophisticated and complicated tables were integrated into the system called LIMSEL. In 1989, the on-line network security analysis software known as LASER was put in service. This software includes a state estimator and on-line post-contingency analysis. This paper briefly describes the experiences of contingency analysis in both on-line and off-line aspects at Hydro-Quebec. Challenges and improvements arising from the practice of contingency analysis are also discussed.
This paper presents some results of the first experiences with the operation of the Dynamic Security Assessment (DSA) system. The DSA is a part of a comprehensive network security solution and is based on fast time domain contingency simulations, considering main stability issues in transmission networks, voltage stability, transient stability and small signal stability. The DSA system uses the actual and near future operating points as starting points for the dynamic contingency simulations. The results of the dynamic security analyzes are reduced to one index, represented by a color scheme similar to a traffic light. In case of security problems, the user is led through different layers of visualization to be able to find out the reasons for the problem and identify the events leading to instability.
Security is a key factor in most of the systems used widely in day to day life. Most of the real time applications and systems are facing security problem very often, therefore we have attempted to build a general framework for all existing and future systems. Wireless or mobile networks emerged to replace the wired networks. MANET is an emerging research area with practical applications. This new generation of networks is different from wired one in many aspects like network infrastructure, resources and routing protocols, routing devices etc. However, MANET is particularly vulnerable due to its fundamental characteristics, such as open medium, dynamic topology, distributed cooperation, and constrained capability. This article provides an overview of past and current work in the area of security research of mobile ad hoc networks - as well as emerging work in different approaches to provide security features to routing in mobile ad hoc networks (MANET). Authentication, integrity and encryption are the key issues pertaining to network security. Traditional authentication schemes cannot be effectively used in such decentralized networks. Here, we present an end-to-end data authentication scheme that relies on mutual trust between nodes. In Mobile Ad-hoc Networks there must be two security systems: one to protect the data transmission and another one to make the routing secure. There are already well studied point to point security systems that can be used for protecting network transmissions. But there is not much work has been done about how make secure routing in MANET for volatile nodes.



Misleading actions have not been considered in previous researches on plan recognition. It leads to poor recognition results for some special domains. This paper introduces new concepts including reliability, correlativity, and correlated action sequence etc. A novel algorithm is proposed to recognize a misleading action by using correlated action sequences. The proposed algorithm is shown to improve the accuracy of plan recognition. Moreover, it could also be applied to intrusion detection and network security problems.
The problem of computer network security is the very hard of detecting new attacks which do not have known signatures of intrusion. Intrusion Detection Systems (IDS) is a program of monitoring the events in a computer network and analyzing them for signature of intrusions. This paper proposed the clustering technique by using hybrid method based on Principal Component Analysis (PCA) and Fuzzy Adaptive Resonance Theory (FART) for identifying various attacks. The PCA is applied to random selects the best attribution and reduction the feature space. FART is implementing used to classifying difference group of data, Normal and Anomalous. The results show that the proposed technique can improve the high performance of the detection rate and to minimize the false alarm rate. The evaluated our approach on the benchmark data from KDDCup'99 data set.
In the present investigation, Rijindael's encryption algorithm (AES) is designed based on co-design methodology. It is a commonly used network security algorithm for wireless transmission systems. It is a symmetric block cipher, which plays a major role in bulk data encryption. Four different modules with specific functions are included in this algorithm. The hardware / software co-design methodology is adopted to implement one of the functional modules in hardware and subsequent remaining modules in software. The module in hardware is implemented on FPGA and added as hardware accelerator to the processor. The proposed hardware / software implementation is done on Altera NIOS II processor platform. An implementation result shows a considerable improvement in speed as compared to software only approach. On the other hand, the significant reduction in area is achieved as compared to hardware only approach.
This paper looks at network security from a game-theoretic point of view. Through the formulation and examination of increasingly complex scenarios, we formulate a model for utility-based security decisions. We look at the decision for one person to buy security software for herself and to buy security software in the context of two or more people. By modeling security as a public good, we examine externalities that players impose upon each other. We then examine Olson's theory of groups in a network security context to evaluate the effect of network size on optimal decision-making. We also discuss network topologies to investigate the limitations of the common game-theoretic interdependent security models. We conclude that these models work well for small to medium-sized networks with fairly uniform topologies.
Real-time analysis is vital to network security and management. Solutions are required that are scalable to modern network speeds while remaining flexible to ensure the latest analysis techniques can be implemented. This paper presents the Internet Traffic And Content Analyser (ITACA), an extendable general analysis tool that enables the implementation of plugins to perform specific tasks. Designed with a modular architecture akin to hardware, it is shown, with experiments on real network traffic, to outperform Bro and Snort IDSs in terms of throughput and scalability while offering increased flexibility for real-time analysis.
To enhance network security, we propose a secret key generation and distribution method for multihop wireless OFDM networks. In this method, the inherent physical features of wireless channel signatures, including randomness and reciprocity, are exploited to generate secret keys. In addition, a network coding approach is introduced for key distribution between a pair of transmitter and receiver in a multi-hop network. Our proposed protocol constructs the secret key from multiple branches at the source and forwards partial key information through separate paths to the destination using a simple and robust network coding operation. Under our protocol, cooperative relay nodes cannot decipher the key due to the partial information available, which provides a strong encryption capability for multi-hop systems. Mathematical analysis and excessive simulations demonstrate the high effectiveness of our proposed scheme.
In recent years, Wireless Sensor Networks (WSNs) have become valuable assets to both the commercial and military communities with applications ranging from industrial automation on a factory floor to reconnaissance of a hostile border. In both examples, the sensors act as data sources and relay information to a central sink or base station (BS). Consider the possible attacks that an adversary could launch against a WSN once the BS is identified. This motivates a significant need for improved network security to protect against such malicious attacks. One such mitigation strategy is to increase the anonymity of the BS such that the adversary is unable to distinguish it from the other nodes of the sensor field. Many techniques have been proposed in the literature to improve BS anonymity by altering network-layer routing algorithms; however, few techniques have been proposed to improve anonymity at the Physical Layer (PHY). This is, in part, due to the lack of metrics to characterize PHY anonymity. In this paper, we propose an algorithm called &#x201C;Intercept, Correlate, and Follow&#x201D; (ICF) to characterize PHY anonymity in a WSN network. Simulation results based on the IEEE 802.15.4a Ultra Wideband (UWB) PHY illustrate the effectiveness of the proposed ICF algorithm.
Honeypots are traps designed to resemble easy-to-compromise computer systems in order to deceive botmasters. Such security traps help security professionals to collect valuable information about botmasters' techniques and true identities. Depending on the complexity of services provided by honeypots, botmasters might be able to detect these traps by performing a series of tests. In particular, to detect honeypots, botmasters can command compromised machines to perform specific actions such as targeting sensor machines controlled by them. If honeypots were designed to completely ignore these commands, then they can easily be detected by the botmasters. On the other hand, full participation by honeypots in such activities has its associated costs and may lead to legal liabilities. This raises the need for finding the optimal response strategy needed by the honeypot in order to prolong its stay within the botnet without sacrificing liability. In this paper, we address the problem of honeypot detection by botmasters. In particular, we present a Bayesian game theoretic framework that models the interaction between honeypots and botmasters as a non-zero-sum noncooperative game with uncertainty. The game solution illustrates the optimal response available for both players. Simulation results are conducted to show the botmasters' behavior update and possible interactions between the game players. The obtained results can be utilized by security professionals to determine their best response to these kind of probes by botmasters.
E-commerce has a great impact on burden of proof and cross-examination, etc, in the traditional evidence rules. After analyzing the cross-examination and certification obstacles encountered during e-commerce disputes, the paper proposes an authentication mechanism of e-evidence combined with network security audit system. Based on graph theory, the mechanism constructs a chain of evidence fragment for a lawsuit to authenticate e-evidence. Experiment is performed to validate the effectivity of proposed mechanism, which can improve the adoption rate as direct evidence in judicial decisions.
During the last few years, it has become more and more conpeling in mobile commerce applications, mobile commerce technology is convenient, but also produces a series of security compromise. Payment is an important part of the network security. In this paper, we proposed a secure roaming payment protocol based on IBS, which considers effective mutual authentication between customer and the forge in domain bank and realizes roaming payment. Based on IBS signature, a signature and verification scheme is designed in one round mutual authentication and roaming payment. Security analysis shows, basing on the security of IBS, the proposed protocol with universally compos able(UC)model", "it proves that the proposed protocol satisfies the definition of UC security defined in the UC model. scheme is sufficient for private key privacy, signature unforgeability.
Nowadays, as the scale of IP network and the number of its users increasing, requirements for higher quality of service and better network security have significantly grown. Network test is an important way to ensure the efficiency and reliability of network. Protocol RFC2544 is an international standard to evaluate network equipments, and testers based on this protocol can achieve more accurate results. This article firstly outlines the contents of RFC2544, introduces algorithm design for the four test parameters presented by the protocol, and finally gives the solution to the technical problems during the implementation of the system.
Network traffic identification is one of the hot research fields for network management and network security; machine learning is an important method during the network traffic identification research.this paper describes the current situation and common methods of network traffic identification, at the same time this paper also states the currently popular Machine learning methods. We compared and evaluated the supervised and unsupervised classification and clustering algorithms, the experiment results show that feature selection algorithm has great effect on supervised machine learning and DBSCAN algorithm which belongs to unsupervised clustering algorithm has great potential in precision.
Intrusion prevention system is an important technique in the network security architecture. Most of the modern intrusion detection systems lack the ability to process massive data streams to achieve anomaly detection. Instead of Intrusion detection, Intrusion prevention system can be used for both servers and end-hosts to handle the dual challenges of accuracy and performance which the former lacks. Intrusion prevention can be done by processing the data stream on fly. It is a difficult issue since the streaming data have some tough characteristics, such as unknown or unbound size, possibly a variable arrival rate, lack of ability to backtrack over previously arrived transactions, and a lack of system control over the order in which the data arrive. Many applications rely directly or indirectly on finding the frequent items, and implementations are in use in large scale industrial systems. This paper will find a network model which is more suitable for high speed processing of massive data streams in real-time from various data sources by considering the frequency property of events. An Intrusion prevention system have been built with online mining of frequent item sets over a stream with Time-sensitive sliding window, which is one of the most important technique in stream data mining with broad applications. Our method is employed to prevent the system with high efficiency and low use of system resources.
In network security goals such as confidentiality, authentication, integrity and non-repudiation can be achieved using cryptographic techniques. Cryptographic techniques are techniques to hide information from unauthorized persons. The fact is, even when strong cryptographic algorithms and protocols are applied, the security of communication systems cannot be guaranteed [1]. Since cryptographic protocols can contain several types of flaws and vulnerabilities that can be exploited by attackers, cryptographic verification for suitability is needed to detect all possible flaws and attacks against them. In Wireless Body Area Networks (WBANs) the cryptographic implementation comes at the expense of performance and power conservation making engineering of security protocols suitable for low powered networks a challenging task. This paper will discuss the possible flaws and attacks on cryptographic protocols, verification methods to detect these flaws and investigate the implementation of these protocols in the WBANs.
This paper presents experimental results for calculating both node- and edge separators on Autonomous System graphs generated from BGP routing information. The separator of a network graph describes a range of interesting properties as it captures components that are critical to overall connectivity. These components play special roles in terms of routing and deserve special attention from those in-charge of network security and resilience. We present empirical evidence showing that the Autonomous System Graph (AS Graph) is hard to separate and large portions always remain connected even in the case of a significant number of concurrent Byzantine failures of Autonomous Systems or connections between Autonomous Systems.
Malware is a serious threat for modern information technology. It is therefore vital to be able to detect and analyze such malicious software in order to develop contermeasures. Honeypots are a tool supporting that task - they collect malware samples for analysis. Unfortunately, existing honeypots concentrate on malware that spreads over networks, thus missing any malware that does not use a network for propagation. A popular network-independent technique for malware to spread is copying itself to USB flash drives. In this article we present Ghost, a new kind of honeypot for such USB malware. It detects malware by simulating a removable device in software, thereby tricking malware into copying itself to the virtual device. We explain the concept in detail and evaluate it using samples of wide-spread malware. We conclude that this new approach works reliably even for sophisticated malware, thus rendering the concept a promising new idea.
Anomaly extraction refers to automatically finding, in a large set of flows observed during an anomalous time interval, the flows associated with the anomalous event(s). It is important for root-cause analysis, network forensics, attack mitigation, and anomaly modeling. In this paper, we use meta-data provided by several histogram-based detectors to identify suspicious flows, and then apply association rule mining to find and summarize anomalous flows. Using rich traffic data from a backbone network, we show that our technique effectively finds the flows associated with the anomalous event(s) in all studied cases. In addition, it triggers a very small number of false positives, on average between 2 and 8.5, which exhibit specific patterns and can be trivially sorted out by an administrator. Our anomaly extraction method significantly reduces the work-hours needed for analyzing alarms, making anomaly detection systems more practical.
This paper investigates reliable and covert transmission strategies in a multiple-input multiple-output (MIMO) wiretap channel with a transmitter, receiver and an adversarial wiretapper, each equipped with multiple antennas. In a departure from existing work, the wiretapper possesses a novel capability to act either as a passive eavesdropper or as an active jammer, under a half-duplex constraint. The transmitter therefore faces a choice between allocating all of its power for data, or broadcasting artificial interference along with the information signal in an attempt to jam the eavesdropper (assuming its instantaneous channel state is unknown). To examine the resulting trade-offs for the legitimate transmitter and the adversary, we model their interactions as a two-person zero-sum game with the ergodic MIMO secrecy rate as the payoff function. We first examine conditions for the existence of pure-strategy Nash equilibria (NE) and the structure of mixed-strategy NE for the strategic form of the game. We then derive equilibrium strategies for the extensive form of the game where players move sequentially under scenarios of perfect and imperfect information. Finally, numerical simulations are presented to examine the equilibrium outcomes of the various scenarios considered.
Researchers in the growing fields of digital and network forensics require new tools and techniques to stay on top of the latest attack trends, especially as attack vectors shift into new domains, such as the cloud and social networks.
It is critical to identify various P2P flows accurately for managing P2P traffic as well as for ensuring network security. In this study, a hidden Markov model -based P2P flow identification (HMM-PFI) method is put forward, which makes use of packet size, the inter-arrival time and the arrival order of packets to build flow identification model, the packets in sample flow are associated with the states of hidden Markov model (HMM). Moreover, some discrete random variables are introduced to depict the characteristics of HMM state. The method dramatically decreases the time needed for building the model and improves the real-time and accuracy in identifying unknown flows. In HMM-PFI, various P2P flows can be identified simultaneously. Meanwhile, the algorithm for selecting the number of HMM state is designed. In a controllable experimental circumstance such as our campus network, HMM-PFI is utilised to identify P2P flows whereas several other identification methods are chosen as benchmark. The results show that HMM-PFI can correctly identify the packet flows produced by various P2P protocols and it is preferably adaptive to different network circumstance.
Covert channels via the widely used TCP/IP protocols have become a new challenging issue for network security. In this paper, we analyze the information hiding in TCP/IP protocols and propose a new effective method to detect the existence of hidden information in TCP initial sequence numbers (ISNs), which is known as one of the most difficult covert channels to be detected. Our method uses phase space reconstruction to create a processing space called reconstructed phase space, where a statistical model is proposed for detecting covert channels in TCP ISNs. Based on the model, a classification algorithm is developed to identify the existence of information hidden in ISNs. Simulation results have demonstrated that our proposed detection method outperforms the state-of-the-art technique in terms of high detection accuracy and greatly reduced computational complexity. Instead of offline processing as the state-of-the-art does, our new scheme can be used for online detection.
Computer network security is one of the important issues in the Internet age. Network administrators of organizations such as companies or universities filter IP packets at network equipment such as Layer 3 switch or firewall between their organizations and the Internet to keep the security of the computer networks. One of the expressions of the filtering rules of IP packets is access control list. Access control lists are lists of rules, which describe permission or denial of packet transition based on source IP address, destination IP address, port numbers and so on. Access control lists are not always fixed; network administrators change access control lists according to the change of network topology or network security policy. After several changes, access control lists may include redundancies and network administrators have to modify the access control list to remove redundancies. This modification must keep the semantics of access control list. After modification, the network administrators must confirm that the semantics of access control list does not change. One of the methods of equivalence of two access control lists is to send test IP packets to the network equipment that filters IP packets and to check the transitions of the IP packets. This paper proposes the method of generating test packets to confirm the equivalence of two access control lists.
With the increased usage of Internet, network security attracts many researchers to propose various kinds of approaches. Data mining techniques are efficient to construct a reliable Intrusion Detection System. Classification is an essential task in data mining. In this paper, a new classification method is proposed to build an accurate and efficient classifier for intrusion detection. The new classification method utilizes the average distances of the new data to its closest neighbor points to classify it as normal or intrusion. Then, the distances of the data to the centroids of normal, misuse intrusion and anomaly intrusion is used to get the accurate class label of the data. In addition, this paper integrates Fuzzy GNP-based class association rule mining method to extract rules. Fuzzy GNP avoids the use of the domain knowledge and solves the continuous attributes efficiently. On the basis of the extracted rules, the multi-feature space is projected into a two-dimensional average matching degree space. The benchmark data KDD Cup 1999 and NSL-KDD are used to evaluate the performance of the proposed method.
Network filtering has become an important security issue worldwide. Network filters are designed and put in place to enforce restrictions for a variety of different motives, such as political, social, economical or merely security reasons. Although network filters can be applied to different networks, their main use is for the Internet. However, as is the case with most network security measures, many network filters are bypassed by users and thus are not completely adequate to perform their tasks. This paper approaches the network filtering concepts from a software engineering perspective. The general purpose of this approach is to utilize automated methodologies to analyze the correctness of the requirements of the filtering mechanisms, and to reduce their vulnerability. In order to achieve this, requirements are expressed using scenario-based specifications. The resulting scenarios are then analyzed for unwanted behavior using automated methodology. To demonstrate the effectiveness of this approach, it is applied to the case study of a real-life Internet-filtering system.
Systems and network defenses currently implementing a Defense in Depth (DiD) strategy frequently slow attackers' progress but do not act as a secure barrier. These systems of network defense methods are primarily comprised of static defenses focused on preventing attacks from entering a network by enabling the features of blocking access, requiring authentication, or analyzing traffic. To adapt to the ever-changing threat profile of network attacks, the DiD model must be adapted to be symmetric and focus on new vectors for defense instead of authenticating, blocking, or analyzing all traffic. Instead of a focusing on feature-centric network defense requirements, the DiD model should be redesigned to be a functional or capability focused model. Symmetry in the DiD model allows for the network defense system to recognize the insider threat, preventing data exfiltration and allowing attacks to be stopped at the originating network instead of being defended by the attacked network. Dynamic defenses must also be enabled, which change attack surfaces to proactively defend a network. New vectors, such as dynamic network addressing, enterprise computing resources, and network architectures, must be used by the DiD model to prevent attacks from reaching network, consuming attackers often limited resources, and securing networks in their design and architecture.
Nowadays, computers and networks have become the infrastructure of people's daily lives. As the Internet becomes more open to public, most people and organizations prefer to use online services. At the same time, the Internet security problem is becoming more severe. An effective and feasible protection measure for network security is necessary. Intrusion detection system (IDS) is a typical measure used to provide protection for information systems. In order to detect unknown attacks, anomaly detection technique is a commonly used method. Meanwhile, many scholars are studying more effective and easy-to-use anomaly detection methods. In this paper, a new intrusion detection approach inspired by the function of biological memory cell is proposed. The detectors in this approach (which called memory detector) are more effective than common detectors. The generation method of memory detector is detailed. In order to analyze and evaluate the performance of this approach, comparative experiments are implemented with training and testing data from &#x201C;Knowledge Discovery and Data mining cup 1999&#x201D; (KDD99) dataset. The results show that our approach provides better performance than ordinary anomaly detection approaches with higher true positive rate and lower false positive rate.
Botnets are the most serious network security threat bothering cyber security researchers around the globe. In this paper, we propose a proactive botnet detection framework using Support Vector Machine (SVM) to identify P2P botnets based on payload independent statistical features. Our investigation is based on the assumption that there exists significant difference between flow feature values of P2P botnet traffic and that of normal web traffic. However, we don't see a significant difference among flow feature values of normal web traffic and that of normal P2P traffic. Therefore, we combined normal web traffic and normal P2P traffic for the purpose of binary classification. Furthermore, we tried to evaluate the optimum SVM model that provides the best classification of P2P botnet data. Our optimized method yields approximately 99.01% accuracy for unbiased training and testing samples with a False Positive rate of 0.11 and 0.003 for bot and normal data flows respectively.
A large amount of research effort is focused on developing methods for correlating network intrusion alerts, so as to better understand a network's current security state. The accuracy of traditional static methods of correlation is however limited in large-scale complex systems, where the degree of human insight and validation necessary is higher, and dynamic attack behaviours are likely. Many recent efforts have centred around visualising security data in a way that can better involve and support a human analyst in the network security triage process but this potentially gives rise to another complex system of analytical and visual components which need to be configured, trained and understood. This paper describes an agent-based framework designed to manage a set of visual analytic components in order to improve a security analyst's understanding and ability to classify the threats to the network that they govern. In the proof-of-concept system an agent selects the most effective method for event aggregation, given a particular set of events which have been generated by an Intrusion Detection System (IDS). We present a novel application of a dynamic response model in order to configure the aggregation component such that the data is best simplified for more effective further analysis.
Recently, even operating systems are often compromised by the attackers. Since a compromised operating system affects all the applications including security software on top of it, the integrity of the operating system should be guaranteed. However, it is difficult to monitor the operating system securely. In this paper, we propose SPE Observer, which is a framework for securely monitoring operating systems using SPEs in Cell/B.E. SPE Observer guarantees the integrity and confidentiality of monitoring systems by the isolation mode of SPEs. To complement the isolation mode, SPE Observer monitors the running status of monitoring systems from an external security proxy. In addition, it schedules monitoring systems to mitigate the performance degradation of applications due to occupying SPEs. We have implemented SPE Observer in PlayStation 3 and developed the integrity monitor of the operating system. According to our experiments, it was shown that the integrity monitor on an SPE could detect a compromised operating system and that the application performance was dramatically improved by scheduling the integrity monitor.
Internet flows characteristics are important reference for network behaviors research. However, there are few latest related studies and comparative analysis on different networks is especially rare. Based on traffic traces from CAIDA and CERNET, this paper presents a detailed and comprehensive comparison on Internet flows characteristics, including the fine-grained distributions of lifetime, size, rate, life stage, port and protocol of flows. Our comparative research and conclusion can provide a latest data support for other studies of network behaviors, traffic classification, network performance and network security etc.
The following topics are dealt with: cooperative relay; wireless network; mobile network; multimedia communication; interference reduction; future Internet; quality of service; MIMO communication; cognitive radio; optical network; compressed sensing; ad hoc network; signal processing; VANET; green communication; coding; OFDMA; WLAN; WPAN; switching and routing; and network communication security.
Intrusion Detection Systems (IDS) play an important role in network security. The main challenge is how to find occurrences of patterns defined in the rule set which describe the signature of malicious activities. In this paper, we propose an efficient exact pattern matching algorithm based on the bit-parallel approach. Experimental results show that our algorithm outperforms the traditional Aho-Corasick automaton at the cost of a small number of false positives.
As a result of continually changing Internet and applications, more and more advanced features are requested to be available in the appliance for more accurately monitoring and managing the network. Therefore, modern networking appliances are equipped with the DPI (Deep Packet Inspection) technology to scan the payload of a packet. A rule (like Snort rules) may consist of several patterns with certain relationships, such as order, relative positions, and offset, etc. The system performance is usually dominated by not only the pattern matching algorithm but also the rule match processing algorithm. This paper proposes a unique-pattern based pre-filtering method for the rule matching. It is employed to filter out unwanted matches after scanning the packet payload by the pattern matching algorithm. The proposed algorithm is also implemented on different multi-core platforms to demonstrate its efficiency and performance. The experimental results indicate that the throughput is improved significantly and can be increased approximately linearly to the number of CPU cores.
Minority identification is an important issue in network security and financial applications. This paper considers the direct maximum reachability distance of an object and the indirect minimum reachability distance of an object for measuring the degree of an object being minority. The data classification is performed by an optimized combination model. We empirically evaluate the proposed approach using a number of UCI data sets, and experiment results demonstrate that our method outperforms the existing methods in terms of the comparisons of ROC curves.
Intrusion Detection has shown great potential in network security research. Most existing intrusion detection methods treat all data in the network as a whole. However, in reality, data in the network could be divided into two categories: upload data and download data. When intrusion takes place, these two types of dataflow may have different characters. Based on this discovery, we proposed a novel intrusion detection method (U-D method) taking both upload and download data into consideration. With the enhanced separately analysis method, we could figure out intrusion clues more effectively and efficiently. We wonder the relationships between these data might contain some instinct clue for discovering important intrusions. Experiment results demonstrate the effectiveness of our approach.
Network vulnerability analysis is one of the important techniques to protect network security. Modeling and classification of network vulnerability are introduced firstly, then the concept of attack capability transfer and the algorithm to produce it are presented, which can aggregate vulnerabilities with the same exploitation attributes and satisfying some constrains to simplify the further analysis. Based on the attack capability transfer, a new method constructing attack graph is presented, and the complexity is O(N2) where N is the number of hosts in a network. Through the analysis of attack graph, network vulnerability quantitative analysis is taken and security hardening method based on approximate greedy algorithm is presented, the complexity of which is O(V), where V is the number of vulnerabilities in a network. Experiment shows the effectiveness of the method.
The following topics are dealt with: Web IR; XML querying; social Web; user behavior; recommendations; clustering; virtual worlds; sponsored search and network security.
PFM, the ProtoFlight Model (GSAT0101), was launched together with FM2 on October 21, 2011. According to CONGO(COoperative Network for GIOVE Observation) network tracking reports, PFM started transmitting E1 signals on December 10, 2011. This paper presents the processing results of the E1 signal transmitted from the Galileo PFM satellite. The PFM E1 signal was acquired in ETRI, Korea. And then, the signal processing is performed using the E1 software receiver implemented by ETRI. The processing results of the signal acquisition and tracking show that the PFM E1 signal is normally being transmitted in Korea.
This paper explores the impact of dynamic generation prices (DGP) on transmission network capacity planning. A model based on the framework of DC optimal power flow is proposed to solve the transmission planning problem considering DGP, as well as the network security constraints. The transmission planning problem is formulated as a mixed-integer optimisation model with the objective of minimizing the costs of network constraints and transmission investment over several typical daily load scenarios. The methodology is illustrated with the IEEE 14-Bus test system under weak, relatively strong and strong network conditions.
Considering the influence of the unknown uncertainties, a robust adaptive neural network control approach is employed to the controller design for permanent magnet spherical motor (PMSM) under the continuous tracking mode. The unknown nonlinear model can be approximately learning by RBF neural network (RBFNN) system. With the aid of robust items, the external disturbance problem and the approximation errors are solved. It is proved that the proposed adaptive control scheme can guarantee the PMSM rotor dynamic system stability based on Lyapunov analysis. Simulation studies show the effectiveness of the proposed approach. Also, the results in this paper could serve as a basis for the future research and experiment.
With the impending era of internet, the network security has become the key foundation for lot of financial and business web applications. Intrusion detection is one of the looms to resolve the problem of network security. Imperfectness of intrusion detection systems (IDS) has given an opportunity for data mining to make several important contributions to the field of intrusion detection. In recent years, many researchers are using data mining techniques for building IDS. Here, we propose a new approach by utilizing data mining techniques such as neuro-fuzzy and radial basis support vector machine (SVM) for helping IDS to attain higher detection rate. The proposed technique has four major steps: primarily, k-means clustering is used to generate different training subsets. Then, based on the obtained training subsets, different neuro-fuzzy models are trained. Subsequently, a vector for SVM classification is formed and in the end, classification using radial SVM is performed to detect intrusion has happened or not. To illustrate the applicability and capability of the new approach, the results of experiments on KDD CUP 1999 dataset is demonstrated. Experimental results shows that our proposed new approach do better than BPNN, multiclass SVM and other well-known methods such as decision trees and Columbia model in terms of sensitivity, specificity and in particular detection accuracy.
Trust management frameworks are used to evaluate and manage trust relationships between network nodes and enhance network security. However, trust management frameworks themselves are vulnerable to attacks. Attacks against trust management frameworks are described in this paper with a trust management framework to resist them. The trustworthiness between nodes is evaluated to classify node behavior using a three-dimensional classifier based on a fuzzy integral. Different behaviors are mapped to different behavioral spaces to detect malicious nodes and identify their behavior types. The security of ad hoc networks is then improved by various measures to handle different types of malicious behavior. Simulations of the model on the System In The Loop (SITL) platform show that this trust management framework can separate normal nodes and malicious nodes and can distinguish different types of malicious nodes.
As networks become ubiquitous in people's lives, users depend on networks a lot for sufficient communication and convenient information access. However, networks suffer from security issues. Network security becomes a challenging topic since numerous new network attacks have appeared increasingly sophisticated and caused vast loss to network resources. Game theoretic approaches have been introduced as a useful tool to handle those tricky network attacks. In this paper, we review the existing game-theory based solutions for network security problems, classifying their application scenarios under two categories, attack-defense analysis and security measurement. Moreover, we present a brief view of the game models in those solutions and summarize them into two categories, cooperative game models and non-cooperative game models with the latter category consisting of subcategories. In addition to the introduction to the state of the art, we discuss the limitations of those game theoretic approaches and propose future research directions.
This work continues a trend of developments aimed at exploiting the physical layer of the open systems interconnection (OSI) model to enhance wireless network security. The goal is to augment activity occurring across other OSI layers and provide improved safeguards against unauthorized access. Relative to intrusion detection and anti-spoofing, this paper provides details for a proof-of-concept investigation involving air monitor applications where physical equipment constraints are not overly restrictive. In this case, RF fingerprinting is emerging as a viable security measure for providing device-specific identification (manufacturer, model, and/or serial number). RF fingerprint features can be extracted from various regions of collected bursts, the detection of which has been extensively researched. Given reliable burst detection, the near-term challenge is to find robust fingerprint features to improve device distinguishability. This is addressed here using wavelet domain (WD) RF fingerprinting based on dual-tree complex wavelet transform (DT-CWT) features extracted from the non-transient preamble response of OFDM-based 802.11a signals. Intra-manufacturer classification performance is evaluated using four like-model Cisco devices with dissimilar serial numbers. WD fingerprinting effectiveness is demonstrated using Fisher-based multiple discriminant analysis (MDA) with maximum likelihood (ML) classification. The effects of varying channel SNR, burst detection error and dissimilar SNRs for MDA/ML training and classification are considered. Relative to time domain (TD) RF fingerprinting, WD fingerprinting with DT-CWT features emerged as the superior alternative for all scenarios at SNRs below 20 dB while achieving performance gains of up to 8 dB at 80% classification accuracy.
Security automation continues to depend on signature models, but vulnerability exploitation is exceeding the abilities of such models. The authors, in reviewing the different types of mathematical-based constructs in anomaly detection, reveal how anomaly detection can enhance network security by potentially solving problems that signature models can't address.
Internet security problems remain a major challenge with many security concerns such as Internet worms, spam, and phishing attacks. Botnets, well-organized distributed network attacks, consist of a large number of bots that generate huge volumes of spam or launch Distributed Denial of Service (DDoS) attacks on victim hosts. New emerging botnet attacks degrade the status of Internet security further. To address these problems, a practical collaborative network security management system is proposed with an effective collaborative Unified Threat Management (UTM) and traffic probers. A distributed security overlay network with a centralized security center leverages a peer-to-peer communication protocol used in the UTMs collaborative module and connects them virtually to exchange network events and security rules. Security functions for the UTM are retrofitted to share security rules. In this paper, we propose a design and implementation of a cloud-based security center for network security forensic analysis. We propose using cloud storage to keep collected traffic data and then processing it with cloud computing platforms to find the malicious attacks. As a practical example, phishing attack forensic analysis is presented and the required computing and storage resources are evaluated based on real trace data. The cloud-based security center can instruct each collaborative UTM and prober to collect events and raw traffic, send them back for deep analysis, and generate new security rules. These new security rules are enforced by collaborative UTM and the feedback events of such rules are returned to the security center. By this type of close-loop control, the collaborative network security management system can identify and address new distributed attacks more quickly and effectively.
The following topics are dealt with: intelligent agents; consciousness and agent oriented software engineering; network security; Web content and service delivery; robotics; biomedical engineering; intelligent control and automation; image and signal processing; intelligent business system; computational biology; bioinformatics; and data mining.
Computer network security is very important for all business sectors. The Intrusion Detection Systems (IDS) is one technique that prevents an information system from a computer networks attacker. The IDS is able to detect behavior of new attacker which is indicated both correct Detection Rate and False Alarm Rate. This paper presents the new intrusion detection technique that applied hybrid of unsupervised/supervised learning scheme. To combine between the Independent Component Analysis (ICA) and the Support Vector Machine (SVM) are the advantage of these new IDS. The benefit of the ICA is to separate these independent components from the monitored variables. And the SVM is able to classify a different groups of data such as normal or anomalous. As a result, the new IDS are able to improve the performance of anomaly intrusion detection and intrusion detection.
With rapid development of the Internet, Internet surfing has been an important daily activity for peoples. There are many websites and web pages containing obscene and malicious information such as phishing, spam and pornography that peoples can easily obtain. In order to improve network security service, we developed and implemented a content-based filter system on an embedded Linux home gateway. We analyzed and compared two feasible approaches that the content-based filter was respectively embedded into user space and kernel space of Linux home gateway. The content-based filter system comprises URL (Universal Resource Locator) blocker and keyword filter in the kernel space. The Linux home gateway embedded with our new content-filter was evaluated by extensive experiments. The experiment results showed that the content filter method that based modification of kernel space outperforms around 35% improvements than that of based on user space.
As an essential and significant part of network security, the security of web applications has received more and more attentions at present. In this paper, we review the security of current web applications, and enumerate the most common attacks on them such as injection, cross site scripting, and insecure direct object references. Then by taking injection attack as an example, we explain the principles of injection attack and analyze the reasons for the vulnerability. Finally, to prevent these attacks, we provide several valuable suggestions.
This paper surveys the field of security separation technology from a network security perspective. The basic idea of security separation is to use physical or logical measures to protect computers with sensitive information from external or internal threats. The paper first discusses two basic concepts of security separation, i.e., end device separation and network separation. After that, it introduces four different techniques and products to implement the separation idea, and presents the primary characteristics of them. Finally, it makes a conclusion and provides some suggestions for future work.
For the characteristics of ad hoc networks, group key management analysis of the current research results and problems, group key management protocol based on NTRU public key cryptosystem was proposed. Trusted center offline to generate key and other initialization parameters for each member of the group. In the group key agreement process useing the NTUR digital signature to realize the user and the private key information authentication. The correctness and security of the protocol are proof.. In satisfies ad hoc network security conditions, The agreement is simple in design and requires limited calculation and limited storage space etc benefits.
Both legitimate users and attackers use web resources to realize their goals. These goals are either desired or malicious in terms of the role of the users. This paper presents a novel approach for testing web security by focusing on the web resources used/exploited by both legitimate users and attackers.
Detection of malicious traffic and network health problems would be much easier if ISPs shared their data. Unfortunately, they are reluctant to share because doing so would either violate privacy legislation or expose business secrets. However, secure distributed computation allows calculations to be made using private data, without leaking this data. This paper presents such a method, allowing multiple parties to jointly infer a Hidden Markov Model (HMM) for traffic and/or user behaviour in order to detect anomalies. We extend prior work on HMMs in network security to include observations from multiple ISPs and develop secure protocols to infer the model parameters without revealing the private data. We implement a prototype of the protocols, and our experiments with the prototype show its has a reasonable computational and communications overhead, making it practical for adoption by ISPs.
A flow is active during a measurement period if it sends at least one packet. It is essential to control resource consumption in network monitoring by traffic sampling, so most high-end routers provide sampling network traffic function. The number of original flow is an important metrics to many network applications such as congestion control and network security. In this paper, we present a EUF algorithm that can accurately infer the number of original flow from the sampled packet traffic. Using the EUF algorithm, the number of unsampled flows can be estimated by an iteration method based on random sampling, and then the number of original flows can be precisely estimated according to both the number of the sampled flows and the unsampled flow. The EUF algorithm is also compared with the EM algorithm using multiple traffic traces collected from a Tier-1 ISP backbone networks. The result shows that the EUF algorithm is superior to EM and can provide highly accurate estimation on the number of the original flow.
This paper proposes a novel distributed agent model for network security evaluation. The method, which uses antibody concentration to quantitatively describe the degree of intrusion danger, is demonstrated. A new network security evaluation method using antibody concentration to quantitatively analyze the degree of intrusion danger level is presented. The method which uses antibody concentration to quantitatively describe the degree of intrusion danger is presented. Additionally, the hierarchical and distributed management framework of the proposed model is built, avoiding neglecting uncertain factors that the traditional method often does. Our experimental results show the model which enhances detection efficiency and assures steady performance in the ability of intrusion detection.
On the basis of in-depth study of the relaying protections' free distribution characteristics in digital substation, a solution to cope with the transitional situation is proposed against the background of little technology maturation in primary apparatus and the communication on process level. A design method of relaying protection software based on VxWorks is provided, and the execution path, interrelations and communication mode between the tasks is elaborated.
Malware is one of the major security threats that can break computer operation. However, commercial anti-virus or anti-spyware that used signature-based matching to detects malware cannot solve that kind of threats. Nowadays malware writers try to avoid detection by using several techniques such as polymorphic, metamorphic and also hiding technique. In order to overcome that issue, we proposed a new framework for malware behavior identification and classification that apply dynamic approach. This framework consists of two major processes such as behavior identification and malware classification. These two major processes will integrate together as interrelated process in our proposed framework. Result from this study is a new framework that able to identify and classify malware based on it behaviors.
At present, as the high speed development of Internet, there are more and more information, some of them are false information. Law enforcement is difficult, but the network forensics method is not only cumbersome, but also not easily admissible in the court. This paper based on the requirement of the network electronic forensics, put forward the network electronic forensics process model based on the requirements, and a clear network electronic forensics model of the system architecture in order to describe the basic principles that the network electronic forensics process should followed.
With the gradual popularization of the Internet, campus network construction has become more sophisticated, to the intelligent direction. Apart from that, the campus network security is growing importance, Design a very effective defense hacker attacks, viruses, data theft, and internal defense system is the focus of the study in this paper. This paper compared the firewall; IDS based on the integrated, then design of a campus network security model, and detail the specific implementation principle, methodology and the specific model implementation chart.
In this paper, an objective function consisting of generators, bus bars and lines vulnerability indices for optimal placement of facts devices including UPFC, SVC and TCSC has been presented. To calculate the vulnerability and network security indices, an appropriate model of mentioned devices to solve the load flow calculation is used. Also, in this paper, the optimal placement of facts devices has been done by two ways. In the first case, placement and installation of them is performed individually and in the next state the placement of these devices have been done simultaneously. For the case studies, IEEE 30-buses test system is selected and a UPFC, SVC and a TCSC are placed in the system simultaneously and non-simultaneously. Results show that these devices have better influences in the simultaneous placement mode than the non-simultaneous mode, and could improve voltage profile and network security margin.
With the rapid development of Internet, network database security has become the focus of network security. The research of database protection technology against SQL attacks has become very urgent. In this paper, we analyse principles of SQL attacks, study a database protection system which is used between the Web application and the database. The system provides different protective measures for ordinary users and administrators to effectively guarantee the security of the database. The role of a Web application and database in the database between the protection system for ordinary users and administrators were made by different users of protective measures to effectively guarantee the security of the database.
Network equipments are developing continually along with the network size expansion. Because switchboard function increasingly intelligent and variety, the relative management system also upgrade and update uninterrupted. Management systems control the network equipments directly, so its property decides the whole network operational capability. Switchboard is the most important equipment of local area network creation and network security. So the switchboard network management system is the emphasis attention for research staff and users. Ethernet, ATM and TDDI are the most extensive applied network at present. Masses of network companies in the domestic and overseas has open out the switchboards on the basis of these three networks. And relative management systems also developed by the software houses at the same time. This is very important for the secondary development of switchboard and its system.
In 3G mobile communications, network security is mainly ensured by the AKA protocol and the complexity of encryption algorithms. After successful operation of AKA protocol, the serving network VLR could corroborate the identity of the user and the user could definite the validity of HLR. If authentication is successful, the corresponding cipher key CK and integrity key IK in the authentication vector shall be taken in use to encrypt data in both the RNC and the ME. This paper described the 3G AKA protocol process and its security features, and made an analysis of the potential security vulnerabilities and possible attacks. To solve these vulnerabilities, the author finally put forward a reliable and effective improved proposal. Finally, an analysis of the improved protocol has been made.
Facing up complex and various Hacker's attacking, a new approach of constructing the dynamic and adaptive virtual network based on virtual honeypot and network scanning is presented in this paper to puzzle adversaries, delay and divert attacks from their real targets, exhaust attacker resources, and collect the attacking information. Firstly the elementary concepts of honeypot and Honeyd is examined, then the techniques of network scanning used in the virtual network system are analyzed especially the active TCP/IP stack fingerprinting, and then this dynamic and adaptive system is designed and implemented, furthermore the virtual network is comprehensively tested, the experimental results prove this design can simulate the real network environment successfully.
In order to make the network authentication convenient, secure and reliable, this paper presents a solution based on USB Key technology. It constructs a logical model based on USB Key, shows the process of network authentication, and gives a specific example using ASP, thus effectively solving the problem of network authentication. Practice has proved that this solution is feasible.
Along with the computer information network technology high-speed development and Internet extensive application, network information and data security is becoming more and more important. The problems of detailed analysis of the confidential computer information system safety protection of the main problems existing in the, Introduce the computer network in today's facing threats. The problems and in the face of such threats should take safety countermeasures. Refer to the computer information system security protection rules and principles, The problems of study the information security management system multi-layer various protection technology measures, The problems presents a comprehensive reliable information safety management system design and realization method.
There are many applications of using association rules in data streams, such as market analysis, network security, sensor networks and web tracking. Mining closed frequent item sets is a further work of mining association rules, which aims to find the subsets of frequent item sets that could extract all frequent item sets. Formally, a closed frequent item set is a frequent item set which has no superset with the same support as it. One of well-known algorithms for mining closed frequent item sets based on the sliding window model is the New Moment algorithm. However, the New Moment algorithm could not efficiently mine closed frequent item sets in data streams, since they will generate closed frequent item sets and many unclosed frequent item sets. Moreover, when data in the sliding window is incrementally updated, the New Moment algorithm needs to reconstruct the whole tree structure. Therefore, we propose the Subset-Lattice algorithm which embeds the property of subsets into the lattice structure to efficiently mine closed frequent item sets over a data stream sliding window. Moreover, when data in the sliding window is incrementally updated, our Subset-Lattice algorithm will not reconstruct the whole lattice structure.
With the rapid development of the Internet, the network structure becomes larger and more complicated and attacking methods are more sophisticated, too. To enhance network security, Network Security Situation Analysis (NSSA) technology is a research hot spot in the network security domain. But at present, the NSSA framework and model which not only analyze the affected results of the network security but also the process how the network security is affected are less. In this paper, a novel NSSA framework is presented. The framework includes two parts: calculate the Network Security Situation Value (NSSV) and discover intrusion processes. NSSA quantitative assesses the impact on network security caused by attacks upon Analytical Hierarchy Process (AHP) and hierarchical network structure. Based on attack classification, intrusion processes discover the process how network security is affected. At last from the experiments results, NSSV exactly changes as attacks take place and the accurate intrusion processes are discovered. The applicability of the framework and algorithms are verified.
This paper presents the simulation results relevant to the 15.2.7 Working Package of the European SESAR Project1. The main goal was to conduct a risk assessment of network security for the AeroMACS airport network. The risk analysis is based on a new approach for network security assessment that measures quantitatively the network risk level. Critical aspects such as the impact of a successful attack on a node and the risk propagation of that attack within an aeronautical wireless airport communication network have been taken into account. We specifically focus on the access network vulnerabilities, and a first network risk study is conducted for a predefined scenario. Some security guideline are provided to enhance the security policies and to improve the end-to-end security using some additional mechanisms such as certificate-based authentication.
Virtual Private Networks (VPNs) are overlay networks established on top of a public network backbone with the goal of providing a low cost but secure network solution. The allocation of bandwidth for VPN tunnels to meet the requirements specified by customers is an important traffic engineering research issue. This paper addresses the general problem of computing a constrained VPN with a tree topology, and having optimum bandwidth allocation. This is a hard combinatorial optimization problem [1]. This issue was studied first by different resource allocation methods [2] and next was studied by general optimization methods [3]. In this paper, we present integer programming formulations (IPFs) assuming a hose as well as a pipe workload models. Earlier work [2] showed that the hose model results in VPN trees over provisioned by a factor of 2-to-3. Extensive simulations using test networks and their corresponding hose workloads, generated with Brite [4] show that our exact methods produce VPN tree solutions with a bandwidth over-provisioning factor as low as 1.6, and never exceeds a factor of 2, compared to solutions obtained using a pipe workload.
The group of security standards in WS-Security is used to secure exchanges of SOAP messages in Web Service environment. However, despite all of these security standards, SOAP messages can still be vulnerable to types of attacks based on the malicious interception, manipulation, and transmission of SOAP messages. We refer to these types of attacks as XML Signature Wrapping Attacks. In this paper, we propose an approach on wrapping-attack tolerant SOAP messages called UNWRAP. In our approach, we first build SOAP message elements structure using ontology and then attach it in SOAP message header. By validating the ontology in the receiving end, we will be able to detect attacks early in validating process. Also, in our approach, all modifications on SOAP messages are written to a log. So if security failures are occurred, we could check this log and recover from effect of successful execution. Experiments show that the proposed solution has better performance in securing the exchange of SOAP messages.
Most of the network security applications in today's networks are based on Deep Packet Inspection (DPI). It is a form of computer network packet filtering that examines not only the header portion but also the payload part of a packet as it passes through an inspection point, searching for protocol noncompliance, viruses, Spam, intrusions or some predefined criteria to decide if the packet can pass it or it needs to be routed to a different destination. Most of the systems that perform deep packet inspection implement basic string matching algorithms to match packets against large but finite strings. However, there is growing interest in the use of regular expression-based pattern matching, since regular expressions offer superior expressive power. DFA is employed to implement regular expression matching. DFA representations of a regular expression sets in network applications require large amounts of memory, limiting their practical application. This paper presents an analysis of different compact representation of DFA such as D2FA, FA, 2FA.
In this paper, we have observed that the performance analysis of one of the most challenging security issue for wireless network i.e. Black hole attack. WiMAX-WLAN interface will play a important role in the NGN (Next generation Network) scenario. This attack is possible in WiMAX-WLAN interface network i.e. in current scenarios. In this paper we analyzed the performance analysis of Performance Analysis of Black Hole Attack on WiMAX-WLAN Interface Network. In this type of attack an intruder is a malicious node with less buffer size moving on its defined trajectory and passing from WiMAX-WLAN converter. In black hole attack, a malicious node uses its routing protocol in order to advertise itself for having the shortest path to the destination node or to the packet it wants to intercept. This aggressive node advertises its availability of fresh routes irrespective of checking its routing table. In this technique attacker node will always have the accessibility in replying to the route request and thus intercept the data packet and retain it.
To visualize the VAST 2012 Mini Challenge 2 datasets, we use the InfoVis Toolkit (IVTK). Custom visualizations as well as extra interaction capabilities have been added to the toolkit. Custom-made Python scripts are used for data preprocessing purposes. In this work, we show how visualization tools may be combined to leverage network forensic analysis tasks.
NetSecRadar is a visual analytics system to aid in monitoring the network security in real time and perceiving the overall view of the security situation using radial graph. At present, we use this tool mainly for IDS alerts to analyze the irregular behavioral patterns, and synthesize interactions, filtering and drill-down to detect the potential intrusions. In conclusion, we describe how this system was used to analyze the mini-challenges of the 2012 VAST challenge.
Security KISS is a popular virtual private network tool used to protect privacy, ensure anonymity and bypass Internet restrictions. In this paper, we use Black-box analysis method and White-box analysis method to analyze the communication behavior and the encryption algorithm of the software. We get the workflow and the internal structures of the software in detail. In addition, we analyze the security of the software, and point out the defects existed. Finally, experimental results verify the accuracy and reliability of our analysis. This shows that the method we proposed to analyze network software is very efficient.
As an active and dynamic security-defense technique, intrusion detection can detect the interior and exterior attacks, and it plays an important role in assuring the network security. A radial basis function (RBF) neural network learning algorithm based on immune recognition algorithm which based on the clonal selection principle recognition principle was studied. In the algorithm, the input data was regarded as antigens, and antibodies are regarded as the hidden layer centers. The weights of the output layer are determined by adopting the Recursive least square method, which can improve convergence speed and precision of the RBF neural network. This algorithm was applied to Intrusion Detection Systems. Theory and experiment show that this algorithm has better ability in intrusion detection, and can be used to improve the efficiency of intrusion detection, reduce the false alarm rate.
Computer and network security incidents have financial and other consequences to organizations, such as direct business losses from theft of proprietary information or from just reputational damage. There are also costs for restoring operations and protecting against threats. Being able to quantify the impact of different factors within an organization may provide additional context for prevention and remediation efforts. This paper examines a large set of security incident data along with some population characteristic data from an organization's network. We discuss the rationale for examining the different population characteristics and their potential influence on computer security incidents. We then create logistic regression models using the population characteristics to forecast which machines in the population may be involved in a computer security incident. We evaluate the models using the forecasts as a set of unequal probability weights combined with repeated sampling. We also explore different time windows used for the inclusion of data during model creation.
High security level network is always physically isolated from other networks, but also has difficulty in information transmission. Net Gap keeps different networks cut, and transmitted information from high security network to low. It uses QR code to carry data, and keeps information transfer unilaterally. MD5 algorithm is used in digital signature and RSA arithmetic is used encoding and decoding process. Experiments approve its efficiency and security. The system is in high security and utility.
In order to realize the full rang of information security, a variety of network equipment, safe equipment have been applied to deal with all aspects of information security and protection by many enterprise. These devices, systems produce a lot of security event log in the network security protection, and these event log data format are different, and different safety equipment may generate the same alerts logs, not only resulting in generating redundant events, but not conducive to the next work of network security situational awareness. Therefore, this paper proposed a method by using the multi-agent technology to collect and analysis the log data generated by network devices and security devices, and then generating a fixed-format data structure and building the log collection and analysis systems to facilitate the later maintenance and use of data.
Aiming at wireless network attacks such as DoS attacks, rouge STA, rouge AP, War Driving attacks and brute force attacks, a Lightweight Intrusion Detection System(IDS) for Wireless Lan(WLAN) is implemented by combining the misuse detection and anomaly detection. In this system, the user can define attack rule set, authorization AP/STA list, illegal AP/STA list, the sensitivity and the threshold value of detection can adjust according to the circumstance and user requirement. The test shows that this system has a better detecting effect than other WLAN intrusion detection in market.
With the increase of network complexity and continuous development of network attack techniques, it is unrealistic to prevent network from attacks or intrusion threats absolutely, a large number of critical network services require that they can provide adequate qualities of services in the face of suffering intrusions or even certain parts of network system be destroyed. Therefore, network survivability technologies in open environment are becoming a research focus in network security field. This paper proposes network threat evolution model based on threat evolutionary behaviors and threat propagations. Then, network survivability is abstracted as a dynamic game process among network attacker, network defender and normal user, thereafter network survivability stochastic game model is established and network survivability analysis algorithm is proposed based on the game model. Finally, the proposed network survivability analysis model and approach are experimented in a typical network environment. The results show that the analysis model and approach proposed are feasible and effective.
The field of information security of data privacy issues in the existing network deployment environment is facing the more and more challenges. The one hand, the physical isolation of high-security domain is specifically requested by the State Secrecy Bureau, On the other hand, the challenge of security and efficiency of high-security domain when low security domain importing data to it. This paper is based on more than the status quo, not only analysis of the traditional isolation devices and introduced the spectrophotometric mechanism into the isolation system, but also use PF_RING technology to optimize the system and designed one-way transmission protocol, designed a new one-way isolation system. This system has a fundamental solution to the problem of data leakage, improved the efficiency of one-way import under the premise of protecting the security.
In this paper, we present a network detection method based on collaborative model of network threat attacks, as well as trend analysis of network structure. First of all, the collaborative model is given a specific framework, build process and collaborative mechanisms. Then ripe for pattern matching algorithm and behavioral sequence template for a simple introduction to this approach, and explains how to use the collaborative model structure. Finally, the security situation of the entire network is analyzed by a quantitative situation evaluating model, The experiment results shows that, during its running in an intranet security guard system of a large enterprise, the next-step attack can be predicted by our algorithm, and the security situation of the entire network can be accurately evaluated as well.
By analyzing the current computer network security situation and the existing intrusion detection system, this paper introduces the data mining technology into the intrusion detection system and structures behavior model and invasion information database of intrusion detection system based on data mining. This model improves the efficiency of intrusion detection.
Trojan is a threat to network security which poses a serious threat to national security. Through research on Trojan communication process, this paper proposes a data stream clustering method based on packet timestamp. This method uses cluster to compress Trojan communication data stream information, extracts cluster characteristics and then detects Trojans according to the cluster characteristics and correlation between the clusters. The experimental results showed that this method achieved good detection results.
Internet security is facing a growing challenge, and many security monitoring systems based on packet inspection have been unable to fully meet the requirements of high-level security monitoring. In this paper, based on full web hyperlinks reconstruction and the hot topic of automatic discovery mechanism, a network security monitoring system is proposed, which is able to achieve the real-time monitoring, to reproduce the complete content of the WEB browser, to cluster and discovery the hot topic automatically, and to monitor web content intelligently.
In this paper a communication monitoring technology for host based on virtual man chine monitor is proposed. It can use the high privilege of virtual machine monitor to monitor the communication by capturing the network interruption of network interface card and analysing the data of network interface card. The results show that it can monitor the communication of host and effective protect the host.
Managing complex enterprise networks requires an understanding at a fine granularity than traditional network monitoring. The ability to correlate and visualize the dynamics and inter-relationships among various network components such as hosts, users, and applications is non-trivial. Network security visualization is a highlighted topic of network security research in recent years, The existing research situation of network security visualization is analyzed. the paper first proposed the network security situation awareness model, and analysis network security situation awareness method, at last, and designed and implemented the security situation visualization prototype system based on geographic information systems, network topology graph, attack paths. The security situation data show in multiple views, multi-angle, multi-level display to the user by visualization technology, therefore the performance of the security situation will be more accurate and vivid, assessment of network security situation become timely and accurate, laying the foundation for rapid decision-making.
In order to solve the problem of the lack of fairness and poor security in online games, this paper presents a multi-granularity reputation management model for P2P MMOG. In order to accurately evaluate the reputation of the different roles of the games players, the model assesses the reputation of different roles taken by players respectively and calculates the player's reputation by fuzzy theory. This paper also gives the algorithm of reputation evaluation. In addition, this paper introduces the incentive mechanism to encourage players to take the initiative to carry out evaluation on the quality of coordinator service and act as a coordinator. The experimental results show that the multi-granularity reputation can provide a more appropriate credit support in related fields and provide a fair and credible gaming environment for the multi-player games.
In Windows kernel drivers, different function paths will be called according to the DeviceIoControlCode parameter in DeviceIoControl request. In order to achieve high-coverage security testing of these function paths, a new testing method using symbolic testing is proposed in this paper. With the automatic detection of Dispatch routine, symbolic testing is used to analyse the Dispatch function, walk through all supported function paths and get all DeviceIoControlCodes and check constraints. More specific test cases are generated with the guide of the codes and constraints. To compare the coverage of function paths, drivers of six famous security software are tested. Traditional testing method performs an average of 35% coverage, while ours performs 90%. Our tool discovers a previously-undiscovered priviledge escalation vulnerability of BitDefender2012 and some denial-of-service vulnerabilities, which prove the validity of our method.
DIDS (Distributed Intrusion Detection System) is the best project for detecting distributed intrusion. On account of complicated architecture, difficulty for deployment and no condition for testing, DIDS has not been emploied. This paper analyzes the existing architecture of DIDS, and proposes the idear of designing a RIDS (Reconfigurable Intrusion Detection System), and proposes the construction principle of component and analysis for reconfiguration. In addition, this paper formally describe the RIDS, and present two measurement for reconfiguration based on the different object.
Introducing the trust evaluation into the access control for the service requester in a distributed network environment could be helpful for enhancing the precision and security of authorization decisions, special to the strange requesters. The existing researches of the trust evaluation could be divided into the objective means and the subjective means. The subjective evaluations could be used to dynamically measure the user's trust degree according to the reputation based on its past actions. But it cannot work when the interaction experience with the user is absence. Compared with other subjective evaluation models using the probability theory, the fuzzy based model could exactly depict the unsure and fuzzy phenomena in process. But the existing fuzzy method based on the fuzzy synthetic evaluation needs to do complex calculations between trust vectors. And the chosen operators are not enough accurate. In this paper we promote a comprehensive trust evaluation framework was a fusion of objective and subjective means. An initial trust view for user is provided based on the certified trust and a reputation is derived by the fuzzy synthetic evaluation method based on user's past actions. The proposed fuzzy method can avoid complex calculations between trust vectors, and adopts a more accurate operator to avert the information wastage.
Along with the development of the Internet, university network system has a lot of potential safety problems. Information security level protection system has been included in the state council of the People's Republic of China concerning the strengthening information security work among the opinion ". In information system adopt to enforce the protection of classification of development idea. In this paper, starting from level protection model to explore network information safety system, analyzes the university. Aimed at sprang out of college, hope more attention, network information safety for university teachers and students a secure network home.
Sina Weibo is the most popular and fast growing microblogging social network in China. However, more and more spam messages are also emerging on Sina Weibo. How to detect these spam is essential for the social network security. While most previous studies attempt to detect the microblogging spam by identifying spammers, in this paper, we want to exam whether we can detect the spam by each single Weibo message, because we notice that more and more spam Weibos are posted by normal users or even popular verified users. We propose a Weibo spam detection method based on machine learning algorithm. In addition, different from most existing microblogging spam detection methods which are based on English microblogs, our method is designed to deal with the features of Chinese microblogs. Our extensive empirical study shows the effectiveness of our approach.
In this paper, an Artificial Immune System based on Hollanda?s Classifier is proposed as a new method for network intrusion detection. This paper is not aimed to provide a comparative study but to give more understanding on the feasibility of combining Artificial Immune System and Hollanda?s Classifier to detect network intrusion. This new Artificial Immune System, named AIS-CS, can attain higher than 90% intrusion detection with a false negative percentage below 10% and a fairly low false positive rate on a network composed of 50 regular nodes and 50 intruders. The experiments appear to suggest that the best performance can be found by setting the tolerization and the simulation parameters differently. Since there are numerous parameters involved, more experiments need to be performed to further measure this Holland classifier based Artificial Immune System.
We innovatively propose a honeypot monitoring system for mobile communication to decrease the possibility of smart phones being attacked by viruses. The system applies the idea of active honeypot theory and combines with communication protection. We initially clarify the high safety protection performance of our honeypot monitoring system in the following steps: we simulates the wireless access environment and then capture the mobile phones; after that, our system can search for potential attacks, as well as analyze and monitor their communication behaviors. We discuss the feasibility and efficiency of our system by testing an Android smart phone which carries a malware that can embezzle the phone's address book. After initially triggering the malware, we use our honeypot system to show the monitoring result. Our honeypot monitoring system for mobile communication, which is mainly applied in radio link safety protection, can effectively find potential hazards and optimize protective environment. Our system will play an important role in various aspects such as commercial activities promotion and personal information protection.
Security in MANET is a grand challenge problem nowadays. The main security issues in MANET are identification and privacy. To combat with these problems, many secure routing protocols have been designed to reduce the security threats. In this paper, we have proposed a Centralized Secure Routing Protocol (CSRP) to enhance the security levels in the architecture to prevent the network against active and passive attacks. A Master Node (MN) is used in this architecture to control and manage the network security and data delivery. In the first step we have recognized the nodes and in the second step we have established session keys between the nodes for safe communication. The data delivery is done by encryption/decryption mechanism using session key. CSRP algorithm is mainly designed for secure and safe routing.
Alert correlation systems attempt to discover the relations among alerts produced by one or more intrusion detection systems to determine the attack scenarios and their main motivations. The main purpose of this paper is to propose a new IDS alert correlation method to detect attack scenarios in real-time. The proposed method is based on causal approach due to the strength of causal methods in practice. Most of causal methods can be deployed offline but not in real-time due to time and memory limitations. In the proposed method the knowledge base of attack patterns is represented in a graph model called Causal Relations Graph. In offline, we construct some trees related to alerts probable correlations. In real-time for each received alert, we can find its correlations with previously received alerts by performing a search only in the corresponding tree. Thus processing time of each alert decreases significantly. In addition, the proposed method is immune to the deliberately slowed attacks. To verify the proposed method, it was implemented in C++ and we used DARPA2000 dataset to test it. Experimental results show the correctness of the proposed alert correlation and its efficiency with respect to the run time.
To confront an attack, a good strategy may save the cost and time. When Detecting and reacting is known to handle predictable nai?ve attacks, prevention strategies should be used to deal with smart unknown ones. These are defined types of misbehavior in wireless MAC layer protocols. To study MAC layer misbehavior in IEEE 802.11 networks more systematically, we defined a framework named PCPD. This methodology introduces four parameters, Possibility, Cost, Profit, and Damage to delineate any assumed attack or threat. By studying these parameters together, it becomes clear what are the most probable and considerable misbehaviors, how much damage they can make, what are profits and cost of such a behavior for the hostile host, and so how much essential is to tackle it. Comparison of PCPD parameters for different types of misbehavior results in a comprehensible identification of their severity, network's reliability, and network's weak points. By recognizing these parameters one can easily go through designing an accurate handling method for any threat.
Since the beginning of research on E-government security problems, many facts have proved that the construction of e-government security assurance system can guarantee the running of the whole e-government system in a safe, reliable and effective mode. In Nepal, government organizations are already using few of these technologies to increase efficiency and effectiveness in service delivery and provide easier interaction between the citizens and the government. However, these benefits do not come without risks for information being misused, service disrupted or any other attacks interrupting the normal operation of computer based information systems. Problems with existing E-government systems security can be identified and taken care of by conducting regular audits. The goal of the paper is to investigate and understand the current E-government security audit readiness situation of Nepalese government organizations and suggest an approach to solve the existing situation. To meet this goal, government organizations that are intended to offer e-service has been purposely selected and assessed using questionnaire as a method of data collection. Security management domains in ISO audit checklist and ICT security readiness checklist are used to design the questionnaire to assess the security audit readiness of Nepal government organizations and suggest a framework for improvement.
The balance between security and performance represents critical and tightly coupled challenges in the design and operation of next generation LTE networks. Security and performance are not necessarily proportional to each other. It is common that; high security measures lead to high reliability results. However over-dimensioned security measures can adversely affect the networks' performance and thus the services' quality of experience. For example a lightweight security algorithm may represent a security breach to an E-commerce application and vice versa. This paper presents a framework of dynamic security architecture that balances the trade-off between resources performance and security to optimize the performance of communication networks via tuning of security profile. The paper explores a framework to define the network security profile concept and identify its usage for security analysis and vulnerability assessment for enhancing the networks' performance. A simulation model is developed using OPNET to ground the model analytics under heavy load conditions.
Computer Network Defense Policy is the rules of computer network and security devices. In order to achieve specific security objectives, the network need to choose the defensive measures under certain conditions. In order to generate the measures implemented by the device, it usually requires manual or automated translation from high-level network defense policy. In the translation process, due to the presence of semantic loss, man-made understanding mistakes, device machinery, etc., the high-level policy requirements cannot be completely satisfied. This will result in hiding network security risks or vulnerabilities. Through analysis of the consistency between high-level policy and low-level measures, and pointing out the lack and redundancy between the policy and measures, it can guide the further translation of policy on the device. This paper presents a novel formal and automated method to verify the consistency. When errors are detected, we will point out the location of the misconfiguration. The same time, based on SMT solving tools, it has been implemented in a prototype of consistency verifier. Experiments demonstrate that this tool is able to check the consistency and have good scalability and efficiency.
Policy is an essential part of computer network defense, which also has important guidance effect in the deployment, implementation, and configuration of the defense system. Thus, the possibility of conflicts existing in defense policies is becoming more and more crucial for ensuring the security of policies themselves. In this paper, we use a computer network defense policy specification language called CNDPSL, according to a model called CNDPM, to describe computer network defense policies, which can provide a uniform method of specifying policies of protection, detection and response. First, this article analyses conflicts existing in defense policies and shows the classification of these conflicts. Then, it gives a computer network defense policy detection model. Finally, this article shows a prototype system of computer network defense policy conflict detection, and validates the effectiveness of the prototype system with experiments.
Mobile Nodes (MN) in Mobile IPv6 (MIPv6) are given the opportunity to eliminate triangle routing that is inefficient with their own corresponding node (CN) using Route Optimisation (RO). This greatly improves the performance of the network. Unfortunately, using this method allows several security vulnerabilities to manifest itself with the MIPv6. Among those, common issues are those concerns the verification of authenticity and authorisation of Binding Updates during the process of RO. These types of unauthenticated and unauthorised BUs are the key to various types of malicious attacks. Since it is expected that MIPv6 will be supported by IPv6, several mechanism to ensure BU security will be crucial in the next generation Internet. This article focuses on Mobile IPv6 and security considerations.
VoIP (Voice over Internet) provides delivery of voice information over unsecured IP-based networks like the Internet. VoIP data, signaling and voice, needs to be secured in such an environment. Security mechanisms take their toll on VoIP system performance. SIP is dominant signaling protocol for VoIP. This paper measures relative decrease in VoIP performance of system with secured SIP signaling over one without it. It compares SIP with authentication enabled over three transport protocols: UDP, TCP and TLS. Peak throughput of concurrent calls, registration request delay, session request delay, SIP server CPU and RAM usage are measured. Testbed environment consists of Asterisk IP private branch exchange (PBX) as a part of Elastix server, several SIP user agents and SIPp traffic generator. Test results show that performance of SIP over TLS based signaling is four times lower than the SIP signaling over UDP in most metrics.
In this paper we propose a nature-inspired approach that can boost the Optimum-Path Forest (OPF) clustering algorithm by optimizing its parameters in a discrete lattice. The experiments in two public datasets have shown that the proposed algorithm can achieve similar parameters' values compared to the exhaustive search. Although, the proposed technique is faster than the traditional one, being interesting for intrusion detection in large scale traffic networks.
Recent news on financial losses due to lack of information security has created certain awareness of its importance among computer users. This paper presents the results arrived from an initial response of 340 questionnaires distributed to the home users, who are not computer savvy. The survey was conducted to assess the awareness of computer and internet security during their day-to-day usage of internet. Results show that a user's awareness of security plays a significant role. This paper also proposes developing a software solution to protect home users' computer system based on the internet usage and behavioral pattern.
The goal of co st-sensitive response system is to ensure that response cost does not outweigh the intrusion cost. In order to ensure this, some cost-sensitive response models have been developed. Some of these models do not consider the effectiveness of previous actions and lack standard approach for estimating associated cost. In this work, we present a model for assessing cost of responses based on three factors, the cost of damage caused by the intrusion, the cost of automatic response to an intrusion and the operational cost. The proposed approach provides consistent basis for response assessment across different systems and environment. In addition, the performance analysis indicates that automated responses systems using this cost metric COSIRS, when deployed can responds quickly enough to thwart active attacks in real time using optimal responses. The results of evaluation show that the design has better performance over existing ones.
The security of web applications is an ongoing dilemma. Hackers and bots are getting more and more innovative in bypassing the various defensive tools implemented to enforce security. e-Commerce Applications, such as those used for the transaction processes, could be in a placed in a position of not providing a fair chance to all consumers because because can interact more quickly. This is especially true when a commerce site offers hot inventory items where many traders are competing to get a limited supply item. The e-Commerce site's security is compromised when some traders utilize pre-formatted scripts or spiders to place orders, thus giving them an unfair advantage The problem is: how to eliminate scripts/spiders in a given web application flow by using a solution that is difficult to crack while requiring no additional actions taken by the end user. Our paper introduces an innovative multi-layer approach to honeypots where cashing or bypassing the honeypot is technically impractical.
The following topics are dealt with: information forensics and security; biometrics; privacy; network forensics; multimedia forensics; data hiding; media forensics; and secure communications.
We present an attack to locate hidden servers in anonymous common networks. The attack is based on correlating the flow of messages that arrives to a certain server with the flow that is created by the attacker client. The fingerprint is constructed by sending requests, each request determines one interval. To improve the performance a prediction of the time of arrival is done for each request. We propose an optimal detector to decide whether the flow is fingerprinted, based on the Neyman-Pearson lemma. The usefulness of our algorithm is shown for the case of locating a Tor Hidden Service (HS), where we analytically determine the parameters that yield a fixed false positive probability and compute the corresponding detection probability. Finally, we empirically validate our results with a simulator and with a real implementation on the live Tor network. Results show that our algorithm outperforms any other flow watermarking scheme. Our design also yields a small detectability.
The fusion model in wavelet domain is proposed. To maximize fusion and enhances the hidden image's capability of resisting attacks. Method: we derive the image related scrambling transform based on least square error principle. The algorithm improves the extent of fusion. The fusion factor, the scrambling matrix and iteration number are used as secret keys with the intension of improving the algorithm's security. Result: Simulation experiments have proved that the proposed algorithm improves the fusion factor and the extent of fusion significantly, Conclusion: The algorithm can be widely used for copyright protection of electronic image products and encrypted transmission of important military information.
The prediction of the evolutionary link in the email network is an important research direction in the field of network security. The weighted correlated Bayesian classification model is an extension of the Naive Bayesian classification model. In this paper, email network users were grouped by the characteristics of email content and the evolutionary links were sorted into two types: the link in the same issue group and between two issue groups respectively. By defining classification attributes for each type of evolutionary link and depending on the weighted correlated Bayesian classification model, an improved method for predicting evolutionary link was proposed. The result of experiment in email dataset showed that the accuracy and precision of the improved method is higher than Common Neighbor algorithm and Adamic-Adar algorithm.
In this short paper, an automatic malware analysis framework is introduced to facilitate the security community to keep the pace of rapidly changing malwares. In our framework, the honeynet technology and Taiwan Malware Analysis Net (TWMAN) can simultaneously collect and analyze the latest malicious software. The well-organized malware database and sharing platform can assist security experts in searching malware patterns. Owing to the prevalence of Bonnet, the number of malware increases quickly. Our automatic malware analysis framework is an excellent solution to deal with the Bonnet problem.
This paper dealt with several security aspects in multimedia systems. The possible threads of various attacks are summarized and described. The methods focused on the intrusion detection and protection, the tools that manage the trusted use of digital media content (Digital Rights Management -DRM), as well as approaches for user's data privacy are also presented in the paper. Within HBB-Next architectural concept the Security Manager (SM) architectural component was proposed. This component is responsible for the management of multi-factor authentication, authorization, and policy enforcement for multifactor levels and for profile data access control.
This paper will focus on the proposed logical key tree, called the LKH (Logical Key the Hierarchy) and analyze this scheme. An optimization method based on the LKH will be proposed. When the network users to join or leave group, the update method and its changes in the structure of original tree will be analyzed. The LKH structure and the improved LKH structure will be simulated by the NS2 simulation software. Besides, the impact of the two options on the network delay in the network communication will be analyzed, and NS2 software will objective analysis the advantages and disadvantages of these two programs and its application environment.
This paper proposes two proactive recovery mechanisms, i.e., periodic grouped proactive recovery and annular proactive recovery. The periodic grouped proactive recovery divides the replicas into groups and each group recover periodically in round. The annular proactive recovery adopts a kind of annular system structure and recovers the replicas dynamically. Although there are some differences between grouped proactive recovery and annular proactive recovery in system size, availability, and security, the results of simulation experiments show that two proactive recovery mechanisms both can improve the availability obviously.
To meet the needs of the digital evidence on legal action proceedings, network forensics technology plays an important role in the process of fighting against computer crime and hacking crime. To try to solve some problems of eliminating intrusion track after hacking and some drawbacks of network forensics products, in this paper, we present a network forensic solution which adopts dynamic and static methods to analyze network intrusion data and make detailed records of the data and log. This network forensics solution is able to carry out deep and multi-angle forensic analysis with network evidence, and can ensure the reliability and credibility of the network evidence through effective technical methods.
Web applications have brought with them new classes of network security vulnerabilities, such as SQL Injection Attack. SQL Injection Attack is a class of attacks that many of the Web-based systems are highly vulnerable to, and there is no know fool-proof defense against such attacks. Static analysis is one of the techniques in defense of SQL Injection. In this paper, we propose an improved technique eliminates the need to modify source code of application scripts. The improved Eliminating SQL Injection Attacks technique bases the regular expressions instead of using SQL Graph representation using SQL-FSM in static analysis.
This paper describes a Cyber Threat, Vulnerability and Defense Modeling and Simulation tool kit used for evaluation of systems and networks to improve cyber resiliency. This capability is used to help increase the resiliency of networks at various stages of their lifecycle, from initial design and architecture through the operation of deployed systems and networks. Resiliency of computer systems and networks to cyber threats is facilitated by the modeling of agile and resilient defenses versus threats and running multiple simulations evaluated against resiliency metrics. This helps network designers, cyber analysts and Security Operations Center personnel to perform trades using what-if scenarios to select resiliency capabilities and optimally design and configure cyber resiliency capabilities for their systems and networks.
The Identity-Based Internet Protocol (IBIP) Network project is experimenting with a new enterprise oriented network architecture using standard Internet Protocol to encode identity (ID) information into the IP packet by a new edge security device referred to as the IBIP policy enforcement point (PEP). This is a variant of a network admission control process that establishes user and host identities as well as provides optional information on host visibility, organizational affiliation, current role, and trust metric (associated with the user and host endpoints). Our motivation is to increase our security posture by leveraging identity, reducing our threat exposure, enhancing situational understanding of our environment, and simplifying network operations. In addition to authentication, we leverage strong anti-spoofing technology to improve accountability. We reduce our threat surface by hiding our client hosts and making all infrastructure devices inaccessible. Any attempt to access a hidden host or infrastructure device results in a policy violation attributable to the user/host that caused the violation and provides enhanced situational awareness of such activities. Our servers can also have a permissible use policy that ensures that the server only operates across the network per that policy. Finally, as users log in and servers are added to the network all dynamic configurations for access control initiated by such changes are automatically carried out without manual intervention, thereby reducing potential vulnerabilities caused by human errors.
Wireless intruder geolocation techniques using received signal strength (RSS) measurements from an array of sensors are very attractive for campus wireless network security because of their low cost implementation and simplicity. Both precision in RSS measurements and the relative geometry between RF sensors and the wireless intruder can affect the geolocation accuracy. This paper presents a comparative performance analysis of three RSS-based geolocation algorithms for non-cooperative wireless intruders in terms of their root mean square errors in wireless campus networks. A comparison study is established in a RF sensor grid and circular sensor geometries to fit general campus topology. Geolocation algorithm accuracy is examined for these particular geometric configurations of RF sensors with respect to various wireless intruder locations in and around a campus area. The results show that certain geolocation algorithms perform much better than others when wireless intruders are inside a campus wireless network, whereas the performance of the same geolocation algorithms is turned around when wireless intruders are outside a campus wireless network. As a result, a high accuracy solution to wireless intruder geolocation in campus wireless networks is discovered and proposed. MATLAB-based numerical simulations were used to evaluate the performance of these algorithms for various scenarios and parameters.
There is increasing interest in using terminal architectures capable of supporting multiple networks. A terminal that is episodically present on a network is a new situation for most Network Managers and for computer network defense needs. Likewise, having to accommodate interaction with multiple network managers is a different paradigm for the terminal. This paper discusses the kinds of node architecture features that would support this kind of interaction. It looks at the extension of traditional control plane architectures to support the use of Cognitive engines to simplify interactions with network managers and network security functions. It also discusses the use of a Service Oriented Architecture for alerting user networks when a terminal becomes part of the backhaul path.
The mobile agent paradigm and security issues in the grid has been successfully used in large number of research areas separately, and specifically contributes to the research endeavors. The proposed paper combines both the technologies and provides avenues for further research. Particularly in ad hoc grid environment, resources are not always available as the nodes can spontaneously connect and disconnect at any time. Thus, these environments demand the correct execution of process for improving the performance criteria. However, there are some intruders that affect the normal operation of ad hoc grids. Therefore, it is essential to use punishment procedures based on trust models. The existing work has proven that the RETENTION a reactive trust based mechanism detected and punished malicious nodes in ad hoc grid environments, without generating any false-positives. We propose a mobile agent based trust model in ad hoc grid environment for improving the performance considerations and security issues. Simulation results demonstrate the effectiveness of the proposed work, to detect and punish nodes having malicious behavior, such as selfish nodes, injected nodes and so on. We observe that the behavior of the proposed model is comparatively good in a scenario where the types of attack implemented in different nodes.
Podslurping is the intentional or unintentional use of a portable USB mass storage device, such as a USB flash drive (or thumb drive), to illicitly download and store confidential and proprietary data from network endpoint [1]. There are many establishments and organization that are unaware of, or choose to remain ignorant about the threat that can be caused by portable devices in their network setting until some events that can be from a minor unfortunate incident to a complete catastrophe. In the information age, cybercrime and information leakage increase, because endpoints are an easy target [2]. The key to managing portable devices in business environment is to give administrator direct control over what devices are in use on your network. In this paper we present the implementation of access and identity management for endpoint protection and data security from USB devices to maintain information security and data theft prevention in a corporate environment.
Distributed computing has grown rapidly in the recent years. In addition to the increase in the size of individual networks, new types of networks have also emerged providing different types of services to clients. While these systems provide an invaluable service, they also face certain practical issues. Security is one of the most important issues, that must be dealt with by the implementers in order to provide a satisfactory service. Trust and trust management have been drawing the attention of security researchers in order to identify the malicious nodes and separate them from good benevolent nodes and also to quantify the quality of services provided by the nodes in a system. Several trust computing models have been proposed for distributed systems by various researchers. These models are based on different approaches from fuzzy logic, Bayesian model, social networking to bio-inspired mechanisms. In this paper, the authors take a critical look at the bio-inspired trust models reported in the literature with respect to their principles, advantages and disadvantages.
Many services in the internet including Email, search engine, social networking are provided with free of charge due to enormous growth of web users. With the expansion of web services, denial of service (DoS) attacks by malicious automated programs (e.g. web bots) is becoming a serious problem of web service accounts. In order to avoid tremendous attack from malicious computer programs, HIP, or Human Interactive Proofs has been introduced to distinguish humans from computers. HIPs are designed to be easy for humans but hard for machines. Unfortunately, the existing HIPs tried to maximize the difficulty for automated programs to pass tests by increasing distortion or noise. Consequently, it has also become difficult for potential users too. In our proposed technique we resolve this problem by making use of human cognitive processing abilities through emoticons focusing mainly on users. Features like language independence, using this for advertising purpose, ease of use interface for the touch-based smart-phone users, easy tuning of security and usability level make it very attractive to web service providers. In the result section, a microscopic large-scale user study was conducted involving 118 users to investigate the actual user views compare to existing state of the art CAPTCHA systems like ESP-PIX and Asirra in terms of usability and security and found our system can be solved with 88.04% average success rate in less than 7 seconds.
Availability of DNSSEC resolution and validation service against man-in-the-middle attacks are analysed in this paper, and possible vulnerabilities are introduced and classified. Experiments show DNSSEC client is vulnerable because the attacks are always successful, but they are failed to recursive server, at the same time, attacks to recursive server will bring about numerous retries, and the number of retries depends on the number of root domain name servers, top-level servers and authority servers, and this can be exploited to launch denial of service attacks to recursive server. The results show the availability of DNSSEC service is poor against man-in-the-middle attacks. Conclusions are valuable to the optimization of DNSSEC recursive server application, as well as DNSSEC security analysis.
Mobile VoIP is wireless communication technology based on peer-to-peer. Wireless devices loaded with Wi-Fi Direct could connect other devices directly with or without a Wi-Fi network. Wi-Fi Direct also supports 802.11 WPA2 PSK for its security. But it has security issues such as Wireless DoS and air sniffing, it is necessary to prepare countermeasures. This study analyzes and verifies security vulnerabilities to use Wi-Fi Direct.
Owing to the strong character expressive power, regular expressions gradually replace explicit string and become the first choice of patterns describing method. In the current network security products, huge amount of patterns lead to dramatic increase of the DFA storage space and affect the efficiency of matching. This paper presents a regular expression grouping algorithm based on partitioning method. It reduces the total number of DFA states and improves the matching performance which is also suitable for multi-core processor.
Network security event correlation can find real threat through correlating security events and logs generated by different security devices and can be aware of the network security situation accurately. This paper propose a network security events correlation scheme based on rough set, build database of network security events and knowledge base, gives rule generation method and rule matcher. This method solves the simplification and correlation of massive security events through combining data discretization, attribute reduction, value reduction and rule generation.
Wireless Local Area Networks (WLANs) are subject to different types of vulnerabilities. Denial of Service (DoS) attacks are among the most challenging issues regarding WLANs. In this paper, we present our approach to simulate the RTS flood DoS attack, and to analyze the effects of this attack on IEEE 802.11 WLANs. OPNET Modeler 14.5 advanced network simulator is used to simulate this type of attack.
This paper explores and quantifies the impact of WPA2 security protocol on network performance of operating systems in IEEE 802.11n wireless networks. Experiments were conducted under laboratory conditions on a wireless network infrastructure. Obtained data analysis throughput in different security scenarios. This paper analyzed the UDP transport flows generated with different packet sizes on IPv4 and IPv6 64-bit operating systems, Windows 7 and Linux Ubuntu 12. The measurement results show that IPv4 systems achieves greater throughput values than IPv6 systems for the same size of UDP packets. The results also show a decrease in throughput value when WPA2 security protocol is used in IEEE 802.11n wireless networks.
Cyber security in interconnected smart grid is vital for reliability of power system operations. Power communication applications are different from cooperate information technology applications. Utilities that follow defense in depth strategy have to confront many network security technologies. This paper presents an application of IPSec tunnel in substation gateways. IPSec Encapsulating Security Payload (ESP) tunnels are selected for our experiments. Since security overhead of cyber security measures introduce additional delays, several performance tests are conducted on IPSec ESP tunnels. The results are used to determine the balance between security and performance of IPSec ESP tunnel on substation gateways.
Genetic fuzzy systems (GFSs) hybridize the approximate reasoning method of fuzzy systems with the learning capability of evolutionary algorithms. The objective of this paper is to focus on an important class of problems in the field of network intrusion detection, namely, the class imbalance problem, one of the problems strongly tied with the classification of the database of intrusion detection. In this work we have used a fuzzy association rule based classification method, to obtain an accurate and compact fuzzy rule-based classifier with a low computational cost. We have proposed the use of a novel fitness function to deal with the problem of imbalance dataset in the genetic post processing phase for rule selection and parameter tuning. The efficiency of the proposed system has been shown through a complete detailed experimental comparative study with well-known classifiers reported in the IDS literature. Experiments were performed with KDD Cup 99 intrusion detection benchmark data set as an example of a network traffic data.
Secret communication via network has always been an area of interest for many. It has not only attracted the trusted parties to communicate with each other secretly but has also attracted the hackers/attackers to find ways to discover and leak the information and use the network in a manner that violate security policies. Steganography and covert channels are most widely used approaches for secret communication. Number of detecting techniques has been proposed for steganography and covert channel detection. This paper covers detecting techniques of covert channel only as the covert channel is a modern way of leaking information and it is difficult to detect such channels. Covert channel falls into two categories: storage covert channel and timing covert channel. Storage covert channel is created by manipulating the header fields of packets whereas timing covert channel is created by timing of event i.e. arrival pattern of packets. In this paper different techniques for detecting storage and timing covert has been surveyed and analysis of these techniques is done.
Limited research conducted on the determinants of user acceptance of visitor application systems have been carried out eventhough there are varieties of similar system available in the market. Hence, in this paper, the researchers aim to adopt and adapt the Use of Technology model (UTAUT) to determine the user acceptance of visitor application system among viewers of the system. This visitor application system is called Visitor Management System (VMS) and with the motto Handling Your Visitor at Your Fingertips. Visitor Management System (VMS) was designed and developed in order to monitor visitor movement in an organization. The application can be viewed from local LAN (Intranet) with a standard web browser such as Microsoft Internet Explorer with no additional software or plug-in to load. VMS will be placed at the main guardhouse and will be handled by the corporate security section. Each of the department will be located with at least one front desk officer to monitor the current visitor to visit their department.
As the volume of network data continues to increase and networks become more complex, the ability to accurately manage and analyze data quickly becomes a difficult problem. Many network management tools already use two-dimensional (2D) and three-dimensional (3D) visualization techniques to help support decision-making and reasoning of network anomalies and activity. However, a poor user interface combined with the massive amount of data could obfuscate important network details. As a result, administrators may fail to detect and identify malicious network behavior in a timely manner. 3D visualizations address this challenge by introducing monocular and binocular visual cues to portray depth and to increase the perceived viewing area. In this work, we explore these cues for 3D network security applications, with a particular emphasis on binocular disparity or stereoscopic 3D. Currently, no network security tool takes advantage of the enhanced depth perception provided by stereoscopic 3D technologies for vulnerability assessment. Compared to traditional 3D systems, stereoscopic 3D helps improve the perception of depth, which can, in turn reduce the number of errors and increase response times of network administrators. Thus, we introduce a stereoscopic 3D visual Framework for Rendering Enhanced 3D Stereoscopic Visualizations for Network Security (FRE3DS). Our novel framework uses state-of-the art 3D graphics rendering to assist in 3D visualizations for network security applications. Moreover, utilizing our framework, we propose a new 3D Stereoscopic Vulnerability Assessment Tool (3DSVAT). We illustrate the use of 3DSVAT to assist in rapid detection and correlation of attack vulnerabilities in a subset of a modified local area network data set using the enhanced perception of depth in a stereoscopic 3D environment.
We introduce a computational model for networks that is suitable for supporting the administrative staff in estimating the threat that is caused by security relevant events, for identifying possible root causes for these events and for making predictions about the impact of attacks or countermeasures against attacks. We refer to expertise from the network management domain and extend the common methodologies to meet the demands of network security. We also describe how this model can be created and updated in an automatic way.
The availability of inexpensive sensing devices capable of wireless communications enables the design of exciting and new distributed sensing applications. The integration of such applications with the Internet will contribute to materialize a vision of a future Web that we nowadays describe as the Web of Things (WoT). Many security-related aspects are still open on how to address security for communications on the WoT, and that must be targeted before such applications can realistically be considered ready for deployment. We discuss the experimental evaluation of mechanisms proposed to secure end-to-end web communications with IPv6-capable sensing applications and devices, as such mechanisms may provide an important contribution towards the WoT. Our experimental evaluation study considers the impact of security on real sensing devices and requirements from sensing applications, allowing the identification of the limitations of current sensing platforms in supporting the proposed mechanisms.
Network Disaster Recovery research has examined behavior of networks after disasters with an aim to restoring normal conditions. In addition to probable loss of connectivity, a disaster scenario can also lead to security risks. However, network security has been examined extensively under normal conditions, and not under conditions that ensue after disasters. Therefore, security issues should be addressed during the period of chaos after a disaster, but before operating conditions return to normal. Furthermore, security should be assured, while still allowing access to the network to enable public communication in order to assist in disaster relief efforts. In general, the desire to help with public assistance requires opening up access to the network, while security concerns add pressure to close down or limit access to the network. In this study, we show that the objectives of availability and confidentiality, two objectives that have not previously been considered together in disaster scenarios, can be simultaneously achieved. For our study, we evaluated six wireless devices with various network configurations, including a laptop, a Kindle Fire e-reader, an Android tablet, a Google Nexus phone, an IP camera, and an Apple TV, to approximate behaviors of a communication network under a disaster scenario. Actual data leakage was tracked and observed for these devices. To the best of our knowledge this has not previously been examined in a systematic manner for post-disaster scenarios. After illustrating the data leakage of various devices, we analyze the risk associated with the various types of leakage. Moving private traffic to a VPN would free the physical network for use as a public resource.
Published studies have focused on the application of one bio-inspired or evolutionary computational method to the functions of a single protocol layer in a wireless ad hoc sensor network (WSN). In a novel departure from previous work, the cross-layer approach to WSN protocol design suggests applying a bio-inspired evolutionary computational method to the functions of each protocol layer to improve the intrusion detection identification (IDID) performance of a WSN cross-layer design beyond that of a single method assigned to only one layer's functions. A cross-layer design, embedding genetic algorithms, anti-phase synchronization, ant colony optimization, and a trust model based on quantized data reputation at the physical (PHY), medium access control (MAC), network, and application layers, respectively, is constructed. Simulation results demonstrate synergies among the bio-inspired methods of the proposed baseline design improve overall IDID performance of networks over that of a single computational method.
Internet of Things (IoT) refers to an inter-connected world where physical devices are seamlessly integrated into the Internet. The emergence of technologies such as Zigbee, Bluetooth low energy, and embedded sensors has transformed simple physical devices into smart objects that can understand and react to their environment. Such smart objects form the building blocks for the Internet of Things.
A new scheme of monoecism watermarking algorithm based on the bionics idea, is presented in this paper. We gave out the key concepts about the monoecism watermarking algorithm. We also described the process of monoecism watermark signal embedding and watermark detection in detail. At last, through the experiments we proved the validity of this algorithm. Monoecism watermarking algorithm can protect ROI (Region of Interest) area from the watermark signal interference, so as to improve the quality of the ROI area. At the same time, it can detect any tampering with the ROI area. For image information safety, especially for the information security on the image that contains important area, this algorithm has practical significance and practical value.
Botnet creates harmful network attacks nowadays. Lawbreaker may implant malware into victim machines using botnets and, furthermore, he employs fast-flux domain technology to improve the lifetime of botnets. To circumvent the detection of command and control server, a set of bots are selected to redirect malicious communication and hides botnet communication within normal user traffic. As the dynamics of fast-flux domains, blacklist mechanism is not efficient to prevent fast-flux botnet attacks. It would be time consuming to examine the legitimacy of the domain of all the network connections. Therefore, a lightweight detection of malicious fast-flux domains is desired. Based on the time-space behavior of malicious fast-flux domains, the network behavior of domains are formulistic in this study to reduce the time complexity of feature modeling. According to the experimental results, the malicious fast-flux domains collected from real networks are identified efficiently and the proposed solution outperforms the blacklists.
GPU and other SIMD stream architecture have been used for accelerating packet processing applications. This paper explores the design space on GPU for sketch-based network traffic change detection application by using OpenCL parallel programming framework. Due to the parallel nature of sketch data structure, the computations can be mapped to OpenCL execution model on GPU efficiently. The sketch data structure is mapped to buffer object in device's global memory, and work-items are executed on the sketch in parallel. Compared to the sequential CPU implementation, the experiment results on Radeon HD 5870 GPU show that the hash computation and ESTIMATE operation achieved about 15 times and 9 times speedup, respectively.
A network security metric is desirable in evaluating the effectiveness of security solutions in distributed systems. Aggregating CVSS scores of individual vulnerabilities provides a practical approach to network security metric. However, existing approaches to aggregating CVSS scores usually cause useful semantics of individual scores to be lost in the aggregated result. In this paper, we address this issue through two novel approaches. First, instead of taking each base score as an input, our approach drills down to the underlying base metric level where dependency relationships have well-defined semantics. Second, our approach interprets and aggregates the base metrics from three different aspects in order to preserve corresponding semantics of the individual scores. Finally, we confirm the advantages of our approaches through simulation.
Network interdiction problems consist of zero-sum games between an attacker and an intelligent network defender, where the attacker seeks to degrade network operations while the defender adapts its operations to counteract the effects of the attacker. This problem has received significant attention in recent years due to its relevance to military problems and network security. In this paper, we study a class of dynamic network interdiction games where the attacker has imperfect knowledge of the network topology, and where the attacker can learn about the topology by monitoring network operations. The network observes the attacker's actions, and can choose to avoid using the observed parts of the network in order to disguise information from the attacker. We pose this problem as a multistage game with nested imperfect information structure, and study the extensive form of this game. This form has special structure that we exploit to develop a novel decomposition algorithm for obtaining recursive solutions to this game. We characterize the payoff function of subgames starting at attacker's information sets as piecewise linear concave functions of the attacker's information state, the beliefs those information sets. We then develop a recursive algorithm based on extensions of partially observed Markov Decision Process algorithms to obtain complete solutions to these multistage games with nested information. The resulting recursive algorithm allows dynamic programming-like solution of dynamic games with partially nested information structure, where signaling between players is possible. We illustrate the algorithm with a simple example.
The price and size reduction of computational devices have increased the number of areas where human-operated equipments are replaced by autonomous and autoconfigurable devices. The IP protocol has taken place in industrial and home automation, specially in wireless applications. It allows applications to use interoperable communication strategies commonly used in the computer networks as the Web Services. Since the standard Web Services are too heavy to be directly used in devices, the DPWS (Device Profile for Web Services) has been created, reducing the resources needed by services. The DPWS specification suggest WS-Security to implement secure communication between devices and their clients. It is a problem considering that WS-Security not fit in current devices applications, because it uses assimetric cryptography for all communication steps. This work evaluates the use of DPWS over a TLS/SSL cryptography layer using compression techniques to reduce the impact of security layer implementation.
The analysis of network traffic is a key area for the management of fault-tolerant systems, since anomalies in network traffic can affect the availability and quality of service (QoS). This work proposes an intrusion detection tool based on the two-dimensional wavelet transform to quickly and effectively detect anomalies in computer networks generated by denial of service (DoS). Experiments were performed using two databases: a synthetic (DARPA) and another one from data collected at the Federal University of Santa Maria (UFSM), allowing analysis of the intrusion detection tool under different scenarios. The wavelets considered for the tests were all from the orthonormal family of Daubechies: Haar (Db1), Db2, Db4 and Db8 (with 1, 2, 4 and 8 null vanishing moments respectively). For the DARPA database we obtained a detection rate up to 100% and 95% for the UFSM database.
While more and more digital application services move to the cloud virtualization environment, the network security challenges are equally striking. In general, these network attacks can be detected by deploying network intrusion detection systems (NIDSs) to the cloud platform. As clients in the cloud can create many virtual machines (VMs) to run their services privately, all detection rules are usually loaded into NIDSs to avoid any oversight, and cause damage to the performance of NIDS. This work presents a new architecture for building an efficient NIDS to the cloud virtualization environment. By resolving the virtual system information from operating systems' kernel map in hypervisor layer, the services in the cloud can be identified exactly and the required detection rules can be adopted dynamically. The experiment results show that the proposed NIDS is efficient and effective.
The measurement of network congestion and degradation of quality of service during distributed denial of service attacks remained an elusive goal. This paper analyzes the impacts that all congested links cause on attack victim and network architecture, introduces the min-cut set and presents a new method to assess the network security situation under DDoS attacks, which computes the influence value that attacks cause on network security situation according to the distance between the congested link and victim and whether the link is in the min-cut set, and this value is used for quantitative situation assessment. The applicability of this method is verified by simulated experiments with the network simulation tool.
The propagation and evolution model of botnet is the most common method to study the spreading features of bots, but current botnets' propagation and evolution models limited to describe active spread process based on worms, don't consider the passive spread process similar to plugged Trojan-horse into webpage, so these models can not exactly describe how the bots spread on the Internet. In this paper, a new botnet propagation and evolution model (WT-SIR) was proposed. This model carefully considers about the real situation of the Internet, especially probability of bots' status transformed from infected and immune to susceptible. Simulation result shows that the WT-SIR model more exactly satisfies the practical propagation laws and infection characteristics of bots on Internet.
Bonnet is extremely harmful to computer network security which could cause many network attacks(like spam, DDoS, phishing etc). In this paper, we design a distributed Bonnet detecting approach based on network traffic analysis. A botnet detection framework is proposed, which composed of two sections: Data Collection and Filter, Bonnet Detection and Identify. The first section is deployed in distributed hosts in order to capture network traffic data, filter data and classify data. The second section is deployed in centralized place which collectes all data from distributed hosts and detected the botnet using data amalgamation algorithms and characteristic identified algorithms. The detecting approach works efficiently and can detect botnet in the experiment environment.
In this paper, we integrate the multi-hop technique with a backup-based clustering algorithm using the residual energy to organize sensors. By using an adaptive backup strategy as well as the residual energy, the algorithm not only realizes load balance among sensor node, but also achieves dynamic cluster head distribution across the network in a timeout manner. Simulation results also demonstrate our algorithm is more energy-efficient compared to other algorithms. Our algorithm is also easily extended to avoid the formation of forced cluster heads, thereby it achieves better network management, energy-efficiency and scalability.
A fusion algorithm for network security situational awareness is proposed based on D-S theory of evidence, processing the vulnerability data from network scanning and host scanning. Though normalizing the vulnerability data, we constructed the corresponding basic probability assignment and fusion the data, fully used the redundancy and complementarity of multisource data, and integrated the results got by different scanning means. The results showed that this method obtained more comprehensive and reliable results.
Low-Rate Denial of Service (LDoS) attack has been a hot topic in network security area in the last decade. LDoS flows are affected by the dynamical environment of the Internet, making it difficult to understand the behaviors of LDoS flows at the receiver side. In this paper, we studied the impact of end-to-end delay on LDoS flows in order to provide a foundation for detecting and defense of LDoS attacks. We gathered data of LDoS flows from the Internet and analyzed their traffic patterns. The experimental results have shown that end-to-end delays can distort LDoS flows greatly although they are sent in regular patterns.
Due to increased use of the internet for various time critical applications such as banks, power stations, mission critical applications and even our defense system its unavailability greatly hinders our day to day operations. We all know that the biggest threat to the availability of internet is Distributed Denial of Service (DDoS) attack. The objective of DDoS attack is to consume the target's resources to such an extent that it becomes incapable to serve legitimate clients. Till date, a wide variety of solutions have been proposed to solve this problem but a very few of them concentrate on protecting critical infrastructures. Secure Overlay Services (SOS) architecture acts as a basis to protect all these time critical applications. It proactively prevents the target by providing layers of security between the authenticated client and the protected target. Although SOS architecture is successful in complicating the job of attacker, but still it is vulnerable to various attacks. Various researchers critically study this architecture and discuss various attacks possible on it. The aim of this paper is to review various attacks possible on this architecture so that more detailed understanding of attacker's strategy can be achieved and use this knowledge to develop more robust solutions.
Wireless Sensor Network (WSN) is a major catalyst in the advancement of the computer networks. WSN is used in outdoor applications like environmental monitoring and military surveillance. These open environments cause WSN to be more vulnerable to various wireless hackinglike spoofing jamming and broadcast attack. Securing a WSN from these attacks is a challenging task. Denial of Service (DoS) attack is one of the major security attacks in WSN. Jamming a single or cluster of nodes is a type of DoS attack on WSN. In this paper, a novel approach in detecting the physical layer DoS style jamming attack is proposed and analyzed. Here, we propose a method called physical layer jamming identification. This method is based on residual energy where few nodes are marked as monitor nodes. These nodes monitor the jamming attack by checking the Receiver Signal Strength Indicator and packet delivery ratio values of other nodes. It improves system performance and increases packet deliver ratio.
Securing Voice over IP (VoIP) is a crucial requirement for its successful adoption. Since SIP is the predominant protocol used for signaling to create, manage, monitor and tear down the VOIP sessions, this paper will focus on building a strong defense system around the signaling protocol layer. In this paper, we examine and investigate the concerns and requirements of VoIP security. After a detailed study of security issues and defense mechanisms, we focus on attacks and threats at signaling protocol layer and countermeasures to it that are essential for current and future VoIP implantations. Then, we analyze two popular industry best practices for securing VoIP networks and conclude this paper with further discussion on future research directions. This paper aims to direct future research efforts and to offer helpful guidelines for practitioners.
Web applications Security is an ongoing dilemma as hackers and bots are getting more and more innovative bypassing the various defensive tools implemented to enforce security. e-Commerce Applications, such as those used for the check-out process, could be in a position of not providing a fair chance to all consumers. This is especially true when a commerce site offers hot inventory items where many traders are competing to get a limited supply item. What happens is the e-Commerce sites security is compromised when some of the traders utilize preformatted scripts/ spiders to place orders, thus giving them an unfair advantage The problem is: how to eliminate scripts/spiders in a given web application flow by using a solution that is non-practical to crack with no additional actions taken by the end user. Our paper introduces an innovative multilayer approach to honeypots cashing or bypassing it is technically impractical, resulting in well secured web forms.
User authentication prevents unauthorized users from accessing resources or getting applications illegally. Due to the rapid development of computer network technologies, users may gain the desired services in a multi-server environment. Recently, Lee et al. proposed a dynamic-ID-based remote user authentication scheme for multi-server environment using smart cards. They adopt dynamic ID to provide user anonymity. However, we found that their scheme suffers from some security drawbacks. in this paper, we show the perceived security drawbacks in detail and give some helpful remedy recommendations.
With the rise of cyber-attack activities in the recent years, research in this area has gained immense emphasis. One of such research efforts is modeling of cyber-attacks. In this context, several modeling approaches have been developed, such as approaches based on attack trees (AT). In this paper, we propose Colored Petri Net (CoPNet) modeling approach by extending the attack trees with new modeling constructs and analysis approaches. CoPNet based attack model is flexible enough to model Internet intrusion, including the static and dynamic features of the intrusion. The process and rules of building CoPNet based attack model from AT are also presented. In order to evaluate the risk of intrusion, some cost elements are added to CoPNet based attack modeling. We show how attack trees can be converted and analyzed in CoPNets. Finally, we provide a case study that illustrates the CoPNet approach.
RADIUS Accounting Server is used for receiving an accounting request and returning a response to a client. When the client is configured to use RADIUS Accounting, it will generate accounting requests towards accounting server. In a system where there are multiple servers configured in round-robin fashion, if some of the servers go down, it takes more time to reach the actual active server after retransmissions to the non-responsive server get exhausted. In case of accounting server not responding or dead, client keeps on archiving the requests until certain point. Once the server becomes active, client will flush all the archived requests. In this process, there are chances that some of the requests may get lost. Also it takes large amount of time to flush all the archived requests. Here we propose a new approach to make Accounting-On and Accounting-off requests to be sent by Accounting Server to Network Access Server for better communication.
The core of Next Generation Network(NGN) IP Multimedia subsystem(IMS) based on SIP as mechanism signaling, is an important challenge for supporting data communication services, voice, video, messaging and web-based technologies. In this work we present a novel design of architecture and turns up some challenges of new IMS architecture and security system. This architecture provides a robustness, reliability, scalability and strategy for extension in the future and responds to the security challenges. We introduced the architecture with clustering database HSS and automatic storage of data that give a secure database. This paper give a classification of security in IMS network, modulate the risk in IMS network and our comparison is giving by cost signaling interworking with and without securing Gateway (SEG). We show that there is a tradeoff between the level of increasing system security and the potential cost incurred. we conclude that this architecture is suitable for operators and services providers for the new business models.
Private overlays, such as Virtual Private Networks (VPN), offer methods for a cheap and yet secure communication over the Internet. However, as our society becomes more and more dependent on it, these structures turn into vital targets for Denial-of-Service (DoS) attacks. As so-called botnets offer an inexpensive way to generate almost arbitrary amounts of traffic, the only effective measure that can be taken by overlay mechanisms is adapting the topology for minimal impact. This article presents novel metrics to estimate the impact of DoS attacks with different strengths. In particular random, greedy, and optimal attacks are considered, whereas for the optimal attacker we show that it involves NP-hard calculations. Based on the attacker models, several prerequisites for resilient overlay topologies, like a low constant node degree and high girth, are derived and validated by a simulation study.
The traffic of digital images is growing rapidly on computer networks. Protection of digital data, especially medical images, becomes important for many reasons such as confidentiality, authentication and integrity. Currently, the most suitable for the transfer of medical images lies in cryptography. However, once decrypted, the image is no longer protected and can be duplicated, copied, falsified and distributed easily. In this context, digital watermarking has quickly emerged as a new advanced technology to enhance the security of digital images. Indeed, the insertion of a watermark in a medical image can authenticate it and guarantee its integrity. The watermark must be generally hidden does not affect the quality of the medical image. The objective of this paper is to develop a watermarking algorithm, in the domain wavelet transform, to ensure an authentication service suitable for medical images to gray level fostering a fragile rather than robust of the watermark inserted. We examine the technique used for the description of the results obtained in terms of imperceptibility and robustness.
The paper explains the concept of a cyber range and its use for performing system security assessments. It shows the advantage of evaluating security from a whole-system perspective rather than individual components and undertaking this with no risks of contamination, damage or degradation of the actual system. Moving the architecture of a real system into a cyber range in a meaningful and costeffective way is the key challenge for performing security assessments. A solution is to use representative models and virtualisation however the paper explains that it is necessary to be clear what side effects this might have.
Information security has become a very important topic especially during the last years. Web services are becoming more complex and dynamic. This offers new possibilities for attackers to exploit vulnerabilities by inputting malicious queries or code. However, these attack attempts are often recorded in server logs. Analyzing these logs could be a way to detect intrusions either periodically or in real time. We propose a framework that preprocesses and analyzes these log files. HTTP queries are transformed to numerical matrices using n-gram analysis. The dimensionality of these matrices is reduced using principal component analysis and diffusion map methodology. Abnormal log lines can then be analyzed in more detail. We expand our previous work by elaborating the cluster analysis after obtaining the low-dimensional representation. The framework was tested with actual server log data collected from a large web service. Several previously unknown intrusions were found. Proposed methods could be customized to analyze any kind of log data. The system could be used as a real-time anomaly detection system in any network where sufficient data is available.
Security is becoming a major concern for many mission-critical applications wireless sensor networks (WSNs) are envisaged to support. This is because WSNs are susceptible to various types of attacks or to node compromises that exploit known and unknown vulnerabilities of protocols, software and hardware, and threaten the security, integrity, authenticity, and availability of data that resides in these networked systems. While various security mechanisms have been proposed for these networks dealing with either MAC layer or network layer security issues, or key management problems, the security benefits that can be obtained from an upper visualization layer have not been adequately considered in their design. In this paper, we explore the issues and concerns surrounding the application of visual analysis for wireless sensor network security purposes. This paper focuses on several distinct advantages information visualization and visual analytics can offer in the security domain. In addition, this paper reviews security visualization tools that are available to network security analysts. Finally, it concludes by identifying challenges for this new area of research.
Because of increasing vulnerabilities, maturing attack tools, and increasing dependence on computer network infrastructure, tools to support network defenders are essential. Course-of-action recommendation research has often assumed a goal of perfect network security. In reality, network administrators balance security with usability and so tolerate vulnerabilities and imperfect security. We provide realistic course-of-action decision support for network administrators by minimizing connectivity in attack graphs, by optimizing network configuration changes to separate defence goals from attackers as much as possible, even when complete security is impractical. We introduce vertex closures and closure-relation graphs in AND/OR digraphs as the underlying framework. Computing an optimal course-of-action is NP-hard but we design a polynomial-time greedy algorithm that almost always produces an optimal solution.
Ensuring transportation network security is one of the most daunting challenges confronting homeland security agencies today. Significant research has been dedicated to model and analyze the vulnerability of transportation systems, yet notably fewer studies propose specific strategies for deploying defensive technologies to safeguard these systems. The ultimate goal of situational awareness for prevention and rapid response remains largely unaddressed. Wireless sensors and ad hoc networks are widely regarded as a promising approach to monitor systems and enhance their security. Furthermore, the growth in smart phone usage can contribute additional relay nodes to the network connecting the deployed sensors and command center. Such sensor networks offer the potential to detect a terrorist plot before it can be executed, support effective response to emergency events, and dynamically monitor traffic flows to facilitate efficient travel within a city. This paper present a scheme which turn the deployment into a optimization to maximize the weighted coverage, and we demonstrate the approach through the simulation, the result of which clear indicates its effectiveness.
The authentication of the IP source address remains one of the most important steps in making the Internet as trustworthy as possible. With existing anti-spoofing solutions, deployed ASes lack cooperation when exchanging routing decisions and disseminations. In general, this makes anti-spoofing mechanisms inefficient and does not adapt to incremental deployment. By introducing routing choice feedback, we propose a distributed inter-domain anti-spoofing solution (Umbrella). In Umbrella, the deployed ASes can acquire approximate global routing choice information. Our approach offers gains in efficiency through the verification of the packets forwarding path and the construction of dynamically spoofing packets filter. Our experimental analysis of Umbrella shows it to be both effective and incrementally deployable.
In recent years, there have been strong interests in the networking community in designing new Internet architectures that provide strong security guarantees. However, none of these proposals back their security claims by formal analysis. In this paper, we use a reduction-based approach to prove the route authenticity property in secure routing protocols. These properties require routes announced by honest nodes in the network not to be tampered with by the adversary. We focus on protocols that rely on layered signatures to provide security: each route announcement is associated with a list of signatures attesting the authenticity of its subpaths. Our approach combines manual proofs with automated analysis. We define several reduction steps to reduce proving route authenticity properties to simple conditions that can be automatically checked by the Proverif tool. We show that our analysis is correct with respect to the trace semantics of the routing protocols.
Cloud computing is becoming a popular paradigm. Many recent new services are based on cloud environments, and a lot of people are using cloud networks. Since many diverse hosts and network configurations coexist in a cloud network, it is essential to protect each of them in the cloud network from threats. To do this, basically, we can employ existing network security devices, but applying them to a cloud network requires more considerations for its complexity, dynamism, and diversity. In this paper, we propose a new framework, CloudWatcher, which provides monitoring services for large and dynamic cloud networks. This framework automatically detours network packets to be inspected by pre-installed network security devices. In addition, all these operations can be implemented by writing a simple policy script, thus, a cloud network administrator is able to protect his cloud network easily. We have implemented the proposed framework, and evaluated it on different test network environments.
Modern consumer devices, like smartphones and tablets, have multiple interfaces (e.g., WiFi and 4G) that attach to new access points as users move. These mobile, multi-homed computers are a poor match with an Internet architecture that binds connections to fixed endpoints with topology-dependent addresses. As a result, hosts typically cannot spread a connection over multiple interfaces or paths, or change locations without breaking existing connections. In this paper, we create an end-to-end connection control protocol (ECCP) that allows hosts to communicate over multiple interfaces with dynamically-changing IP addresses and works with multiple data-delivery protocols (i.e., reliable or unreliable transport). Each ECCP connection consists of one or more flows, each associated with an interface or path. Through end-to-end signaling, a host can move an existing flow from one interface to another, or change its IP address, without any support from the underlying network. We develop formal models to verify that ECCP works correctly in the presence of packet loss, out-of-order delivery, and frequent mobility, and to identify bugs and design limitations in earlier mobility protocols.
Internet Service Providers (ISPs) need to balance multiple opposing objectives. On one hand, they strive to offer innovative services to obtain competitive advantages; on the other, they have to interconnect with potentially competing ISPs to achieve reachability, and coordinate with them for certain services. The complexity of balancing these objectives is reflected in the diversity of policies of the Border Gateway Protocol (BGP), the standard inter-domain routing protocol. Unforeseen interactions among the BGP policies of different ISPs can cause routing anomalies. In this work, we propose a methodology to allow ISPs to check their BGP policy configurations for guaranteed convergence to a single stable state. This requires that a set of ISPs share their configurations with each other, or with a trusted third party. Compared to previous approaches to BGP safety, we (1) allow ISPs to use a richer set of policies, (2) do not modify the BGP protocol itself, and (3) detect not only instability, but also multiple stable states. Our methodology is based on the extension of current theoretical frameworks to relax their constraints and use incomplete data. We believe that this provides a rigorous foundation for the design and implementation of safety checking tools.
Data stealing botnets pose a great risk to the security of networks and the privacy of their users. Most of these botnets use the web as a medium for communication, making them difficult to detect given that web traffic constitutes about 70% of Internet traffic. In addition, they use obfuscation techniques, primarily encryption, to hide their communications and data exfiltration attempts making current botnet detection techniques that depend on content inspection ineffective. In this paper, we present an analysis of the data stealing behaviors of one of the most notorious data stealing botnets, Zeus. In addition, we propose a classification algorithm to identify malicious data stealing attempts within web traffic. Our classifier uses entropy and byte frequency distribution of HTTP POST request contents as features. Our evaluation of the classifier shows high accuracy and high efficiency making it applicable at network perimeter monitoring devices and web proxies.
Wireless Sensor Network (SENSORNET) is vulnerable to various security threats due to its deployment in open and hostile environment. Fast battery exhaustion of sensor nodes in SENSORNET is a challenging issue because it shortens the normal lifetime of sensor network. Affected nodes give rise to denial of service that resists to get the objective of SENSORNET in real life. In this paper, Battery Exhaustion Anomaly Monitoring System (BEAMS) model based on Absorbing Markov Chain (AMC) is proposed to detect battery exhaustion attack in SENSORNET. A mathematical model is considered to analyze whether SENSORNET is affected by battery exhaustion attack.
Ad hoc wireless network with black hole attacks, denial of service attacks and malicious attacks are investigated to study if we can detect harmful activities in a timely manner. A popular simulator NS2 was used to create a controlled wireless network environment. The network was built with 33 nodes with AODV protocol and traffic data was collected from the network. The probability based Nai?ve Bayesian classifier was used with 10 attributed derived from the traffic data. The classification rates obtained is over 97% when numeric attribute values are discretized in Nai?ve Bayesian classifier. It is observed that in our network simulation, denial-of-service attacks and black hole attacks are always correctly detected while malicious attacks and normal traffic are not.
In this paper, we focus on the detection of the Spam over Internet Telephony (SPIT) attacks targeting the Session Initiation Protocol (SIP) protocol during its signaling process. Though, a host of solutions have been proposed to detect and mitigate SPIT attacks in VoIP and SIP based systems, cooperation among local detection systems is so far neglected as a scheme to leverage the entire security of the system and lower the attacks detection duration. We propose a distributed cooperative detection method to detect SPIT attacks based on cooperation between several detection gateways in order to decrease the detection duration of any given SPIT source. An analytical study is conducted to demonstrate the betterments achieved by our distributed cooperative scheme in terms of reduced attack detection duration.
With the popularity of the mobile internet, the thin client terminals play an important role in our daily life. Because the thin client terminals have limited computation ability, the light-weight authentication scheme is a urgent problem to be solved. In this paper, we propose a novel mutual lightweight authentication protocol called HB-MAP. In the protocol the client just need store a small amount of messages and do some simple bit-wise operations, the computation-consuming operations such as the generation of random challenges are all done by the server. Finally we will also analysis the security vulnerabilities of HB-MAP protocol.
For many smaller and larger entities over the last couple of decades, information systems and technologies have become an integral part of their operations and played a major role in drastically changing and often improving their business processes. As computers become more and more integrated into our business organizations, we end up leaving and storing confidential, vital business and sensitive information on them. In general, larger organizations have the technical expertise and resources to better secure computing services. The Small to Medium Enterprises (SMEa?s), however, often lack the platforms, infrastructure, technical expertise, and the required financial resources to be able to utilize modern secure technologies for computing services. This paper discussed the importance of network security, analyzed different type of threats to network infrastructure, different methodologies that can be used to mitigate network infrastructure threats and have proposed an approach for securing SMEa?s network infrastructure. This approach suggested Unified Threat Management (UTM) as the first line of protection to the network, based on the links between each distribution switch layers which offers a zone based monitoring and controlling system to prevent the network from any possible threats.
While realizing that most of the applications currently envisioned for the Internet of Things (IoT) are critical in respect to security, we may expect that such sensing applications may benefit from the availability of end-to-end IPv6 communications with Internet hosts. Research and standardization work is starting to produce mechanisms that may enable end-to-end communications using IPv6-enabled constrained sensing devices, and such communications will raise serious security challenges that must be addressed in the context of a proper integration architecture that is yet to be standardized for the IoT. In our work we target the fundamental question on the effectiveness of the usage of security mechanisms to protect end-to-end communications involving Internet hosts and constrained sensing devices. We describe mechanisms to enable security at the network-layer and at the application-layer and perform an extensive experimental evaluation study with the goal of identifying the most appropriate secure communications mechanisms and the limitations of current sensing platforms in supporting end-to-end secure communications in the context of Internet-integrated sensing applications.
The paper suggests a framework for attack modeling and security evaluation in Security Information and Event Management (SIEM) systems. It is supposed that the common approach to attack modeling and security evaluation is based on modeling of a malefactor's behavior, generating a common attack graph, calculating different security metrics and providing risk analysis procedures. Key elements of suggested architectural solutions for attack modeling and security evaluation are using a comprehensive security repository, effective attack graph (tree) generation techniques, taking into account known and new attacks based on zero-day vulnerabilities, stochastic analytical modeling, and interactive decision support to choose preferred security solutions. The architecture of the Attack Modeling and Security Evaluation Component (AMSEC) is proposed, its interaction with other SIEM components is described. We present the prototype of the component and the results of experiments carried out.
With increasingly more businesses engaging in offshore outsourcing, organisations need to be made aware of the global differences in network security, before entrusting a nation with sensitive information. In July 2011, Syn and Nackrst1 explored this topic by analysing seven countries from a wide spectrum across the globe for network security vulnerabilities. The countries selected were China, the United Kingdom, Germany, Russia, India, Mexico and Romania. Their method utilises Nmap and Nessus to probe and test for network vulnerabilities from each respective nation, in order to collect quantitative data for national vulnerability volumes. The Vulnerability statistics collected are of four categories, High, Medium, Low and Open Ports. This paper extends Syn and Nackrst1's work by constructing a more detailed analysis of their results, showing the number of real-world vulnerabilities per nation, the differences between national levels of network security, the ratios of vulnerabilities/IP address, and vulnerability summary rankings. Multiple causal factors are also looked at to quantify the reasoning behind the varying levels of vulnerabilities per nation. This paper concludes that each nation has millions of vulnerabilities of varying amounts, and therefore, each nation differs in network security levels. Mexico and India exhibited the most worrying statistics, with the highest number of high level vulnerabilities/IP address ratio. Ultimately, this paper highlights the vulnerability levels that organisations are faced with when engaging in foreign and domestic outsourcing.
The technology of Security Information and Event Management (SIEM) becomes one of the most important research applications in the area of computer network security, including distributed networks of internet enabled objects (as in the Internet of Things). The overall functionality of SIEM systems depends largely on the quality of solutions implemented at the data storage level, which is purposed for the representation of heterogeneous security events, their storage in the data repository and the extraction of relevant data for the analytical modules of SIEM systems. An ontological approach at present becomes more applicable for realizing these tasks in various spheres of information security. The paper discusses the possibilities of applying the ontological approach for implementation of the data repository of SIEM systems for distributed networks of Internet enabled objects. Based on the analysis of existing SIEM systems and standards, the choice of ontological approach is done, an example of the ontological data model of vulnerabilities is presented, a hybrid architecture of the ontological repository is proposed and the issues of developing and testing the repository for attack modelling and secure evaluation tasks are discussed.
Wireless Sensor Networks consist of sensor nodes with wireless communication devices and sink nodes (here-inafter referred to as destination nodes). Intermediate nodes relay the source nodes' data packets to destination nodes. When an intermediate node steals a source node's ID within relayed data packets, it can impersonate the source node. In this paper, we have proposed a new detection method against such impersonation attacks using Bloom Filters and a Secret-Sharing-Scheme-based Dispersed Data Transfer method. In addition, we simulated this scheme to confirm its effectiveness.
It is now necessary to have a better understanding of the mindset of a hacker in order to provide better protection for your network systems. The large scale and well organized targeted attacks that have been recently uncovered, demonstrate that system administrators can ill afford to take the wait and see approach. Also it has been shown that on-the-job training is limited in how well it can prepare organization's defensive capacity. Another issue is the lack of a standardized approach on gauging the technical proficiency of staff or the robustness of the network they protect. Attacks to computer networks are on the increase as the tools used by attackers are getting more automated and easier to use for the non-technical person. Staying one step ahead of the enemy has never been more important with the rise of the number of script kiddies, the proliferation of increasingly advanced one click automated attack tools and the apparent destructive force available to hacker groups such as anonymous. Defining and profiling the enemy is a large part of this problem. Recent anonymous arrests have shown these attacks originated from stereotypical disgruntled teenagers whom lack the cause and effect understanding that adults posses. We must somehow begin to deliver effective industrial training to the system administrators. If a system has not been compromised to-date, does it mean it will not be compromised tomorrow? and How do we know that the defenses of the system can withstand an attack if it has not already done so. This paper outlines the merits of utilizing the Security Shepherd white-hat gaming framework as a mechanism for rapid up skilling of front-line computer network defensive staff to the mindset of hackers.
This paper aims to investigate and identify distinguishable TCP services, that comprise of both attack and normal types of TCP packets, using J48 decision tree algorithm. A predictive model capable of discriminating between normal and abnormal behavior of network traffic is developed by integrating Hidden Markov Model (HMM) technique with anomaly intrusion detection approach for each distinguishable TCP service. The model has been trained for each TCP session of the KDD Cup 1999 dataset using Baum-Welch training (BWT) and Viterbi training (VT) algorithms. Evaluation of the developed HMM model is performed using Forward and Backward algorithms. Results show that the proposed model is able to classify network traffic with approximately 76% to 99% accuracy. The overall performance of model is measured using standard evaluation method ROC curves.
Internet Information Services (IIS) is a modular TCP/IP network server application and a Software Development Kit from Microsoft. As a web server, it provides a platform for hosting and managing web applications and as a software development kit, it facilitates the developers to create applications to manage IIS server, or web applications that run on an IIS server. IIS stores all its configuration settings (server and site level) in plaintext XML files. The reliable functioning of IIS relies heavily on the integrity and confidentiality of these files. The protection provided to these files is; they can be accessed under the administrator's account only and the passwords are stored in encrypted form. But all other configurations relating to sites and the server are present in plaintext and are always accessible to the logged-in administrator. As there is no other protection layer except the administrator account login, therefore if someone manages to get into the system by some means, he can easily modify the files the way, he wants. As the web server is always running (or runs for long time intervals), these files are almost; constantly subjected to threats of integrity and confidentiality. This paper proposes and presents that another security layer be applied on these files, so that the threats to integrity and confidentially be minimized when the configuration files are not being edited by the administrator.
On demand allocation of honeypots at right places on the network and at right time would considerably make the network more secure and harder to sneak into. This paper proposes an idea of dynamically creating, modifying and managing virtual honeypots-Honeydoop. Honeydoop is a system of dynamically creating, modifying and managing virtual honeypots. It combines the concept of honeypots and uses big data analyzer Hadoop for quick information retrieval and analysis. The goal of the system is to create evanescent honeypots at right places and times, on demand, to achieve better security in this ever changing environment. The system finds the machines on the network which attackers are interested in using IDS alerts and network traffic analysis. Virtual honeypots replicating those systems are then created and deployed on the network. Suspicious traffic destined for the target system is then redirected to the newly created honeypot. Dormant honeypots are deleted periodically. Honeydoop can also be used to analyze existing honeypot logs.
The paper firstly analyzes the limitation of traditional defense methods, including the firewall and the IDS. Then the paper discusses the traditional honeypot system in detail. Unfortunately, the traditional honeypot is a passive active defense mechanism in nature, and could be unfavorable for cyber defense. Inspired by the mimetic phenomenon of the nature, we introduce the mimicry concept into the network defense, define the concept of protective coloration and warning coloration for cyber defense, and then formalize a mimicry honeypot model, which could perceive and adaptive the change of the network service and perform better camouflage. Several important issues are discussed in this paper. So far as we know, this is the first paper about the mimicry honeypot.
In underwater sensor network, security becomes a primary concern in order to provide protected communication. In this paper, we present a novel network simulation component, wormhole attack, in NS2, for underwater sensor networks, after analyzing the current methods of simulating wormhole attack. The core idea of wormhole implementation is making a "tunnel" between two malicious nodes. If this "tunnel" is built on normal wireless or wired channel, there would be some unexpected channel loss. So in order to solve the problem, we selected the existing wired object Link, in the NS2 wired simulation package, to simulate the "tunnel" by copying the local packets and directly sending them to the wormhole object installed in a remote node. Experimental results demonstrate the effectiveness of the wormhole components in NS2 worked smoothly and comply with the actual.
Among the current information security prevention systems such as firewalls and intrusion detection systems, there exist several shortages such as alert overload, high false alarm rate, absence of effective alert management mechanism etc. As a result, there is a tremendous amount of alert data overload in the network, and this data could be redundant, irrelevant or meaningless. The result of this information flooding is the inability to correctly correlate the events to locate the security breach. In this paper, we aim to present the architecture of an integrated computer network defense system that is efficient, distributed and adaptable; in short, a good match for the dynamic environment of cloud computing. The use of peer-to-peer architecture is investigated for computer network defense. The architecture consists of an advanced intrusion detection system for identification of malicious traffic in such a manner that a centralized controller correlating the events is not overwhelmed by the deluge of alerts. We investigate the Content Addressable Network Distributed Hash Table for the event aggregation.
Proxy Mobile IPv6 (PMIPv6) is a network-based mobility management protocol. Mobile node (MN) does not participate in mobility-related signaling in PMIPv6. PMIPv6 can support multi-homing hosts with the help of virtual interface to perform inter-technology handover or to support flow mobility. However, a single virtual interface has the problem of swapping link-layer identifier (LL-ID) when the neighbor discovery is performed after inter-technology handover. Whenever the single virtual interface receives an Neighbor Solicitation (NS) message, the virtual interface analyzes the NS message and it changes the LL-ID of virtual interface to the LL-ID of physical interface for keeping communication. This approach is not natural and is not recommended because it has a security problem. In this paper, the efficient inter-technology handover scheme with the multiple virtual interfaces are proposed to solve the problem of swapping LL-ID with Proxy Neighbor Advertisement message in Neighbor Discovery. This paper explains the difference based on the result of comparison with a single virtual interface. NS-3 simulation is performed to provide a clear and practical procedure of multiple virtual interfaces in PMIPv6 network.
The role of Intrusion Detection System (IDS) has been inevitable in the area of Information and Network Security - specially for building a good network defense infrastructure. Anomaly based intrusion detection technique is one of the building blocks of such a foundation. In this paper, the attempt has been made to apply hybrid learning approach by combining k-Medoids based clustering technique followed by Na?ve Bayes classification technique. Because of the fact that k-Medoids clustering techniques represent the real world scenario of data distribution, the proposed enhanced approach will group the whole data into corresponding clusters more accurately than k-Means such that it results in a better classification. An experiment is carried out in order to evaluate performance, accuracy, detection rate and false positive rate of the classification scheme. Results and analyses show that the proposed approach has enhanced the detection rate with minimum false positive rates.
Firewall is the first line to protect network security. In this paper, current state, role and classification of firewall technologies are introduced in detailed, the shortcomings and the future development trend are explored.
Although IPv6 protocol has considered and implemented many security mechanisms compared with IPv4, there are still many security threatens in IPv6 Networks. In this paper, we has designed and implemented a network forensics prototype 6Foren in IPv6 environment based on the protocol analysis technology, its functions include packet capture, data reconstruct and messages replay etc. the 6Foren can be used as the online digital forensics which support the online forensic of HTTP, FTP, SMTP and POP3 protocols.
One of the challenges facing information technology (IT) security professionals is the laborious task of sifting through numerous log files in an attempt to identify malicious traffic and conduct a forensics analysis to determine an appropriate course of action. This process is complicated significantly by the volume of traffic that can be associated with a production device environment. A honey net can provide a mechanism to identify much of the forensically interesting traffic by creating a representative system to collect traffic data. However, it is challenging to maintain an accurate representation of a dynamic system in order to consistently collect the appropriate data of interest. This research effort addresses a current challenge identified by researchers at the Honey net Project by describing a methodology for automatically creating and dynamically updating a honey net in order to facilitate IDS support.
Recent years have witnessed an unprecedented growth in the security software market. This market is fiercely competitive with about seven dozen vendors, yet, the price is high and coverage low. Although current research has examined these idiosyncrasies and has found the existence of a negative network effect as a possible explanation, an important question still remains: what possibly discourages product differentiation in this competitive market? We develop a comprehensive quantitative model to answer this question. Our analyses reveal that the negative network effect makes quality differentiation less likely, as well. Hence, our contributions are two-fold: (i) we explain why quality differentiation is often suboptimal for vendors in this market, and (ii) we generalize prior work to include asymmetric equilibriums and find that the presence of the negative network effect still provides insights into the unique structure of this market, with important strategic implications for vendors.
Mobile devices are now well integrated with advanced capabilities and technologies such as the Internet. Today, mobile security has become a globally critical issue due to the high usage of mobile devices, their convenience and mobility. However, they are not properly protected compared to computer and computer networks, and the users pay less attention to the security updates. Recently, mobile devices and networks have been targeted by one of the most dangerous cyber threats, known as botnets. Mobile botnets have not yet been fully explored as they have only recently migrated to mobile infrastructures. Therefore, in this paper, we present an overview of mobile botnets including studies on the new command and control mechanisms, actual examples and malicious activities. We also review the current challenges and limitations of botnet detection in mobile environments, as well as existing solutions.
A tremendous number of low-level alerts reported by information security systems makes it challenging for security administrators to do an effective analysis and initiate a timely response. Alert correlation techniques have been proposed to reduce the number of alerts and provide a succinct and high-level view of attacks. Most of the existing approaches rely on a priori and hard-coded domain knowledge that leads to their difficult implementation and limited capabilities of detecting new attack strategies. To address the drawbacks of these approaches, the recent trend of research in this area has gone towards extracting attack strategies through automatic analysis of intrusion alerts. In this paper, we present new algorithms to mine attack behavior patterns from a large number of intrusion alerts without specific prior knowledge about attacks. Unlike expert knowledge-based methods, our proposed scheme automatically generates correlation rules from the previously observed alerts using a Bayesian causality mechanism. The attack activity patterns learned by this way can help us to correlate alerts, reconstruct attack scenarios and predict possible forthcoming attacks in a real-time system. Our experimental results clearly show efficiency of the proposed method in learning new attack strategies.
Networked control systems present an inviting target for adversaries seeking to attack physical infrastructure through cyber attacks alone. A diverse set of possible attacks, including node compromise, false data injection, malware propagation, and denial of service have been identified and studied in isolation. Currently, however, there is no framework for composing multiple attacks, mounted by one or more adversaries, and designing a system defense that guarantees stability and allows flexible performance. In this paper, we introduce a passivity framework for modeling and mitigating multiple, interdependent attacks on networked control systems. Under our framework, multiple adversaries are modeled as passive individual blocks, either in parallel or negative feedback interconnections depending on the interdependencies between the attacks, leading to an overall system that is passive and stabilizable. We present two case studies within this framework, namely joint node capture and malware propagation, as well as joint node capture and control channel jamming, and derive a stabilizing network response to the attacks. Our results are illustrated through a numerical study.
Start of the above-titled section of the conference proceedings record.
Start of the above-titled section of the conference proceedings record.
Greylisting and real-time block listing are two common mechanisms for spam filtering. The former can efficiently reduce the number of spam sessions, but can be easily evaded if spammers retry spam delivery within a period of time. The latter can detect and reject the clients with poor reputation during SMTP sessions by looking up an external blacklist. Its weaknesses are the high false-positive rate and a lack of locality. We design a method of dynamically updating the greylist and block list based on the delivery behavior of spammers to block spam sessions in time. The method is implemented on a mail gateway in a senior high school. In our experiments with the real-world mail traffic for a month, the method can block 70.29% and 69.21% of the known and possible messages in the spam sessions with greylisting and block listing. The forward and reverse DNS lookups are also adopted to further reduce the false-positive rate to under 0.01%. Therefore, the required system resources for further spam filtering on mail servers can be greatly saved by blocking most of the spam sessions in time.
Start of the above-titled section of the conference proceedings record.
The NFC (Near Field Communication) is a promising technology for the Internet of Things (IoT). It enables proximity communications (a few centimeters) with modest throughputs (hundreds Kbit/s) and low power consumption (a few mW). Although this technology is deployed for payment, access control, transport, or file transfer applications, it does not support a security framework. This demonstration presents Peer to Peer (P2P) transactions protected by a new protocol, LLCPS, i.e. the Logical Link Control protocol (LLCP) secured by TLS. LLCPS should enable a wide range of trusted services for the IoT.
This paper investigates studies how cooperative jamming helps improve the secrecy throughput of large decentralized networks where the locations and channel state information (CSI) of eavesdroppers are both unknown. The spatial distributions of legitimate nodes(transmitter/receiver pairs and helping jammers) and eavesdroppers are modeled as Poisson point processes. The helping jammers, equipped with multiple antennas, broadcast artificial noise that confuses eavesdropper but zero-forcing to the legitimate receiver. A jamming protocol based on the RTS/CTS handshake of IEEE 802.11 standard is proposed for decentralized implementation. Closed-form and numerical results analyze the benefits of jamming on secure communications according to parameters such as the Tx/Rx/jammers/eavesdroppers densities and guard zone.
Phishing attacks have frequently used homographs since they were first described a decade ago. Though a wide variety of tools and techniques have been used to mitigate the effectiveness of these attacks, none have offered a comprehensive solution. This paper describes the homograph attack as a mathematical problem, identifies the various flavors of the attack, and provides a solution applicable to a wide variety of scenarios.
To explore the passive detecting mechanism of traditional intrusion detection system, based on autonomic computing, an autonomic proactive intrusion detection model is proposed in this paper. Centered on an autonomic manager, this model introduces an auction mechanism in the agent coordination layer to perceive environmental changes, to manage and allocate resources, and to achieve active intrusion response. Experimental results show that through rational parameter configuration, the model can effectively improve adaptability and detection accuracy of IDS.
The Border Gateway Protocol (BGP) is the path vector routing protocol that connects different autonomous systems. The traditional BGP protocol is weak to provide security for AS path and verification of Autonomous system number ownership as well as network prefix. The BGP remains vulnerable to various types of misconfiguration and attacks. Many secure BGP algorithms have been proposed but complexity of algorithm and attack on that models still remain open problem. In this paper, we propose a single solution of all types of attacks on BGP. We use an efficient hybrid mechanism like cyclic shift and SHA-1. In such a construction create trust between BGP speakers only one time at during TCP session establishment; Instead of distribute key in plaintext, first we generate hash code of key and then we use optional field of OPEN message of TCP for key distribution process. In our work we authenticate BGP speakers through hash code of key, if hash code matches then establish secure connection and exchange routing UPDATE. If hash code does not match then connection not set up between BGP speakers. In our simulation studies, we noticed efficient reduction of cost of router memory requirement, time and traffic between BGP speakers.
The goal of this work is to provide a systematic virus detection hardware solution for network security for real time systems. Virus Detection Processor is a complete network simulation from virus threats can be handled only using VLSI chip without any software dependency and it is scalable to large pattern sets. Bloom filter algorithm is used for filtering the input patterns. Safety shift strategy algorithm is used for exact matching process. Pipelining technique is added to the processor to speed up the processor. Pipelining is used to work parallely to achieve the end function. High speed is achieved using pipelining technique.
Software defect prediction studies have shown that defect predictors built from static code attributes are useful and effective. On the other hand, to mitigate the threats posed by common web application vulnerabilities, many vulnerability detection approaches have been proposed. However, finding alternative solutions to address these risks remains an important research problem. As web applications generally adopt input validation and sanitization routines to prevent web security risks, in this paper, we propose a set of static code attributes that represent the characteristics of these routines for predicting the two most common web application vulnerabilities-SQL injection and cross site scripting. In our experiments, vulnerability predictors built from the proposed attributes detected more than 80% of the vulnerabilities in the test subjects at low false alarm rates.
In this work, we consider detecting unknown or new network attack types with a Fuzzy Genetic Algorithm approach. The fuzzy rule is a supervised learning technique and genetic algorithm make fuzzy rule able to learn new attacks by itself. Moreover, this technique has high detection rate and robust. Therefore, we apply the fuzzy genetic algorithm approach to our real-time intrusion detection system implementation i.e. the data is detected right after it arrived to the detection system. In our experiments, various denial of service (DoS) attacks and Probe attacks are considered. We evaluate our IDS in terms of detection time, detection rate and false alarm rate. From the experiment, we obtain the average detection rate approximately over 97%.
Active Peer-to-Peer (P2P) worms present serious threats to the global Internet by exploiting popular P2P applications to perform rapid topological self-propagation. Active P2P worms pose more deadly threats than normal scanning worms because they do not exhibit easily detectable anomalies, thus many existing defenses are no longer effective. We propose an immune system with Phagocytes - a small subset of elected P2P hosts that are immune with high probability and specialized in finding and eating worms in the P2P overlay. The Phagocytes will monitor their managed P2P hosts' connection patterns and traffic volume in an attempt to detect active P2P worm attacks. Once detected, local isolation, alert propagation and software patching will take place for containment. The Phagocytes further provide the access control and filtering mechanisms for communication establishment between the internal P2P overlay and the external hosts. We also design an adaptive and interaction-based computational puzzle scheme at the Phagocytes to restrain external worms attacking the P2P overlay, without influencing legitimate hosts' experiences significantly. We implement a prototype system, and evaluate its performance based on realistic massive-scale P2P network traces. The evaluation results illustrate that our Phagocytes are capable of achieving a holistic immunity against active P2P worms.
The existing P2P routing algorithms generally don't take routing security into consideration. Therefore P2P overlay systems are vulnerable to a kind of attacks called misrouting attacks. This is especially serious in structured P2P overlays due to the deterministic routing algorithms. What's more, in dynamic network environments, the continuous process of node arrival and departure, churn, makes peers' routing table lose accuracy and may lead to the increase of system latency. In this paper, we propose a trust-based routing strategy in structured P2P systems to solve these problems, using quantified trust degree to select next-hops and neighbors. Not only direct trust, but also indirect trust is used for optimization. At the end of this paper, the simulation results show that this trust-based routing strategy leads to higher look-up success rate compared to the original routing algorithm, with the same magnitude of cost.
IEEE 802.16m standard redefined with many improvements on IEEE 802.16e standard to provide the best connectivity and to perform the error-free data transmission. In this paper, an improved Traffic based Duo triggered handoff algorithm for the buffer management is proposed to enhance the quality of service features in the air interface standard of IEEE 802.16 m. The standard aims to reduce overhead, improved coverage through optimized parameterization and excessive security measures. These parameters define the set of services that are provided for protection and they are agreed upon at the time of security association establishment. It includes the request and response of both the MS and the BS in order to measure the parametric values and the respective threshold values to find the optimum for handoff operations. Optimization in the threshold value is achieved through the calculation both SNR and delay in a combined manner to evaluate the heuristic handoff for a given scanning interval. We evident that our proposed system framework performs efficient handoff management with effort from base station scheduler and subscriber station scheduler. These processes incur least bandwidth underutilization thereby reducing the transmission delay under heavy traffic condition with QoS features conciliation.
The selection of the most appropriate network in heterogeneous Wireless environment is one of the critical issues to provide the best Quality of Service (QOS) to the users. The selection of an apt network among various alternatives is a kind of Multi Criteria Decision Making (MCDM) problem. This paper describes a novel Multi Criteria Decision Making (MCDM) method to evaluate and select the suitable network for homogeneous wireless network environment. The proposed MCDM technique involves Fuzzy Analytical Hierarchy Process (FAHP) is integrated with Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) and VlseKriterijumska Optimizacija I Kompromisno Resenje in Serbian (VIKOR) techniques. FAHP is used to determine the criteria weights, whereas TOPSIS and VIKOR used to find the performance ranking of the alternative networks. This study focuses on five network alternatives such as WLAN, GPRS, UMTS, WIMAX, and CDMA and ten evaluation criteria such as bandwidth, latency, jitter, BER, Retransmission, Packet loss, through put, preference, security, cost to select the appropriate network.
TCP veto is a detection-resistant variation of the TCP connection hijacking attack. While not limited to SCADA protocols, Modbus TCP, the Ethernet Industrial Protocol (EtherNet/IP), and the Distributed Network Protocol (DNP3) each meet the necessary assumptions of the attack. Experimental results reveal that the integrity of messages transmitted using each of the three SCADA protocols are vulnerable to TCP veto. Additionally, TCP veto produces up to 600 times less network traffic during its attack than connection hijacking. This work underscores the vulnerability of current SCADA protocols that communicate over TCP/IP to network attack. A method to definitively identify TCP veto requires a detection system to perform deep packet inspection on every TCP packet of a monitored connection. Methods for mitigating the attack through message authentication include implementing DNP3 with Secure Authentication, tcpcrypt, or Internet Protocol Security (IPsec).
Cyber attacks on a smart grid aiming at misleading the control center with incorrect topology information are considered. In such attacks, an adversary intercepts network and meter data from the remote terminal units, modifies part of them, and forwards the modified data to the control center. A necessary and sufficient condition for an undetectable topology attack is presented, and an undetectable attack that requires the modification of only a few meter data is proposed. When the adversary has limited local information, a heuristic attack strategy is proposed. The proposed attacks are tested with IEEE 14-bus and 118-bus systems, and their effect on real-time locational marginal pricing is examined.
This paper presents a dynamically updating key distribution strategy to enhance mesh network security against cyber attacks. The scheme has been applied to two security protocols known as Simultaneous Authentication of Equals (SAE) and Efficient Mesh Security Association (EMSA). The former is based on a password-authenticated key exchange and the latter relies on mesh key holders through the use of a mesh key hierarchy. Since both SAE and EMSA protocols utilize 4-way handshaking, the resiliency of the network in a situation where an intruder carries a denial of service attack has been evaluated. This includes the effect of the proposed dynamic key refreshment strategy on the network delay and overhead performance.
Vulnerability of a Network (can be an office LAN or computer systems connected together for secure data communication) is defined as susceptibility of a network for a successful intrusion or attack. Network's vulnerability depends on some specific factors. In this paper we have used Rough Set theory to trim down the massive data of factors. The paper considers various possibilities with different possible combinations of attack factors and deducting rules, based on Rough set theory. Thus, determining Vulnerability factor of the Network; describing proneness of the network for a successful attack. Albeit the paper considers only few factors, there can be enormous number of factors.
We propose a distributed and self-organized framework for collaboration of multiple heterogeneous IDS sensors. The framework is based on a game-theoretical approach that optimizes behavior of each IDS sensor with respect to other sensors in highly dynamic environments. We formalize the proposed collaborative architecture as a game between defenders and attackers and transform the hard problem of heterogeneous collaboration into an easier problem of finding two functions that are used in the game-theoretical model to specialize the detection mechanisms on a specific type of malicious activity. The collaboration of such more specialized IDS nodes covers much wider range of attack classes, allowing the collaborating system to maximize the overall network security awareness. We have evaluated the proposed concept on real networks, where we have shown considerable improvements in the detection capabilities of intrusion detection devices thanks to the proposed collaboration model.
Voice over IP (VoIP) is increasingly replacing fixed line telephone systems globally due to lower cost, call quality improvements over digital lines and ease of availability. At the same time, criminals have also transitioned to using this environment, creating challenges for law enforcement, since interception of VoIP traffic is more difficult than a traditional telephony environment. One key problem for proprietary VoIP algorithms like Skype is being able to reliably identify and characterize network traffic. In this paper, the latest Skype version and its components are analyzed, in terms of network traffic behavior for logins, calls establishment, call answering and the change status phases. Network conditions tested included blocking different port numbers, inbound connections and outbound connections. The results provide a clearer view of the difficulties in characterizing Skype traffic in forensic contexts. We also found different changes from previous investigations into older versions of Skype.
Controlling the flow of sensitive data has been widely acknowledged as a critical aspect for securing web information systems. A common limitation of previous approaches for the implementation of the information flow control is their proposal of new scripting languages. This makes them infeasible to be applied to existing systems written in traditional programming languages as these systems need to be redeveloped in the proposed scripting language. This paper proposes a methodology that offers a common interlinqua through the use of Semantic Web technologies for securing web information systems independently of their programming language.
We propose Pecan, a circuit-less, peer-to-peer low-latency anonymous system. In Pecan, users do not need to construct a circuit for communications. The anonymity is provided by Pecan's highly peer-to-peer, circuit-less structure and the feature that all re-encryption keys are generated locally by the intermediate nodes themselves. Pecan also has stronger resistance against several DoS attacks that the circuit-based systems do not. The high anonymity is obtained without harming user experience: a user could send anonymous requests and get corresponding responses with less stringent requirements on the intermediate nodes involved on the route. Pecan's design is based on a revised onion routing and a new proxy re-encryption algorithm. These cryptographic algorithms maintain a small amount of processing overhead for system nodes. Our analysis and experiment showed the total processing delay is relatively small in comparison to the whole end-to-end latency. The throughput is satisfactory for normal users' online activities.
Distributed hash tables (DHT) are a key building block for modern P2P content-distribution system, for example in implementing the distributed tracker of BitTorrent Mainline DHT. DHTs, due to their fully distributed nature, are known to be vulnerable to certain kinds of attacks and different kinds of defenses have been proposed against these attacks. In this paper, we consider two kinds of attacks on a DHT, one already known attack and one new kind of an attack, and show how they can be targeted against Mainline DHT. We complement them by an extensive measurement study using honeypots which shows that both attacks have been going on for a long time in the network and are still happening. We present numbers showing that the number of sybils in the Mainline DHT network is increasing and is currently around 300,000. We analyze the potential threats from these attacks and propose simple countermeasures against them.
As one of the most popular low-latency anonymous communication systems, Tor has been a great success but still faces some challenges, e.g., subject to low-resource attacks and no explicit mechanisms to explore tradeoffs between anonymity and performance. In this paper, we propose a Relay Recommendation System (RRS) for Tor to provide reliable relay information for building paths with better performance, mitigate low-resource attacks, and enable users to explore the tradeoffs between performance and anonymity based on their needs. We first present the design of RRS, and show its performance improvement and resistance to low-resource, high-resource, and reputation attacks. We further analyze the potential anonymity decrease, and propose new path selection schemes to enable selective anonymity based on user needs. We have evaluated RRS via both analysis and experiments on a Tor simulation platform. Our results show that RRS achieves significant performance improvement with a small decrease of anonymity.
With the proliferation of the use of peer-to-peer networks for distributing multimedia and software content, there is an increasing need for efficient and reliable data distribution mechanisms that can scale to large numbers of users. The peer-to-peer protocol BitTorrent is scalable and is widely used to share content. The lack of reputation techniques in BitTorrent makes it impossible to get any information about how reliable a peer is. In this paper, we propose a secure and reliable architecture for reputation in BitTorrent. Our framework allows a peer to securely share its feedback about another peer after a transaction. Then, allow other peers to securely retrieve this information when required. Furthermore, we discuss how secure our system is by studying how it protects against the well knows peer-to-peer attacks.
We present Backward Traffic Throttling (BTT), an efficient, decentralized mechanism for congestion and bandwidth-flooding attacks mitigation. Upon congestion, BTT employs three basic mechanisms to throttle excessive traffic, namely: prioritize legitimate flows, shape traffic, and request upstream BTT nodes to similarly prioritize and shape traffic. Flow prioritizing parameters are determined independently by each BTT server, based on typical traffic estimations. BTT is easily deployed: it requires no changes to routers, and does not modify traffic. Instead, BTT configures routers' queuing discipline and traffic shapers. Both simulation and testbed experiments were performed to asses the effectiveness of BTT during distributed denial-of-service (DDoS) attacks. Results show that even limited BTT deployment alleviates attacks damage and allows legitimate TCP traffic to sustain communication, whereas larger deployments maintain larger portions of the original bandwidth.
The detection of anomalies in backbone networks is posing serious performance issues, not only in terms of accuracy, but also in terms of detection speed. Indeed current software solutions to the problem, even promising from the point of view of detection and false alarm rates, suffer from the inability of performing the required operations in real time, when working in high speed backbone networks. On the other hand, hardware solutions are based on costly and inflexible niche systems.
Fast, easy and inexpensive deployment of wireless networks has made them one of the most popular communication environments. Wireless networks are becoming ubiquitous and widely used to transfer critical information such as banking accounts, credit cards, e-mails and social network credentials. The more pervasive the wireless technology is going to be, the more important its security issue will be. The current security protocols for wireless networks have addressed the privacy and confidentiality issues, but failed to address other important security attributes such as availability and integrity (e.g. denial of service, session hijacking and MAC address spoofing attacks). In this paper we describe an anomaly-based intrusion detection system for the IEEE 802.11 wireless networks, based on tempo-spatial data analysis technique to detect deviations from normal behaviors that are triggered by wireless network attacks. Our anomaly behavior analysis of the 802.11 protocol is based on n-gram pattern analysis. We apply statistical techniques to quantify the n-transition patterns in the protocol and determine the probabilities of these transitions being normal.
Traffic classification has been extensively examined in recent years, as it is widely used in network management, design, security, advertising and research. In the past few years, the traffic classification techniques have been evolved along with the development of Internet protocols and applications, and many approaches have been investigated, proposed and developed. Nowadays, the ever increasing network bandwidth, the constantly sophisticated applications and the growth incentives to confuse classification systems to avoid filtering or blocking are among the reasons that traffic classification remains one of the hot areas in network research. In this paper, we first attempt to present an analysis of the existing traffic classification techniques, and dwell on their issues and challenges, then outline some recommendations that can improve the performance of traffic classification systems.
Over the past decade malware, i.e., malicious software, has become a major security threat on the Internet. Today anti-virus companies receive thousands of malicious samples every day. However the vast majority of these samples are variants of the existing malware. Due to the sheer number of malware variants it is important to accurately determine whether a sample belongs to a known malware family or exhibits a new behavior and thus requires further analysis and separate detection signature. Despite of the importance of network activity, the existing research on malware analysis does not fully leverage the malware network behavior for classification. In this paper, we propose an automated malware classification system that focuses on network behavior of malware samples. Our approach employs behavioral profiles that summarize the network behavior of malware samples. The proposed approach is applied to a real world malware corpus. Our experimental results show the effectiveness of the proposed approach in classifying malware samples only based on the network activity exhibited by the samples.
Protecting against cyber attacks is no longer a problem of organizations and home users only. Cyber security programs are now a priority of most governments. Cyber criminals have been using botnets to gain control over millions of computer, steel information and commit other malicious activities. In this paper we propose a self-healing architecture that was originally inspired from a nature paradigm and applied in the computer field. Our solution is designed to work within a network domain. We present the initial design of our solution based on the principles of self healing systems and the analysis of botnet behaviour. We discuss how to either neutralize or reverse (correct) their actions ensuring that network operations continue without disruption.
Wireless Personal Area Networks provide a pivotal role in local area network technology complementing traditional Wireless Local Area Network technologies. Bluetooth, ZigBee and NFC (Near Field Communications) have emerged as key WPAN technologies with UWB (Ultra Wide Band) standards currently evolving. They are however subject to the usual range of security vulnerabilities found in wireless LANs such as spoofing, snooping, man-in-the-middle, denial of service and other attacks. However security in WPANs is not as mature as it is in Wireless LANs and further work is needed in order to provide comparable protection. This paper examines a range of WPAN technologies and security issues and proposes protection mechanisms that can mitigate risk in each case.
Analysis based on Internet Background Radiation (IBR) has been shown to be effective for detecting Internet threats such as worms and DDOS attacks. In contrast with traditional methods using darknets, this paper proposes a scheme of extracting IBR from raw traffic gathered at a point of presence (PoP) by its ISP. This method is proceeding from a different angle based on redefined greynet and IBR's own characteristics. The method's basic principle is introduced first and then it is qualitatively analyzed using precision and recall. On this basis, the method is implemented facing raw traffic in a particular format and applied to measured data with certain scale. Based on the successfully extracted IBR, subsequent analysis reveals that this scheme is effective and feasible.
Wireless sensor network (WSN) deploys tiny wireless sensor nodes to communicate with each other with limited processing speed, power and security measures. A recent WSN routing protocol defined as Secure Real-Time Load Distribution (SRTLD) has been developed to provide realtime transfer, high delivery ratio, and longer sensor node lifetime. SRTLD has been compared with LQER, MMSpeed, RTPC and RPAR. However, SRTLD uses broadcast packets to perform neighbour discovery for every packet transfer every hop, thus consume high energy. A novel Biological inspired self-organized Secure Autonomous Routing Protocol (BIOSARP) to enhance SRTLD with self-optimized and autonomous secure routing mechanism. The BIOSARP routing protocol depends on the optimal forwarding decision obtained by Ant Colony Optimization (ACO). The pheromone value in ACO is computed based on end-to-end delay, remaining battery power, and packet reception rate metrics similar to SRTLD. The proposed BIOSARP has been designed to reduce overhead broadcast packet in order to minimize the delay, packet loss and power consumption in WSN. In this paper we have presented the improved ACO algorithm that has been utilized in BIOSARP to perform self-optimized routing in WSN. BIOSARP has been studied and verified through simulation in network simulator 2 (NS-2). In simulation study BIOSARP normalized overhead is 12.1% less as compare to E&D ANTS and achieves 14% higher delivery ratio with 9% less power consumption when compared to SRTLD. Hence, the results confirm that BIOSARP offers better performance and can be practically implemented in WSN applications as structural and environmental monitoring, battlefield surveillance.
In the network security cybercrime technologies have brought many good things by means of the internet: electronic commerce, easy access to vast stores of reference material, collaborative computing, e-mail, and new avenues for advertising and information distribution, to name a few. As with most technological advances, there is also a other side: criminal hackers. Governments, companies, and private citizens around the world are anxious to be a part of this revolution, but they are afraid that some hacker will break into their Web server and replace their logo with pornography, read their e-mail, steal their credit card number from an online shopping site, or implant software that will secretly transmit their organization's secrets to the open Internet. With these concerns and others, the ethical hacker can help. This paper describes ethical hackers: their skills, their attitudes, and how they go about helping their customers find and plug up security holes. Hacking is the word that shakes everyone whenever it is said or heard by someone. Everyone born in this world with attitude wants to be a Hacker. But it is not a job of a new born baby or an old grown lady. A Hacker needs a brilliant mind to hack anything. There are many rules that he should learn to become an Ethical Hacker which is also called as penetrate testing. These rules include knowledge of HTML, Java Scripts, Computer Tricks, Cracking & Breaking etc.etc. In this paper I explain about the hacking techniques and the functions of how it takes place in the network and the methods to be solved.
The following topics are dealt with: network security; pervasive computing; cloud computing; advanced computing technologies; mobile computing; wireless communications; data mining; knowledge discovery; advanced networking; information security; information retrieval; artificial intelligence; soft computing; grid computing; 3G/4G wireless technologies; advanced communication systems; optical communication; networking sensor / ad hoc networks; medical electronics; RF antenna design; RF antenna modeling; RF antenna measurement; computational electromagnetics; image processing; speech processing; real time embedded systems; MEMs and NEMs; electrical drives; embedded control; embedded systems; nano-photonics; fuzzy logic; neural network technologies; embedded networking; embedded system design ; VLSI automation & distributed control systems; remote sensing.
In recent years, internet and computers have been utilized by many people all over the world in several fields. In order to come up with efficiency and up to date issues, most organizations rest their applications and service items on internet. On the other hand, network intrusion and information safety problems are ramifications of using internet. The growing network intrusions have put companies and organizations at a much greater risk of loss. In this paper, propose a new learning methodology towards developing a novel intrusion detection system (IDS) by back propagation neural networks (BPN). The main function of Intrusion Detection System is to protect the resources from threats. It analyzes and predicts the behaviours of users, and then these behaviours will be considered an attack or a normal behaviour. There are several techniques which exist at present to provide more security to the network, but most of these techniques are static. Test the proposed method by a benchmark intrusion dataset to verify its feasibility and effectiveness. Results show that choosing good attributes and samples will not only have impact on the performance, but also on the overall execution efficiency. The proposed method can significantly reduce the training time required. Additionally, the training results are good. It provides a powerful tool to help supervisors analyze, model and understand the complex attack behavior of electronic crime.
A secure network partially depends on user authentication and unfortunately authentication schemes used at present are not utterly secure. Most of the systems today rely on static passwords to verify the user's identity. This paper describes a method of implementing two factor authentication using mobile phones. The proposed system involves using a mobile phone as a software token for One Time Password generation. OTP algorithm powered with user's unique identifications like International Mobile Equipment Identification and Subscriber Identification Module; makes a finite alphanumeric token valid for a session and for a single use. The generated One Time Password is valid for only a short user defined period of time and is generated by factors that are unique to both, the user and the mobile device itself. Additionally, an SMS-based mechanism is implemented as both a backup mechanism for retrieving the password and as a possible mean of synchronization.
Even though a lot security is given in the wireless sensor networks yet the information is being exposed. Such information can then used by the adversary for the attack. The existing privacy techniques defend against a local adversary. There are two main categories of privacy preservation in Wireless Sensor Networks. They are data privacy and the context privacy. In this paper, we describe Location privacy. Location privacy is extremely important in Wireless Sensor Networks. Information on location of events or on location of base stations can be of a primary concern of adversary. Location privacy is very important in hostile environments. It is sufficient for the adversary to find out location of sensors currently monitoring the location of the source to successfully localize and capture the source. Similarly, the adversary only needs to find out location of the base station to be able to mount a physical or other DoS attack on the base station and thus inactivate the whole network. In this paper, they are two main categories of privacy preserving techniques for Wireless sensor network that have been presented, data-oriented and context-oriented. So different techniques against a universal adversary with respect to context privacy are discussed. The paper presents recurrent clustering mechanism.
Many data encryption techniques have been employed to ensure both personal data security and network security. But few have been successful in merging both under one roof. The block cipher techniques commonly used for personal security such as DES and AES run multiple passes over each block making them ineffective for real time data transfer. Also, ciphers for network security such as Diffie-Hellman and RSA require large number of bits. This paper suggests a simple block cipher scheme to effectively reduce both time and space complexities and still provide adequate security for both security domains. The proposed Reverse Circle Cipher uses `circular substitution' and `reversal transposition' to exploit the benefits of both confusion and diffusion. This scheme uses an arbitrarily variable key length which may even be equal to the length of the plaintext or as small as a few bits coupled with an arbitrary reversal factor. This method of encryption can be utilized within stand alone systems for personal data security or even streamed into real time packet transfer for network security. This paper also analyses the effectiveness of the algorithm with respect to the size of the plaintext and frequency distribution within the ciphertext.
This paper describes a method to solve the problem that IP address pool is too small in the SPDnet access layer by introducing The Private Network into SPDnet , such as internal server PAT mapping technique. It is the critical practice that distribution local communication network acts as the private network, only IEC 60870-5-104 protocol's service port (2404) are permitted to open, and it is forbidden that Distribution Terminal Unit in the distribution local communication network actively initiates connection to SPDnet. The method can effectively control the business data to access SPDnet from distribution local communication network, and can reliably ensure the security of SPDnet.
Solve the power communications network security issues, so that from malicious attacks. Through the establishment of electric power communications system dynamic security model for the whole network, the introduction of quantum cryptography communications technologies, building electric power communications system network security protection systems and communications network security framework system, and access to services on the smart grid of electric power communications network security technology system. And defence of the communications network to avoid controllability attacks (namely, illegal use), confidentiality attacks (namely, the secret to detect) and availability attacks (namely, vandalism). Support the smart grid of the power generation, transmission, substations, distribution, customer service, power dispatch, six links, and information platform for normal operation. Based on traditional cryptography communications technology, whole network dynamic security model of electric power communications system was established, quantum cryptography communications technology was introduced. Those are unique keys for cyber security infrastructure of smart grid communications system.
Information system security of power corporations is mainly exposed to physics security risk, network security risk, application security risk and management security risk. The success of information security normally relies on 30% technology support but 70% management investment. Therefore, rigid management plays a crucial role in threat prevention. After a critical summery of the current research on power information system security technology and security management home and abroad, the paper points out a necessary emphasis on information security of production control system and Internet power information system and security management in the future, based on the features and requirements of power information system. First of all, the paper reviews information security and its classification. Then it puts forwards corresponding strategies based on the analysis of various risks, such as network equipment security reinforcement, service security reinforcement, and application security reinforcement. Meanwhile, it gives an introduction of physics security, data security prevention and management regulations.
The advent of the IEC61850 Standard has ushered in a new age for substation automation. This standard has gained a very strong footing in Europe, and its application is spreading across the world. In the USA, the progress towards IEC 61850 implementation is relatively slower. IEC 61850 holds great promises for substation automation and protection. There are many advantages that IEC61850 brings to the area of protection and automation in the form of ease of data collection, reduced construction and design costs, and interoperability between various multi-vendor IEDs. Although this standard is viewed as being very useful and cost-effective, there are several new challenges posed by the implementation of this standard in the US. Issues that concern utilities in the US include the level of cyber security that this standard provides. At this point, the IEC 61850 in the US is limited to the walls of the substation. For the present time, the cyber security issues are within the regulation imposed by the various regulating agencies in the USA. Another issue that is very critical is the new techniques and tools required for commissioning a substation that has multi-vendor IEDs. This main thrust of this paper is to highlight some of the installation and commissioning issues that were encountered during one of the pilot projects at a utility in the US. This pilot project involved the First IEC 61850 substation built in the USA with multi vendor IEDs. There were several challenges that the commissioning team had to face, and it was a learning experience for everybody involved. This paper will highlight all the experiences associated with making this First IEC 61850 substation a great success in the US. This paper will consist of four sections. The First section will cover the hardware needed to support the IEC61850 communication over a TCP/IP network. The second section will explain what and how IEC61850 GOOSE works. This section will include an example of a line relay applica- ion. It will also cover the security aspects of using IEC61850 for substation control and protective relay applications. The third section will cover testing the individual measuring units in the relays, and the overall protection scheme tests. The fourth section is about lessons learned during the initial design and substation commissioning.
According to Maslow's hierarchy of needs, personal security plays an important role in everyone's life. Thus, over the years, surveillance systems have increased in both complexity and performance evolving from video cameras monitored by human operators to systems capable of automatic intrusion detection. Recent technological development and the increased demand for mobility have led to devices equipped with a variety of sensors that are able to handle a considerable workload. A personal security system is therefore, now more than ever, accessible to anyone. This paper presents an implementation of a system of mobile devices, organized as a fault tolerant network, capable of automatic monitoring and detection of intrusions. We quantify how well various devices can perform in such a system and we analyze the fault tolerance it can provide.
This paper presents a study concerning the current implementation of 802.11 wireless local area networks in Romania and their security. The data that was gathered throughout the year 2012 presents the structure of the wireless networks in rural areas compared to urban areas, identifies potential security problems and situates the Romanian national development in the global context. This paper is part of a study concerning the security of computer networks related to the new threats to this segment of mobile networking.
With the growing sophistication of computer worms, information security has become a prime concern for individuals, community and organizations. Traditional signature based IDS, though effective for known attacks but failed to handle the unknown attack promptly. This paper describes a novel honeypot system which capture worm based on their characteristics of self replication. We introduce combination of unlimited and limited outbound connections to capture different payload of single or multiple worms. The proposed system isolate the suspicious traffic and able to effectively control the malicious traffic and capture most useful information regarding the worm's activities, without attacker's knowledge. Our system will be used for critical study of structure and behavior of most sophisticated worms and then forwards the necessary input to Signature Generation Module for automatically generating signature of unknown worms. Our attempt is to generate signature of unknown especially polymorphic worms with low false positive and high coverage. Our system is able to enhance the capability of IDS signature library and increases the probability of detecting most variant of unknown worms.
Like any other wireless communication settings, mobile ad-hoc network inherits potential dangerous vulnerabilities in network security. A proactive security defense such as intrusion detection system has become a recent research topic in this area. Many intrusion detection models are proposed by the researchers and most of them are promising. How-ever, the problem in mobile ad-hoc environment is that communication and power resources are very limited. Thus, any additional features which need to be implemented in this environment must be as efficient as possible. This paper presents a comparison study between the co-operative detection model and the aggregative detection model to evaluate the efficiency of resource usage. We use a sample case study of disaster recovery operations system to have a real scenario of mobile ad-hoc practice. An experiment is conducted using that scenario under different treatments. The contribution of our work is to suggest an intrusion detection model that is efficient but still reliable to use.
Wireless communication has become a very interesting sector for the provision of wireless access network, and available almost anytime and anywhere and the popularity is high. As wireless devices are used in offices, houses and universities, the need for strong and secure transport protocols seems to be one of the most important issues in wireless standards. Many organizations today are faced with all kinds of challenges related to security and privacy of their wireless network. In this case, IIUM community wireless access is not an exception, having thousands of subscribers from both staffs and students. Therefore, this paper discusses the current challenges faced by IIUM community wireless network. Specifically, issues that affects the security and privacy of student's data. The target of the study is to examine the awareness and usage of wireless security policy among IIUM students. The results of the in-depth interview and survey revealed that there several wireless security challenges to IIUM wireless security and low student's awareness. Other findings and recommendations were also highlighted in this paper.
The Internet of Things (IoT) is a flagship project that aims to connect objects to the Internet to extend their use. For that, it was needed to find a solution to combine between the IEEE 802.15.4 protocol for Low Power Wireless Personal Area Networks (LoWPANs) and IPv6 network protocol that its large address space will allow million devices to integrate the internet. The development of 6LoWPAN technology was an appropriate solution to deal with this challenge and enable the IoT concept becoming a reality. But this was only the beginning of several challenges and problems like the case of how to secure this new type of networks, especially since it includes two major protocols so the combination of their problems too, over and above new problems specific to that network. In this paper, we analyze the security challenges in 6LoWPAN, we studied the various countermeasures to address these needs, their advantages and disadvantages, and we offer some recommendations to achieve a reliable security scheme for a powerful 6LoWPAN networks.
The following topics are dealt with: network security and high performance computing; information systems; artificial intelligence; software engineering; image processing and pattern recognition.
In order to improve networks' total security, a method of assessing network security risks based on vulnerability correlation graph is proposed in this paper. Firstly, it proposed a definition of vulnerability correlation graph based on the basis of network security dependency. Secondly, according to the size of network topology, the method of assessing the potential risk based on the vulnerability correlation graph is explained in detail. The experiment results show that it's possible to calculate potential risk indexes of three hierarchies: hosts, subnets and networks so that system administrators could adjust the security strategies in order to reduce the potential risk value of the whole network. It is also possible to solve the problem of network state explosion, thus improving expansibility of the assessment method.
During the last decade, network monitoring and intrusion detection have become essential techniques of cyber security. Nowadays, many institutions are using advanced solutions for detecting malicious network traffic, discovering network anomalies, and preventing cyber attacks. However, most research in this area has not been conducted specifically for organizational private networks, and their special properties have not been considered. In this paper, we first present a study of traffic patterns in a corporate private network, and then propose two novel algorithms for detecting anomalous network traffic and node behavior in such networks.
Every computer network needs some of the mechanism to protect itself against the malicious attacks. The common process for this is to divide the whole network into various coverage areas. Every coverage area elects its coverage agent who is responsible to collect the malicious information from its coverage area and share it with the other coverage areas. This will help to protect the computer network against malicious attacks. The core for this process is cooperative agent framework and enhancement of it will lead to increase in the security & performance of the computer network. The paper deals to enhance the cooperative framework in respect of the security of the function of cooperative agent as it will be having the more degree of the sensitivity of the malicious attack.
IEEE 802.11 based Wireless LAN (WLAN) standard has been one of the most successful wireless technology standards with total expected sales rising to a staggering $6.1Billion by 2015. The proliferation of 802.11 based WLANs highlights the need to focus on development of new solutions for security as enterprises and campuses increasingly being covered by WLANs. Denial of Service (DoS) is one of the popular attacks that prevents WLAN users from accessing the wireless network resources. Most DoS attacks such as the Clear-to-Send (CTS) duration attack is easy to carry out by an attacker. This work focuses on the use of clustering techniques on wireless traffic datasets for detecting CTS-based DoS attacks on 802.11 WLANs. Performance evaluation shows that, under the cases of nai?ve CTS duration attacker as well as the sophisticated CTS duration attacker, the k-means clustering technique is able to achieve high detection rates and low false positive rates with relatively small values of k (i.e., number of clusters).
One of the main aspects of Smart Grid communication network is providing a secure communications network. Smart Grid Network operating center and back office are pretty much secure. The problem starts when smart grid network is extended to connect to homes, commercial building and factories. With this the smart grid network access is opened up to millions of smart grid end user access points. In this paper the different security threats at each OSI layer are discussed. An overview of layer 2 security standards 802.1AE, 802.1X are elucidated. The paper finally concludes by proposing to implement the layer 2 security to achieve complete Smart Grid communication network security.
With inherent design defects, the current Internet is facing with security threats and application bottlenecks. In this paper, a network security model of host real-name access was established by introducing host identity protocol and PKI system. It consisted of host authentication, host identifier management, and expansion domain name query. The source address and host name authentication, addressing and fine-grained access control were implemented by the model. It greatly improves the Internet security by providing an exploration scheme for construction of reliable next generation Internet.
VLAN's are widely used in today's enterprise networks to improve scalability & flexibility at core, distribution and access layers. VLAN's are no longer confined to LAN environments and are becoming more widespread in their use. Unfortunately VLAN security issues has raised concerns and caused some network architects to re-focus on the associated issues. Our paper focused on two key issues required to implement inter-VLAN communication i.e. Effective VLAN design according to organizational need to reduce the much complex administrative work, and to overcome security issues related with VTP design model. We propose mechanism to use router sub-interfaces for Inter-VLAN communication with the ability to filter ingress & egress traffic using Access Control List, and to overcome VTP issues like VLAN hopping attack and counter the effect of inserting a rogue switch with high config revision number. The proposed architecture and VTP issues are demonstrated by analysis done in simulated network. Unless otherwise stated this paper is based upon configuration & hardware implementation in a Cisco environment.
The main aim of this paper is to provide a broad review of network security and cryptography, with particular regard to digital signatures. Network security and cryptography is a subject too wide ranging to coverage about how to protect information in digital form and to provide security services. However, a general overview of network security and cryptography is provided and various algorithms are discussed. A detailed review of the subject of network security and cryptography in digital signatures is then presented. The purpose of a digital signature is to provide a means for an entity to bind its identity to a piece of information. The common attacks on digital signature were reviewed. The first method was the RSA signature scheme, which remains today one of the most practical and versatile techniques available. Fiat-Shamir signature schemes, DSA and related signature schemes are two other methods reviewed. Digital signatures have many applications in information security, including authentication, data integrity, and non-repudiation was reviewed.
Trust is a vital control mechanism in computer network and social network, but there is a big gap between web trust and social trust. To eliminate this difference to make web trust more similar to social trust, we proposed a theory of quantum trust based on quantum mechanics. And then, the framework of quantum trust computation is presented for resolving network security problems. Finally, we use a scenario to illustrate the need for a quantum trust management in a distributed environment.
Many methods were designed in previous literatures to protect systems from IP and TCP layers distributed denial of service attacks instead of the application layer. However, they will not work well any more when encountering with application layer distributed denial of service. We will introduce clustering method to analysis application layer ddos in this paper. To capture users' browsing behavior, we cluster users' sessions. We consider bots' browsing behavior as abnormally behavior. That is, different from normal human behavior. We first extract four features from session to cluster users sessions-average size of objects requested in the session, request rate, average popularity of all objects in the session, average transition probability. Then, we use large amount of legitimate request sequence to get normal user browsing behavior models. Finally, conduct simulation experiments with attack dataset to validate the models.
Botnet is a serious information safety problem in the recent network. How to effectively find out the victim host and how to make the victim host free from the control of the botnet have become an urgent problem to be solved in the current network safety. In the paper, the use of the network online failure can distinguish the normal flow, P2P flow and the flow infected by the botnet. It can abstract the relevant characteristic values by observing the normal flow, the P2P flow and the online failure from the botnet intranet to the outer net, and then the characteristic values can create the detection model through the machine learning. The use of the detection model can distinguish the different kinds of flows.
Community detection is an important issue due to its wide use in designing network protocols such as data forwarding in Delay Tolerant Networks (DTN) and worm containment in Online Social Networks (OSN). However, most of the existing community detection algorithms focus on binary networks. Since most networks are weighted such as social networks, DTN or OSN, in this paper, we address the problems of community detection in weighted networks and exploit community for data forwarding in DTN and worm containment in OSN. We propose a novel community detection algorithm, and then introduce two metrics called intra-centrality and inter-centrality, to characterize nodes in communities. Based on these metrics, we propose an efficient data forwarding algorithm for DTN and an efficient worm containment strategy for OSN. Extensive trace-driven simulation results show that the data forwarding algorithm and the worm containment strategy significantly outperform existing works.
Malware poses a big threat to computer systems now a days. Malware authors often use encryption/compression methods to conceal their malicious executables data and code. These methods that transform some or all of the original bytes into a series of random looking data bytes appear in 80 to 90% of malware samples. This fact creates special challenges for anti-virus scanners who use static and dynamic methods to analyze large malware collections. In this paper we propose a method to identify malware executables by reading initial 2500 byte patterns of the sample. Our method reduces overall scanner execution time by considering 2500 bytes instead of whole file. Experimental results are evaluated using different classification algorithms (Random Forest, Ada-Boost, IBK, J48, Nai?ve-Bayes) followed by a feature selection method.
Wireless Sensor Network (WSN) consists of number of distributed autonomous sensors spatially, that has many applications. These are often used in potentially adverse as well as in hostile environment. Hence security is an important factor during communication. There are three critical issues for wireless sensor networks such as network lifetime, saving energy and security. A sensor node has limited battery power so need effective key distribution and management mechanism for secure communication. Enormous key distribution and management mechanism have been proposed in research literatures. Here, we provide a survey of various key management schemes in WSNs and made an extensive study to categorize available key management techniques and analyze the possible network security on them.
Cloud storage (CS) is a type of data storage system widely accepted in information technology communities. Users of CS require reliable and continuous access to data on their stored data storage. Therefore, this study presents a data backup method for data retrieval and access from distinct places based on the access frequencies of a shared folder. The proposed method improves the quality of CS services by studying the data duplication of a CS system. This approach aims to minimize the data access time, which is subject to the required transmission bandwidth and location of data storage. Therefore, this study develops a network model, access frequency model, delay model, and budget to develop a new duplication mechanism. This study proposes a bar center method and evaluates its performance to determine how well the predicted CS access time outperforms the related methods.
Topology control problems in wireless sensor networks (WSN) have important relationship with energy saving, network efficiency, network security and so on. In this paper, MATC, a mobile agent based topology control algorithm is proposed, which could solve three problems in data collection and transmission in WSN: routing void, isolated node and sleeping control. Simulation results show that MATC could save energy and prolong network lifetime effectively, and decrease network traffic load.
With the recent surge in cyber attacks, there is a growing demand for effective security analytics tools. Though, there are advanced data collection techniques in the form of honeypots and malware collectors, the value of data are only as useful as the analysis technique used. One of the primary drawbacks of current security analytic tools is the lack of visualization controls to effectively analyze the data. In this paper, we develop a visualization tool to analyze the geographical locations of spammers based on the integration of MaxMind and WhoIS databases with Google Maps API. The visualization tool provides an insight into spam origins, along with patterns of spammers identified from spam activity. A key component in the development of this tool is its extensible framework allowing for the addition of resources to retrieve more information about a spammer and analyze additional patterns of spammers for spam analysis.
In the last decade, the progress of internet technologies has led to a significant increase in security and privacy issues for users. This study aims to investigate how computer science students perceive computer network security. Thirty three students participated in the study in which we gathered data through a questionnaire. In this paper, we present an analysis that is inspired by the phenomenographic approach. Our conclusion is that the students have different levels of understanding of computer network security depending on their usage of the concepts they have learned, their theoretical or practical orientation to the subject, and their interest in the field.
Web applications have become one of the most popular targets of attacks during the last years. Therefore it is important to identify the vulnerabilities of such applications and to remove them to prevent potential attacks. This paper presents an approach that is aimed at the vulnerability assessment of Web applications following a black-box approach. The objective is to detect vulnerabilities in Web applications and their dependencies and to generate attack scenarios that reflect such dependencies. Our approach aims to move a step forward toward the automation of this process. The paper presents the main concepts behind the proposed approach and an example that illustrates the main steps of the algorithm leading to the identification of the vulnerabilities of a Web application and their dependencies.
Our contribution through this paper is to provide a novel approach for securing hosts inside the home network using predictive attack detection and honeypots. We have given a unique method to secure hosts inside the home network. After securing these hosts we redirect attacker traffic to honeypots for further analysis. Our system also offers techniques to roam an attacker over multiple installed honeypots. This in turn helps to load share the attack traffic between honeypots and to record maximum number of attacks. This method also helps to address key problem of determining honeypot location for maximum attack exposure.
Robot Network or BOTNET is the biggest network security threats faced by home users, organizations, and governments. Botnet is created by intelligent and up to date hackers, which challenges IT Community in detection, prevention and mitigation from Botnet attacks. This paper discuss about life cycle, topologies, detection and future prospects required to be safe from Botnet attacks.
Network attacks based on source address spoofing have become one of the most serious threats to the network security. In this paper, we propose an active source validation scheme based on path identification, referred to as Active SI, which combines the router marking mechanism and the end-system filtering mechanism. In router marking, the router mark each received packet before forward it. In end-system filtering, end-systems send probe packet and perform path learning according to response messages. The learning ability enable end-systems to efficiently verify source address authenticity of received packets. Based on the result of active learning, we establish a trust table which provides support for the subsequent validation. The trust table well solves the imprecision problem of positive learning including such based on probability, threshold and so on. We also present performance analysis of our scheme, and results show that our scheme can efficiently defense against source address spoofing attacks with high filtering precision and good adaptability. Moreover, Active SI support incremental deployment which demonstrates the practicality of our scheme.
Trust management frameworks are used to evaluate and manage trust relationships between network nodes and enhance network security. However, trust management frameworks themselves are vulnerable to attacks. Attacks against trust management frameworks are described in this paper with a trust management framework to resist them. The trustworthiness between nodes is evaluated to classify node behavior using a three-dimensional classifier based on a fuzzy integral. Different behaviors are mapped to different behavioral spaces to detect malicious nodes and identify their behavior types. The security of ad hoc networks is then improved by various measures to handle different types of malicious behavior. Simulations of the model on the System In The Loop (SITL) platform show that this trust management framework can separate normal nodes and malicious nodes and can distinguish different types of malicious nodes.
As networks become ubiquitous in people's lives, users depend on networks a lot for sufficient communication and convenient information access. However, networks suffer from security issues. Network security becomes a challenging topic since numerous new network attacks have appeared increasingly sophisticated and caused vast loss to network resources. Game theoretic approaches have been introduced as a useful tool to handle those tricky network attacks. In this paper, we review the existing game-theory based solutions for network security problems, classifying their application scenarios under two categories, attack-defense analysis and security measurement. Moreover, we present a brief view of the game models in those solutions and summarize them into two categories, cooperative game models and non-cooperative game models with the latter category consisting of subcategories. In addition to the introduction to the state of the art, we discuss the limitations of those game theoretic approaches and propose future research directions.
An industrial control network is a system of interconnected equipment used to monitor and control physical equipment in industrial environments. These networks differ quite significantly from traditional enterprise networks due to the specific requirements of their operation. Despite the functional differences between industrial and enterprise networks, a growing integration between the two has been observed. The technology in use in industrial networks is also beginning to display a greater reliance on Ethernet and web standards, especially at higher levels of the network architecture. This has resulted in a situation where engineers involved in the design and maintenance of control networks must be familiar with both traditional enterprise concerns, such as network security, as well as traditional industrial concerns such as determinism and response time. This paper highlights some of the differences between enterprise and industrial networks, presents a brief history of industrial networking, gives a high level explanation of some operations specific to industrial networks, provides an overview of the popular protocols in use and describes current research topics. The purpose of this paper is to serve as an introduction to industrial control networks, aimed specifically at those who have had minimal exposure to the field, but have some familiarity with conventional computer networks.
Covert channels via the widely used TCP/IP protocols have become a new challenging issue for network security. In this paper, we analyze the information hiding in TCP/IP protocols and propose a new effective method to detect the existence of hidden information in TCP initial sequence numbers (ISNs), which is known as one of the most difficult covert channels to be detected. Our method uses phase space reconstruction to create a processing space called reconstructed phase space, where a statistical model is proposed for detecting covert channels in TCP ISNs. Based on the model, a classification algorithm is developed to identify the existence of information hidden in ISNs. Simulation results have demonstrated that our proposed detection method outperforms the state-of-the-art technique in terms of high detection accuracy and greatly reduced computational complexity. Instead of offline processing as the state-of-the-art does, our new scheme can be used for online detection.
This work continues a trend of developments aimed at exploiting the physical layer of the open systems interconnection (OSI) model to enhance wireless network security. The goal is to augment activity occurring across other OSI layers and provide improved safeguards against unauthorized access. Relative to intrusion detection and anti-spoofing, this paper provides details for a proof-of-concept investigation involving air monitor applications where physical equipment constraints are not overly restrictive. In this case, RF fingerprinting is emerging as a viable security measure for providing device-specific identification (manufacturer, model, and/or serial number). RF fingerprint features can be extracted from various regions of collected bursts, the detection of which has been extensively researched. Given reliable burst detection, the near-term challenge is to find robust fingerprint features to improve device distinguishability. This is addressed here using wavelet domain (WD) RF fingerprinting based on dual-tree complex wavelet transform (DT-CWT) features extracted from the non-transient preamble response of OFDM-based 802.11a signals. Intra-manufacturer classification performance is evaluated using four like-model Cisco devices with dissimilar serial numbers. WD fingerprinting effectiveness is demonstrated using Fisher-based multiple discriminant analysis (MDA) with maximum likelihood (ML) classification. The effects of varying channel SNR, burst detection error and dissimilar SNRs for MDA/ML training and classification are considered. Relative to time domain (TD) RF fingerprinting, WD fingerprinting with DT-CWT features emerged as the superior alternative for all scenarios at SNRs below 20 dB while achieving performance gains of up to 8 dB at 80% classification accuracy.
Intrusion detection systems (IDSs) and intrusion prevention systems (IPSs) that use signatures cannot protect servers from new types of internet worms. Therefore it is important to collect information about new attacks because the detection rules employed by IDSs and IPSs are formulated using this information. Honeypots are valuable security resources that act as baits for attackers. They can monitor intrusions by being probed, attacked or compromised and can detect zero-day attacks and provide researchers intending to improve security with information about the attacks. However, it is almost impossible to immediately generate detection rules from the information collected by honeypots. This study presents an agent-based honeynet framework for protecting servers in a campus network. In this framework, agents remove malicious processes and executable files on servers infected by zero-day attacks as soon as the honeynet detects them. The proposed framework provides a novel defense mechanism that protects servers from new types of internet worms effectively, without the use of signatures.
Attack graph is a popular tool for modelling multi-staged, correlated attacks on computer networks. Attack graphs have been widely used for measuring network security risks. Majority of the works on attack graph use host-based or state-based approaches. These attack graph models are either too restrictive or too resource consuming. Also, a significant portion of these works have used `probability of successfully exploiting a network` as the metric. This approach requires that the `probability of successfully exploiting individual vulnerabilities` be known a priori. Finding such probabilities is inherently difficult. This present study uses exploit dependency graph, which is a space efficient and expressive attack graph model. It also associates an additive cost with executing individual exploits, and defines a security metric in terms of the `minimum cost required to successfully exploit the network`. The problem of calculating the said metric is proved to be NP-complete. A modified depth first branch and bound algorithm has been described for calculating it. This study also formulates, a linear-time computable, security metric in terms of the `expected cost required to successfully exploit the network` assuming a random attacker model and an uncorrelated attack graph.
Security automation continues to depend on signature models, but vulnerability exploitation is exceeding the abilities of such models. The authors, in reviewing the different types of mathematical-based constructs in anomaly detection, reveal how anomaly detection can enhance network security by potentially solving problems that signature models can't address.
Internet security problems remain a major challenge with many security concerns such as Internet worms, spam, and phishing attacks. Botnets, well-organized distributed network attacks, consist of a large number of bots that generate huge volumes of spam or launch Distributed Denial of Service (DDoS) attacks on victim hosts. New emerging botnet attacks degrade the status of Internet security further. To address these problems, a practical collaborative network security management system is proposed with an effective collaborative Unified Threat Management (UTM) and traffic probers. A distributed security overlay network with a centralized security center leverages a peer-to-peer communication protocol used in the UTMs collaborative module and connects them virtually to exchange network events and security rules. Security functions for the UTM are retrofitted to share security rules. In this paper, we propose a design and implementation of a cloud-based security center for network security forensic analysis. We propose using cloud storage to keep collected traffic data and then processing it with cloud computing platforms to find the malicious attacks. As a practical example, phishing attack forensic analysis is presented and the required computing and storage resources are evaluated based on real trace data. The cloud-based security center can instruct each collaborative UTM and prober to collect events and raw traffic, send them back for deep analysis, and generate new security rules. These new security rules are enforced by collaborative UTM and the feedback events of such rules are returned to the security center. By this type of close-loop control, the collaborative network security management system can identify and address new distributed attacks more quickly and effectively.
A proposed Internet of Things system architecture offers a solution to the broad array of challenges researchers face in terms of general system security, network security, and application security.
In part I of this article we presented the design alternatives, issues, and challenges for designing backhaul for 2G (GSM, CDMA) and 3G (UMTS, CDMA2000) radio access networks (RANs). Part II extends the survey of backhaul technologies to address LTE-based RANs. We present various alternatives to deal with the specific requirements imposed by Evolved Packet System architecture on the backhaul design. In particular, we address handling of the X2 interface, network security through IPSec, distribution of frequency and phase synchronization, the impact of small cell design, self-organizing networks, and endend QoS management within backhaul. We also present a brief overview of active debates with respect to some of these design options as open issues, in particular the impact of LTE-Advanced requirements on LTE backhaul design.
Recently, a kind of lightweight and resource-efficient biometrics-based security solutions were proposed for the emerging body sensor network (BSN). In such security solutions, physiological characteristics that can be captured by individual sensors of BSN were proposed to generate entity identifiers (EIs) for securing keying materials by a biometric approach. In this study, the authors focus on an improved key distribution solution with the energy distribution information of physiological signals (EDPSs) -based EIs. Firstly, different EDPS-based EI generation schemes are studied. Based on the existing multi-windows Fourier transform scheme, a modified one with single-window is proposed to improve the identification performance of the generated EIs. Then, a different method based on the discrete cosine transform of the autocorrelation sequence of physiological signals is proposed aiming for a significant increase in identification rates. The performances of time-varying randomness and identification rates are evaluated to examine EI??s feasibility in securing the transmission of keying materials. Based on the characteristics of generated EIs, the corresponding key distribution solution, that is, user-dependent fuzzy vault, is proposed. A detailed system performance analysis in terms of half total error rate, antiattack ability, as well as computational complexity, is conducted to demonstrate the effectiveness of the proposed solution.
Firewalls are important network security devices that protect networks by blocking unwanted traffic based on filtering policies. However, the structure of firewall policies has a major impact on firewall security and performance. In this article, we classify, describe, and compare traffic-aware firewall policy management techniques based on their objectives, schemes, complexity, applicability, and limitations. We classify traffic-aware firewall policy techniques into two categories based on their goals: matching optimization and early rejection optimization schemes. Matching optimization techniques try to minimize the matching time of normal network traffic. Early rejection techniques create a minimum set of policy preamble rules (constraints) that can potentially filter out the maximum amount of denied traffic. Both categories are self-adaptive to ensure that the performance gain will always supersede the dynamic management maintenance overhead. We believe that our work provides important insights on the operation and use of trafficaware filtering.
In this paper we consider the problem of routing traffic between k source-destination pairs. Using game theoretic modeling we provide randomized strategies to minimize the threat of attacks on links by an adversary. The adversary is assumed to have a choice of c edges for attack. We propose iterative methods to find the Nash Equilibrium of the zero-sum game. The proposed schemes have been implemented using existing network models (GEANT in Europe and the AT
In this paper, we investigate a new type of denialof- service attack in dynamic spectrum access networks - Sybilenabled attack. In this attack, the attacker not only launches the primary user emulation (PUE) attacks but also creates and infiltrates multiple Sybil identities to compromise the decision making process of the secondary network via Byzantine attacks. We implement this attack in our cognitive radio testbed to show its feasibility and attack impact. We further analyze the optimal attack strategy from the perspective of the malicious attacker, i.e., the optimal allocation of Sybil interfaces for different attacks, to maximize the impact on the secondary network. The attack models are analyzed under two different scenarios: with and without a reputation system in the network fusion center. Numerical analysis and simulations are conducted to solve the optimal attack strategy and demonstrate the impact of attacks on the secondary network.
Protection and performance are the major requirements for any Intrusion Detection and/or Prevention System (IDPS). Existing IDPSs do not seem to provide a satisfactory method of achieving these two conflicting goals. Intrusion Detection Systems (IDSs) fulfill the network performance requirement but exhibit poor protection under successive attacks. On the other hand, Intrusion Prevention Systems (IPSs) can protect the network by dropping the malicious packets that match any attacking pattern; however, this can have a negative impact on network performance in terms of delay as the attacking patterns increase. This results in a tradeoff between security enforcement levels on one hand and the performance and usability of an enterprise information system on the other. This paper aims to study the impact of security enforcement levels on the performance and usability of an enterprise information system. We propose a rule mode selection optimization technique that aims to determine an appropriate IDPS configuration set in order to maximize the security enforcement levels while avoiding any unnecessary network performance degradation. Simulation was conducted to validate our proposed technique. The results demonstrate that it is desirable to strike a balance between system security and network performance.
This paper presents a routing framework that embeds location and communication privacy into the routing mechanisms. It conceals endpoint identification by introducing waypoints, through encrypted routing hints, where each waypoint has knoweldge of the next hop, assuring network privacy over several waypoints. Based on IPv6 extension headers and Onion Routing techniques, the network waypoints comply with normal routing procedures, avoiding explicit tunneling or full packet encryption. By focusing on the network as a cooperative entity for privacy preservation, we propose a lightweight approach that can be easily deployed, establishing a good compromise between privacy and optimal routing.
Rapid advancements of wireless technologies allow users to access real-time data, and stay connected with friends and business while they are on the move. However, most emerging mobile applications assume users have cellular data services, and hence not everyone can enjoy new mobile applications. In addition, some emerging mobile applications e.g. mobile recommender system are data-centric but existing IP oriented communication paradigms are not flexible enough to support such applications. In this paper, we present a new secure content centric mobile network that supports content centric communication paradigm. Users can exchange information using peer to peer mode without having to rely on cellular data services. Content-centric security solution is provided where data owners can share encrypted published data items with others without knowing a priori who the interested users may be. Our preliminary prototype deployed in the ORBIT testbed demonstrates some of the key features we have designed.
The deployment of security measures and the provision of quality of service represents two critical and tightly coupled challenges in the design and operation of network services, architectures, and protocols in today's converged communication networks. Security and performance are not necessarily proportional to each other. It is common that; high security measures lead to high reliability results. However over-dimensioned security measures can adversely affect the networks performance and thus the services' quality of experience. For example a lightweight security algorithm may represent a security breach to an Ecommerce application. On the other hand, heavyweight algorithm could impact negatively the service performance. This paper presents a dynamic security system architecture that weighs the trade-off between resources, costs and risks for tuning the network security profile to optimize the performance of communication networks. The paper explores a framework to define the network security profile concept and identify its usage for security analysis and vulnerability assessment for enhancing the networks' performance. A simulation model is developed using OPNET to ground the model analytics under heavy load conditions.
Due to extensive use of network services and applications, most of the enterprise networks today deploy policy based security devices (e.g. routers, firewalls, IPSec etc.) for controlling accesses to network resources based on organizational security policy. The organizational network security policy is becoming more fine-grained, where access control list (ACL) configuration depends on various constraints like, service priority, time, location etc. The major challenge that the network administrators are facing today is to determine the correct access control configurations that satisfy the organizational policy. Throughout the last two decades, a significant amount of research has been done in formally verifying the correctness and consistency of access control policy configurations in enterprise network. However, this bottom-up analysis may not be useful because of its high state-space requirement for large scale networks. In addition, this approach requires repairing sequences of misconfigurations iteratively to meet a specific requirement. This paper presents a framework for synthesizing correct and conflict-free ACL configuration model, given the global organizational security policy and underlying network topology. This framework includes two major functions: (i) deriving the conflict-free model of the organizational security policy, and (ii) extraction of the correct ACL distributions for the network. The framework formally models the organizational security policy and generates the conflict-free policy model by resolving the policy rule conflicts. Then, ACL model is extracted based on the conflict-free policy model and the underlying network topology. The efficacy of the proposed framework has been demonstrated through a case study.
In this paper, we use ns2 to simulate routing methods. Not only compare the delivery ratio but also understand the factors that affect the delivery ratio. We choose four routing methods including our new routing method just been accepted in ICCNT2012 [11], named OOPFE-Routing. We will focus on the packet-drop-problems that don't depend on buffer queue and don't consider the network security in delay tolerant networks. From our results, the number of delivery ratio of OOPFE is even better than Epidemic-Routing which is high-bound in theory. The major reason is the overhead of OOPFE lower than Epidemic. Therefore, we discuss about the reasons of overhead. There are three; the first reason is the numbers of repeatedly received packets. The second is the numbers of packets dropped by our setting, when the value of buffer queue is less than 100%. The last is the numbers of packets dropped by other five type reasons in the routing time.
Malicious code detection is a major concern in computer science community in this decade. With the rapid growth of web applications, web sites have been become the attacker's main target. Innocent users' machines become compromised by just visiting a malicious page. This paper presents a malicious web page detection based on static feature classification. We classified features into three groups: explicit features, replicated features, and miscellaneous features. We employed Greasemonkey to develop the detection script. It provides the alert when an innocent user is visiting a malicious page. The accuracy of our detection system is 97.9% with 1.42 % of false positive and 2.76% of false negative. The average detection time is 2.49 seconds per page.
The vastness of IPv6 address space and rapid spread of its deployment attract us to usage of IPv6 network. Various types of devices, including embedded systems, are ready to use IPv6 addresses and some of them have already been connected directly to the Internet. Such situation entices attackers to change their strategies and choose the embedded systems as their targets. We have to deploy various types of honey pots on IPv6 network to trace his activities and infer his objective. Huge address space and wide variety of devices, however, suggest the limitation of conventional honey pots. In this paper, we propose a system that dynamically assigns an address to a honey pot by detecting an access to an unassigned address. We also present our strategy against IPv6 address scans by making honey pots collaborate each other.
Network Intrusion Detection Systems (NIDS) are considered as essential mechanisms to ensure reliable security. In an intrusion detection context, none of the main detection approaches (signature-based and anomaly-based) are fully satisfactory. False positives (detected non-attacks) and false negatives (non-detected attacks) are the major limitations of such systems. The generated alerts are elementary and in huge numbers. Hence, alert correlation techniques are used to provide a complementary analysis to link elementary alerts and provide a more global intrusion view. We propose an alert correlation and aggregation framework based on requires/provides model. The objective is to discover the logical relationships between atomic alerts potentially incorporated in multi-stage attacks. The obtained results illustrate that the proposed system can effectively detect coordinated attack with minimum false positives.
Network Intrusion Detection System (NIDS) plays an important role in providing network security. Efficient NIDS can be developed by defining a proper rule set for classifying network audit data into normal or attack patterns. Generally, each dataset is characterized by a large set of features, but not all features will be relevant or fully contribute identifying an attack. Since different attacks need different subsets to have better detection accuracy, this paper describes an improved feature selection algorithm to identify most appropriate subset of features for a certain attack. The proposed method is based on MAHALANOBIS Distance feature ranking and an improved exhaustive search to choose a better combination of features. We evaluate the approach on the KDD CUP 1999 datasets using SVM classifier and KNN classifier. The results show that classification is done with high classification rate and low misclassification rate with the reduced feature subsets.
Since being stored and transferred in the form of text, it is possible for the Cookies to be collected, modified or embezzled, this paper analyzes the working principle of Cookies, proposes the requirements of making Cookies against the security threats. Three options to meet the security requirements are presented.
In the network security, the identity authentication technology holds the extremely important status, is the information security system first checkpoint. From this the identity authentication is the most basic safe service, other securities serve all must rely on it. This article will describe identity authentication base on speech feature which is a very important way of identity authentication in network security.
The following topics are dealt with: wireless sensor network security; network security system; communication security; encryption algorithm; and information security algorithm.
Several countries have invested in technologies for Smart Grids. Among such protocols designed cover this area, highlights the DNP3 (Distributed Network Protocol version 3). Although the DNP3 be developed for operation over the serial interface, there is a trend in the literature to the use of other interfaces. The Zigbee wireless interface has become more popular in the industrial applications. In order to study the challenges of integrating of these two protocols, this article is presented the analysis of DNP3 protocol stack through state machines The encapsulation of DNP3 messages in P2P (point-topoint) ZigBee Network, may assist in the discovery and solution of failures of availability and security of this integration. The ultimate goal is to merge the features of DNP3 and Zigbee stacks, and display a solution that provides the benefits of wireless environment, without impairment of security required for Smart Grid applications.
In this paper, we discuss the remaining security risk of Wi-Fi AP spoofing with current AP joining approaches and how a new solution has been developed as part of the ULOOP project in order to be more user-friendly and secure. It was an important step for increased security because our evaluation shows that even computer aware users do not know or do not bother about this issue although it is a real risk that current approaches do not solve.
The increasing volume of data in large networks to be analyzed imposes new challenges to an intrusion detection system. Since data in computer networks is growing rapidly, the analysis of these large amounts of data to discover anomaly fragments has to be done within a reasonable amount of time. Some of the past and current intrusion detection systems are based on a clustering approach. However, in order to cope with the increasing amount of data, new parallel methods need to be developed in order to make the algorithms scalable. In this paper, we propose an intrusion detection system based on a parallel particle swarm optimization clustering algorithm using the MapReduce methodology. The use of particle swarm optimization for the clustering task is a very efficient way since particle swarm optimization avoids the sensitivity problem of initial cluster centroids as well as premature convergence. The proposed intrusion detection system processes large data sets on commodity hardware. The experimental results on a real intrusion data set demonstrate that the proposed intrusion detection system scales very well with increasing data set sizes. Moreover, it achieves close to the linear speedup by improving the intrusion detection and false alarm rates.
In the 21st century network security is rated as one of most demanding domain in computer science and engineering where numerous organizations and researchers are giving their contribution to enhance the existing network security framework in various ways. This paper is aimed at discussing about the enhancement of firewalls policies in the network. The network security and firewalls are two words which seem to be closely attached to each other as we know firewall provides security to network in organization efficiently. The firewalls are no doubt very efficient in securing data in network but still there is wide scope of improvement to utilize it in network with the best usage. The scope of improvement in firewalls policies configuration proves to be a good step towards better security in network. The timely firewalls policies start becoming obsolete and start getting conflicts complaints. Our objective is to eliminate the unused polices and provide updated and timely required policies to provide better security to network to avoid any untoward network security attacks.
Image authentication plays an important role in the peer-to-peer (P2P) file sharing system. In the file sharing process, there is a chance for alteration of the original contents by unknown intermediaries. In the present study, an attempt is made to identify the malicious attack in the original image by predictive lossless coding. In order to authenticate the received image, a predictive encoded quantized image is used. This method provides the desired robustness against legitimate variations while detecting illegitimate modification. Predictive lossless coding provides rate-efficient encoding of the projection by exploiting the correlation between the projections of the original and received images.
The advent of the internet age has led to the increase of prominent network security issues. Information encryption has long been a method used for information security. With the rapid development of parallel computing capacities of computer hardware, this method alone could not be trusted to ensure security by increasing the key sizes, thus bringing in the information hiding techniques into the scenario. Cryptography scrambles the data to be secured while information hiding embeds the information into files which do not reveal the presence of information. Steganography and water marking are two information hiding techniques. While steganography is used for secretly embedding the sensitive information in files, watermarking is used to implement copyright protection. Steganographic techniques are being widely used these days to increase the security of information. A combination of cryptography and steganography results in very strong cryptosystems. This is a paper of unexampled prosperity, which elicits easy-to-implement, difficult-to-sense proficiency for image steganography enforcing the statistical distribution's profound laws. Bit length for embedding is adjudicated via some delineated conditions and is done so by making use of Least Significant Bit . These conditions are the index for increasing complexity as well as security. Experimental ensues in terms of BPP, embedding capacity, and stego outputs also vindicate this paper. This paper prognosticates the crucial imputes of steganography.
Intrusion Detection System (IDS) has increasingly become a crucial issue for computer and network systems. Intrusion poses a serious security risk in a network environment. The ever growing new intrusion types pose a serious problem for their detection. The acceptability and usability of Intrusion Detection Systems get seriously affected with the data in network traffic. A large number of false alarms mean a lot in terms of the acceptability of Intrusion Detection Systems[1].In this paper we consider the dataset with multi classes and propose the classification for each type of attacks in a separate layer. In this work, a multi-classification approach for detecting network attacks is designed and developed to achieve high efficiency and improve the detection and classification rate accuracy [6].
In computer network security, a Network Intrusion Detection (NID) is an Intrusion Detection mechanism that attempts to discover unauthorized access to a computer network by analyzing traffic on the network for signs of malicious activity. There are many areas of research in this vast field of Network Intrusion Detection (NID) but in this survey paper, we will focus on its technology, development & strategic importance. Virus attacks, unauthorized access, theft of information and denial-of-service attacks were the greatest contributors to computer crime, a number of techniques have been developed in the past few years to help cyber security experts in strengthening the security of a single host or the whole computer network. Intrusion Detection is important for both Military as well as commercial sectors for the sack of their Information Security, which is the most important topic of research for the future networks.
The security in IMS network is a big challenge for both users and service providers over IMS. Many research works are interested in this field to build more secure networks and to deal with security problems. This paper aims to present firstly a synthesis of the most important existing model used to secure IMS network. Secondly, the paper introduces our approach to address security in IMS. This approach defines light model for IMS network, identifies and focuses in the most vulnerable components of IMS network. It gives more importance to secure these components in order to build a safety network.
This paper proposed the improved grey diagnosis method of network security named area relation degree method based on the grey relation degree theory. The method reflects the similar degree of changing trend of sequence curves. The simulation results show that the method is feasible in network security fault diagnosis, and can make accurate judgements of the network which the state is in. It can greatly widen range of application of grey model.
This paper considers stability analysis of a discrete-time computer SEIQR model in networks. The disease-free equilibrium and the disease equilibrium are first derived from the mathematical model. Then the sufficient condition of stability for the disease-free equilibrium is obtained by the first Lyapunov method. And the sufficient conditions of stability for the disease equilibrium are given by disc theorem. Simulation results demonstrate the effectiveness of the stability conditions.
The following topics are dealt with: computer communications; radio networks; name prefixes management; DNS analysis; consumer behavior; content distribution; smart grid energy efficiency; Internet; traffic management; telecommunication network management; computer network performances; scheduling; telecommunication network security; passive monitoring; smart grid communications; and smart grid security.
This extended abstract briefly introduces Obsidian, a scalable and efficient Python implementation of the extended version of the (k, j)-obfuscation technique. Obsidian improves the previous version by supporting the incremental obfuscation of network flows. This extension enables the obfuscation of larger datasets of network flows as required by networking research. As such it has been evaluated with billions of flows generated by the border router of a commercial Autonomous System (AS).
As more and more authority DNS servers turn on DNS security extensions (DNSSEC), it becomes increasingly important to understand whether, and how many, DNS resolvers perform DNSSEC validation. In this paper we present a query-based measurement method, called Check-Repeat, to gauge the presence of DNSSEC validating resolvers. Utilizing the fact that most validating resolver implementations retry DNS queries with a different authority server if they receive a bad DNS response, Check-Repeat can identify validating resolvers by removing the signatures from regular DNS responses and observing whether a resolver retries DNS queries. We tested Check-Repeat in different scenarios and our results showed that Check-Repeat can identify validating resolvers with a low error rate. We also cross-checked our measurement results with DNS query logs from .COM and .NET domains, and confirmed that the resolvers measured in our study can account for more than 60% of DNS queries in the Internet.
In the last years, botnets have become one of the major sources of cyber-crime activities carried out via the public Internet. Typically, they may serve a number of different malicious activities such as Distributed Denial of Service (DDoS) attacks, email spam and phishing attacks. In this paper we validate the Domain Name System (DNS) failure graph approach presented earlier in [1]. In our work we apply this approach in an operational 3G mobile network serving a significantly larger user population.Based on the introduction of stable host identifiers we implement a novel approach to the tracking of botnets over a period of several weeks. Our results reveal the presence of several groups of hosts that are members of botnets. We analyze the host groups exhibiting the most suspicious behavior and elaborate on how these participate in botnets and other malicious activities. In the last part of this work we discuss how the accuracy of our detection approach could be improved in the future by correlating the knowledge obtained from applying our method in different networks.
In this paper we review state-of-the-art botnet detection algorithms that reveal the control traffic of malicious peer-to-peer (P2P) networks by targeting topological properties of their interconnectivity graph. This class of detection methods does not rely on the exchanged content and therefore is also applicable to encrypted control traffic. However, in practice, an ISP monitoring customer traffic over an edge router will usually see only a fraction of the overall botnet, thus restricting the available bot connectivity information and limiting the applicability of general community detection approaches. In this paper we critically review graph based detection methods suitable for edge router monitoring using two types of real network traces. We show experimentally that using meta-graphs of mutual contacts proposed by Coskun et al. 2010 has the highest potential on result quality. We improve this approach by presenting a computationally less complex algorithm with similar result quality. Furthermore we explain ways to alleviate the cost of dealing with false positives in the result set.
Capturing packets to disk at line rate and with high precision packet timestamping is required whenever an evidence of network communications has to be provided. Typical applications of long-term network traffic repositories are network troubleshooting, analysis of security violations, and analysis of high-frequency trading communications. Appliances for 10 Gbit packet capture to disk are often based on dedicated network adapters, and therefore very expensive, making them usable only in specific domains. This paper covers the design and implementation of n2disk, a packet capture to disk application, capable of dumping 10 Gbit traffic to disk using commodity hardware and open-source software. In addition to packet capture, n2disk is able to index the traffic at line-rate during capture, enabling users to efficiently search specific packets in network traffic dump files.
The deployment of smart metering provides an immense amount of data for power grid operators and energy providers. By using this data, a more efficient and flexible power grid can be realized. However, this data also raises privacy concerns since it contains very sensitive information about customers. In this paper, we present Elderberry, a peer-to-peer protocol that enables near real-time smart metering while preserving the customer's privacy. By forming small groups of cooperating smart meters, their consumption traces are anonymized before being aggregated and sent to the grid operator. Through aggregation, Elderberry realizes efficient monitoring of large numbers of smart meters. It reaches this goal without computationally complex cryptography and adds only little communication overhead.
Bot detection methods that rely on deep packet inspection (DPI) can be foiled by encryption. Encryption, however, increases entropy. This paper investigates whether adding high-entropy detectors to an existing bot detection tool that uses DPI can restore some of the bot visibility. We present two high-entropy classifiers, and use one of them to enhance BotHunter. Our results show that while BotHunter misses about 50% of the bots when they employ encryption, our high-entropy classifier restores most of its ability to detect bots, even when they use encryption.
Internet Background Radiation (IBR) is unsolicited network traffic mostly generated by malicious software, e.g., worms, scans. In previous work, we extracted a signal from IBR traffic arriving at a large (/8) segment of unassigned IPv4 address space to identify large-scale disruptions of connectivity at an Autonomous System (AS) granularity, and used our technique to study episodes of government censorship and natural disasters [1]. Here we explore other IBR-derived metrics that may provide insights into the causes of macroscopic connectivity disruptions. We propose metrics indicating packet loss (e.g., due to link congestion) along a path from a specific AS to our observation point. We use three case studies to illustrate how our metrics can help identify packet loss characteristics of an outage. These metrics could be used in the diagnostic component of a semiautomated system for detecting and characterizing large-scale outages.
The Internet routing infrastructure is vulnerable to the injection of erroneous routing information resulting in BGP hijacking. Some spammers, also known as fly-by spammers, have been reported using this attack to steal blocks of IP addresses and use them for spamming. Using stolen IP addresses may allow spammers to elude spam filters based on sender IP address reputation and remain stealthy. This remains a open conjecture despite some anecdotal evidences published several years ago. In order to confirm the first observations and reproduce the experiments at large scale, a system called SpamTracer has been developed to monitor the routing behavior of spamming networks using BGP data and IP/AS traceroutes. We then propose a set of specifically tailored heuristics for detecting possible BGP hijacks. Through an extensive experimentation on a six months dataset, we did find a limited number of cases of spamming networks likely hijacked. In one case, the network owner confirmed the hijack. However, from the experiments performed so far, we can conclude that the fly-by spammers phenomenon does not seem to currently be a significant threat.
A major challenge in today's network is to maintain a secure interconnected world of computing where confidentiality, integrity, availability of information and resources are restored. Traditionally, security is enforced by access control and authentication. However, these security best practices do not take operating system, or network service-based vulnerabilities into account. With the evolution of sophisticated hacking tools, attackers exploit these vulnerabilities and gain legitimate access to network resources, bypassing the access control and authentication policies. Exploit dependency graph models service or application-based attacks and depicts all possible multi-host multi-step attack scenarios that an attacker can launch to penetrate into a network. An important step in the generation of exploit dependency graph is to characterize exploits in terms of a set of precondition and postcondition. Most of the reported works have generated exploit dependency graphs using proprietary vulnerability databases not available in the public domain. This work proposes a customized exploit dependency graph generation through modeling of exploits from open-source databases. Analysis of the developed algorithm shows considerable improvement in terms of time and space complexity in comparison to the reported works.
One of the most common current problems in medical data exchange is the lack of a standardized form of exchange. This paper presents a medical data exchanging platform for communication between a Web personal health record (WWW-PHR) and a mobile personal health record (m-PHR). The increased number of smart phones with high processing capabilities and applications help many health institutions use these devices for medical purposes. The contribution of this paper is to develop a communication module between m-PHRs and WWW-PHRs that provide interoperability between different medical entities in unified manner by using a common message standard Continuity of Care Record (CCR) and message vocabulary standards. In addition the communication module uses the available transmission channels anywhere, even with the lack of internet or broadband coverage. It also ensures the security of the transmitted data. It is developed using free tools and open source software to enable future enhancements.
Trusted insiders that betray an organization can inflict substantial harm. In addition to having privileged access to organization resources and information, these users may be familiar with the defenses surrounding valuable assets. Computers systems at the organization need a mechanism for communicating suspicious activity that is difficult for a malicious insider (or even an outsider) to detect or block. In this work, we propose a covert channel in the Ethernet frame that allows a computer system to report activity inside other, unrelated network communication. The covert channel leverages the differences in the framing approaches used by Ethernet and IP packets to append hidden information to IP packet and transmit it to an organization's administrator. This stealthy communication is difficult for even advanced attackers and is challenging to block since it opportunistically uses unrelated communication. Further, since the transmission is tied to the Ethernet frame, the communication cannot traverse network routers, preventing security information from leaving the organization. We introduce the covert channel, incorporate it into a working prototype, and combine it with an intrusion detection system to show its promise for security event reporting.
Authentication using centralized methods is a primary trust mechanism within most large-scale, enterprise computer networks. This paper proposes using graphs to represent user authentication activity within the network. Using this mechanism over a real enterprise network dataset, we find that non-privileged users and users with system administration privileges have distinguishable graph attributes in terms of size and complexity. In addition, we find that user authentication graphs provide intuitive insights into network user behavior. We believe that understanding these differences in even greater detail will lead to improved user behavior profiling and the elusive detection of authentication credential misuse.
The manual forensics investigation of security incidents is an opaque process that involves the collection and correlation of diverse evidence. In this work we conduct a complex experiment to expand our understanding of forensics analysis processes. During a period of four weeks we systematically investigated 200 detected security incidents about compromised hosts within a large operational network. We used data from four commonly-used security sources, namely Snort alerts, reconnaissance and vulnerability scanners, blacklists, and a search engine, to manually investigate these incidents. Based on our experiment, we first evaluate the (complementary) utility of the four security data sources and surprisingly find that the search engine provided useful evidence for diagnosing many more incidents than more traditional security sources, i.e., blacklists, reconnaissance and vulnerability reports. Based on our validation, we then identify and make available a list of 138 good Snort signatures, i.e., signatures that were effective in identifying validated malware without producing false positives. In addition, we compare the characteristics of good and regular signatures and highlight a number of differences. For example, we observe that good signatures check on average 2.14 times more bytes and 2.3 times more fields than regular signatures. Our analysis of Snort signatures is essential not only for configuring Snort, but also for establishing best practices and for teaching how to write new IDS signatures.
Due to the rapid and continuous increase of network intrusion, the need of protecting our systems becomes more and more compelling. In many situations, there exists a weak anomaly signal detection problem: due to the little number of anomalous system calls, the anomalous patterns of some intrusions may not be enough to distinguish themselves from normal activities so the existing anomaly detection systems can not detect this kind of sequences accurately. Motivated by this, we propose a multi-module anomaly detection scheme to solve this problem through utilizing system call prediction to enlarge the patterns of weak anomaly signal sequences and make them more distinguishable. Besides this, a variation of the Viterbi algorithm (called VV algorithm) is developed to predict the most probable future system calls more efficiently and a Markov-based intrusion detection method is adopted for the pattern value calculation and anomaly detection. The results of our experimental study conclude the followings: (i) the proposed scheme can greatly improve the intrusion detection accuracy of this Markov-based intrusion detection method in terms of hit rates under small false alarm rate bounds; (ii) the performance of the proposed scheme depends on the prediction accuracy of the adopted prediction technique; (iii) the developed VV algorithm is exponentially more efficient than a baseline method.
Thousands of cases each year of child exploitation on P2P file sharing networks lead from an IP address to a home. A first step upon execution of a search warrant is to determine if the home's open Wi-Fi or the closed wired Ethernet was used for trafficking; in the latter case, a resident user is more likely to be the responsible party. We propose methods that use remotely measured traffic to disambiguate wired and wireless residential medium access. Our practical techniques work across the Internet by estimating the perflow distribution of inter-arrival times for different home access network types. We observe that the change of inter-arrival time distribution is subject to several residentialfactors, including differences between OS network stacks, and cable network mechanisms. We propose a model to explain the observed patterns of inter-arrival times, and we study the ability of supervised learning classifiers to differentiate between wired and wireless access based on these remote traffic measurements.
The operation of a wireless network relies extensively on exchanging messages over a universally known channel, referred to as the control channel. The network performance can be severely degraded if a jammer launches a denial-of-service (DoS) attack on such a channel. In this paper, we design quorum-based frequency hopping (FH) algorithms that mitigate DoS attacks on the control channel of an asynchronous ad hoc network. Our algorithms can establish unicast as well as multicast communications under DoS attacks. They are fully distributed, do not incur any additional message exchange overhead, and can work in the absence of node synchronization. Furthermore, the multicast algorithms maintain the multicast group consistency. The efficiency of our algorithms is shown by analysis and simulations.
Accurate online network monitoring is crucial for detecting attacks, faults, and anomalies, and determining traffic properties across the network. With high bandwidth links and consequently increasing traffic volumes, it is difficult to collect and analyze detailed flow records in an online manner. Traditional solutions that decouple data collection from analysis resort to sampling and sketching to handle large monitoring traffic volumes. We propose a new system, Pegasus, to leverage commercially available co-located compute and storage devices near routers and switches. Pegasus adaptively manages data transfers between monitors and aggregators based on traffic patterns and user queries. We use Pegasus to detect global icebergs or global heavy-hitters. Icebergs are flows with a common property that contribute a significant fraction of network traffic. For example, DDoS attack detection is an iceberg detection problem with a common destination IP. Other applications include identification of top talkers, top destinations, and detection of worms and port scans. Experiments with Abilene traces, sFlow traces from an enterprise network, and deployment of Pegasus as a live monitoring service on PlanetLab show that our system is accurate and scales well with increasing traffic and number of monitors.
In this paper, we study several important issues that can be used to prevent pirated content propagation in BitTorrent (BT) Distributed Hash-Tables (DHT) networks. We design a system called PPBD to stop pirated content propagation by utilizing several attacking methods. First, the system can efficiently deal with massive concurrent connections to reduce bandwidth consumption, schedule peers to cooperate and optimize the protection methods according to clients. Second, we construct two mathematical models for BT DHT attacks, and we theoretically analyze the system performance. Third, we take into account some countermeasures of different BT clients and make corresponding optimizations of our PPBD system. Our realworld experiments show that: (1) our system can extend the download duration at least three times by the fake-block attacking method and it is more effective in a small swarm; (2) DHT index poison and routing pollution methods can limit the sharing swarm to a small swarm.
As more and more authority DNS servers turn on DNS security extensions (DNSSEC), it becomes increasingly important to understand whether, and how many, DNS resolvers perform DNSSEC validation. In this paper we present a query-based measurement method, called Check-Repeat, to gauge the presence of DNSSEC validating resolvers. Utilizing the fact that most validating resolver implementations retry DNS queries with a different authority server if they receive a bad DNS response, Check-Repeat can identify validating resolvers by removing the signatures from regular DNS responses and observing whether a resolver retries DNS queries. We tested Check-Repeat in different scenarios and our results showed that Check-Repeat can identify validating resolvers with a low error rate. We also cross-checked our measurement results with DNS query logs from .COM and .NET domains, and confirmed that the resolvers measured in our study can account for more than 60% of DNS queries in the Internet.
In the last years, botnets have become one of the major sources of cyber-crime activities carried out via the public Internet. Typically, they may serve a number of different malicious activities such as Distributed Denial of Service (DDoS) attacks, email spam and phishing attacks. In this paper we validate the Domain Name System (DNS) failure graph approach presented earlier in [1]. In our work we apply this approach in an operational 3G mobile network serving a significantly larger user population.Based on the introduction of stable host identifiers we implement a novel approach to the tracking of botnets over a period of several weeks. Our results reveal the presence of several groups of hosts that are members of botnets. We analyze the host groups exhibiting the most suspicious behavior and elaborate on how these participate in botnets and other malicious activities. In the last part of this work we discuss how the accuracy of our detection approach could be improved in the future by correlating the knowledge obtained from applying our method in different networks.
The Internet Autonomous System (AS) topology has important implications on end-to-end routing, network economics and security. Despite the significance of the AS topology research, it has not been possible to collect a complete map of the AS interconnections due to the difficulties involved in discovering peering links. The problem of topology incompleteness is amplified by the increasing popularity of Internet eXchange Points (IXPs) and the flattening AS hierarchy. A recent study discovered that the number of missing peering links at a single IXP is larger than the total number of the observable peering links. As a result a large body of research focuses on measurement techniques that can alleviate the incompleteness problem. Most of these proposals require the deployment of additional BGP vantage points and traceroute monitors. In this paper we propose a new measurement methodology for improving the discovery of missing peering links through the publicly available BGP data. Our approach utilizes the traffic engineering BGP Communities used by IXPs' Route Servers to implement multi-lateral peering agreements. We are able to discover 36K additional p2p links from 11 large IXPs. The discovered links are not only invisible in previous BGP-based AS topology collections, but also 97% of those links are invisible to traceroute data from CAIDA's Ark and DIMES projects for June 2012. The advantages of the proposed technique are threefold. First, it provides a new source of previously invisible p2p links. Second, it does not require changes in the existing measurement infrastructure. Finally, it offers a new source of policy data regarding multilateral peering links at IXPs.
In this paper we review state-of-the-art botnet detection algorithms that reveal the control traffic of malicious peer-topeer (P2P) networks by targeting topological properties of their interconnectivity graph. This class of detection methods does not rely on the exchanged content and therefore is also applicable to encrypted control traffic. However, in practice, an ISP monitoring customer traffic over an edge router will usually see only a fraction of the overall botnet, thus restricting the available bot connectivity information and limiting the applicability of general community detection approaches. In this paper we critically review graph based detection methods suitable for edge router monitoring using two types of real network traces. We show experimentally that using meta-graphs of mutual contacts proposed by Coskun et al. 2010 has the highest potential on result quality. We improve this approach by presenting a computationally less complex algorithm with similar result quality. Furthermore we explain ways to alleviate the cost of dealing with false positives in the result set.
Capturing packets to disk at line rate and with high precision packet timestamping is required whenever an evidence of network communications has to be provided. Typical applications of long-term network traffic repositories are network troubleshooting, analysis of security violations, and analysis of high-frequency trading communications. Appliances for 10 Gbit packet capture to disk are often based on dedicated network adapters, and therefore very expensive, making them usable only in specific domains. This paper covers the design and implementation of n2disk, a packet capture to disk application, capable of dumping 10 Gbit traffic to disk using commodity hardware and open-source software. In addition to packet capture, n2disk is able to index the traffic at line-rate during capture, enabling users to efficiently search specific packets in network traffic dump files.
Internet Background Radiation (IBR) is unsolicited network traffic mostly generated by malicious software, e.g., worms, scans. In previous work, we extracted a signal from IBR traffic arriving at a large (/8) segment of unassigned IPv4 address space to identify large-scale disruptions of connectivity at an Autonomous System (AS) granularity, and used our technique to study episodes of government censorship and natural disasters [1]. Here we explore other IBR-derived metrics that may provide insights into the causes of macroscopic connectivity disruptions. We propose metrics indicating packet loss (e.g., due to link congestion) along a path from a specific AS to our observation point. We use three case studies to illustrate how our metrics can help identify packet loss characteristics of an outage. These metrics could be used in the diagnostic component of a semiautomated system for detecting and characterizing large-scale outages.
Discovering high risk network flows and hosts in a high throughput network is a challenging task of network monitoring. Emerging complicated attack scenarios such as DDoS attacks increase the complexity of tracking malicious and high risk network activities within a huge number of monitored network flows. To address this problem, we propose an iterative framework for assessing risk scores for hosts and network flows. To obtain risk scores of flows, we take into account two properties, flow attributes and flow provenance. Also, our iterative risk assessment measures the risk scores of hosts and flows based on an interdependency property where the risk score of a flow influences the risk of its source and destination hosts, and the risk score of a host is evaluated by risk scores of flows initiated by or terminated at the host. Moreover, the update mechanism in our framework allows flows to keep streaming into the system while our risk assessment method performs an online monitoring task. The experimental results show that our approach is effective in detecting high risk hosts and flows as well as sufficiently efficient to be deployed in high throughput networks compared to other algorithms.
Distributed Denial of Service attacks (DDoS) are coordinated efforts, by human or machine, to overwhelm web sites, and at a minimum, to cause them to shut down. The use of this type of malicious software has grown exponentially in the past decade, and despite considerable research, it has proven very difficult to identify, detect or prevent such attacks. On the other hand, increases in traffic at Web sites may not be the result of a DDoS attack, but a legitimate increase in demand for the Web service. Our current research attempts to find indicators that will enable a system or network to distinguish between a DDoS attack and legitimate heavy traffic in real time.
Network traffic anomaly detection is an important part in network security. Identifying abnormal activities in a timely manner has been a demand in network anomaly detection. Conventional detection methods include Hurst parameter method, wavelet transform and Markov model. This article proposes a new method using weighted self-similarity parameter to detect abnormal activities over the internet. By performing a real-time Empirical Mode Decomposition (EMD) on the network traffic, we calculate the weighted self-similarity parameter based on the first Intrinsic Mode Function to analyze and detect suspicious activities. This approach provides the benefits of faster and accurate detection, as well as low computational cost.
Web application security has become a big issue because of common vulnerabilities found in web applications. This paper illustrates a case study on conducting security testing on an example application, Tunestore. The example application was tested using a number of tools such as Paros, WebScarab, JBroFuzz, Acunetix, and Fortify. Manual testing was also conducted. The testing results of different tools and manual testing are compared and discussed. Our case study shows manual testing is very important since some vulnerability types can only be found through manual testing and tester's observations, and it is important to utilize a variety of tools as well as conduct careful manual testing in order to find the most number of vulnerabilities in a web application. Based on this case study, hands-on labs can be developed for teaching web security, software security testing, and other topics.
Based upon the command and control network infrastructure of the Army, now electronic military systems are being built to store, transfer, query and manipulate data for various military tasks. However, in the open and interconnected network environment, military systems are facing two main typical security threats, i.e., broken authentication and broken access control. This paper proposes an idea of building a security Web-based electronic military system by utilizing network security trust units. An authorization component based on PKI (Public Key Infrastructure) and LDAP (Lightweight Directory Access Protocol), as well as an access control component based on T-RBAC (Task-Role Based Access Control) are designed. The implemented security components are embedded into an online manuscript submission and editing system for CAPF (Chinese Armed Police Force) newspaper office to ensure data and workflow security. Practical application shows the implemented security components can effectively guarantee the security and reliability of the system.
The paper suggests a framework for cyber attack modeling and impact assessment. It is supposed that the common approach to attack modeling and impact assessment is based on representing malefactors' behavior, generating attack graphs, calculating security metrics and providing risk analysis procedures. The main aspects outlined are achieving near-real time mode, event analysis and prognosis mechanisms, security and impact assessment. To optimize the attack graph generation and security evaluation we apply an anytime approach to have the result at any time by applying a set of algorithms with different timelines and precision. The architecture of the Cyber Attack Modeling and Impact Assessment Component (CAMIAC) is proposed. We present the prototype of the component, the results of experiments carried out, and comparative analysis of the techniques used.
With the recognition of cyberspace as a warfighting domain by the U.S. Department of Defense, we anticipate increased use of malicious software as weapons during hostilities between nation-states. Such conflict could occur solely on computer networks, but increasingly will be used in conjunction with traditional kinetic attack, or even to eliminate the need for kinetic attack. In either context, precise targeting and effective limiting of collateral damage from cyber weaponry are desired goals of any nation seeking to comply with the law of war. Since at least the Morris Worm, malicious software found in the wild has frequently contained mechanisms to target effectively, limit propagation, allow self-destruction, and minimize consumption of host resources to prevent detection and damage. This paper surveys major variants of malicious software from 1982 to present and synthesizes the control measures they contain that might limit collateral damage in future cyber weapons. As part of this work, we provide a framework for critical analysis of such measures. Our results indicate that a compelling framework for critical analysis emerges by studying these measures allowing classification of new forms of malware and providing insight into future novel technical mechanisms for limiting collateral damage.
This paper explores the Internet Background Radiation (IBR) observed across five distinct network telescopes over a 15 month period. These network telescopes consisting of a /24 netblock each and are deployed in IP space administered by TENET, the tertiary education network in South Africa covering three numerically distant /8 network blocks. The differences and similarities in the observed network traffic are explored. Two anecdotal case studies are presented relating to the MS08-067 and MS12-020 vulnerabilities in the Microsoft Windows platforms. The first of these is related to the Conficker worm outbreak in 2008, and traffic targeting 445/tcp remains one of the top constituents of IBR as observed on the telescopes. The case of MS12-020 is of interest, as a long period of scanning activity targeting 3389/tcp, used by the Microsoft RDP service, was observed, with a significant drop on activity relating to the release of the security advisory and patch. Other areas of interest are highlighted, particularly where correlation in scanning activity was observed across the sensors. The paper concludes with some discussion on the application of network telescopes as part of a cyber-defence solution.
Research in the field of IT security - in this case especially the Evaluation and Correlation of Intrusion Detection Systems (IDS) - implies special demands for the construction and operation of IT systems. In order to (i) evaluate multiple IDS under absolutely identical conditions and to (ii) check their reactions especially against novel attack patterns / attacker behaviour, all attack related actions (i.e. all traffic) have to be forwarded to all IDS in parallel at real-time. In addition, an attractive target needs to be offered to potential attackers, awaking the outward semblance of real-productive systems / networks including the corresponding behaviour. In particular, the correlation of IDS seems a promising approach to compensate the individual deficiencies of IDS. For example, while knowledge based systems are only able to detect previously known attacks, anomaly based systems suffer from higher False Alarm Rates (FARs). Even more, periodic performance evaluation studies, e.g., by NSS-Labs, have illustrated that numerous IDS are not configured properly and have a much worse system performance and detection capability than announced by the vendors. However, changing parameters of systems in productive networks (for the correlation of IDS as well as for their evaluation) can result in an enhanced endangerment of the security or even a breakdown of the network in case of horrible misconfigurations. To overcome these shortcomings, we present an architecture that supports research in the field of IT security and simultaneously ensures that all actions associated with an attack get recorded and a spill over of the attack from the research to the productive environment is prevented. Each test system is supplied with an unaltered live record of the network traffic. This allows an assessment of the detection as well as a comparison of different NIDS concepts/products. In addition, different correlation strategies of alerts of multiple systems can be evaluated. Furt- ermore, superior configurations can be identified and assessed without endangerment of the productive network.
This paper examines how cyber attacks, if indeed conducted by nation states, have been unsuccessful in supporting states' foreign policy objectives. By analyzing three prominent case studies, I show that as a result of geopolitical tensions, cyber attacks were implemented to further nation state objectives in support of foreign policy considerations and failed to achieve their respective outcomes despite successful deployment against their intended targets. The three case studies, hypothetical scenarios because attribution has not been confirmed, include: (1) the October 2012 distributed denial of service attacks targeting the U.S. banking sector; (2) the 2012 Stuxnet attack against Iran; and (3) the 2007 cyber attacks against Estonia. I work with the assumption that nation states were orchestrating the attacks through proxies, or else were actual participants, based on intent, motive, and a plethora of circumstantial evidence presented in each scenario. Data has been collected from newspapers, information technology security periodicals, and expert analysis. This paper challenges the notion that states can use the threat of cyber attack to influence an adversarial nation state's behavior, much the same way the threat of nuclear weapons holds other states in check.
Typically, there is a direct correlation between the time to resolve an incident and the damage sustained by an organization, with faster resolution of the incident resulting in less damage to the organization. Therefore, improving coordination between organizations experiencing the same or related incidents allows faster resolution and hence less damage to each organization. Coordination, however, means more than simply communicating during an incident - effective communication is critical. In this paper we explore how effective communication might be improved by the development of a mental model internalized by the group's technical staff prior to an incident. In this paper, we present the results of an exercise we conducted to determine whether an ad-hoc group of incident responders share a schema for decision making, and, if not, what some of the decision criteria (questions) and types of values (answers) might be that would allow the creation of a shared mental model for incident response.
Zero configuration networking aims to support users in seamlessly connecting devices and services. However, in public networks associated service announcements pose substantial privacy risks. A major issue is the inclusion of identifying information in device names, often automatically set or suggested by devices upon initial configuration. Focusing on mDNS, we assess this issue by studying its actual extent, awareness about the problem, and potential consequences for privacy. We collected a one-week dataset of mDNS announcements in a semi-public Wi-Fi network at a university. Of 2,957 unique device names, 59% contained real names of users, with 17.6% containing first and last name. An online survey (n=137) revealed that 29% of the participants did not know the current device name of their smartphone, but that the vast majority considered periodic announcement of their full names worrisome. We further discuss specific potential privacy threats and attack scenarios stemming from mDNS device names.
P2P protocols are responsible for a large amount of the Internet traffic. Given that in such systems each node functions both as a client and as a server, peer connectivity is an important element that influences the overall system performance. The introduction of IPv6 solves a part of the NAT traversal connectivity problems that IPv4 had, and allows a larger number of peers to be connectable from the Internet. Moreover, it introduces built-in IPsec capabilities, lowering the overhead security features bring on the network stack. This places P2P systems in an area that would most benefit from IPv6 deployment. Considering that P2P systems mostly rely of the resources provided by each user, the information collected by measuring such systems offers a real image on the hardware and Internet connection available to the home users. For many years BitTorrent has been the P2P protocol with the largest contribution to the P2P traffic in the Internet. This paper analyses one of the peer discovery methods available in BitTorrent, DHT, and compares the number of IPv4 and IPv6 node addresses that are available in the DHT system. The paper results are based on an experiment that measured the largest public swarms available on the file-sharing community called The Pirate Bay.
Computer networks are overwhelmed by self propagating malware (worms, viruses, trojans). Although the number of security vulnerabilities grows every day, not the same thing can be said about the number of defense methods. But the most delicate problem in the information security domain remains detecting unknown attacks known as zero-day attacks. This paper presents methods for isolating the malicious traffic by using a honeypot system and analyzing it in order to automatically generate attack signatures for the Snort intrusion detection/prevention system. The honeypot is deployed as a virtual machine and its job is to log as much information as it can about the attacks. Then, using a protected machine, the logs are collected remotely, through a safe connection, for analysis. The challenge is to mitigate the risk we are exposed to and at the same time search for unknown attacks.
The following topics are dealt with: software and system reliability; mobile policy enforcement and access control; malware detection and performance modeling; evaluation and measurement; survivability and malware monitoring; security threat and network availability; web security; and security assessment.
Web applications and server environments hosting them rely on configuration settings that influence their security, usability, and performance. Misconfiguration results in severe security vulnerabilities. Recent trends show that misconfiguration is among the top critical risks in web applications. While effective at uncovering numerous classes of vulnerabilities, generic web application vulnerability scanners are limited in identifying configuration vulnerabilities. In this paper, we present an approach that effectively combines hierarchical configuration scanning and preliminary source code analysis of web applications to pinpoint potential configuration vulnerabilities, quantify the degree of severity based on standard metrics, and facilitate fixing of vulnerabilities found therein. We implemented our approach in a tool called Confeagle and evaluated it on 14 widely deployed PHP web applications. Unlike generic web vulnerability scanners, on the subject applications, Confeagle detected potential configuration vulnerabilities that could result in information disclosure, denial-of-service, and session hijacking attacks on the applications.
Computer networks support many of the services that our society relies on. Therefore, ensuring their resilience to faults and challenges, such as attacks, is critical. To do this can require the execution of resilience strategies that perform dynamic reconfiguration of networks, including resilience-specific functionality. It is important that resilience strategies are evaluated prior to their execution, for example, to ensure they will not exacerbate an on-going problem. To facilitate this activity, we have developed a toolset that supports the evaluation of resilience strategies that are specified as event-driven policies. The toolset couples the Ponder2 policy-based management framework and the OMNeT++ simulation environment. In this paper, we discuss the network resilience problem and motivate simulation as a suitable way to evaluate resilience strategies. We describe the toolset we have developed, including its architecture and the implementation of a number of resilience mechanisms, and its application to evaluating strategies that detect and mitigate Internet worm behaviour.
The risk analysis is an important process for enforcing and strengthening efficient and effective security. Due to the significant growth of the Internet, application services, and associated security attacks, information professionals face challenges in assessing risk of their networks. The assessment of risk may vary with the enterprise's requirements. Hence, a generic risk analysis technique is suitable. Moreover, configuring a network with correct security policy is a difficult problem. The assessment of risk aids in realizing necessary security policy. Risk is a function of security threat and impact. Security threats depend on the traffic reachability. Security devices like firewalls are used to selectively allow or deny traffic. However, the connection between the network risk and the security policy is not easy to establish. A small modification in the network topology or in the security policy, can change the risk significantly. It is hard to manually follow a systematic process for configuring the network towards security hardening. Hence, an automatic generation of proper security controls, e.g., firewall rules and host placements in the network topology, is crucial to keep the overall security risk low. In this paper, we first present a declarative model for the qualitative risk analysis. We consider transitive reachability, i.e., reachability considering one or more intermediate hosts, in order to compute exposure of vulnerabilities. Next, we formalize our risk analysis model and the security requirements as a constraint satisfaction problem using the satisfiability modulo theories (SMT). A solution to the problem synthesizes necessary firewall policies and host placements. We also evaluate the scalability of the proposed risk analysis technique as well as the synthesis model.
The distribution of malicious hosts over the IP address space is far from being uniform. In fact, malicious hosts tend to be concentrated in certain portions of the IP address space, forming the so-called Bad Neighborhoods. This phenomenon has been previously exploited to filter Spam by means of Bad Neighborhood blacklists. In this paper, we evaluate how much a network administrator can rely upon different Bad Neighborhood blacklists generated by third-party sources to fight Spam. One could expect that Bad Neighborhood blacklists generated from different sources contain, to a varying degree, disjoint sets of entries. Therefore, we investigate (i) how specific a blacklist is to its source, and (ii) whether different blacklists can be interchangeably used to protect a target from Spam. We analyze five Bad Neighborhood blacklists generated from real-world measurements and study their effectiveness in protecting three production mail servers from Spam. Our findings lead to several operational considerations on how a network administrator could best benefit from Bad Neighborhood-based Spam filtering.
We present the longitudinal trending analysis of traffic anomalies on a trans-Pacific backbone network over nine years. Throughout our analysis, we try to answer several questions: how frequent do such anomalies appear and how long do they last? Does a set of anomalous hosts occur correspondingly? We answer these by applying the state-of-the-art anomaly detectors to (un)anonymized packet traces and look into interesting insights from the long-term analysis. The key observations are as follow. The sources of anomalies are decreasing over the recent years, but take a significant portion of traffic volume during the measurement period (i.e., 0.03% of all IP addresses take up to 30% of traffic volume). The frequency analysis reveals that there is a clear periodicity of anomalies and anomalous host occurrences in various durations. Finally, we find the influences of anomaly detectors to the overall trending and how they differ from each other.
DNSSEC deployment for large Internet Service Provider (ISP) is an important issue. With the current architecture, the migration of current DNS resolving platforms requires 5 times more nodes. This paper introduces alternative architectures where the DNS traffic is split between the nodes of the platform according to the queried Fully Qualified Domain Names (FQDN), rather than the IP addresses of the queries. We show that such type of architecture requires up to 30% less nodes. However, this load balancing techniques results in a non-uniform distribution of the resources among the nodes of the platform. Furthermore, operational teams are reluctant to modify the existing load balancing infrastructure. Thus, we investigate how pro-active caching over a Distributed Hash Table (DHT) protocol, can optimize the resources of an ISP operational DNSSEC resolving platform. We find out that it can reduce the number of nodes by 3.5.
Traditional intrusion detection systems (IDSs) work in isolation and are not effective to detect unknown threats. An intrusion detection network (IDN) is a collaborative IDS network intended to overcome this weakness by allowing IDS peers to share detection knowledge and experience, and hence improve the overall accuracy of intrusion assessment. However, malicious insiders and free riders may compromise the efficiency of IDNs. In this work, we design a collaborative IDN system and particularly focus on four research problems, namely, trust management, collaborative intrusion decision, resource management, and collaborators selection. We evaluate our design in terms of several desired properties such as efficiency, robustness, scalability and incentive-compatibility.
As the number of Internet applications grows, the number of applications that use data encapsulation increases as well. Flow monitoring using NetFlow version 5 or 9 is only able to analyze the encapsulating protocol, therefore it becomes too limited to detect new threats. For this reason, more thorough knowledge of such traffic is needed. The IPFIX protocol can be used in such situations, because it provides enough flexibility for monitoring tools to be extended by new elements. Along with greater flexibility, IPFIX support results in a higher performance footprint on collectors and tools for querying the collected data. Currently, there is a lack of flow collection frameworks with IPFIX support that would allow flow data to be extended. The aim of this paper is to compare open-source flow collectors that provide support for the IPFIX protocol. We focus on evaluating performance of query tools and the level of IPFIX support provided by the collection frameworks.
The deployment of devices at remote or distributed locations is a typical scenario e.g. in large enterprise networks or industrial applications. In the deployment of a device identification of the device and the establishment of security relations (trust) between the device and other elements of the infrastructure is crucial. Usually, the process requires either to pre-configure the device or to let administrators physically access the device for configuration. Both options induce costs. For functional configurations and software distribution zero configuration solutions are available. One important step hereby is the establishment of trust into the individual device by the device owner. This trust establishment requires in typical schemes a large organizational involvement of the device owner resp. operator. The approach presented in this demo addresses the step of initial trust establishment. The demonstration shows the process of a device being securely configured and provides a visualization through a meta-data graph generated from an IFMAP server.
This technical demonstration presents PReSET, a toolset for the simulation and evaluation of network resilience strategies. The toolset is based on an integration between the Ponder2 policy framework and the OMNeT++ simulator. It permits the offline evaluation of policy-based strategies that perform dynamic reconfiguration of resilience mechanisms to contain malicious attacks and other challenges to a network.
Distributed denial-of-service attack (DDoS Attack) is one of the types of attacks that use multiple hosts as attacker against a system. There is a difference between Distributed Denial-of-Service (DDoS Attack) and Denial-of-Service (DoS Attack). DDoS attacks are distributed, meaning spread using multiple hosts, while the DoS attack is one-on-one. DoS attacks requires a powerful host, either from the resource or operating system used to carry out the attack. In this study, we discuss how to handle DDoS attacks in the form of detection method based on the pattern of flow entries and handling mechanism using layered firewall. Tests carried out using three scenario that is simulations on normal network environment, unsecured network, and secure network. Then, we analyze the simulations result that has been done. The method used successfully filtering incoming packet, by dropped packets from the attacker when DDoS attack happen, while still be able to receive packets from legitimate hosts.
The following topics are dealt with: cloud security; operating system security; debugging; diagnosis; stochastic modeling techniques; distributed dependability; virtualization; software-based memory error detection; caches; data analysis; system stack; wireless networks; storage systems; network security; and Internet security.
Several research projects in the past have built intrusion detection systems and honeypot architectures based on virtual machine introspection (VMI). These systems directly benefit from the use of virtualization technology. The VMI approach, however, requires direct interaction with the virtual machine monitor, and typically is not available to clients of current public clouds. Recently, nested virtualization has gained popularity in research as an approach that could enable cloud customers to use virtualization-based solutions within a cloud by nesting two virtual machine monitors, with the inner one under control of the client. In this paper, we compare the performance of existing nested-virtualization solutions and analyze the impact of the performance overhead on VMI-based intrusion detection and honeypot systems.
We consider the problem of achieving uniform node sampling in large scale systems in presence of a strong adversary. We first propose an omniscient strategy that processes on the fly an unbounded and arbitrarily biased input stream made of node identifiers exchanged within the system, and outputs a stream that preserves Uniformity and Freshness properties. We show through Markov chains analysis that both properties hold despite any arbitrary bias introduced by the adversary. We then propose a knowledge-free strategy and show through extensive simulations that this strategy accurately approximates the omniscient one. We also evaluate its resilience against a strong adversary by studying two representative attacks (flooding and targeted attacks). We quantify the minimum number of identifiers that the adversary must insert in the input stream to prevent uniformity. To our knowledge, such an analysis has never been proposed before.
The domain name system plays a vital role in the dependability and security of modern network. Unfortunately, it has also been widely misused for nefarious activities. Recently, attackers have turned their attention to the use of algorithmically generated domain names (AGDs) in an effort to circumvent network defenses. However, because such domain names are increasingly being used in benign applications, this transition has significant implications for techniques that classify AGDs based solely on the format of a domain name. To highlight the challenges they face, we examine contemporary approaches and demonstrate their limitations. We address these shortcomings by proposing an online form of sequential hypothesis testing that classifies clients based solely on the non-existent (NX) responses they elicit. Our evaluations on real-world data show that we outperform existing approaches, and for the vast majority of cases, we detect malware before they are able to successfully rendezvous with their command and control centers.
This paper proposes an autolinking approach to help analysts investigate spatial details of suspicious sections from an overview temporal visualization. Analysis of spatial-temporal network security data takes place both conditionally and in sequence. Many systems use time-series curves to visualize the temporal perspectives of the data and maps to show the spatial information. To identify anomalies, the analysts frequently shift across different visualizations. In essence, time-series curves provide a temporal overview of data, and the map anchors the locations for the users to drill down for details. Anomalies may be reflected in a time-series curve as a jump, a dive, a peak, or a valley. With the autolinking mechanism, after the analyst selects a segment of a curve, the system can automatically highlight the related area on the map for further investigation. This approach adopts the slicing operation of the Online Analytical Process (OLAP) to find the basic granularities that contribute to the overall value change. This approach is implemented in our award-winning visual analytics system SemanticPrism. In this paper, we describe its structure and demonstrate three examples of use with the VAST 2012 Minichallenge 1 data.
Click Fraud Bots pose a significant threat to the online economy. To-date efforts to filter bots have been geared towards identifiable useragent strings, as epitomized by the IAB's Robots and Spiders list. However bots designed to perpetrate malicious activity or fraud, are designed to avoid detection with these kinds of lists, and many use very sophisticated schemes for cloaking their activities. In order to combat this emerging threat, we propose the creation of Bot Signatures for training and evaluation of candidate Click Fraud Detection Systems. Bot signatures comprise keyed records connected to case examples. We demonstrate the technique by developing 8 simulated examples of Bots described in the literature including Click Bot A.
Toward the ultimate goal of enhancing human performance in cyber security, we attempt to understand the cognitive components of cyber security expertise. Our initial focus is on cyber security attackers - often called hackers. Our first aim is to develop behavioral measures of accuracy and response time to examine the cognitive processes of pattern-recognition, reasoning and decision-making that underlie the detection and exploitation of security vulnerabilities. Understanding these processes at a cognitive level will lead to theory development addressing questions about how cyber security expertise can be identified, quantified, and trained. In addition to behavioral measures our plan is to conduct a functional magnetic resonance imaging (fMRI) study of neural processing patterns that can differentiate persons with different levels of cyber security expertise. Our second aim is to quantitatively assess the impact of attackers' thinking strategies - conceptualized by psychologists as heuristics and biases - on their susceptibility to defensive techniques (e.g., decoys, honeypots). Honeypots are an established method to lure attackers into exploiting a dummy system containing misleading or false content, distracting their attention from genuinely sensitive information, and consuming their limited time and resources. We use the extensive research and experimentation that we have carried out to study the minds of successful chess players in order to study the minds of hackers with the ultimate goal of enhancing the security of current systems. This paper outlines our approach.
Because it is impossible to find attacker's real IP address in hacking attacks against Korean users from other countries although the tracking method is used, investigators thereof have great difficulty. This is because attackers use the IP spoofing method to conceal their IP address while they go through a plurality of proxy servers. This study aims to examine how attackers use the IP spoofing method, and apply the proxy servers to attempt attacks. This study suggests a method of defending users against IP spoofing attacks through proxy servers, and a method of IP traceback, and will contribute to technical development for global hacking and security defense technology.
DDoS attacks are becoming more advanced to attack with a small traffic that is difficult to detect. In this paper, we propose two DDoS attacks that send a small traffic. One named SQL Search attack that is modified DoS attack using SQL wildcards. The other named Mass Contents Request attack that request large files. We prove experimentally that they are possible for the real homepages to DDoS attack with 10Megabytes/sec to 1Gigabytes/sec traffic. Also, we prove that proposed DDoS attacks have a small traffic and no patterns that not only it is difficult to detect of threshold-based but also it is hardly possible to detect of pattern-based through an experiment.
The following topics are dealt with: 4G cellular system; LTE-A; MIMO; energy efficiency; multicarrier technique; combinatorial method; wireless sensor network; Internet technology; computational intelligence; software engineering; smart grid communication; cryptography; network security; multihop network; heterogeneous network; mobile cloud computing; image processing; multiple antenna technique; wireless small cell network; and digital information management.
Privacy is one of the main concerns for Internet users. In many situations, users would like to conceal their identities when accessing the Internet. Some would want to hide their identities when accessing specific services, and others may even want to be anonymous to other hosts they are communicating with. Anonymity is sometimes a must for applications like electronic voting, and in other situations it is optional such as the case of Web browsing. Moreover, hiding the identities of communicating parties helps alleviate problems related to traffic analysis. In this paper, we propose two anonymity models to ensure the privacy of communicating parties and to protect the sender address during the routing process and hence render such sender unidentifiable and untraceable. The results analyses show that our models have anti-spoofing capability and high anonymity degree even when some nodes on the path are compromised. In the first proposed model, the request and the reply can be linked, however, in the second model they cannot be linked at an additional overhead cost. The measurements showed that the additional incurred delay on each collaborative router varies between 1.35 and 3.3 ms, and hence the end-to-end delay is negligible for typical Internet paths.
Traditionally supervisory control and data acquisition (SCADA) networks were physically isolated, providing some inherent level of security; yet, as these networks slowly converged with both corporate intranets and the Internet, their security continually eroded. The gradual evolution of SCADA systems has introduced many novel and previously unknown security risks. During the advent of SCADA technologies, a heavy focus was put on providing system robustness, safety, and reliability. Because of this, widely deployed SCADA protocols like Modbus and DNP3 provide no inherent security controls. In this paper, we will propose a novel Modbus alternative capable of providing secure, backward-compatible Modbus message transmission using stream control transmission protocol and hash-based message authentication code technologies. This proposed protocol improvement ensures the availability and integrity of Modbus messages while providing a robust and secure mutual authentication mechanism. Improvements upon the legacy Modbus protocol aim to mitigate common SCADA protocol attack vectors.
We would like to welcome you to the IEEE International Wireless Communications and Mobile Computing Conference (IEEE IWCMC 2013) in the beautiful city of Cagliari, Sardinia, Italy (inside the campus of the University of Cagliari)! We are delighted that this year's IEEE IWCMC accomplished its goal under the conference theme Connectivity Everywhere, and continues its tradition of providing the premier forum for presentations of research results and experience reporting on the cutting edge research in the general areas of wireless communications and mobile computing. This year, we received more than 630 submissions from many countries around the world. Each paper received at least three peer technical reviews, comprised of more than 480 TPC members from academia, government laboratories, and industries. After carefully examining all review reports, the IEEE IWCMC 2013 TPC finally selected about 35% high-quality papers for presentation at the conference and publication in the IEEE IWCMC 2013 proceedings. The conference program starts on Monday July 1st with a full day of Tutorials that are free of charge to all attendees. Then, each day starts with a keynote speaker chosen from renowned world-class leaders in the area: Dr. Lajos Hanzo, Dr. Robert Schober, and Dr. Daniele Franceschini, highlighting the latest research trends in cooperative wireless communications, how much can we gain by exploiting buffers in wireless relay networks? And giving the industry's perspectives and overview on key technology milestones in the Ultra Mobile Broadband evolution path. This year, the technical sessions reflect the continued and growing interests in a wide range of spectrum, including wireless communications and networks, design and optimization, mobile computing, wireless sensor networks, network security, cloud-assisted computing and pervasive services and use of wireless technologies in different social applications. There are two special sessions composed of- invited papers from renowned experts from around the world. Outstanding papers will be selected for four Special Issues in well known international journals. Our objective in the future is to reduce the acceptance rate further.
Advancements in wireless networking and embedded systems technology have given rise to Wireless sensor networks (WSN). WSNs promise ubiquitous data collection and processing for variety of commercial and military applications. Recently proposed Internet of things concept utilises WSN as a medium connecting physical world to virtual world. Practical realization of these applications is possible only after assuring network security. Cryptographic key distribution is a critical stage in the implementation of network security. In WSNs, due to the resource constrained nature of the nodes, it is important to design a key management protocol with minimum resource overhead. At the same time, to meet increasing security demands resource consuming asymmetric key primitives based implementations are needed. In order to address the same, hybrid key management technique is proposed. Both symmetric and asymmetric key distribution techniques are compared with the proposed scheme and also a detailed security analysis is presented.
Traditionally, RSA is being used for authentication and key exchange for symmetric key cryptography (SKC). Improved network security demands forward secrecy also. Even though, RSA, a widely used key exchange approach can not provide forward secrecy, the same can be achieved by making used Elliptic Curve Diffie-Hellman Ephemeral (ECDHE) technique for SKC and RSA for the purpose of authentication. However, ECDHE_RSA based approach is more compute intensive compared to the RSA alone. The predominant operation in the ECDHE technique is Elliptic Curve (EC) based scalar multiplication. Hence, speeding up of ECDHE operation demands faster EC scalar multiplication algorithm. Binary method, Non-adjacent form (NAF) method and sliding window method are used to carry out the EC scalar point multiplication. An algorithm based on both the NAF and the sliding window techniques is considered. This technique is more efficient in terms of EC point operations. There is a trade-off between the number of EC point addition operations and the number of pre-computed values. A fuzzy based controller method is proposed to determine an optimum window width, resulting in faster scalar multiplication.
Detection and prevention of DDoS is still an area of ongoing research. A carefully crafted DDoS attack can fool present methodologies and overwhelm a server; such attacks may be the next wave of cyber warfare and cyber crime. It can be shown that only application level methods are capable of differentiating legitimate flash traffic from DDoS. In this paper we explain why existing application layer methodologies cannot work and propose a novel set of algorithms that are capable of detecting and blocking DDoS attacks whilst allowing through legitimate user traffic, including flash traffic. The required functionality can be added to existing web servers with a minimum of interference with the application code, or implemented in a separate network device.
The following topics are dealt with: security policies formalization; trust management; security management; application security; network security; cloud computing security; collaborative systems security; cryptology; digital watermarking; and steganography.
The following topics are dealt with: green ICT; computer communication; information theory; information security; network security; and communication technology.
Consider mobile targets in a plane and their movements being monitored by a network such as a field of sensors. We develop distributed algorithms for in-network tracking and range queries for aggregated data (for example, returning the number of targets within any user given region). Our scheme stores the target detection information locally in the network and answers a query by examining the perimeter of the given range. The cost of updating data about mobile targets is proportional to the target displacement. The key insight is to maintain in the sensor network a function with respect to the target detection data on the graph edges that is a differential form such that the integral of this form along any closed curve $C$ gives the integral within the region bounded by $C$ . The differential form has great flexibility, making it appropriate for tracking mobile targets. The basic range query can be used to find a nearby target or any given identifiable target with cost  $O(d)$, where  $d$ is the distance to the target in question. Dynamic insertion, deletion, coverage holes, and mobility of sensor nodes can be handled with only local operations, making the scheme suitable for a highly dynamic network. It is extremely robust and capable of tolerating errors in sensing and target localization. Targets do not need to be identified for the tracking, thus user privacy can be preserved. In this paper, we only elaborate the advantages of differential forms in tracking of mobile targets. Similar routines can be applied for organizing many other types of informationfor example, streaming scalar sensor data (such as temperature data field)to support efficient range queries. We demonstrat- through analysis and simulations that this scheme compares favorably to existing schemes that use location services for answering aggregate range queries of target detection data.
Pattern-matching techniques have recently been applied to network security applications such as intrusion detection, virus protection, and spam filters. The widely used AhoCorasick (AC) algorithm can simultaneously match multiple patterns while providing a worst-case performance guarantee. However, as transmission technologies improve, the AC algorithm cannot keep up with transmission speeds in high-speed networks. Moreover, it may require a huge amount of space to store a two-dimensional state transition table when the total length of patterns is large. In this paper, we present a pattern-matching architecture consisting of a stateful pre-filter and an AC-based verification engine. The stateful pre-filter is optimal in the sense that it is equivalent to utilizing all previous query results. In addition, the filter can be easily realized with bitmaps and simple bitwise-AND and shift operations. The size of the two-dimensional state transition table in our proposed architecture is proportional to the number of patterns, as opposed to the total length of patterns in previous designs. Our proposed architecture achieves a significant improvement in both throughput performance and memory usage.
Nowadays malware can be spread over the Internet using botnets to download. This preliminary work presents temporal download behavior of Top-10 malware based on 2010 and 2011 CCC (Cyber Clean Center) datasets in terms of number of downloads per day and per hour. The datasets contain download logs of several independent honeypots in Japan to observe malware traffic and its activities. Our results show sequences and similar patterns of malware downloads in 2010. On the other hand, the behaviors in 2011 are quite different from those of 2010 that no obvious sequences and patterns can be detected.
Spreading behavior of passive worms, in mobile social networks has raised a lot of concerns for malicious worms researchers in network security for some time now. However, as a result of dynamic and complex nature of mobile social networks, studies are still evolving. This paper introduces the spreading behavior of passive worms in mobile social networks, to contribute find effective antidote to fight against the spreading of especially, passive worms in mobile social networks. To study its effectiveness in mobile social networks, this paper tackles the peers that constitute the mobile topology in social networks and user behavior as consideration and puts forward a five-state spreading model for passive worms spreading. The model also simplified the fact that, size of a mobile social network is large enough, but devices have limited resources like memory, processor, battery etc. The simulation result has also not only validated the effectiveness of our passive worms behavior spreading model, but closely evaluated the efficient performance of the model compared to similar recent researches.
Intrusion Detection Systems (IDS) have become crucial components in computer and network security. NSL-KDD intrusion detection dataset which is an enhanced version of KDDCUP'99 dataset was used as the experiment dataset in this paper. Because of inherent characteristics of intrusion detection, still there is huge imbalance between the classes in the NSL-KDD dataset, which makes harder to apply machine learning effectively in the area of intrusion detection. In dealing with class imbalance in this paper Synthetic Minority Over sampling Technique (SMOTE) is applied to the training dataset. A feature selection method based on Information Gain is presented and used to construct a reduced feature subset of NSL-KDD dataset. Random Forests are used as a classifier for the proposed intrusion detection framework. Empirical results show that Random Forests classifier with SMOTE and information gain based feature selection gives better performance in designing IDS that is efficient and effective for network intrusion detection.
The demand for protecting the enterprise network infrastructure from network security threats has shown an increase in recent years. Therefore, a security enforcement mechanism for the network is required to protect the network against the threats especially from internal. Generally, staffs and visitors that use their computer everywhere could bring a threat as it escape from the protective measures imposed by an organization. Therefore, it is necessary to secure enterprise network from being compromised by using endpoint security solution. Network Access Control (NAC) is capable to provide solution for determining the integrity of endpoints which serve as a basis for trustworthy communication. However, literature review reveals several types of NAC architecture that have been implemented by solution providers such as CISCO NAC and Microsoft NAP employs proprietary standard and the deployment method used is not comprehensive. In addition, previous architecture only complies with one of the NAC characteristic such as in-band or out-band, managed or unmanaged LAN, agent or agentless, pre-admission or postadmission and limited OS support. Hence, this study will focus on reviewing all those NAC architecture as a baseline to produce an enhanced NAC architecture which can cater for all NAC characteristics. The results shows that proposed NAC architecture which is combination of all the NAC characteristics can effectively control the network access by endpoint device. This proposed NAC architecture maybe useful as a basis for reference not only for researchers in this field but also for network administrator. It is necessary to review the NAC architecture from time to time to ensure that the security is remain intact.
SQL Injection Attack (SQLIA) has made to the top of the OWASP, Top 10 Web Application Security Risks in 2013 and in 2010. The explosive use of web application with very little emphasis lay on securing it make this attack becoming more popular. Various methods have been discussed and proposed as countermeasure to the attack. Unfortunately, most of them are seen to be not comprehensive enough to address any kind of issues an organization might have when it comes to hardening the web security such as technical and financial matter for instance. This study presents a way to prevent and detect intrusion through the deployment of reverse proxy with an intrusion and prevention mechanism built in against web attacks especially SQLIA. With the flexibility offered in server logging process, we obtain and analyse preferred data to visualize the type of attack based on logs information. Our graph visualization development monitors three web security aspects, i.e. the top traffic blocked attempted by IP address, number of regular expression rules violated and detect the rules of intrusion detection.
Nowadays, many organizations find it economically attractive to host their services in a cloud environment. Still, security remains the number one concern for many organizations in adopting the cloud services. In this paper, we assess and study the security of a typical cloud environment as that of the popular Amazon AWS Cloud. The cloud security is assessed by deploying and running Dionaea honeypots for a few months in the cloud provide networks. Collected data and logs from Dionaea honeypots show that the cloud environment is surprisingly insecure and is in fact a target of malicious activities and attacks from different countries.
It is a fact that any computer or device connected to the Internet can potentially be infected by malicious software. This paper investigates this risk further, and makes the basic statement that any Internet connected computer in a country should be seen as part of that country's critical information infrastructure protection program because such a computer can be herded into a botnet and used against that country.
A Distributed Denial-of-Service attack is an attempt to make a computer resource unavailable to its intended users. Typically, a large number of bots are triggered by an attacker simultaneously to create a huge load on a web server and bring it down. However, when processing SQL queries on a web server, owing to huge resource requirements, even a small number of queries from smaller set of bots can create huge load on the server. Such sophisticated application layer attacks go undetected by network security solutions under deployment today. Therefore, we propose an SQL DDoS Mitigator device that focuses on preventing such attacks targeting SQL database resources. It can parse packets at line speed, with a maximum latency of 20s for detecting HTTP GET packets with embedded SQL queries. The query pattern information for requester IP addresses are stored in a red-black tree data structure. Clients crossing the limit of server load, dynamically set on the basis of server state, will be re-directed to a CAPTCHA server for identification of bots. The IPs confirmed as bots are black-listed for a configurable timeout period. The complete system, except the CAPTCHA server, is built on Xilinx Virtex-II Pro 50 FPGA based NetFPGA-1G platform. The device achieved a throughput of 400 Kilo Packets/s in a 1 Gbps network.
A rash of well-publicized database hacking incidents brings security to the forefront of issues faced by the database community. Anomalous requests are processed by the Intrusion Detection System (IDS). The intrusion response component is the major responsible component in IDS for anomalous requests processing. Response component manages some of the policy for authenticating the user requests for further access. The system is proposed with the Database Response Policies with two issues Policy Matching and Policy Administration. This system is termed as Joint Threshold Administrative Module (JTAM). The principle used in JTAM is Separation of Duty. The major idea is used to prevent the malicious modification of the policies by database administrators (DBA's). The database modification proposed ought to be signed by k DBA's for complete modified policy. Cryptographic algorithms are used to encrypt the policy and the signatures. The policies are limited for the user's view and for this pattern concept is used. An Intrusion Prevention Protocol (IPP) is proposed which detects the users that attacks the system and revokes the access rights of that user.
The main objective of the Nymble system is to allow its users a responsible, secured anonymous online access to other networks. It provides a reliable mechanism for the server administrators to block misbehaving users while allowing the other users to stay anonymous and also in fact even the blocked users remain anonymous but only discarded from the system. The main aim is to implement a system for the server administrators to block anonymous misbehaving users by keeping the utilization of anonymizing networks more acceptable for the server administrators. All users remain anonymous and the misbehaving users can be blocked without deanonymization.
An intrusion detection system (IDS) monitors network traffic and monitors for suspicious activity and alerts the system or network administrator. It identifies unauthorized use, misuse, and abuse of computer systems by both system insiders and external penetrators. Intrusion detection systems (IDS) are essential components in a secure network environment, allowing for early detection of malicious activities and attacks. By employing information provided by IDS, it is possible to apply appropriate countermeasures and mitigate attacks that would otherwise seriously undermine network security. However, current high volumes of network traffic overwhelm most IDS techniques requiring new approaches that are able to handle huge volume of log and packet analysis while still maintaining high throughput. Hadoop, an open-source computing platform of MapReduce and a distributed file system, has become a popular infrastructure for massive data analytics because it facilitates scalable data processing and storage services on a distributed computing system consisting of commodity hardware. The proposed architecture is able to efficiently handle large volumes of collected data and consequent high processing loads using Hadoop, MapReduce and cloud computing infrastructure. The main focus of the paper is to enhance the throughput and scalability of the IDS Log analysis. Once enough data is gathered, it is necessary to rapidly analyze it and determine whether any attacks or malicious activities are present, which is the main issue that impacts IDS performance.
Distributed denial-of-service (DDoS) flooding attacks are still great threat to the network security, although methodologies and tools have been implemented to combat this problem. In this paper, a variation of Lyapunov exponent is proposed to detect anomalies in network traffic, based on entropy. Experimental results show that our approach outperforms entropy-based method while reflecting relationship between source IPs and destination IPs, which is enabled by the possibility of combining their entropies.
Intrusion detection is one of the major research problems in network security. It is the process of monitoring and analyzing network traffic data to detect security violations. Mining approach can play very important role in developing an intrusion detection system. The network traffic can be classified into normal and anomalous in order to detect intrusions. In our paper, top-ten classification algorithms namely J48, BayesNet, Logistic, SGD, IBK, JRip, PART, Random Forest, Random Tree and REPTree were selected after experimenting with more than twenty most widely used classification algorithms. The comparison of these top-ten classification algorithms is presented in this paper based upon their performance metrics to find out the best suitable algorithm available. Performance of the classification models is measured using 10-fold cross validation. Experiments and assessments of these methods are performed in WEKA environment using NSL-KDD dataset.
Network intrusion detection systems are considered as one of the basic entities widely utilized and studied in the field of network security that aim to detect any hostile intrusion within a given network. Among many network intrusion detection systems (NIDS), open source systems have gained substantial preference due to their flexibility, support and cost effectiveness. Snort, an open source system is considered as the de-facto standard for NIDS. In this paper, effort has been made to gauge Snort in terms of performance (packet handling) and detection accuracy against TCP Flooding Distributed Denial of Service attack. The evaluation has been done using a sophisticated test-bench under different hardware configurations. This paper has analyzed the major factors affecting the performance and detection capability of Snort and has recommended techniques to make Snort a better intrusion detection system (IDS). Experimental results have shown significant improvement in Snort packet handling capability by using better hardware. However; Snort detection capability is not improved by improving hardware and is dependent upon its internal architecture (signature database and rate filtration). Furthermore, the findings can be applied to other signature based intrusion detection systems for refining their performance and detection capability.
The Tasmanian transmission network is owned and operated by Transend Networks Pty Ltd and is connected to mainland Australia through the Basslink HVDC interconnector. The increasing amount of non-synchronous generation supplying the Tasmanian load has resulted in the need to re-evaluate network security risks due to the changes in system dynamic behaviour. Following certain network contingencies, rates of change of system frequency may breach particular limits resulting in the loss of generation and/or activation of the existing under frequency load shedding scheme. Appropriate limit advice and constraint equations are required to operate the power system securely. This paper describes the studies and methodology used in constructing limit advice as defined by the boundary conditions of the system.
In the case of network applications become more common and computer network security issues become more prominent, intercepting and analyzing network packets is important. However, network environment traffic is large. How to intercept and analyze network packet effectively is a crucial issue. This paper research the process of NDIS intermediate driver capture packets in the Windows platform and propose a mode of shared memory blocks that transport data packets to the application layer software to analyze. This way can improve the efficiency of communication between kernel layer and application layer and reduce the probability of losses during the packets transmission.
With the rapid growth of communication technologies, the widespread use of the Internet, and the recent introduction of e-services, the number of computer network security threats is dramatically increasing. This paper presents an efficient method for anomaly detection in network traffic. In this method, network traffic is decomposed into control and data planes. As the data traffic generation is based on control traffic, the behavior of the two planes is expected to be similar during normal behavior. Therefore, detecting dissimilarity (via cross-correlation) between the traffic of the two planes can indicate an abnormal behavior. Constant and adaptive thresholding techniques have been developed in this paper for the design of a false alarm rate intrusion detection processors. Simulation experiments have been carried out on a real traffic obtained at King Saud University at the end of 2012.
Due to the rapid growth of the network application, new kinds of network attacks are endlessly emerging. Thus, it is of paramount importance to protect the networks from attackers. Consequently, the Intrusion Detection Systems (IDS) are quickly becoming a popular requirement in building a network security infrastructure. Most existing and commercial IDS are generally centralized and suffer from a number of drawbacks, e.g., high rates of false positives, low efficiency, etc, especially when they face distributed attacks. In this paper, we introduce a novel mobile agent-based intrusion detection system focusing on the misuse detection approach, called DIDMAS (Distributed Intrusion Detection using Mobile Agents and Snort). DIDMAS takes advantages of the mobile agent paradigm to implement an efficient distributed system, as well as the integration of existing techniques, i.e., the well-known IDS SNORT. Carried out experiments showed that our proposed system presents better performance as well as a good scalability compared to the pioneer known centralized IDS SNORT system over real traffic and a set of simulated attacks.
Security information event management (SIEM) technologies focus on developing effective methods and tools to assist network administrators during the whole network security management. Though there is a vast number of novel initiatives and contributions in providing adaptiveness and intelligence in this research field, there are still many problems that need be solved. In particular, event correlation are currently emerging as an essential field to be optimized specially due to the widespread adoption of botnets to launch attacks. This position paper explores the biological immune system's characteristics of learning and memory to solve the semi-automatic generation of event correlation rules by applying Artificial Immune Systems (AISs).
Regular Expression (RegEx) matching is the core operation of various network security devices such as IPSes. Despite much effort, it has remained an unsolved problem to achieve both high speed and low memory requirements.XFA, the state-of-the-art software RegEx matching solution, has two fundamental limitations: (1) XFA construction is hard to automate as it requires manual annotation by human experts, and (2) XFA is hard to implement in ASIC as the program executed upon reaching a state requires much of the complexity of a general purpose CPU. In this paper, we propose HASIC, a History-based Finite Automaton (HFA [11]) based RegEx matching scheme. HASIC can exponentially reduce state explosion by testing, setting, and clearing an auxiliary vector of history bits. Compared with XFA, HASIC advances the state of the art because it can be fully automated and it is ASIC friendly. HASIC only uses three simple bit operations and they are easy to implement in ASIC. We conducted experiments using real-world RegEx sets and various traffic traces. Experimental results show that for packet processing speed, software HFA runs an average of 3.34 times faster than XFA, for automata construction speed HFA is orders of magnitude faster than DFA, and for memory image size HFA is an average of 20 times smaller than DFA.
Exploring the symbiotic nature of biological systems can result in valuable knowledge for computer networks. Biologically inspired approaches to security in networks are interesting to evaluate because of the analogies between network security and survival of human body under pathogenic attacks. Wireless Sensor Network (WSN) is a network based on multiple low-cost communication and computing devices connected to sensor nodes which sense physical parameters. While the spread of viruses in wired systems has been studied in-depth, applying trust in WSN is an emerging research area. Security threats can be introduced in WSN through various means, such as a benevolent sensor node turning fraudulent after a certain period of time. The proposed research work uses biological inspirations and machine learning techniques for adding security against such threats. While it uses machine learning techniques to identify the fraudulent nodes, consecutively by deriving inspiration from human immune system it effectively nullify the impact of the fraudulent ones on the network. Proposed work has been implemented in LabVIEW platform and obtained results that demonstrate the accuracy, robustness of the proposed model.
This paper introduces an innovative packet filter application called oftables. The application provides a new network security enforcement opportunity using OpenFlow, with a lot of advantages which will be further explained. Even more, it covers all firewall requirements and evolve the whole concept concerning performance, management, granularity and reliability. Altogether it presents an major improvement regarding packet based network traffic filtering in future data-center, enterprise or campus networks.
Capturing the right security requirements is crucial when developing a security software. Poor elicited security requirements can lead to a failure in software development, thus it needs to be accurately defined. This study evaluates various security requirement engineering tools and analyses the existing gaps in security requirement engineering tools. Based on a literature search conducted manually, we report our findings from the review and analysis of different studies of security requirements engineering tool. Consequently, the gaps and motivations found from this literature study are discussed. Future directions of this study is to develop a more useful tool that can perform a better function in capturing security requirements are also discussed.
Modern covert channel communication is the art of hiding secret information in legitimate network traffic in a way that cannot normally be detected by anyone other than the intended receiver. It is growing in its presence and sophistication. This type of communication enables the distribution of malicious or sensitive information and poses a significant network security problem to individuals, organizations, and governments. One popular method of covert communication in RTP streams is the transmission of one or more packets after significantly delaying them. As a result, any normal receiver will discard them as arriving late, whereas covert receivers successfully receive them to extract their payload subverted by the covert transmitter. This provides a covert channel method with significant throughput potential and thus high risk. In this paper we propose a method that can restrict this type of covert communication and prevent the distribution of secret information. Our proposed method takes advantage of buffering the sequence number of the received packets and thus detecting late packets, allowing it to discard them instead of delivering them to the receiver. Therefore, the covert receiver will not be able to intercept and observe these intentionally delayed packets, nor extracting the covert message. The in-depth analysis and our simulation results demonstrate that the proposed method is effective and capable of preventing this type of covert communication.
We consider arbitrary risk-averse users, whose costs of improving security are given by an arbitrary convex function. In our model, user probability to incur damage (from an attack) depends on both his own security and network security: thus, security is interdependent. We introduce two user types (normal and malicious), and allow one user type (malicious users) to subvert insurer monitoring, even if insurers perfectly enforce (at zero cost) security levels of normal users. We prove that with malicious users present, equilibrium contract that specifies user security fails to exist. We demonstrate, in a general setting, a failure of cyber-insurers to underwrite contracts conditioning the premiums on security. We consider arbitrary risk-averse users, whose costs of improving security are given by an arbitrary convex function. In our model, user probability to incur damage (from an attack) depends on both his own security and network security: thus, security is interdependent. We introduce two user types (normal and malicious), and allow one user type (malicious users) to subvert insurer monitoring, even if insurers perfectly enforce (at zero cost) security levels of normal users. We prove that with malicious users present, equilibrium contract that specifies user security fails to exist. We demonstrate, in a general setting, a failure of cyber-insurers to underwrite contracts conditioning the premiums on security.
In recent years, there has been a growing interest in analyzing functional connectivity networks estimated from neuroimaging technologies using graph theory. Previous studies of the functional brain networks have focused on extracting static or time-independent networks to describe the long-term behavior of brain activity. In this paper, we propose a dynamic functional brain network tracking and summarization approach to describe the time-varying evolution of connectivity patterns in functional brain activity. The proposed approach is based on two-dimensional SVD of the three-mode tensor representation of dynamic graphs. First, the event intervals are identified based on the change in the reconstruction error in the lower dimensional space and then the activity in the event intervals are summarized. The proposed method is evaluated for characterizing time-varying network dynamics from event-related potential (ERP) data indexing the well-known error-related negativity (ERN) component related to cognitive control.
The research of Network security situational awareness (NSSA) is very important because it can improve the network monitoring abilities, Emergency response capacity and predict the development trend of network security. Based on the concept model of situational awareness, This paper expounds on the situational awareness of the three main research content: extraction the factors of NSSA, situation understanding and situation prediction. The Focuses on the core problem each stage needs to be solved, main algorithms and advantages and disadvantages of these algorithms. Finally, the implementation of the development trends of the related theory and application in different stages are analyzed and prospected.
Network signature matching is an important task in many applications such as network security or traffic analysis, which generally rely on a flexible signature matching system to extract important packet information from each processed packet. This task is computation and data intensive, and requires significant processing time in sequential manner. In order to accelerate signature matching of giga-bit network traffic, we aim to exploit the inherent parallelism of signature matching through the use of parallel graphics processor units. In this paper, we present detailed analysis of signature matching along with the system design for parallel graphics processors(GPUs). The signature matching schema proposed is based on port matching and keyword matching in each packet header. A real system on graphics processor units was implemented to evaluate the efficacy of our design. Experimental results proved that signature matching can be efficiently done on graphics processor units.
With the advent of technology and the rapid development of network applications, network security issues become a priority need to consider a variety of network applications, in a variety of spoofing attacks, how to prevent spoofing TCP session hijacking became the focus of network intrusion prevention one. This article first explains the concept of TCP session hijacking, and its principle and the resulting harm caused elaborated, describing the attacker to conduct a session hijacking attack process, the final against session hijacking attacks can not prevent or eliminate a fundamental characteristic, put forward the corresponding defense strategies.
Association Rule is one of the main technologies of Data Mining, this paper combining with the particularity of network security, applying the association rule to large scale networks of situational awareness. Against the limitation of the classical algorithm for mining association rules Apriori algorithm, proposes an efficient algorithm to save time consumption. According to the three levels of reducing the number of non-frequent sets, avoiding calculating the items that is not existed and reducing the database redundancy items to improve the Apriori algorithm, saving lots of time and space in the algorithm process. Experiments results show that this algorithm can quickly and effectively mining association rules, and help to identify potential, malicious attacks, safeguard the overall network security situation.
A novel day-ahead generation scheduling and spinning reserve determination model was proposed for power grid with large-scale wind power. The load shedding losses and the wind curtailment losses were estimated by using the probability density function. And they are taken as two operation risk costs integrated into the generation cost objective function. While fully meeting the unit ramp rate constraints and the network security constraints, the day-ahead generation schedule of conventional units and the up/down spinning reserve capacities were solved simultaneously. The priority list method and the minimum marginal cost based economic dispatch algorithm are used to solve the proposed model. Numerical results of modified IEEE reliability test system show that the proposed model and method are effective.
The dendritic cells algorithm (DCA) is an immune-inspired intelligent algorithm and is based on an abstract model of the behavior of dendritic cells (DCs) in immunology. Meanwhile, a number of bots based on diverse protocols, including P2P, IRC, DNS and HTTP, have become an increasing threat to network security. So many researchers use the DCA to detect a single bot in computer system. But the DCA's input is the time series data consisting of antigens and signals. The complex raw data must be preprocessed to form the input stream of antigens and signals. The data preprocessing of the DCA includes raw data collection phase, selection, extraction and mapping phase of antigens and signals. This paper proposed the several methods of input data preprocessing for bots detection using the DCA. These methods are suit for the different bots detection, such as IRC and P2P bots detection.
The escalating complexity of computer networks on a daily basis has increased the probability of malicious exploitation. Even a rare vulnerability in a single computer might compromise the network security of an entire organisation. Intrusion Detection Systems form an integral component of the mechanisms designed to prevent internet and data communication systems from such attacks. The attacks on the network comprise of information gathering and modification through unauthorized access to resources and denial of service to legitimate users. IDS play a key role in detecting the patterns of behaviour on the network that might be indicative of impending attacks. Majority of groundbreaking research on IDS is carried out onKDD'99 dataset and focuses on either all the attacks in the network or the attacks corresponding to TCP/IP protocol. This paper presents a step forward in this direction where the IDS model addresses a specific part of the network attacks commonly detected at port 7 in UDP. Port scans in UDP account for a sizable portion of the internet traffic and comparatively little research characterizes security in UDP port scan activity. To meet the growing trend of attacks and other security challenges in the constantly evolving internet arena, this is paper presents a computationally intelligent intrusion detection mechanism using swarm intelligence paradigm, particularly ant colony optimisation, to analyze sample network traces in UDP port scans. This work aims at generating customised and efficient network intrusion detection systems using soft computing to increase general network security through specific network security.
Information and network security have received a great attention in the past two decades and this attention will increase as the world moving dramatically towards making governmental and organizational systems computerized. Several events and scandals occurred that show that there are security breaches in most the powerful information security systems. These have enforced governments and organization to grant researches on information security and also educate their people in such aspects. Thus the research paper will investigate the ways to educate any organization members with the concepts of information and network security and taking the advantages of e-learning to make such training efficient and easy to use. The research paper attempts to develop an E-Laboratory that will achieve the above goals via the training of users with the security concepts to be able to protect their information and networks. The paper presents the design and implementation of the e-laboratory that covers most of the important aspects of information and network security. The e-lab can also be used to detect the security breaches of the system and even the development of information and network security in future. Several well known techniques for defending the information and networks have been included in the e-lab with a simple user friendly interface.
The security situation of the Internet of Things (IoT) is serious. IoT encounters security problems more than traditional computer networks does. The attributes of dispersity and mass of IoT require that approaches to IoT security should be dynamic. Inspired by immunology, a novel approach to IoT security is proposed in this paper. Traditional network security models are used for reference and special requests of IoT security are taken into account. A dynamic defense frame for IoT security is formed in the proposed approach. The links in the frame are correlated with relative data of IoT security. Performance in biological immunology is applied into some links to make the proposed approach be adaptive to IoT environment. The immunity-based antigen, self and detector in the real IoT environment are simulated. They are adopted to imitate the mechanisms which are used to recognize pathogens in biological immune systems. Simulation experiment results show that the proposed approach may provide a novel effective method to ensure IoT security.
Processing massive data flow in intrusion detection systems (IDS) become a serious challenge. It is considered as a major deficiency while handling heterogeneous and non-stationary data stream to uncover anomaly in the online operational mode. This paper proposes a novel online method that constructs connections from the massive data flow for evaluating IDS models. The proposed method overcomes this challenge by using a queuing concept of dynamic window size. It captures network traffic and hosts events constantly and handles them synchronously within time slot windows inside the queue in order to construct connection vectors based on certain features. We have evaluated the method in offline mode using DARPA dump data flow and in online mode using a simulated network at the university campus. In addition, we have evaluated our IDS model using the constructed connections to proof the feasibility and plausibility of the proposed method in IDS area. The performance evaluation confirms that, the proposed method is able to operate in offline as well online modes efficiently. Moreover, constructed connections are very adequate for training and evaluating IDS models.
As the network security events emerge in endlessly, security situational awareness has become a hot topic in the area of network security research in recent years. Considering the characteristics of multi-source information, complicated data structure in the network security research, this paper puts forward a multilevel fuzzy comprehensive evaluation model, which is based on improved Analytic Hierarchy Process and Fuzzy Comprehensive Evaluation (AHP_FCE), for network security situational assessment. In this method, the hierarchical network index system which consists of three layers, namely, index layer, criterion layer and target layer from bottom to top, is established. The data of the index layer processed by the fuzzy mathematics is obtained from the massive snort alarm information and vulnerability information. The evaluation index weights each layer are calculated by the improved AHP. Comprehensive evaluation is started from the index layer, and by using the evaluation index weights, the factors of security situational are obtained in the criterion layer; by calculating layer by layer upwards, the hosts' security situational values are obtained in the target layer. Finally, the network security situational value is obtained using the hosts' weights. The effectiveness of this method is evaluated using DARPA 2000 data sets.
HCloud (Healthcare Cloud) is a proposed private cloud platform aims at providing ubiquitous healthcare services. Security issues are crucial for applications and popularization. Apart from the traditional network security solutions, trusted computing technology is combined into more and more aspects of cloud computing environment to guarantee the integrity of platform and provide attestation mechanism for trustworthy services. We extend the structure of HCloud by introducing trusted central node (TCN), which is a host with a built-in TPM (trusted platform module) for each service layer to check process integrity online and using trusted front node (TFN) as a broker to relay the certificate application, as well as under the context of proposed dual-CA regime. Two algorithms are also designed for scheduling request from different category terminals and determine which service would be probed at specific time. Security analysis shows us that the trustworthy services are available after improving the system architecture.
With the launch and development of the Android system, it is urgent to solve its terminal security protection problem. Based on three aspects, i.e., Android features, security architecture and security mechanisms, this paper analyses the security threats and the corresponding risks faced by the terminal from the technical level and management level respectively. Moreover, a security ecosystem architecture based on Android platform is designed reasonably and effectively to regulate the Android management system. The proposed ecosystem is benefit to solve the security problems such as open-source vulnerability and App store management chaos. The achievement of this paper promotes researches of security risk issues positively.
In the last decade of Wireless Network security, using text and biometrics for various types of security problems has gained popularity. This research work merges both, the concepts of packet scheduling and the network security issues of the wireless data packets exchange. To achieve the high Quality-of-Service (QoS) in WLAN, we have replaced the existing Automated Security Aware Packet Scheduling Algorithm (ASPS) with novel Bio-cryptic Security- Aware Packet Scheduling-Algorithm (BSPS). This BSPS Algorithm besides assuring the best performance in packet delivery ratio, load on network switch and also intensifying the security level to the desirable WN by applying Bio-cryptographic methods in each security level. Our simulation outcome proves that our scheme is performing better than existing algorithms in terms of the quality of security.
The market of security software solutions for smartphones has grown considerably in the last few years. A wide range of products is available. The objective of our work is to develop an overview of security software solutions for smartphones. At first we identify basic threats and typical security measures for smartphones. Then we explore current security software solutions and describe how these can be classified. Furthermore, we analyse which security software solutions (respectively classes of security software solutions) can prevent or mitigate which threats.
Scalable video using Cloud Computing is a potential solution for the distribution of media content to a large number of users. This may occur over a heterogeneous network connected to devices with different capabilities and diverse set of users. Although some of the problems are well known and understood in information and network security, there is still a need to improve the existing solutions to produce a solution that is both adequately secure and efficient in highly distributed and scalable environments. In this paper, we describe such improvements using a cloud computing scenario where video content is made available through a cloud platform. When put on an Infrastructure as a Service (IaaS) cloud video content should then be viewable by different consumers using different levels of bandwidth and security requirements depending on their identity. This requires a mechanism through which a cloud service could be authenticated and encrypted by end users. This paper describes the novel solution of securing scalable video in the cloud discussing the various threats for video distribution and how these can be made more secure in terms of confidentiality, availability and integrity, particularly threw source authentication and encryption.
This paper explores traditional intrusion response mechanisms in the context of mobile ad hoc networks (MANET). We first identify the unique difficulties involved in deploying traditional intrusion responses in MANETs. Then, using as a backdrop a hierarchical intrusion detection system (IDS) architecture, we propose a general solution to those difficulties, relying on the optimized link state routing protocol (OLSR) database.
This paper deals with an approach to security analysis of IP-based computer networks for cyber-physical applications. The method developed stems from a formal model of network topology with changing link states, and deploys bounded model checking of network security properties supported by SAT-based decision procedure. Its implementation consists of a set of tools that provide automatic analysis of router configurations, network topologies, and states with respect to checked properties.
With the development of networks countermeasure technology, network security early-warning has become a key technology of constructing networks defense in depth architectures. Focusing on network real environment, upgrading comprehensive capacity of the network security defense, a complete set of network security early-warning control mechanism are first discussed; then, based on network defense in depth model, the design ideas, reaching goals, design principle and implementation technology of network security early-warning system are presented; and finally, from the dynamic monitoring, intrusion detection, real-time early-warning and process status tracking, the system function design and the procedure design of main function module are also given. This design model is valuable for guiding the developing practice of network security early-warning system.
Internet worm attacks reduce the network security and cause economic losses. As different kinds of worms spread in the Internet, the interactions among them have strong impact on the propagation of worms. In order to eliminate the negative effect, it is necessary to characterize these interaction behaviors. Accounting for these factors, a model with two mutualistic symbiosis worms is proposed in this paper. The stability of the infection-free equilibrium and positive equilibrium are discussed. Through theoretical analysis, the stability conditions are obtained mathematically. Numerical results from the experiment are in favor of the theoretical analysis. It also indicates that the mutualistic symbiosis worms indeed promote each other's spread in the Internet, which is helpful to design strategies to control the spread of worms.
An ample amount of evidence is available discussing the barriers to e-government adoption and initiatives. Of the many barriers or challenges mentioned, security concerns are a recurring theme. However, the majority of exiting e-government research does not focus or discuss security considerations for e-government systems. This is even more notorious when looking specifically at literature focused on municipal or local e-government. This paper focuses on bringing additional attention to e-government security in municipal government agencies. To accomplish this, the paper presents the findings from a survey conducted of the 34 incorporated cities within the county of Orange, California. The results show an increased need for e-government security at the municipal government level when benchmarked against federal e-government security requirements.
Markov zero-sum games arise in applications such as network interdiction, where an informed defender protects a network against attacks. This problem has received significant attention in recent years due to its relevance to military problems and network security. In this paper, we focus on finite games where the attacker knows imperfectly the network state, and formulate this as a Markov game with nested information. By exploiting the nested information structure, we decompose the multistage game into a sequence of one-stage subgames and develop an algorithm that computes the value of the game and the saddle point strategies for the game. This decomposition method computes the value of the game using backward induction as in stochastic dynamic programming, then identifies saddle-point strategies that achieve this value. Using the Markov structure of the game, we show that the value of the game can be computed efficiently in terms of a single value function of an information state at each stage. The resulting single stage optimization problems are much smaller than the original multistage game. We illustrate our results with an example of multistage network interdiction where the attacker may not be able to observe outcomes of the attacks.
3G-WLAN heterogeneous network handover technology as one of the key technology of next generation wireless mobile communication technology, other countries have conducted in-depth research on it. Network heterogeneity is one of the biggest challenges of vertical handover, the traditional vertical handover algorithm can no longer adapt to the booming of network integration. This thesis mainly researches the 3G-WLAN heterogeneous network security of access and seamless handover algorithm simulation and performance analysis. Comparing with the traditional vertical handover algorithm, 3G-WLAN network handover algorithm has adopted tight coupling integration schemes and considered the location, speed and angle information of the mobile terminal, distinguish handover mode, set the variable into the community out distance, and assisted by receiving signal strength, effectively reduced the handover frequency, limited the ping-pong effect, improved the handover performance.
Optical code-division multiple-access (OCDMA) technique has been considered as a good candidate to provide optical layer security and regards as an important benefit to the enhanced security. In this paper, an enhanced security mechanism is structured over spectral-amplitude coding (SAC) scheme of OCDMA network coder/decoders (codecs). The network security is obtained through pseudo-noise (PN) type of shift register (SR) state switching to combat with eavesdropping. We construct the dynamic reconfigurable codec over arrayed-waveguide gratings (AWGs) by utilizing the cyclic properties of AWG routers and maximal-length sequence (M-sequence) codes. Signature code unique to each OCDMA network user is reconfigured in accordance with the memory cell state of the controlling electrical shift registers. Examples for both network user and eavesdropper are numerically illustrated. It is shown that, the eavesdroppers cannot accurately detect the cell-shift values of the shift register when the scheme is triggered. The proposed scheme thus makes the probability of achieving a correct detection result for the eavesdroppers become much lower. The degree of OCDMA network confidentiality is hence largely improved.
In terms of network security, software-defined networks (SDN) offer researchers unprecedented control over network infrastructure and define a single point of control over the data flows routing of all network infrastructure. OpenFlow protocol is an embodiment of the software-defined networking paradigm. OpenFlow network security applications can implement more complex logic processing flows than their permission or prohibition. Such applications can implement logic to provide complex quarantine procedures, or redirect malicious network flows for their special treatment. Security detection and intrusion prevention algorithms can be implemented as OpenFlow security applications, however, their implementation is often more concise and effective. In this paper we considered the algorithm of the information security management system based on soft computing, and implemented a prototype of the intrusion detection system (IDS) for software-defined network, which consisting of statistic collection and processing module and decision-making module. These modules were implemented in the form of application for the Beacon controller in Java. Evaluation of the system was carried out on one of the main problems of network security  identification of hosts engaged in malicious network scanning. For evaluation of the modules work we used mininet environment, which provides rapid prototyping for OpenFlow network. The proposed algorithm combined with the decision making based on fuzzy rules has shown better results than the security algorithms used separately. In addition the number of code lines decreased by 2030%, as well as the opportunity to easily integrate the various external modules and libraries, thus greatly simplifies the implementation of the algorithms and decision-making system.
The growing popularity of smart mobile devices such as smartphones and tablets has made them an attractive target for cyber-criminals, resulting in a rapidly growing and evolving mobile threat as attackers experiment with new business models by targeting mobile users. With the emergence of the first large-scale mobile botnets, the core network has also become vulnerable to distributed denial-of-service attacks such as the signaling attack. Furthermore, complementary access methods such as Wi-Fi and femtocells introduce additional vulnerabilities for the mobile users as well as the core network. In this paper, we present the NEMESYS approach to smart mobile network security, to develop novel security technologies for seamless service provisioning in the smart mobile ecosystem, and to improve mobile network security through a better understanding of the threat landscape.
Nowadays Information security is an important issue in Information Technology world. The computer viruses, worms, hackers, crackers, electronic eavesdropping and electronic fraud, intrusions are some of the problems that Computer Security experts are facing. The Intrusion Detection System is a common and widely used approach in a well formed network security policy. Information systems must be monitored and audited for potential attacks; but the challenge in this process is analyzing heavy loads of event logs and network traffic. Also to be able to recognize new kinds of threads that tack place in network every day in a timely and efficient manner. In this paper we considered Differential Evolution algorithm for training neural network for the intrusion detection system. We used KDD dataset for our experiments that is derived from the standard KDD CUP ??? ??? Intrusion Dataset. We also provided the comparative results of the differential evolution with the state of the art classification algorithm like RBF, Probabilistic Neural network (PNN) and Multilayer Perceptron (MLP) neural network. We reduced the dimension/features of the KDD datasets using PCA. The results of our study showed higher accuracy in intrusion detection.
In this study, we proposed a method of hiding the stego image in cover image by using a technique called fractional fourier transform with wavelet coefficients. Application of steganography is internet/web security. To maintain higher security Arnold transform is performed on host image with key, key is only known to a receiver/sender. For embedding, perform a fractional Fourier transform of cover image and secret image. Then apply DWT on both images. Cover image and secret image will added by using a technique called alpha blending which can add foreground with background color and wavelet coefficients of both images. Stego image will be extracted by applying IDWT (Inverse Discrete Wavelet Transform). For extracting, cover image will subtracted from stego image by using alpha blending. Secret image will be produced. By considering different aspects we have investigated the results of our scheme. This proposed method provides higher security, robustness against different attacks, good feasibility.
Medical information is extremely high sensitive in nature. Tampering by a third party may result in identity theft and incorrect diagnosis. Hence, it is important to secure the transfer of medical information from the patient to the remote system where the specialist. The proposed work decomposes this problem into two sub-problems and proposes security solutions to each of them: (1) to secure the link between the patient and the device, and (2) to secure the link between the device and the network. Biometric identification is extremely an effective authentication. Thus we push the limits of the network security to the edge by authenticating the user using their biometric information; authenticating the device to the network at the physical layer and strengthening the security of the wireless link using steganography. The proposed authentication methods can be used for recording the medical data in a central database and for accessing medical records in various settings. An algorithm for visual cryptography and steganography is built using MATLAB platform. The proposed work considers iris images for authentication it was taken from CASIA Iris Image Database V4.0
Anomaly detection is one of the important challenges of network security associated today. We present a novel hybrid technique called G-LDA to identify the anomalies in network traffic. We propose a hybrid technique integrating Latent Dirichlet Allocation and genetic algorithm namely the G-LDA process. Furthermore, feature selection plays an important role in identifying the subset of attributes for determining the anomaly packets. The proposed method is evaluated by carrying out experiments on KDDCUP'99 dataset. The experimental results reveal that the hybrid technique has a better accuracy for detecting known and unknown attacks and a low false positive rate.
Any communication system will be having an encryption, error correcting block and decryption, decoding block at the transmitter and receiver side respectively. But, combining encryption and encoding, is what is the focus in research now a days. Multimedia wireless applications are very popular, mainly due to high data rates. Majority of encrypted transmission schemes do not consider well the effect of bit errors occurred during transmission and are considered a problem that can be handled by an efficient coding scheme and a proper encryption technique. In this paper turbo product code (TPC) and advanced encryption standard (AES), is combined together for getting better bit error rate (BER). TPC used for encoding is an effective technique with reasonable decoding complexity. Original information is encoded, encrypted and sent over an AWGN channel using BPSK modulation. Promising results are obtained for BER with less complexity. So, a wireless data transmission scheme that combines encryption and encoding technique into one processing step can be used for various applications.
When it comes to security risks, especially malware, Mac OS X has the questionable reputation of being inherently safe. While there is a substantial body of research and implementations dealing with malware on Windows and, more recently, Android systems, Mac OS X has received little attention so far. To amend this shortcoming, we built a Mac OS X based high-interaction honeypot and used it to evaluate over 6,000 blacklisted URLs to estimate how widespread malware for Mac OS X is today. We further built a dynamic analysis environment and analyzed 148 malicious samples to gain insight into the current state of Mac OS X malware. To the best of our knowledge, we are the first to tackle this task.
The future cyber security workforce needs radio frequency identification (RFID) information systems security ( INFOSEC ) and threat modeling educational materials. A complete RFID security course with new learning materials and teaching strategies is presented here. A new RFID Reference Model is used in the course to organize discussion of RFID, much as the open systems interconnection (OSI) model is used in a computer networking course. Students use a general-purpose threat modeling process named STRIDE and a risk analysis model named DREAD to determine and to mitigate security risks. Class modules on topics such as the threat modeling process and privacy can be integrated into fourth-year undergraduate or first-year graduate-level computer science and computer engineering courses such as network security, wireless security, computer networks, sensor/RFID networks, or network performance.
By enabling a direct comparison of different security solutions with respect to their relative effectiveness, a network security metric may provide quantifiable evidences to assist security practitioners in securing computer networks. However, research on security metrics has been hindered by difficulties in handling zero-day attacks exploiting unknown vulnerabilities. In fact, the security risk of unknown vulnerabilities has been considered as something unmeasurable due to the less predictable nature of software flaws. This causes a major difficulty to security metrics, because a more secure configuration would be of little value if it were equally susceptible to zero-day attacks. In this paper, we propose a novel security metric, $(k)$-zero day safety, to address this issue. Instead of attempting to rank unknown vulnerabilities, our metric counts how many such vulnerabilities would be required for compromising network assets; a larger count implies more security because the likelihood of having more unknown vulnerabilities available, applicable, and exploitable all at the same time will be significantly lower. We formally define the metric, analyze the complexity of computing the metric, devise heuristic algorithms for intractable cases, and finally demonstrate through case studies that applying the metric to existing network security practices may generate actionable knowledge.
Inspired by swarm intelligence observed in social species, the artificial self-organized networking (SON) systems are expected to exhibit some intelligent features (e.g., flexibility, robustness, decentralized control, and self-evolution, etc.) that may have made social species so successful in the biosphere. Self-organized networks with swarm intelligence as one possible solution have attracted a lot of attention from both academia and industry. In this paper, we survey different aspects of bio-inspired mechanisms and examine various algorithms that have been applied to artificial SON systems. The existing well-known bio-inspired algorithms such as pulse-coupled oscillators (PCO)-based synchronization, ant- and/or bee-inspired cooperation and division of labor, immune systems inspired network security and Ant Colony Optimization (ACO)-based multipath routing have been surveyed and compared. The main contributions of this survey include 1) providing principles and optimization approaches of variant bio-inspired algorithms, 2) surveying and comparing critical SON issues from the perspective of physical-layer, Media Access Control (MAC)-layer and network-layer operations, and 3) discussing advantages, drawbacks, and further design challenges of variant algorithms, and then identifying their new directions and applications. In consideration of the development trends of communications networks (e.g., large-scale, heterogeneity, spectrum scarcity, etc.), some open research issues, including SON designing tradeoffs, Self-X capabilities in the 3^rd Generation Partnership Project (3GPP) Long Term Evolution (LTE)/LTE-Advanced systems, cognitive machine-to-machine (M2M) self-optimization, cross-layer design, resource scheduling, and power control, etc., are also discussed in this survey.
Distributed mesh sensor networks provide cost-effective communications for deployment in various smart grid domains, such as home area networks (HAN), neighborhood area networks (NAN), and substation/plant-generation local area networks. This paper introduces a dynamically updating key distribution strategy to enhance mesh network security against cyber attack. The scheme has been applied to two security protocols known as simultaneous authentication of equals (SAE) and efficient mesh security association (EMSA). Since both protocols utilize 4-way handshaking, we propose a Merkle-tree based handshaking scheme, which is capable of improving the resiliency of the network in a situation where an intruder carries a denial of service attack. Finally, by developing a denial of service attack model, we can then evaluate the security of the proposed schemes against cyber attack, as well as network performance in terms of delay and overhead.
The security issue of complex networks has drawn significant concerns recently. While pure topological analyzes from a network security perspective provide some effective techniques, their inability to characterize the physical principles requires a more comprehensive model to approximate failure behavior of a complex network in reality. In this paper, based on an extended topological metric, we proposed an approach to examine the vulnerability of a specific type of complex network, i.e., the power system, against cascading failure threats. The proposed approach adopts a model called extended betweenness that combines network structure with electrical characteristics to define the load of power grid components. By using this power transfer distribution factor-based model, we simulated attacks on different components (buses and branches) in the grid and evaluated the vulnerability of the system components with an extended topological cascading failure simulator. Influence of different loading and overloading situations on cascading failures was also evaluated by testing different tolerance factors. Simulation results from a standard IEEE 118-bus test system revealed the vulnerability of network components, which was then validated on a dc power flow simulator with comparisons to other topological measurements. Finally, potential extensions of the approach were also discussed to exhibit both utility and challenge in more complex scenarios and applications.
A suitable key agreement protocol plays an essential role in protecting the communications over open channels among users using voice over Internet protocol (VoIP). This study presents a robust and flexible password authenticated key agreement protocol with user anonymity for session initiation protocol (SIP) used by VoIP communications. Security analysis demonstrates that the proposed protocol enjoys many unique properties, such as user anonymity, no password table, session key agreement, mutual authentication, password updating freely, conveniently revoking lost smartcards and so on. Furthermore, the proposed protocol can resist the replay attack, the impersonation attack, the stolen-verifier attack, the man-in-middle attack, the Denning-Sacco attack and the offline dictionary attack with or without smartcards. Finally, the performance analysis shows that the protocol is more suitable for practical application in comparison with other related protocols.
Preserving the availability and integrity of networked computing systems in the face of fast-spreading intrusions requires advances not only in detection algorithms, but also in automated response techniques. In this paper, we propose a new approach to automated response called the response and recovery engine (RRE). Our engine employs a game-theoretic response strategy against adversaries modeled as opponents in a two-player Stackelberg stochastic game. The RRE applies attack-response trees (ART) to analyze undesired system-level security events within host computers and their countermeasures using Boolean logic to combine lower level attack consequences. In addition, the RRE accounts for uncertainties in intrusion detection alert notifications. The RRE then chooses optimal response actions by solving a partially observable competitive Markov decision process that is automatically derived from attack-response trees. To support network-level multiobjective response selection and consider possibly conflicting network security properties, we employ fuzzy logic theory to calculate the network-level security metric values, i.e., security levels of the system's current and potentially future states in each stage of the game. In particular, inputs to the network-level game-theoretic response selection engine, are first fed into the fuzzy system that is in charge of a nonlinear inference and quantitative ranking of the possible actions using its previously defined fuzzy rule set. Consequently, the optimal network-level response actions are chosen through a game-theoretic optimization process. Experimental results show that the RRE, using Snort's alerts, can protect large networks for which attack-response trees have more than 500 nodes.
Exploring the symbiotic nature of biological systems can result in valuable knowledge for computer networks. Biologically inspired approaches to security in networks are interesting to evaluate because of the analogies between network security and survival of human body under pathogenic attacks. Wireless Sensor Network (WSN) is a network based on multiple low-cost communication and computing devices connected to sensor nodes which sense physical parameters. While the spread of viruses in wired systems has been studied in-depth, applying trust in WSN is an emerging research area. Security threats can be introduced in WSN through various means, such as a benevolent sensor node turning fraudulent after a certain period of time. The proposed research work uses biological inspirations and machine learning techniques for adding security against such threats. While it uses machine learning techniques to identify the fraudulent nodes, consecutively by deriving inspiration from human immune system it effectively nullify the impact of the fraudulent ones on the network. Proposed work has been implemented in LabVIEW platform and obtained results that demonstrate the accuracy, robustness of the proposed model.
This paper introduces an innovative packet filter application called &#x201C;oftables&#x201D;. The application provides a new network security enforcement opportunity using OpenFlow, with a lot of advantages which will be further explained. Even more, it covers all firewall requirements and evolve the whole concept concerning performance, management, granularity and reliability. Altogether it presents an major improvement regarding packet based network traffic filtering in future data-center, enterprise or campus networks.
Inspired by swarm intelligence observed in social species, the artificial self-organized networking (SON) systems are expected to exhibit some intelligent features (e.g., flexibility, robustness, decentralized control, and self-evolution, etc.) that may have made social species so successful in the biosphere. Self-organized networks with swarm intelligence as one possible solution have attracted a lot of attention from both academia and industry. In this paper, we survey different aspects of bio-inspired mechanisms and examine various algorithms that have been applied to artificial SON systems. The existing well-known bio-inspired algorithms such as pulse-coupled oscillators (PCO)-based synchronization, ant- and/or bee-inspired cooperation and division of labor, immune systems inspired network security and Ant Colony Optimization (ACO)-based multipath routing have been surveyed and compared. The main contributions of this survey include 1) providing principles and optimization approaches of variant bio-inspired algorithms, 2) surveying and comparing critical SON issues from the perspective of physical-layer, Media Access Control (MAC)-layer and network-layer operations, and 3) discussing advantages, drawbacks, and further design challenges of variant algorithms, and then identifying their new directions and applications. In consideration of the development trends of communications networks (e.g., large-scale, heterogeneity, spectrum scarcity, etc.), some open research issues, including SON designing tradeoffs, Self-X capabilities in the 3rd Generation Partnership Project (3GPP) Long Term Evolution (LTE)/LTE-Advanced systems, cognitive machine-to-machine (M2M) self-optimization, cross-layer design, resource scheduling, and power control, etc., are also discussed in this survey.
The escalating complexity of computer networks on a daily basis has increased the probability of malicious exploitation. Even a rare vulnerability in a single computer might compromise the network security of an entire organisation. Intrusion Detection Systems form an integral component of the mechanisms designed to prevent internet and data communication systems from such attacks. The attacks on the network comprise of information gathering and modification through unauthorized access to resources and denial of service to legitimate users. IDS play a key role in detecting the patterns of behaviour on the network that might be indicative of impending attacks. Majority of groundbreaking research on IDS is carried out onKDD'99 dataset and focuses on either all the attacks in the network or the attacks corresponding to TCP/IP protocol. This paper presents a step forward in this direction where the IDS model addresses a specific part of the network attacks commonly detected at port 7 in UDP. Port scans in UDP account for a sizable portion of the internet traffic and comparatively little research characterizes security in UDP port scan activity. To meet the growing trend of attacks and other security challenges in the constantly evolving internet arena, this is paper presents a computationally intelligent intrusion detection mechanism using swarm intelligence paradigm, particularly ant colony optimisation, to analyze sample network traces in UDP port scans. This work aims at generating customised and efficient network intrusion detection systems using soft computing to increase general network security through specific network security.
Nowadays authentication has become an important technology in the fields of network security. Because of the limitation of the resource and computing capability in wireless sensor network, a kind of simple and efficient authentication protocol needs to be taken into account. Moreover, a few security flaws have been found in a few related protocols. Therefore, an authentication protocol has been presented based on Chinese remainder theorem (CRT), and security analysis and performance testing related to this protocol have been performed. The results hereby show that the protocol has the high security level and stable performance.
Network security is a critical aspect of Internet operations. Most network security research has focused on protecting end-systems from hacking and denial-of-service attacks. In our work, we address hacking attacks on the network infrastructure itself. In particular, we explore data plane stack smashing attacks that have demonstrated successfully on network processor systems. We explore their use in the context of software routers that are implemented on top of general-purpose processor and operating systems. We discuss how such attacks can be adapted to these router systems and how stack protection mechanisms can be used as defense. We show experimental results that demonstrate the effectiveness of these stack protection mechanisms.
Voice over Internet Protocol (VoIP) is an emerging trend of applications on the internet today. As with any recent technology, VoIP also introduces both fortuity and problems. Existing VoIP honeypot experimental set ups based on SIP (Session Initiation Protocol) deals with the basic attacks like DoS (Denial of Service), enumeration detection, signature collection and SPIT (Spam over Internet Telephony). These VoIP service abuse attacks cause discrepancy between the services offered to the VoIP users and service providers. We executed successive attempts with different sets of attributes and sample subsets to collect exact traffic records used for detecting and categorizing the attack packets using honeypot. Finally, a comparison of both the algorithms with its true and false positive rates is evaluated. For result analysis, we propose a test-bed using Zoiper (SIP clients), Asterisk server, Artemisa honeypot and Wireshark as network analyzer. The test-bed demonstrates how the honeypot effectively works in improvising the robustness of the VoIP security system from billing attacks and toll frauds.
Intrusion analysis, i.e., the process of combing through IDS alerts and audit logs to identify real successful and attempted attacks, remains a difficult problem in practical network security defense. The major contributing cause to this problem is the high false-positive rate in the sensors used by IDS systems to detect malicious activities. The goal of our work is to examine whether a machine-learned classifier can help a human analyst filter out non-interesting scenarios reported by an IDS alert correlator, so that analysts' time can be saved. This research is conducted in the open-source SnIPS intrusion analysis framework. Throughout observing the output of SnIPS running on our departmental network, we found that an analyst would need to perform repetitive tasks in pruning out the false positives in the correlation graphs produced by it. We hypothesized that such repetitive tasks can yield (limited) labeled data that can enable the use of a machine learning-based approach to prune SnIPS' output based on the human analysts' feedback, much similar to spam filters that can learn from users' past judgment to prune emails. Our goal is to classify the correlation graphs produced from SnIPS into "interesting" and "non-interesting", where "interesting" means that a human analyst would want to conduct further analysis on the events. We spent significant amount of time manually labeling SnIPS' output correlations based on this criterion, and built prediction models using both supervised and semi-supervised learning approaches. Our experiments revealed a number of interesting observations that give insights into the pitfalls and challenges of applying machine learning in intrusion analysis. The experimentation results also indicate that semi-supervised learning is a promising approach towards practical machine learning-based tools that can aid human analysts, when a limited amount of labeled data is available.
The swift growth of communication facilities and ever decreasing cost of computer hardware has brought tremendous possibilities of expansion for commercial and academic rationales. With widely incremented communiqu like Internet, not only the good guys, but also bad guys have advantage. The hackers or crackers can take advantage of network vulnerabilities and pose a big threat to network security personnel. The information can be transferred by means of textual data, digital images, videos, animations, etc and thus requires better defense. Especially, the images are more visual and descriptive than textual data; hence they act as a momentous way of communication in the modern world. Protection of the digital images during transmission becomes more serious concern when they are confidential war plans, top-secret weapon photographs, stealthy military data and surreptitious architectural designs of financial buildings, etc. Several mechanisms like cryptography, steganography, hash functions, digital signatures have been designed to provide the ultimate safety for secret data. When the data is in form of digital images; certain features of images like high redundancy, strong correlation between neighbouring pixels and abundance in information expression need some extra fortification while transmission. This paper proposes a new cryptographic cipher named Digital Image Chaotic Encryption (DICE) to convene the special requisites of secure image transfer. The strength of DICE lies in its partial-symmetric key nature i.e. even discovery of encryption key by hacker will not guarantee decoding of the original message.
Wireless Sensor Networks (WSNs) are used in many applications in military, environmental, and health-related areas. These applications often include the monitoring of sensitive information such as enemy movement on the battlefield or the location of personnel in a building. Security is important in WSNs. However, WSNs suffer from many constraints, including low computation capability, small memory, limited energy resources, susceptibility to physical capture, and the use of insecure wireless communication channels. These constraints make security in WSNs a challenge. In this paper, we try to explore security issue in WSN. First, the constraints, security requirements and attacks with their corresponding countermeasures in WSNs are explained. Individual sensor nodes are subject to compromised security. An adversary can inject false reports into the networks via compromised nodes. Furthermore, an adversary can create a Gray hole by compromised nodes. If these two kinds of attacks occur simultaneously in a network, some of the existing methods fail to defend against those attacks. The Ad-hoc On Demand Distance (AODV) Vector scheme for detecting Gray-Hole attack and Statistical En-Route Filtering is used for detecting false report. For increasing security level, the Elliptic Curve Cryptography (ECC) algorithm is used. Simulations results obtain so far reduces energy consumption and also provide greater network security to some extent.
A class of algebraic codes for incoherent optical code-division multiple access, so-called extended multilevel prime codes, was recently studied. They have asymptotic optimal cardinality and possess a unique tree structure of multiple levels of subsets of codewords; codewords in different subsets have different cross-correlation values for adjustable code performance. In this paper, the performance of the codes under an optimized method of assigning codewords to simultaneous users is formulated. In addition, how the tree structure, large cardinality, and the codes' algebraic properties potentially support rapid codeword switching and physical-layer network security are investigated.
There has been considerable interest in the scientific community in the nature of the future power system. Having distributed resources, energy storage, and plug-and-play capability makes the future power system analogous to a computer network and some refer to this future power system as the energy internet. Ironically, concerns of the future power system, such as reliability, survivability, availability, and security also parallel those of computer networks. This paper introduces a circuit whereby power can be sent in energy packets much like information is routed in a computer network. This opens up the possibility of applying information network theory, such as path diversity, packet acknowledgement, resource management, ad hoc network routing, network topology control, network security, etc. to the power system. Simulation examples and laboratory measurements validate the fundamental circuit. Implications for future power networks are discussed.
Outage load caused by N?1 contingencies can be restored automatically or manually in distribution network. Therefore, the restored load can be considered as an index to represent the distribution network security level. In this paper, a security evaluation method is proposed, in which a MIQCP (Mixed Integer Quadratic Constrained Programming) based restoration model is used. Meanwhile, methods are introduced to improve the computation efficiency of evaluation procedure. Numerical tests on a PG&E 69-node system are given to illustrate the rationality of the proposed security evaluation method. A larger 269-node system is used to show the computation efficiency.
Situation element extraction of network security situation awareness can be transformed into the vast amounts of data recognition and classification. Due to the difficulty of situation element extraction of network security situation awareness, a mechanism for situation extraction based on Logistic Regression (LR) and Improved Particle Swarm Optimization (LR-IPSO) model is proposed. In order to improve local and global search capability of Particle Swarm Optimization(PSO), this paper takes the nonlinear decreasing random strategy for weight value to improve PSO, because of the inherent implicit parallelism and good global optimization ability of IPSO, it is used to estimate parameters and optimize the learning ability of the LR model. Experiment results show that this model is an effective extraction technology of situation element.
After analyzing and quantifying the network information security elements: confidentiality, integrity and availability, this paper defines the network security confidentiality vector, the network security integrity vector and the network security availability vector, and also builds the hierarchical indicator system of network security evaluation. Based on the positive and negative ideal comparison standards, the evaluation indicator elements are processed in the dimensionless grey method, and a qualitative-quantitative evaluation model with multilayer linear weight for the network security is put forward. Finally, the feasibility and validity of the model are verified by analyzing some practical examples.
Packet classification is widely used as a core function for various applications in network infrastructure. With increasing demands in throughput, performing wire-speed packet classification has become challenging. Also the performance of today's packet classification solutions depends on the characteristics of rulesets. In this work, we propose a novel modular Bit-Vector (BV) based architecture to perform high-speed packet classification on Field Programmable Gate Array (FPGA). We introduce an algorithm named StrideBV and modularize the BV architecture to achieve better scalability than traditional BV methods. Further, we incorporate range search in our architecture to eliminate ruleset expansion caused by range-to-prefix conversion. The post place-and-route results of our implementation on a state-of-the-art FPGA show that the proposed architecture is able to operate at 100+ Gbps for minimum size packets while supporting large rulesets up to 28 K rules using only the on-chip memory resources. Our solution is ruleset-feature independent , i.e. the above performance can be guaranteed for any ruleset regardless the composition of the ruleset.
An exploration of active and passive malware honeypots reveals that the two systems yield vastly different malware collections and that peer-to-peer file sharing is an important, but often overlooked, malware source.
The proliferation of digital devices in a networked industrial ecosystem, along with an exponential growth in complexity and scope, has resulted in elevated security concerns and management complexity issues. This paper describes a novel architecture utilizing concepts of autonomic computing and a simple object access protocol (SOAP)-based interface to metadata access points (IF-MAP) external communication layer to create a network security sensor. This approach simplifies integration of legacy software and supports a secure, scalable, and self-managed framework. The contribution of this paper is twofold: 1) A flexible two-level communication layer based on autonomic computing and service oriented architecture is detailed and 2) three complementary modules that dynamically reconfigure in response to a changing environment are presented. One module utilizes clustering and fuzzy logic to monitor traffic for abnormal behavior. Another module passively monitors network traffic and deploys deceptive virtual network hosts. These components of the sensor system were implemented in C++ and PERL and utilize a common internal D-Bus communication mechanism. A proof of concept prototype was deployed on a mixed-use test network showing the possible real-world applicability. In testing, 45 of the 46 network attached devices were recognized and 10 of the 12 emulated devices were created with specific operating system and port configurations. In addition, the anomaly detection algorithm achieved a 99.9% recognition rate. All output from the modules were correctly distributed using the common communication structure.
A number of detection and defense mechanisms have emerged in the last decade to tackle the botnet phenomenon. It is important to organize this knowledge to better understand the botnet problem and its solution space. In this paper, we structure existing botnet literature into three comprehensive taxonomies of botnet behavioral features, detection and defenses. This elevated view highlights opportunities for network defense by revealing shortcomings in existing approaches. We introduce the notion of a dimension to denote different criteria which can be used to classify botnet detection techniques. We demonstrate that classification by dimensions is particularly useful for evaluating botnet detection mechanisms through various metrics of interest. We also show how botnet behavioral features from the first taxonomy affect the accuracy of the detection approaches in the second taxonomy. This information can be used to devise integrated detection strategies by combining complementary approaches. To provide real-world context, we liberally augment our discussions with relevant examples from security research and products.
With the evolution of fourth generation (4G) technologies like Worldwide Interoperability for Microwave Access (WiMAX), Ultra Mobile Broadband (UMB) and Long Term Evolution (LTE), broadband speeds are destined to be pushed ever higher to the next level. Given the flat, distributed architectures that most evolving 4G technologies encourage, issues with better throughput and network security become all the more important. The challenges of securing the 4G networks are not too dissimilar from those encountered in securing the third generation (3G) networksconfidentiality, authentication, network separation, network planning considerations, and the like. The paper proposes a generic 4G network planning scheme for enhanced security and possibly better throughput, and also a scalable Internet Protocol security (IPsec) management system based on a trusted certificate authority (CA), which will go a long way in addressing the need of service providers to deploy secure Internet Protocol (IP) based 4G networks.
The primary motivation for the evolution from proprietary networks to Internet Protocol (IP)-based radio access network (RAN) systems is cost savings in terms of both capital expenditures (CAPEX) and operating expenditures (OPEX). This evolution, however, given the open network environment, introduces new security threats. Such threats can result in significant potential costs that need to be mitigated through additional investments in security solutions. Finding the optimal balance between investment and potential security risk is a complex endeavor. This paper presents rendezvous point (RP), a method to find the optimal point for a balanced security solution. ? 2007 Alcatel-Lucent.
A network providing Voice over Internet Protocol (VoIP) service requires many network elements. Each network element may have its own set of security capabilities, but not all security capabilities on all network elements are necessary at the same time for a given network configuration. An end-to-end network view is necessary to choose appropriate security capabilities while minimizing network overhead. For VoIP, using an IP Multimedia Subsystem (IMS) core network and wireless fidelity (Wi-Fi?) access, the service provider can offer the feature functionality of the core network to both enterprise and residential customers simultaneously. However, both market segments provide their own set of unique security challenges, and what is appropriate for one market segment is not necessarily appropriate for the other. This paper explores various security implications for both of these market segments and proposes options for securing each network configuration. Security aspects of the control plane, bearer plane, and management plane are considered. ? 2007 Alcatel-Lucent.
The host connection degree distribution (HCDD) is an important metric for network security monitoring. However, it is difficult to accurately obtain the HCDD in real time for high-speed links with a massive amount of traffic data. In this paper, we propose a new sketch method to build a probabilistic traffic summary of a host's flows using a uniform Flajolet-Martin sketch combined with a small bitmap. To study its performance in comparison with previous sampling and sketch methods, we present a general model that encompasses all these methods. With this model, we compute the Cramr-Rao lower bounds and the variances of HCDD estimations. The theoretic analysis and numerical experimental results show that our sketch method is six times more accurate than state-of-the-art methods with the same memory usage.
This article introduces a unified framework for quantitative characterization of various wireless networks. We first revisit the evolution of centralized, ad-hoc and hybrid networks, and discuss the trade-off between structure-ensured reliability and efficiency, and ad-hoc enabled flexibility. Motivated by the observation that the number of hops for a basic node in the network to reach the base station or the sink has a direct impact on the network capacity, delay, efficiency and their evaluation techniques, we introduce the concept of the N-hop networks. It can serve as a general framework that includes most existing network models as special cases, and can also make the analytical characterization of the network performance more tractable. Moreover, for network security, it is observed that hierarchical structure enables easier tracking of user accountability and malicious node detection; on the other hand, the multi-layer diversity increases the network reliability under unexpected network failure or malicious attacks, and at the same time, provides a flexible platform for privacy protection.
With the rapid pace of globalization and outsourcing, supply chain integrity of ICT system is gaining more and more attentions. Integrity of ICT Supply Chain has a slightly different focus from network security. Due to the increasing complexity and formidable cost and timing, it's by no means enough to guarantee the integrity of a modern ICT product through technical testing and vendor screening. An architectural approach, named as Architectural Solution Integration (ASI), has been proposed to improve the supply chain integrity during the topology design stage of ICT systems. In this paper, the architecture design methodology for ASI is proposed. Supplier trust model and supply chain integrity model are established with an algorithm based on ranked attack graph to quantitatively evaluate the integrity of ICT supply chain. Finally, a case study is presented to demonstrate the feasibility of the proposed ICT supply chain integrity model and evaluation algorithm, which are the key elements of the ASI approach.
Digital forensics technology is an indispensable means to combat computer crime and cyber crime. In order to meet the needs of the legal proceedings on digital evidence, and to avoid accidental changes to the evidence, the protection of data evidence is an important principle throughout the process of digital forensics. The risks are existed for the current commonly used means of digital evidence consciously protection, such as evidence of the hard disk media easily damaged, the data evidence has been modified without being discovered. In this paper, we present a digital evidence enforcement protection program based on third party notary to improve the deficiencies of digital evidence consciously protection, we analyzed the techniques and implementation process of the program, and discussed the security of it in the end.
Network intrusion detection plays an important role in network security, and this paper presents an approach of hybrid feature selection combined with improved incremental Support Vector Machine (SVM) classification. First, Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is used to trim the original dataset, and a feature selection method, called GATS, which is based upon Genetic Algorithm (GA) embedded tabu search (TS), is used to extract the optimal subset from the reduced dataset. GATS integrates the concept of tabu list which may increase local search performance. Then, an incremental SVM with reserved set method (R-ISVM) is developed to deal with the problem of intrusion detection. Thirdly, according to the variations of classification hyperplane in incremental training, R-ISVM utilizes a concentric-circle model based reserved set strategy to maintain the samples that are most likely to be support vectors in future computation. Data experiments and comparisons with other popular intrusion detection approaches show that our presented method achieves better performance as well as stability.
Signature customization is a technique to help the misuse network based IDS to select an appropriate signature for the protected hosts. Additionally, it eliminates unnecessary signature matching in order to enhance the detection capabilities for the NIDS. This paper assesses the effectiveness of depending only on vulnerability scanners to perform signature customization. In addition, it introduces the integration of vulnerability scanners with patch management tools to limit the number of false positive and false negative customizations. The results show that adding the patch management tools to the integration between the NIDS and vulnerability scanners can reduce the false signature customization. The proposed system will insure tuning accuracy for average of 30% of all shielded rules in the original signature customization system, accordingly improving the overall detection efficiency for the IDS.
Database is an important part for any organization and this database needs to make secure from various attacks in the network. Although there are various techniques implemented in the network for the security of these databases. Hare we proposed detection of intrusion is detected that are possible in the databases and various authentication techniques are implemented to made this databases secure. Database security and authentication such as 3 factor authentication, intrusion response system, timestamp, triggers such factors provides more security to the database. The proposed methodology implemented here provides security, authentication, and database policies.
Defensive techniques against Internet-scale attacks can significantly benefit from sharing network security data among different domains. One compelling example, proposed in this paper, is the case of whitelists for DDoS mitigation, where domains broadcast, for each possible DDoS target (!), the set of legitimate customers (client IP addresses) whose traffic should not be blocked while a DDoS attack is in progress. However, such a fine-grained whitelist sharing approach appears hardly appealing (to say the least) to operators; not only the indiscriminate sharing of customers' addresses raises privacy concerns, but also it discloses, to competitor domains, business critical information on the identity and activity of customers. In a previous work, we proposed a cryptographic approach called conditional data sharing, devised to permit disclosure of cross-domain shared fine-grained organized subsets of network monitoring data, only when a threshold number of domains are ready to reveal their data. In this paper, we cast such technique to a realistic scenario of whitelist sharing for DDoS mitigation, and we significantly extend the underlying cryptographic approach so as to support disclosure not only for threshold-based policies, but for more general (monotone) access structures.
In the modern world, a rapid growth of malicious software production has become one of the most significant threats to the network security. Unfortunately, widespread signature-based anti-malware strategies can not help to detect malware unseen previously nor deal with code obfuscation techniques employed by malware designers. In our study, the problem of malware detection and classification is solved by applying a data-mining-based approach that relies on supervised machine-learning. Executable files are presented in the form of byte and opcode sequences and n-gram models are employed to extract essential features from these sequences. Feature vectors obtained are classified with the help of support vector classifiers integrated with a genetic algorithm used to select the most essential features, and a game-theory approach is applied to combine the classifiers together. The proposed algorithm, ZSGSVM, is tested by using a set of byte and opcode sequences obtained from a set containing executable files of benign software and malware. As a result, almost all malicious files are detected while the number of false alarms remains very low.
Network vulnerability can be analyzed automatically by attack graph. Attack graph tools can generate attack paths in network and show users the network vulnerabilities analyzing process for network security risk analysis. There are some problems such as state space explosion, the high complexity of algorithms, being difficult to demonstrate graphically, and so on, for attack graph generation and visualization techniques. Therefore, we surveyed and analyzed the attack graph generation and visualization technology. We summarized the open source tools like MulVAL, TVA. Attack Graph Toolkit, NetSPA and so on, and the commercial tools, for example, Cauldron, FireMon, Skybox View. We compared and analyzed these tools from the aspects of the attack graph types, scalability, or complexity of attack graph generation algorithm, the degree of attack graph visualization. Their common denominator was summarized, and their different points were analyzed. The future and applications for attack graph were forecasted, for example its applications in industrial control systems, and in the network security defense and risk assessment.
In current scenario, wired and wireless networks are widely used in educational organizations to meet the various needs of education institutions. New types of security threats and vulnerabilities are increasing day by day, making wired and wireless networks insecure and unreliable. In this case study paper, a survey of different types of security threats and security mechanisms in educational environment and ways to counteract them has been discussed. To address the security issue, we have proposed an integrated model for security in wired and wireless network. The model includes network topology and associated network security mechanisms. The network model incorporates the concept of Bring Your Own Device (BYOD) with its security implications. The proposed model is generalized and holistic to fulfill the network requirements and address emerging security issues of any type of educational organization. The proposed model is implemented in our educational organization and initial encouraging results have been obtained.
Security of electronic systems has been intensively research, covering systems used in computing, communication, environmental, biomedical, space, and many other applications. While software and network security is reasonably advanced, hardware security research is still in its infancy. This presentation reviews various types of attacks on low-energy digital circuits and systems popular in mobile, medical, and environmental applications. The paper describes selected methods to defend against these attacks both invasive and non-invasive, including authentication and cryptographic procedures. Using a simple model for digital circuit power, design considerations for power reduction are discussed from security perspectives. On-line detection of possible invasive attacks is emphasized.
Accurate performance evaluation for network algorithms is vital to meet various requirements of different applications, such as QoS, network security, traffic engineering. Although worst-case and average-case analysis are widely used in algorithm evaluation, they are often insufficient due to the lack of practicality. Smoothed Analysis (SA) introduces a new concept of smoothed complexity, remedying the shortcomings in worst-case and average-case analysis. However, recent research towards SA focuses on theoretical evaluation, and thus those methods tend to be too complicated for the analysis of network algorithms. To address the problem, Sampling-based Smoothed Analysis (SSA) for network algorithm evaluation is proposed. SSA extends feasibility for practical performance evaluation and achieves promising experimental results. As examples, two algorithms for typical network problem are evaluated using the proposed SSA framework, and the results explicitly illustrate their significant performance difference in spite of the same theoretical worst-case complexity. Besides evaluation accuracy, SSA also provide more insight for algorithms to facilitate current algorithms improvement and new algorithms design.
This paper describes the main contents and major types of the current Web security testing to introduce several programmers' fallible aspects, and also gives some cases introduction to Web security testing problems. Finally this paper gives the security testing tools and the corresponding coping Web security testing strategy, hoping to improve the efficiency of Web security testing to further improve the level of Web security testing and to avoid causing damage to the enterprise. This paper makes a contribution to Web security testing not only from theoretical but also from practical fields. By using the testing tools and applying testing strategy can improve web security, which has great significance to software companies.
The performance of communication networks can be affected by a number of factors including misconfigu-ration, equipments outages, attacks originated from legitimate behavior or not, software errors, among many other causes. These factors may cause an unexpected change in the traffic behavior, creating what we call anomalies that may represent a loss of performance or breach of network security. Knowing the behavior pattern of the network is essential to detect and characterize an anomaly. Therefore, this paper presents an algorithm based on the use of Digital Signature of Network Segment (DSNS), used to model the traffic behavior pattern. We propose a clustering algorithm, K-Harmonic means (KHM), combined with a new heuristic approach, Firefly Algorithm (FA), for network volume anomaly detection. The KHM calculate a weighting function of each point to calculate new centroids and circumventing the initialization problem present in most center based clustering algorithm and exploits the search capability of FA from escaping local optima. Processing the DSNS data and real traffic adata is possible to detect and point intervals considered anomalous with a trade-off between the 90% true-positive rate and 30% false-positive rate.
Fuzzy logic approach is used in this risk assessment to analyze the generic threats in the education institution towards firewall. This assessment is very crucialas the firewall is the main network security appliance and itrepresents security of the whole network. The genericthreats list was chosen based on MyRAM (2005) guideline from Malaysian Administrative Modernization and Management Planning Unit (MAMPU). The qualitative method that leads to the understanding and holistic description of a phenomenon was applied in this study. It also generates rich, detail data from a multiple perspectives. Overall result obtained from this research shows that firewall faced moderate level of threat, which means it asunder an acceptable risk that needs suitable plan to cope with the foreseen threat.
Although a few studies have been done to investigate the issue of new service development (NSD) in an emerging market, it is deficiency in the development of home delivery service (HDS) for specialty foods in traditional market. Thus, this study takes a Taiwan's HD companies as a subject and uses quality function deployment (QFD) to develop the home delivery service model based on NSD model. In the area of voice of customer (VOC), the results reveal that e-shoppers put emphasis on the security of personal information and trading mechanism. As for HDS for specialty foods in traditional market, customers pay attention to the speed of delivery service, freshness of foods and quick responses from HD companies when any problems happen during delivery. Furthermore, in the area of voice of engineering (VOE), the main suggestions for improvement are training staff, setting up a brand, and strengthening system effectiveness and information safety.
Distributed Denial of Service (DDoS) attacks are one of the challenging network security problems to address. The existing defense mechanisms against DDoS attacks usually filter the attack traffic at the victim side. The problem is exacerbated when there are spoofed IP addresses in the attack packets. In this case, even if the attacking traffic can be filtered by the victim, the attacker may reach the goal of blocking the access to the victim by consuming the computing resources or by consuming a big portion of the bandwidth to the victim. This paper proposes a Trace back-based Defense against DDoS Flooding Attacks (TDFA) approach to counter this problem. TDFA consists of three main components: Detection, Trace back, and Traffic Control. In this approach, the goal is to place the packet filtering as close to the attack source as possible. In doing so, the traffic control component at the victim side aims to set up a limit on the packet forwarding rate to the victim. This mechanism effectively reduces the rate of forwarding the attack packets and therefore improves the throughput of the legitimate traffic. Our results based on real world data sets show that TDFA is effective to reduce the attack traffic and to defend the quality of service for the legitimate traffic.
To solve the lack of collaborative control and whole effect of security modules in the traditional early-warning systems, a novel network security collaborative early-warning technology is studied and a collaborative control framework and early-alert info quick-publish mechanism is proposed. This framework was based on multi-agent, which can make secure modules in the framework be associated with each other to accomplish to communication and work together. Furthermore, two corresponding multicast trees of early-alert information publish are established based on two multi-agent network typology structures: the bottom random overlay network and top hypercube structured overlay network. Simulation results show that this network security collaborative early-warning can make all module functions work normally and reduce this info load and publish time, furthermore, improve the network security system's collaborative defense capability well.
Network virtualization allows multiple virtual networks to coexist on a shared physical substrate infrastructure. As network virtualization becomes popular, the problem of efficiently mapping a virtual network while guaranteeing its survivability in the event of failures becomes increasingly important. In this paper, the reliability problem of virtual network is solved based on a node and link redundant backup strategy. We model the virtual network mapping problem as an integer linear programming and present a discrete particle swarm optimization based algorithm to solve the problem. Experimental results show that the proposed algorithm has higher recovery success ratio and can reduce backup bandwidth simultaneously.
Neither the Real-time Transport Protocol security (RTP) nor the Secure Real-time Transport Protocol (SRTP) key management mechanisms is adequate, the complexity of SRTP based on Data gram Transport Layer Security (DTLS) is too high to reduce the scope of use, so this paper designs a real-time encrypted transport mechanism, using DTLS to achieve key management and negotiating encryption algorithms. Extend the DTLS, and then achieve encryption of RTP based on the DTLS and packaging transmission. To be compared in the particular case, the method, under certain safety requirements, is better suited for the high efficiency transmission network. The mechanism provides a good foundation for real-time multipath transmission.
The integration of Cyber-Physical Energy Systems (CPES) via the technique of virtualization has the potential to notably advance power system automation. Future power systems - especially Smart Grids - will increasingly rely on software and therefore extend the interwoven dependencies between the power system and its Information and Communication Technology (ICT) infrastructure. This, in turn, will inevitably lead to increased system complexity and as a consequence to new management, safety and security challenges. We believe that encapsulating the monitoring, control or protection software in Virtual Machines (VMs) is able to provide a remedy for many of these challenges. However, to the best of our knowledge there exists no work on how to develop or test virtualized power system applications nor on how the complex dependencies between the power system and the ICT could be taken into account during the development of such an application. In this paper we present a Hardware-in-the-Loop (HiL) co-simulation architecture which addresses both of these issues. Further, we provide a proof of concept of our development and evaluation platform for virtualized CPES applications and discuss the benefits and challenges to this approach.
Along with the development and application of computer network, the network security problem has become an urgent task, and the security problem research also becomes an important topic to the network development. The analytic hierarchy process as a powerful method in system science analyzing was introduced into the research of network security situation awareness. The common model of networks security situational awareness architecture is described, and studies the evaluation method relevantly: from security situation numeric computation, the analytic hierarchy process arithmetic is analysis. When a network includes multiple subnets, the network situation of the total network need to be further aggregated. So a new aggregation model and method was proposed.
Big Data possesses profound information of our society, and therefore, it impacts numerous aspects of human society, with a large amount of data from large-scale heterogeneous network environments, we should analyze some special issues of network security monitoring on Big Data environments. This paper propose data cleaning for various types of data sources and analysis Big Data on network security through security event on multiple associations rules. This study provides some ideas for network security monitoring on Big Data environments.
Classifying network traffic in a real-time fashion on large-scale communication networks has been extensively studied in recent years due to its importance in many areas such as network security, QoS provisioning, and network management. To address this issue, port numbers and packet payload signatures have been widely used in many existing traffic classification tools. They, however, are far away from completed due to for example the increase of new Internet applications and traffic encryption. In this paper, we propose a hybrid framework to classify the Internet traffic, combining a classifier based on the well-known port numbers and packet payload signatures, and a novel heuristic-based co-clustering algorithm for classifying the leftover unknown Internet traffic. Taking advantage of a fast unsupervised co-clustering algorithm with simple flow-based features, our traffic classifier can perform a real-time computing online for application discovery on the Internet. Experimental evaluations with over 200,000 network flows collected over two consecutive days on a large-scale WiFi ISP show that the proposed approach successfully classifies a large portion of the Internet traffic missed by the signature based classifier while also reducing the false alarm rate.
Optical code-division multiple-access (OCDMA) has been considered as a good candidate to provide optical layer security. In this paper, an enhanced OCDMA network security mechanism with pseudo-noise (PN) type of M-sequence coding switching is presented to against eavesdropping. Signature code unique to each OCDMA network user is reconfigured in accordance with the register state of a controlling electrical shift register. Examples of signature reconfiguration following state switching of controlling shift register for both network user and eavesdropper are numerically illustrated. It is shown that, by dynamically changing PN state of shift register to reconfigure user signature sequence, eavesdropper will encounter difficulty on decoding correct data sequences. The proposed scheme hence raises a high error probability for eavesdroppers. Accordingly, the degree of the proposed optical network confidentiality can be largely improved.
Privacy Enhancing Techniques (PET) are key to the success in building the trust among the users of the digital world. Enhancing the communication privacy is getting attention nowadays. In this direction, anonymity schemes such as mix, mix networks, onion routing, crowds etc., have started in roads into the deployment at individual and community network levels. To measure the effectiveness and accuracy of such schemes, degree of anonymity is proposed as a privacy metric in literature. To measure the degree of anonymity, many empirical techniques are proposed. We observe that these techniques are computationally intensive and are infeasible for real-time requirements and thus may not be suitable to measure the degree of anonymity under the dynamic changes in the configuration of the network in real-time. In this direction, we propose a novel lightweight privacy metric to measure the degree of anonymity for mix, mix networks and their variants using graph theoretic approach based on Connectivity Index (CI). Further, we also extend this approach with Weighted Connectivity Index (WCI) and have demonstrated the usefulness of the metric through analytical analysis.
With the development of computer and information technology, computer and network have been popular. At the same time, security problem has greatly aroused people's attention, especially in campus environment. Campus network as an important part of the campus life, people also arouse enough attention to the problems of personal computer safety management. Running a set of effective campus network monitoring software is real needed for the campus network management and maintenance personal safety. In order to ensure the safety, reliability and steady running of the campus network, according to the present situation and development of the campus network, people plan and design to use network monitoring system on the campus network security management. The system can effectively control and manage the reliable operation of the campus network.
Protocol reverse from network traces is widely used in the field of network security. But most of the studies focuse on application-level unknown protocols in Ethernet network system. However, in some special wireless systems, the protocol stack is proprietary. It is in urgent need to do the study on the unknown protocol stack. This paper proposed a new method to delimit frames in the bit stream which generated by signal process. By fully exploiting the characteristics of the wireless protocol data, two levels of frequent items mining are employed and a comprehensive index is applied to recognize the preamble. In the experiment, the method is indicated effective.
The virtual machine technology as new teaching aids in the teaching of computer network security has very good application prospect. At the same time, the virtual machine technology is adopted to simulate the real network teaching environment and improve teachers' teaching effect and has become a kind of effective means to ensure the quality of teaching. The virtual machine technology is adopted to enhance the teaching effect of Network Security. The installation and use of the virtual machine software VMware,setting a virtual network environment on single computer, and simulating attacks and defense on network safety, the teaching effect in both theory and practice teaching is promoted.The virtual machine technology application in computer network has very broad prospects.
To provide a solution for security scheme is a common concern not only of researchers but also of providers, integrators and users of wireless sensor networks. In this paper, an SA-LEACH network security enhancement protocol is proposed. The protocol consumes less energy with the same security requirements, and when the base station is comparatively far from the network deployment area, it is more advantageous in terms of energy consumption and more suitable for wireless sensor networks.
In recent years, with the rapid growth of the Internet applications and services, phishing attacks seriously threaten the web security. Due to the versatile and dynamic nature of phishing patterns, the development and maintenance of the anti-phishing prevention system is difficult and costly. Hence, how to acquire and update the phishing knowledge and the phishing model in the anti-phishing detection system become an important issue. In this study, we use the EMCUD (Extended Embedded Meaning Capturing and Uncertainty Deciding) method to build up the phishing attack knowledge according to the identification of phishing attributes. Since users have been aware of some anti-phishing methods, phishers often evolve phishing attack to gain in the environment. The phishing attack knowledge also needs to be dynamically evolved over time. How to systematically evolve the phishing knowledge becomes a major concern of this study. Hence, we use the VODKA (Variant Objects Discovering Knowledge Acquisition) method, a dynamic EMCUD, to evolve existing phishing knowledge. These methods can facilitate the acquisition of new inference rules for the phishing attack knowledge and the observation of the variation and the trend of the phishing attack. In the experiment, 1, 762 phishing URL of the APNOW (Anti-Phishing Notification Window) phishing database of Taiwan have been partitioned into 7 representative phishing cases, and 10 phishing attributes have been obtained by the VOKDA method. Finally, we successfully evolve detection rules of phishing models and observe the trend of the phishing attack model to show the feasibility of this study.
The specification of Trusted Network Connect (TNC) proposed by Trusted Computing Group (TCG) can improve network security from the inside, which has become the focus of security research field.. A policy server based on TNC architecture, which focus on endpoint integrity, is proposed. We discuss its work flow and communication process. and the simulations show the feasibility of this server used in controlling the endpoints access the trusted network.
Recent work in security has illustrated that solutions aimed at detection and elimination of security threats alone are unlikely to result in a robust cyberspace. As an orthogonal approach to mitigating security problems, some have pursued the use of cyber-insurance as a suitable risk management technique. Such an approach has the potential to jointly align with the incentives of security vendors (e.g., Symantec, Microsoft, etc.), cyber-insurers (e.g., ISPs, cloud providers, security vendors, etc.), regulatory agencies (e.g., government), and network users (individuals and organizations), in turn paving the way for comprehensive and robust cyber-security mechanisms. To this end, in this work, we are motivated by the following important question: can cyber-insurance really improve the security in a network? To address this question, we adopt a market-based approach. Specifically, we analyze regulated monopolistic and competitive cyber-insurance markets, where the market elements consist of risk-averse cyber-insurers, risk-averse network users, a regulatory agency, and security vendors. Our results show that (i) without contract discrimination amongst users, there always exists a unique market equilibrium for both market types, but the equilibrium is inefficient and does not improve network security, and (ii) in monopoly markets, contract discrimination amongst users results in a unique market equilibrium that is efficient, which in turn results in network security improvement - however, the cyber-insurer can make zero expected profits. The latter fact is often sufficient to de-incentivize the insurer to be a part of a market, and will eventually lead to its collapse. This fact also emphasizes the need for designing mechanisms that incentivize the insurer to permanently be part of the market.
In transmission system planning, deterministic criteria like the (N-1) security criterion have to be fulfilled for predefined contingencies. The main drawback of the (N-1) security criterion is that it only gives information about whether it is fulfilled or not, making a comparison between possible candidates for power system expansion or a deeper analysis regarding network security rather difficult. This paper presents a method for probabilistic assessment of the grid security based on the stress level of the power system, e. g. the proximity of violating the technical restrictions. The method quantifies the network security using probabilistic risk indexes. Throughout the information obtained by the method, a comparison of the stress level for different scenarios or power system configurations can be done. This method also helps identifying prone regions as well as those situations that lead the power system to higher stress levels. This methodology is intended to be used as a complement to the (N-1) security criterion in the transmission network expansion planning process. The method was implemented in the Northern Chilean Transmission System (SING) for different possible generation scenarios in the year 2025.
While many protocols for sensor network security provide confidentiality for the content of messages, contextual information usually remains exposed. Such contextual information can be exploited by an adversary to derive sensitive information such as the locations of monitored objects and data sinks in the field. Attacks on these components can significantly undermine any network application. Existing techniques defend the leakage of location information from a limited adversary who can only observe network traffic in a small region. However, a stronger adversary, the global eavesdropper, is realistic and can defeat these existing techniques. This paper first formalizes the location privacy issues in sensor networks under this strong adversary model and computes a lower bound on the communication overhead needed for achieving a given level of location privacy. The paper then proposes two techniques to provide location privacy to monitored objects (source-location privacy)periodic collection and source simulation  and two techniques to provide location privacy to data sinks (sink-location privacy)  sink simulation and backbone flooding. These techniques provide trade-offs between privacy, communication cost, and latency. Through analysis and simulation, we demonstrate that the proposed techniques are efficient and effective for source and sink-location privacy in sensor networks.
Key management is the core to ensure the communication security of wireless sensor network. How to establish efficient key management in wireless sensor networks (WSN) is a challenging problem for the constrained energy, memory, and computational capabilities of the sensor nodes. Previous research on sensor network security mainly considers homogeneous sensor networks with symmetric key cryptography. Recent researches have shown that using asymmetric key cryptography in heterogeneous sensor networks (HSN) can improve network performance, such as connectivity, resilience, etc. Considering the advantages and disadvantages of symmetric key cryptography and asymmetric key cryptography, the paper propose an efficient and hybrid key management method for heterogeneous wireless sensor network, cluster heads and base stations use public key encryption method based on elliptic curve cryptography (ECC), while using symmetric encryption method between adjacent nodes in the cluster. The analysis and simulation results show that the proposed key management method can provide better security, prefect scalability and connectivity with saving on storage space.
With the development of internet technologies and communication services, message transmissions over the internet still have to face all kinds of security problems. Hence, how to protect secret messages during transmission becomes a challenging issue for most of the researchers. It is worth mentioning that many applications in computer science and other related fields rely on steganography and watermarking techniques to ensure information safety during communication. In this paper, we propose a new steganographic method to embed the secret data inside a cover image based on least-significant-bit (LSB) replacement method. The embedding process predominantly concentrates on distributing the secret message inside one share of a color image to appear like a 3D geometric shape. The dimensions of the geometric shape are variable pursuant to the size of secret message. Data distribution process makes our method to be of a great interest as of being so difficult for the hackers or intruders to reconstruct the shape from stego-images, thereby the security is improved. Furthermore, we compare the performance of our approach with two other relevant approaches in terms of peak signal-to-noise ratio (PSNR). The contribution of our approach was immensely impressive.
Internet access by wireless networks has grownconsiderably in recent years. However, these networks are vulnerable to security problems, especially those related to denial of service attacks. Intrusion Detection Systems(IDS)are widely used to improve network security, but comparison among the several existing approaches is not a trivial task. This paperproposes building a datasetfor evaluating IDS in wireless environments. The data were captured in a real, operating network. We conducted tests using traditional IDS and achievedgreat results, which showed the effectiveness of our proposed approach.
As the electric transmission and distribution networks gain smartness from the use of renewable energies and latest measurement and communication technologies, also the utilities become smart. In a modern utility several energy sources are present and energy efficiency has to be guaranteed. Therefore, in order to be a smart utility, it has to be equipped with a measurement and control network to efficiently manage the various energy sources and loads. In this paper a low cost solution for the real-time energy management in a smart grid is presented. It provides several power meters, that continuously monitor connected loads communicating with a Data Concentrator via Power Line bus. Through the implemented web server the users can remotely control their consumption using a web browser. To prevent external attacks, a low computational cost protection software, based on Advanced Encryption Standard Code, was implemented. The paper illustrates the hardware architecture, discusses the adopted communication protocol solutions and is completed with an example of energy monitoring for a smart utility.
Firewalls are essential components in network security solutions. They implement a network security policy which represents the highest level requirements for controlling the resource accesses. The effectiveness of security protection provided by a firewall mainly depends on the quality of the configuration implemented in it. Unfortunately, different conflicts between filtering rules may occur which make the network vulnerable to attacks. Manual management of this problem can be overwhelming and potentially inaccurate. Therefore, there is a need of automated methods to analyze, detect and correct misconfigurations. Prior solutions have been proposed but we note their drawbacks are threefold: First, common approaches deal only with pairwise filtering rules. In such a way, some other classes of configuration anomalies could be uncharted. Second, syntactic anomalies could be intentional (i.e., not perforce misconfigurations). This substantial distinction is not often highlighted. Third, although anomalies resolution is a tedious and error prone task, it is generally given to the network administrator. We present, in this paper, a formal approach whose contributions are the following: Detecting new classes of anomalies, bringing out real misconfigurations and finally, proposing automatic resolution method by considering the security policy. We prove the soundness of our method and demonstrate its applicability and scalability by the use of a Satisfiabilty Solver. The first results we obtained are very promising.
The delimitation of bidding areas and the overall European congestion management have been heavily discussed in the recent years. Although main evaluation criteria are defined in the network code on congestion management, the question of adequate or optimized bidding area delimitations remains open. Thus, this paper presents an approach for an optimized delimitation of bidding areas based on a clustering of security-constraint nodal prices. Relevant evaluation criteria, like the level of generation and redispatch costs or the level of network security, are derived via a multi-step optimization approach. Results show, that optimized bidding area delimitations differ significantly from the current ones. However, these optimized delimitations yield only small cost savings compared to the current bidding area delimitation.
Honeynet represents the new theory in network protection. Unlike purely defensive methods such as Intrusion Detection Systems (IDS), Data Encryption and Firewalls that try to avoid interaction with the attacker, Honeynet is based on the idea of interaction with the attacker. During this interaction, the Honeynet records all of the attacker's actions and tools used without the attacker knowing, representing them to the security analyzers to study and prepare defensive methods. A hybrid honeynet including both the high-interaction and low-interaction honeypots was developed and implemented utilizing virtualization technology, representing a typical laboratory environment.
The wide spread of cyber-attacks made the need of gathering as much information as possible about them, a real demand in nowadays global context. The honeypot systems have become a powerful tool on the way to accomplish that. Researchers have already focused on the development of various honeypot systems but the fact that their administration is time consuming made clear the need of self-adaptive honeypot system capable of learning from their interaction with attackers. This paper presents a self-adaptive honeypot system we are developing that tries to overlap some of the disadvantaged that existing systems have. The proposed honeypot is a medium interaction system developed using Python and it emulates a SSH (Secure Shell) server. The system is capable of interacting with the attackers by means of reinforcement learning algorithms.
Many studies analyze the encrypted transmission using the synchronization of chaotic signals. This requires the exchange of an analog synchronization signal, which almost always is a state of the chaotic generator. However, very few different chaotic structures are used for this purpose, still. The uniqueness of their dynamics allows the identification of these structures by simple autocorrelation. In order to thwart all cryptanalytic attacks based on the identification of this dynamics, we propose a numerical method without memory in order to reversibly destroy the shape of the transmitted signal. After analog-to-digital conversion of the synchronization signal, we apply permutations of the weights of its bits to each binary word. These permutations significantly change the shape of the transmitted signal, increasing its versatility and spreading its spectrum. If the message is simply added to the synchronization signal, being the easiest to decrypt, it undergoes the same transformation. It is therefore extremely difficult to detect the message in the transmitted signal by using a temporal analysis, as well as a frequency one. The present work illustrates the proposed method for the chaotic Colpitts oscillator. Nevertheless, the algorithm does not depend on the chosen chaotic generator. Finally, by only increasing the size of the permutation matrix, the complexity of the change in the waveform is increased in a factorial way.
Hands-on experiments are essential for computer network security education. Existing laboratory solutions usually require significant effort to build, configure, and maintain and often do not support reconfigurability, flexibility, and scalability. This paper presents a cloud-based virtual laboratory education platform called V-Lab that provides a contained experimental environment for hands-on experiments using virtualization technologies (such as Xen or KVM Cloud Platform) and OpenFlow switches. The system can be securely accessed through OpenVPN, and students can remotely control the virtual machines (VMs) and perform the experimental tasks. The V-Lab platform also offers an interactive Web GUI for resource management and a social site for knowledge sharing and contribution. By using a flexible and configurable design, V-Lab integrates pedagogical models into curriculum design and provides a progressive learning path with a series of experiments for network security education. Since summer 2011, V-Lab has served more than 1000 students from six courses across over 20 experiments. The evaluation demonstrates that the platform and curriculum have produced excellent results and helped students understand and build up computer security knowledge to solve real-world problems.
The increasing use of smartphones, tablets, and other mobile devices poses a significant challenge in providing effective online security. CAPTCHAs, tests for distinguishing human and computer users, have traditionally been popular; however, they face particular difficulties in a modern mobile environment because most of them rely on keyboard input and have language dependencies. This paper proposes a novel image-based CAPTCHA that combines the touch-based input methods favored by mobile devices with genetically optimized face detection tests to provide a solution that is simple for humans to solve, ready for worldwide use, and provides a high level of security by being resilient to automated computer attacks. In extensive testing involving over 2600 users and 40000 CAPTCHA tests, fgCAPTCHA demonstrates a very high human success rate while ensuring a 0% attack rate using three well-known face detection algorithms.
Techniques for network security analysis have historically focused on the actions of the network hosts. Outside of forensic analysis, little has been done to detect or predict malicious or infected nodes strictly based on their association with other known malicious nodes. This methodology is highly prevalent in the graph analytics world, however, and is referred to as community detection. In this paper, we present a method for detecting malicious and infected nodes on both monitored networks and the external Internet. We leverage prior community detection and graphical modeling work by propagating threat probabilities across network nodes, given an initial set of known malicious nodes. We enhance prior work by employing constraints that remove the adverse effect of cyclic propagation that is a byproduct of current methods. We demonstrate the effectiveness of probabilistic threat propagation on the tasks of detecting botnets and malicious web destinations.
This paper focuses on the study and analysis of a radar network for the surveillance of the airport terminal area. This work is studied in the framework of the European project SOS, aimed to increase the security of the airport and proposes innovative solutions to guarantee a continuous passenger screening. The active radar networks described in this paper will be designed taking into account different constraints such as modularity, scalability and interoperability. By performing both tracking and data fusion features, the system is able to obtain a complete coverage of the surveillance area and to improve the accuracy respect to a single radar node.
The use of sensor networks has increased rapidly over the last years. Due to their low resources, sensors come along with new issues regarding network security and energy consumption. Focusing on the network availability, previous studies proposed to protect the network against denial of service attacks with the use of traffic monitoring agents on some nodes. But if the control nodes go down or get compromised, they leave the network unprotected. To better fight against attacks, we try to enhance this solution by introducing an energy-aware and secure method to select these monitoring nodes (called cNodes) in a clustered wireless sensor network. Our election process is done in accordance to their remaining reserves: nodes with the higher residual energy are selected. We discuss limitations of this deterministic process concerning security and cluster coverage, and suggest as a workaround to designate new control nodes (called vNodes). Those vNodes are responsible for monitoring the cNodes by periodically enquiring about their remaining energy and ensuring that they do not lie during the election process (in attempt to keep their cNode role). Finally, we present some experimental results obtained with the ns-3 simulator in order to analyze the impact of our proposal on the energy repartition in the network.
This paper presents a new approach to infer malware-infected machines by solely analyzing their generated probing activities. In contrary to other adopted methods, the proposed approach does not rely on symptoms of infection to detect compromised machines. This allows the inference of malware infection at very early stages of contamination. The approach aims at detecting whether the machines are infected or not as well as pinpointing the exact malware type/family, if the machines were found to be compromised. The latter insights allow network security operators of diverse organizations, Internet service providers and backbone networks to promptly detect their clients' compromised machines in addition to effectively providing them with tailored anti-malware/patch solutions. To achieve the intended goals, the proposed approach exploits the darknet Internet space and employs statistical methods to infer large-scale probing activities. Subsequently, such activities are correlated with malware samples by leveraging fuzzy hashing and entropy based techniques. The proposed approach is empirically evaluated using 60 GB of real darknet traffic and 65 thousand real malware samples. The results concur that the rationale of exploiting probing activities for worldwide early malware infection detection is indeed very promising. Further, the results demonstrate that the extracted inferences exhibit noteworthy accuracy and can generate significant cyber security insights that could be used for effective mitigation.
Software-defined network (SDN) is the next generation of networking architecture that is dynamic, manageable, cost-effective, and adaptable, making it ideal for the high-bandwidth, dynamic nature of today's applications. In SDN, network management is facilitated through software rather than low-level device configurations. However, the centralized control plane introduced by SDN imposes a great challenge for the network security. In this paper, we present a secure SDN structure, in which each device is managed by multiple controllers rather than a single one as in a traditional manner. It can resist Byzantine attacks on controllers and the communication links between controllers and SDN switches. Furthermore, we design a cost-efficient controller assignment algorithm to minimize the number of required controllers for a given set of switches. Extensive simulations have been conducted to show that our proposed algorithm significantly outperforms random algorithms.
Covert channels are a network security risk growing both in sophistication and utilization, and thus posing an increasing threat. They leverage benign and overt network activities, such as the modulation of packet inter-arrival time, to covertly transmit information without detection by current network security approaches such as firewalls. This makes them a grave security concern. Thus, researching methods for detecting and disrupting such covert communication is of utmost importance. Understanding and developing analytical models is an essential requirement of covert channel analysis. Unfortunately, due to the enormous range of covert channel algorithms available it becomes very inefficient to analyze them on a case-by-case basis. Hence, a unified model that can represent a wide variety of covert channels is required, but is not yet available. In other publications, individual models to analyze the capacity of interrupt-related covert channels have been discussed. In our work, we present a unique model to unify these approaches. This model has been analyzed and we have presented the results and verification of our approach using MATLAB simulations.
Honeypots are systems aimed at deceiving threat agents. In most of the cases the latter are cyber attackers with financial motivations, and malicious software with the ability to launch automated attacks. Honeypots are usually deployed as either production systems or as research units to study the methods employed by attackers. In this paper we present the results of two distinct research honeypots. The first acted as a malware collector, a device usually deployed in order to capture self-propagating malware and monitor their activity. The second acted as a decoy server, dropping but logging every malicious connection attempt. Both of these systems have remained online for a lengthy period of time to study the aforementioned malicious activity. During this assessment it was shown that human attackers and malicious software are constantly attacking servers, trying to break into systems or spread across networks. It was also shown that the usage of honeypots for malware monitoring and attack logging can be very effective and provide valuable data. Lastly, we present an open source visualization tool which was developed to help security professionals and researchers during the analysis and conclusion drawing phases, for use with one of the systems fielded in our study.
In this paper, an observer-based adaptive neural network (OBANN) tracking control scheme is proposed for uncertain nonlinear systems with time-delays and external disturbances. The adaptive neural network model is used to approximate the dynamics of the nonlinear system, while an observer-based control scheme is to stabilize the system. By applying the adaptive neural dynamics, we can on-line tune the weights of the neurons of the neural model and the bounds of the gains of delay states directly using linear analytical results. From Lyapunov criterion and Riccati-inequality, it is shown that the stability of the closed-loop system is guaranteed and the closed loop system signals are uniform ultimate boundedness and achieve H? tracking performance. Finally, a numerical example of a two-links rolling cart is given to illustrate the effectiveness of the proposed control scheme.
The following topics are dealt with: system security; system quality; network security; format specification; software testing; mobile security; system performance analysis; system accuracy analysis; failure prediction; fault prediction; software reliability; and software security.
Distributed Denial of Service (DDoS) has been one of the greatest threats to network security for years. In recent years, DDoS attackers turn to application layer, which makes DDoS attack detection systems based on net layer and transport layer lost their performance. In this layer, Web service is the most vulnerable application. The study in this paper analyzed the differentiation between user behavior based on web log, as we proposed a series of features based on user behavior to represent characteristics of user behavior, and then, transformed web logs which contain authentic legal users' records and attackers' records to an 14 dimensional feature space. In particular, through the transformation, our work aims to obtain a better representation for users' behaviors, as well as to investigate the relative differences and/or similarities between DDoS attackers and normal users. Finally, we simulated four kinds of prevalent application layer DDoS attack and conducted experiments using three classical data mining classification algorithms to certify the effectiveness of our method. Experimental results show that proposed features are good to distinguish legal users and attackers in application layer.
Regular expression matching plays an important role in network security. Regular expression matching is achieved by NFA and DFA. DFA is suitable for high-speed IDS due to its efficiency. However, the combined compilation of multiple rules containing .? may blow up in state and storage space. In this paper, we give an explanation to this problem from the prospective of information theory, and propose a multidimensional mathematical model focusing on the most serious state explosion. We divide redundant states into zero-dimensional ones and one-dimensional ones. The former are compressed by dimension, and the later are dynamically built. Theory proof illustrates that the space complexity of the model reaches the theoretical lower bound. Then we propose the multi-dimensional finite automata (MFA) based on the model. Experimental results show that, MFA reduces greatly the construction time, memory and matching time, compared with several typical state-of-the-arts DFA improved algorithms.
Montgomery multiplication (MM) in GF(2m) is a popular technique to speedup network security protocols such like digital signature provided by elliptic curve cryptography (ECC) and key distribution supported by ECC or Diffie-Hellman. MM in GF(2m) is defined as ABr?1 mod f(x), where f(x) is the irreducible polynomial of degree m and r is a fixed element in the field. In this paper, a low complexity Montgomery multiplier in GF(2m) using Linear Feedback Shift Registers (LFSR) is proposed for the class of fields generated with an irreducible all-one polynomial. The latency of the proposed architecture is shown to be lower than the best among existing works found in the literature. Furthermore, highly regular architecture in LFSR and available LFSR based low power techniques make our proposal more attractive than non-LFSR architectures. On the other hand, the constraint of the new multiplier is that it will not have speed advantage when the system clock rate is higher than 2GHz.
Attacks to Web systems have shown an increasing trend in the recent past. A contributing factor to this trend is the deployment of Web 2.0 technologies. While work related to characterization and classification of malicious Web traffic using supervised learning exists, little work has been done using semi-supervised learning with partially labeled data. In this paper an incremental semi-supervised algorithm (CSL-Stream) is used to classify malicious Web traffic to multiple classes, as well as to analyze the concept drift and concept evolution phenomena. The work is based on data collected in duration of nine months by a high-interaction honeypot running Web 2.0 applications. The results showed that on completely labeled data semi-supervised learning performed only slightly worse than the supervised learning algorithm. More importantly, multiclass classification of the partially labeled malicious Web traffic (i.e., 50% or 25% labeled sessions) was almost as good as the classification of completely labeled data.
Computing a prioritized set of vulnerabilities to patch is important for system administrators to determine the order of vulnerabilities to be patched that are more critical to the network security. One way to assess and analyze security to find vulnerabilities to be patched is to use attack representation models (ARMs). However, security solutions using ARMs are optimized for only the current state of the networked system. Therefore, the ARM must reanalyze the network security, causing multiple iterations of the same task to obtain the prioritized set of vulnerabilities to patch. To address this problem, we propose to use importance measures to rank network hosts and vulnerabilities, then combine these measures to prioritize the order of vulnerabilities to be patched. We show that nearly equivalent prioritized set of vulnerabilities can be computed in comparison to an exhaustive search method in various network scenarios, while the performance of computing the set is dramatically improved, while equivalent solutions are computed in various network scenarios.
In this paper we propose a methodology and a prototype tool to evaluate web application security mechanisms. The methodology is based on the idea that injecting realistic vulnerabilities in a web application and attacking them automatically can be used to support the assessment of existing security mechanisms and tools in custom setup scenarios. To provide true to life results, the proposed vulnerability and attack injection methodology relies on the study of a large number of vulnerabilities in real web applications. In addition to the generic methodology, the paper describes the implementation of the Vulnerability & Attack Injector Tool (VAIT) that allows the automation of the entire process. We used this tool to run a set of experiments that demonstrate the feasibility and the effectiveness of the proposed methodology. The experiments include the evaluation of coverage and false positives of an intrusion detection system for SQL Injection attacks and the assessment of the effectiveness of two top commercial web application vulnerability scanners. Results show that the injection of vulnerabilities and attacks is indeed an effective way to evaluate security mechanisms and to point out not only their weaknesses but also ways for their improvement.
This paper proposes relay selection to increase the physical layer security in multiuser cooperative relay networks with multiple amplify-and-forward relays, in the presence of multiple eavesdroppers. To strengthen the network security against eavesdropping attack, we present three criteria to select the best relay and user pair. Specifically, criteria I and II study the received signal-to-noise ratio (SNR) at the receivers, and perform the selection by maximizing the SNR ratio of the user to the eavesdroppers. To this end, criterion I relies on both the main and eavesdropper links, while criterion II relies on the main links only. Criterion III is the standard max-min selection criterion, which maximizes the minimum of the dual-hop channel gains of main links. For the three selection criteria, we examine the system secrecy performance by deriving the analytical expressions for the secrecy outage probability. We also derive the asymptotic analysis for the secrecy outage probability with high main-to-eavesdropper ratio. From the asymptotic analysis, an interesting observation is reached: for each criterion, the system diversity order is equivalent to the number of relays regardless of the number of users and eavesdroppers.
The longstanding debate on a fundamental science of security has led to advances in systems, software, and network security. However, existing efforts have done little to inform how an environment should react to emerging and ongoing threats and compromises. The authors explore the goals and structures of a new science of cyber-decision-making in the Cyber-Security Collaborative Research Alliance, which seeks to develop a fundamental theory for reasoning under uncertainty the best possible action in a given cyber environment. They also explore the needs and limitations of detection mechanisms; agile systems; and the users, adversaries, and defenders that use and exploit them, and conclude by considering how environmental security can be cast as a continuous optimization problem.
Network intrusion forensics is an important extension to present security infrastructure, and is becoming the focus of forensics research field. However, comparison with sophisticated multi-stage attacks and volume of sensor data, current practices in network forensic analysis are to manually examine, an error prone, labor-intensive and time consuming process. To solve these problems, in this paper we propose a digital evidence fusion method for network forensics with Dempster-Shafer theory that can detect efficiently computer crime in networked environments, and fuse digital evidence from different sources such as hosts and sub-networks automatically. In the end, we evaluate the method on well-known KDD Cup 1999 dataset. The results prove our method is very effective for real-time network forensics, and can provide comprehensible messages for a forensic investigators.
In recent years, wireless and mobile communication systems have become increasingly popular as the advance of hardware technologies makes the implementation of novel communication technologies feasible. With growing demand of software downloading and mobile multimedia services, the quality of service (QoS) provisioning and the management of network security have been critical in determining the success of future generation wireless communications. This special issue includes cutting-edge research achievements on the provisioning of QoS and security in wireless and mobile networks. The included papers present new research related to theory or practice of all aspects of security issues in ad hoc and sensor networks and QoS provisioning.
Software-based network security is constantly challenged by the increase in network speeds and number of attacks. At the same time, mobile network access underscores the need for energy efficiency. In this paper, we present a new way to improve the throughput and to reduce the energy consumption of an anomaly-based intrusion detection system for probing attacks. Our framework implements the same classifier algorithm in software (C++) and in hardware (synthesizable VHDL), and then compares the energy efficiency of the two approaches. Our results for a decision tree classifier show that the hardware version consumed only 0.03% of the energy used by the same algorithm in software, even though the hardware version operates with a throughput that is 15 times that of the software version.
Darknets can be used to monitor unexpected network traffic destined for allocated but unused IP address blocks, thus providing an effective traffic measurement technique for viewing certain remote network security events. Past works in this field discussed the possible causes (events) of darknet traffic and applied their classification schemes on short-range traces. Our interest lies, however, in how darknets have evolved since those works and the effectiveness of a darknet taxonomy for real long-range traffic. We thus propose a simple but effective taxonomy of darknet traffic, on the basis of observations, and evaluate it on real darknet traces covering six years. The evaluation results show that we can detect and label anomalous events defined by the taxonomy for over 96% of all sources, making the unlabeled source rate extremely low. We also obtain some interesting findings on the evolution of different anomalous events since 2006 (especially in recent years), determine the most appropriate time bin for traffic analysis of our traces, and highlight the general applicability of our taxonomy on different darknet datasets. Finally, we conclude that most sources in our traces are characterized by just one or two events with simple attack mechanisms.
This paper proposes and subsequently delineates quantification of network security metrics using software defined networking approach in real time using a progressive testbed. This comprehensive testbed implements computation of trust values which lend sentient decision making qualities to the participant nodes in a network and fortify it against threats like blackhole and flooding attacks. AODV and OLSR protocols were tested in real time under ideal and malicious environment using the testbed as the controlling point. With emphasis on reliability, interpreting voluminous data, monitoring attacks immediately with negligible time lag, the paper concludes by justifying the use of SAP HANA and UI5 for the testbed.
Network security refers to any activity designed to protect the network. These activities intend to protect the usability, reliability, and safety of network and data. Effective network security targets a variety of threats and stops them from entering or spreading on network. In network security, Complex Event Processing (CEP) system can be used for correlating events across different security devices and applications for complicated attack detection and response. The events will be recorded in sys log files. There will be millions of events generated by each security device. Hence, the CEP engine has to process massive amount of logs. We describe a method for pre-processing the vast input to extract relevant data, the CEP engine shall be concerned about. The CEP engine which we used in this system is ESPER. The sys log is preprocessed based on risk taxonomy. Risk taxonomy is built in a hierarchical structure with respect to the attacks the CEP is looking for.
The increasing demand of network security, access control, and service differentiation over IP networks drives Internet Service Providers and network administrators to deploy ever more sophisticated and faster traffic recognition mechanisms. Unfortunately this is complicated by the continuous development of new application protocols, increasing network bandwidth, and spreading of complicated tunneling and encryption techniques. In this paper we describe a statistical technique for blind recognition and classification of application sessions amongst aggregated traffic. Packets are assigned to known applications/protocols on the basis of a restricted set of information extracted from each packet: packet addresses, sizes, and timestamps. We analyzed three modes with different degrees of correlation among packets belonging to the same session. Albeit its simplicity, the studied technique has demonstrated very good performances, also when used for real-time classification.
Network traffic is a rich source of information for security monitoring. However the increasing volume of data to treat raises issues, rendering holistic analysis of network traffic difficult. In this paper we propose a solution to cope with the tremendous amount of data to analyse for security monitoring perspectives. We introduce an architecture dedicated to security monitoring of local enterprise networks. The application domain of such a system is mainly network intrusion detection and prevention, but can be used as well for forensic analysis. This architecture integrates two systems, one dedicated to scalable distributed data storage and management and the other dedicated to data exploitation. DNS data, NetFlow records, HTTP traffic and honeypot data are mined and correlated in a distributed system that leverages state of the art big data solution. Data correlation schemes are proposed and their performance are evaluated against several well-known big data framework including Hadoop and Spark.
In order to fight against severe risks of security and privacy for implantable medical devices, an on-chip security guard based on zero-power authentication is proposed and verified in this paper. The conception of zero-power is realized by virtue of wireless power recovery instead of fetching power from primary battery. The security guard recovers both data and clock from external wireless signals based on amplitude shift keying pulse width modulation. It features a wireless data rate of 500 Kbps and no need for on-chip clock generator. The Hash encryption is adopted in authentication and features a 32-bit ALU to speed up the SHA-1 computation with an estimated peak power of about only 1 mW, which is affordable by wireless power recovery which has a capacity of several mW. FPGA and chip implementation verify the feasibility of this system.
As the network based applications are growing rapidly, the network security mechanisms require more attention to improve speed and precision. The ever evolving new intrusion types pose a serious threat to network security. Although numerous network security tools have been developed, yet the fast growth of intrusive activities is still a serious issue. Intrusion detection systems (IDSs) are used to detect intrusive activities on the network. Machine learning and classification algorithms help to design Intrusion Detection Models which can classify the network traffic into intrusive or normal traffic. In this paper we present the comparative performance of NSL-KDD based data set compatible classification algorithms. These classifiers have been evaluated in WEKA (Waikato Environment for Knowledge Analysis) environment using 41 attributes. Around 94,000 instances from complete KDD dataset have been included in the training data set and over 48,000 instances have been included in the testing data set. Garrett's Ranking Technique has been applied to rank different classifiers according to their performance. Rotation Forest classification approach outperformed the rest.
Critical Care IT systems such as life support devices, vitals monitoring systems, information systems that provide point of care guidance to care teams are a key component of a lifesaving effort in Healthcare. The mega trends of mobility, social, cloud combined with wide spread increase and sophistication of malware, has created new challenges and the point in time detection methods at the hospitals are no longer effective and pose a big threat to the critical care systems. To maintain the availability and integrity of these critical care systems, new adaptive, learning security defense systems are required that not only learns from the traffic entering the hospital, but also proactively learns from the traffic worldwide. Cisco's Cloud web security (CWS) provides industry-leading security and control for the distributed enterprise by protecting users everywhere, anytime through Cisco worldwide threat intelligence, advanced threat defense capabilities, and roaming user protection. It leverages the big data to perform behavioral analysis, anomaly detection, evasion resistance, rapid Detection services using flow based, signature based, behavior based and full packet capture models to identify threats. This tech talk looks at how big Data Analytics is used in combination with other security capabilities to proactively identify threats and prevent wide spread damage to healthcare critical assets.
With the rapid development of mobile computing, more and more mobile devices, such as smart phones and tablets are able to access Internet. As these mobile devices are usually battery powered, energy efficiency is a very important issue. For most mobile applications, energy saving should be considered at the design stage. Of course, security application is no different. Public key cryptography plays an important role in network security, and it is still essential in mobile computing despite it needs high energy consumption. Considering Elliptic Curve Cryptography (ECC) is easy to perform in hardware and needs lower energy than other public key algorithms. We propose an ECC-based certificate-less public key cryptography scheme. The scheme is lightweight and can save energy for mobile devices. Firstly, it does not need certificate to prove the authenticity of a public key, which can save energy for certificate transmission. Secondly, it is constructed on the traditional ECC instead of bilinear pairing, which makes it lightweight and can save energy for computation. In addition, it avoids the key escrow issue, which makes it has higher security strength than traditional public key cryptography. These advantages make it very suitable for resources-constrained mobile devices.
Multi-tenant infrastructures deployed in cloud datacenters need network security protection. However, the rigid control mechanism of current security middleboxes induces inflexible orchestration, limiting the agile and on-demand security provision in virtualized datacenters. This paper presents Tualatin, a consolidated framework of delivering security services in multi-tenant datacenters. It meets security requirements of different scenarios by hardware and software co-design. Leveraging Software-Defined Networking (SDN) and OpenFlow techniques, Tualatin provides fine-grained security protection in dynamically changing network topologies, where both switches and security middleboxes are programmatically controlled by logically centralized controllers. With service-level APIs exposed, Tualatin could be easily integrated with other Cloud Management System (CMS). A proof-of-concept system has been deployed in a Tier-IV datacenter, providing customizable network security services for tenant Virtual Private Cloud (VPC) infrastructure.
Modeling and characterizing Internet traffic has been a widely studied problem since the conception of the Internet. The self-similar, bursty nature of the traffic has led to a number of conventional statistical models that unfortunately provide relatively weak modeling power. Recently, fractal analysis techniques have emerged to better characterize and model Internet traffic data. However, past research studies have focused on describing and quantifying the fractal nature of Internet traffic on user groups, instead of a single user. In this paper the authors investigate the issue of individual users exhibiting fractal (self-similar behavior) behavior across multiple application types. Using real Internet traffic traces (collected via Net-Flow logs) collected at a college campus for 30 days, our investigations reveal that in a number of application categories (http, chatting, p2p, email etc.) at least one user exhibits long-range correlations typical of fractal behavior. Of the 10 application groups, 7 had over 80% of users demonstrating self-similar behavior with 3 of those groups having > 98%. Potential benefits of our study in the realm of smart health and network security, by reducing the dimensionality of large Internet traffic datasets, are discussed.
Archiving of Internet traffic is essential for analyzing network events in the field of network security. Currently, bitmap indexing is used to accelerate the indexing and search queries for archival traffic data. However, the generation of bitmap index needs large storage space, which makes bitmap index compression is a must-have function. In this paper, we propose a new bitmap index encoding algorithm named SECOMPAX (Scope-Extended COMPressed Adaptive indeX), which performs better compression ratio and fast encoding speed compared with the state-of-art bitmap index compression algorithm WAH (Word-Aligned-Hybrid), PLWAH(Position list word aligned hybrid) and COMPAX (COMPressed Adaptive indeX). The comparison among WAH, PLWAH, COMPAX and SECOMPAX shows that SECOMAX accomplishes the smallest bitmap index in size and the comparable encoding time with other three methods. We also use real Internet trace from CAIDA to prove the validity of SECOMPAX. SECOMPAX has the best compression ratio in compared with other bitmap index encoding algorithms in our experiments. The encoding time is measured, and statistics of the distribution of codeword used in SECOMPAX is also investigated in experiments. It shows that SECOMPAX's extra time consumption is acceptable as the new designed codebook work effectively in encoding bit sequence which cannot be compressed in other bitmap encoding schemes.
In recent years, botnet is one of the major threats to network security. Many approaches have been proposed to detect botnets by comparing bot features. Usually, these approaches adopt traffic reduction strategy as first step to reduce the flow to following strategies by filtering packets. With the rapid development of network hardware and software the network speed has reached to multi-gigabit. However, analyzing header and payload of every packet consumes huge amount of computational resources and is not suitable for many realistic situations. Although signature-based solutions are accurate, it is not possible to detect bot variants in real-time. In this study, we proposed a GPU-based botnet detection approach. The experimental results show that the network traffic reduction stage on GPU can achieve about 8x times over CPU based botnet detection tool. The proposed algorithm can used to improve the performance of botnet detection tools efficiently.
The development of information and network technology makes network security become important. Intrusion is one of the issues in network security. To prevent intrusion happens, intrusion detection system (IDS) is built. One of IDS category is anomaly detection. This category detects intrusion event based on data profile. Clustering is one way to observe data profile. There are a lot of clustering algorithms proposed for anomaly detection on IDS, but most of them find clusters in the highest dimension of data. CLIQUE Partitioning (CP) is one of the clustering algorithm that can find clusters from the subspace of data. Testing is done to analyze system's performance based on computational time, completeness, and false alarm rate. CP algorithm shows good performance from completeness point of view (94.59%) and false alarm rate (2.54%). From computational time, CP shows good performance based on the amount of tuple, but the performance is not too good from the quantity of feature side.
Data and network system security is the most important roles. An organization should find the methods to protect their data and network system to reduce the risk from attacks. Snort Intrusion Detection System (Snort-IDS) is a security tool of network security. It has been widely used for protecting the network of the organizations. The Snort-IDS utilize the rules to matching data packets traffic. If some packet matches the rules, Snort-IDS will generate the alert messages. However, Snort-IDS contain many rules and it also generates a lot of false alerts. In this paper, we present the procedure to improve the Snort-IDS rules for the network probe attack detection. In order to test the performance evaluation, we utilized the data set from the MIT-DAPRA 1999, which includes the normal and abnormal traffics. Firstly, we analyzed and explored the existing the Snort-IDS rules to improve the proposed Snort-IDS rules. Secondly, we applied the WireShark software to analyze data packets form of attack in data set. Finally, the Snort-IDS was improved, and it can detect the network probe attack. This paper, we had classified the attacks into several groups based on the nature of network probe attack. In addition, we also compared the efficacy of detection attacks between Snort-IDS rules to be updated with the Detection Scoring Truth. As the experimental results, the proposed Snort-IDS efficiently detected the network probe attacks compared to the Detection Scoring Truth. It can achieve higher accuracy. However, there were some detecting alert that occur over the attack in Detection Scoring Truth, because some attack occur in several time but the Detection Scoring Truth indentify as one time.
In the fi eld of cyber security, ill-defi ned concepts and inconsistently applied terminology are further complicating an already complex issue1. This causes diffi culties for policy-makers, strategists and academics. Using national cyber security strategies to support current literature, this paper undertakes three tasks with the goal of classifying and defi ning terms to begin the development of a lexicon of cyber security terminology. The fi rst task is to offer for consideration a defi nition of active cyber defence (ACD). This defi nition is based upon a number of characteristics identifi ed in current academic and policy literature. ACD is defi ned here as the proactive detection, analysis and mitigation of network security breaches in real-time combined with the use of aggressive countermeasures deployed outside the victim network. Once defi ned, ACD is contextualised alongside two further approaches to cyber defence and security. These are fortifi ed and resilient cyber defence, predicated upon defensive perimeters and ensuring continuity of services respectively. This contextualisation is postulated in order to provide more clarity to non-active cyber defence measures than is offered by the commonly used term passive cyber defence. Finally, it is shown that these three approaches to cyber defence and security are neither mutually exclusive nor applied independently of one another. Rather they operate in a complementary triptych of policy approaches to achieving cyber security.
With increasing crimes and attacks being committed online by adversaries from remote sites, it is vital for law enforcement and public security that forensics investigation into the nature and source of these network attacks be effective and successful in bringing the criminals to justice. The network forensics investigation process is complex and processing-intensive such as sifting through network traffic and examining them for evidence, thus it is desirable to approach this task systematically and efficiently with as much structure as is feasible. This paper proposes a model for network forensics analysis that captures appropriately defined adversarial capability and structured by a layered approach to investigation. The former approach eliminates the need to presume on the adversarys behaviour and is independent of specific attack styles, thus is generic; while the latter approach facilitates a more network-intuitive and modular investigation process. We discuss the layered approach and propose the forensics model by defining adversarial capabilities and the experiment setting played between an adversary, a collection of node instances and a forensics analyst. We apply the model in our investigation against samples of traffic captured and show the feasibility of this model on two common network attack instances. Results of evidence collected and conclusions confirm that analysis based on this model is objectively done, and trustworthy evidence successfully gathered and produced.
The objective of the paper is to propose a social network security management model for a multi-tenancy SaaS application using Unified Communications as a Service (UCaaS) approach. The earlier security management models do not cover the issues when data inadvertently get exposed to other users due to poor implementation of the access management processes. When a single virtual machine moves or dissolves in the network, many separate machines may bypass the security conditions that had been implemented for its neighbors which lead to vulnerability of the hosted services. When the services are multi-tenant, the issue becomes very critical due to lack of asynchronous asymmetric communications between virtual when more number of applications and users are added into the network creating big data issues and its identity. The TRAIN model for the security management using PC-FAST algorithm is proposed in order to detect and identify the communication errors between the hosted services.
The following topics are dealt with: telecommunication and computer network security;radiocommunications;video and image processing;power engineering computing;circuit analysis;speech processing;integrated circuits;biomedical signal processing;optical fiber communications;antennas;analog circuits;digital circuits;power systems;microelectromechanical devices;power electronics;energy conservation;power supply quality;electric machine;control of power system and devices;and renewable energy resources.
Nowadays, the security of applications and Web servers is a new trend that finds its need on the Web. The number of vulnerabilities identified in this type of applications is constantly increasing especially SQL injection attack. It is therefore necessary to regularly audit Web applications to verify the presence of exploitable vulnerabilities. Web vulnerability scanner WASAPY is one of the audit tool, it uses an algorithm which bases on a classification techniques of pages obtained by sending HTTP requests especially formatted. We propose in this paper a new algorithm which was built in a vision to improve rather to supplement the logic followed in modeling WASAPY tool. The tool was supplemented by a new class reflecting the legitimate appearance or referential, therefore, the detection mechanism was solidly built on a statistic in a fairly clear mathematical framework described by a simple geometric representation or interpretation.
Today Networks are protected using many firewalls and security software's. Several of them are not sufficient and effective. Many intrusion detection systems for mobile ad hoc networks are focusing on either routing protocols or their efficiency, but they do not address the security problems. A number of nodes may be selfish meaning that by not forwarding the packets to the target, thereby cutting back the battery power. Some others may act malevolent by commencement of security attacks similar to denial of service or hack the data. The ultimate aim of the protection solutions for wireless networks is to offer security services, for instance confidentiality, authentication, accessibility, integrity, and secrecy to mobile users. This paper proposes a muiltier intrusion detection system. Here three tiers are application, routing and trust. The data transfer takes place between different nodes. First a trusted connection is established between different nodes. Second the routing policy is conformed for all nodes. Finally at the application layer data is routed on the type of application. The node not following trust or routing policy is considered a malicious node. In this research we have simulated different types of attacks and also provided pseudo code for different attacks like route invasion, eavesdrop, sinkhole. Then the performance of these attacks against the proposed algorithm has been calculated and result is displayed.
Since now -a-days security is the primary concern for any organization. This paper provides the network security with the help of quantum cryptography and biometric. We use the BB84 protocol and biometric to secure the network. In this proposed method we create secret key by the help of fingerprint and quantum cryptography. Thus proposed method can be most secure and also easily understandable.
DDoS attack distributed nature causes immense danger to network security. Their ability to send large amount of malicious traffic through multiple agents is a barrier in defending these attacks. Their detection still remains exigent. The situation gets worst as these attacks share similar characteristics with Flash Events where large quantity of legitimate requests come to server on spread of a newsworthy event. In this paper, we classify the DDoS attack from Flash Event using entropy as a metric based on randomness of source IP addresses on a web server. In this work, real-world datasets are used depicting real time scenario of both DDoS attack as well as Flash Event.
Current communication approaches assume network associations as a prerequisite to both content discovery and access. This network-centric paradigm incurs substantial communication and time overhead,as devices build knowledge of content availability only after blindly associating to a network. In pervasive mobile networking,this prerequisite and associated overhead prevents devices to efficiently identify desired communication partners,i.e.,devices running an application or providing content of interest,within the broad mass of devices in communication range as found in everyday scenarios. SO-Fi,instead,realizes content-centric wireless networking by enabling pervasive content discovery before establishing a network infrastructure. SO-Fi builds on the IEEE 802.11 wireless broadcast medium to instantly achieve a discovery scope covering all devices in communication range. Realizing content discovery outside of secure network associations,SO-Fi supports use-case-specific communication security,i.e.,confidentiality,WPA2 network security,DoS robustness,and user authentication. We show SO-Fi's feasibility and performance through real-world experimentation. Indeed,SO-Fi makes instant,content-centric wireless networking readily accessible to application designers.
With the rapid growth of Internet, the amount of malicious codes is exploding. Some security software vendors provide new cloud-based safeguard software for client users. These software, as part of Internet ware, consist of many modules with different functions and Internet behaviors. The Trojan scanning module, for instance, is based on cloud scanning function, which is achieved by collecting a large number of suspicious files on users' hosts and scanning them in remote cloud platforms. While providing security, they also bring a serious problem of user privacy. In this paper, we use black box testing method to analyze the network behavior of four safeguard software, especially the Trojan scanning module based on cloud scanning function. In specific, we conduct extensive experiments to examine the network behaviors of the major function modules used by these safeguard software. In this paper we present a reasonable network behavior model that can help the safeguard software to protect users' privacy. One way of looking at our contributions is the network behavior comparison and analysis of four safeguard software which are widely used in China. And the experimental results validate our claims either.
In this paper a novel technique for user behavior classification is proposed using Fuzzy Rule Based System (FRBS). Using this technique a network user can be monitored and his/her behavior can be classified depending on his/her activities like unauthorized websites usage, attempting to breach in network security, firewalls, unauthorized services access and frequency of attempts etc. The information about a user is obtained by his/her web, database, hardware and other applications logs. FRBS classifies a user to one of the predefined categories based on the information extracted from user logs. This would great help in network security and privacy as well as users may be guided for sincere mistakes and other measures may be taken by the organization. Significance of the proposed scheme is shown by examples and results.
Montgomery multiplication in finite fields has been paid more and more attention recently since it shows advantageous over regular multiplication in speeding up elliptic curve cryptography based network security protocols. In this paper, a most-significant-bit first bit-serial Montgomery multiplication algorithm in GF(2m) using weakly dual bases is proposed for the first time. Then a new bit-serial Montgomery multiplier architecture is proposed using a linear feedback shift register (LFSR). Complexity comparison has shown that the proposed multiplier is comparable to or has certain advantage over the best among the existing similar works found in the literature.
Today's evolving cyber security threats demand new, modern, and cognitive computing approaches to network security systems. In the early years of the Internet, a simple packet inspection firewall was adequate to stop the then-contemporary attacks, such as Denial of Service (DoS), ports scans, and phishing. Since then, DoS has evolved to include Distributed Denial of Service (DDoS) attacks, especially against the Domain Name Service (DNS). DNS based DDoS amplification attacks cannot be stopped easily by traditional signature based detection mechanisms because the attack packets contain authentic data, and signature based detection systems look for specific attack-byte patterns. This paper proposes a chaos based complexity measure and a cognitive machine classification algorithm to detect DNS DDoS amplification attacks. In particular, this paper computes the Lyapunov exponent to measure the complexity of a flow of packets, and classifies the traffic as either normal or anomalous, based on the magnitude of the computed exponent. Preliminary results show the proposed chaotic measure achieved a detection (classification) accuracy of about 66%, which is greater than that reported in the literature. This approach is capable of not only detecting offline threats, but has the potential of being applied over live traffic flows using DNS filters.
Intrusion detection system (IDS) is an important component to ensure network security. In this paper we build an online Na?ve Bayes classifier to discriminate normal and bad (intrusion) connections on KDD 99 dataset for network intrusion detection. The classifier starts with a small number of training examples of normal and bad classes; then, as it classifies the rest of the samples one at a time, it continuously updates the mean and the standard deviations of the features (IDS variables). We present experimental results of parameter updating methods and their parameters for the online Na?ve Bayes classifier. The obtained results show that our proposed method performs comparably to the simple incremental update.
In the Smart grid, network security is the important part. In this paper, we will introduce a new method detection based on Support Vector Machines to detect Masquerade attack, and test it and other methods on the dataset from keyboard commands on a UNIX platform. The presence of shared tuples would cause many attacks in this dataset to be difficultly detected, just as other researchers shown. In order to eliminate their negative influence on masquerade detection, we take some preprocessing for the dataset before detecting masquerade attacks. Our results show that after removing the shared tuples, the classifiers based on support vector machines outperforms the original approaches presented.
The inappropriate use of features intended to improve usability and interactivity of web applications has resulted in the emergence of various threats, including Cross-Site Scripting(XSS) attacks. In this work, we developed ETSS Detector, a generic and modular web vulnerability scanner that automatically analyzes web applications to find XSS vulnerabilities. ETSS Detector is able to identify and analyze all data entry points of the application and generate specific code injection tests for each one. The results shows that the correct filling of the input fields with only valid information ensures a better effectiveness of the tests, increasing the detection rate of XSS attacks.
The growing popularity of smart mobile devices such as smartphones and tablets has made them an attractive target for cyber-criminals, resulting in a rapidly growing and evolving mobile threat as attackers experiment with new business models by targeting mobile users. With the emergence of the first large-scale mobile botnets, the core network has also become vulnerable to distributed denial-of-service attacks such as the signaling attack. Furthermore, complementary access methods such as Wi-Fi and femtocells introduce additional vulnerabilities for the mobile users as well as the core network. In this paper, we present the NEMESYS approach to smart mobile network security. The goal of the NEMESYS project is to develop novel security technologies for seamless service provisioning in the smart mobile ecosystem, and to improve mobile network security through a better understanding of the threat landscape. To this purpose, NEMESYS will collect and analyze information about the nature of cyber-attacks targeting smart mobile devices and the core network so that appropriate counter-measures can be taken. We are developing a data collection infrastructure that incorporates virtualized mobile honeypots and honeyclients in order to gather, detect and provide early warning of mobile attacks and understand the modus operandi of cyber-criminals that target mobile devices. By correlating the extracted information with known attack patterns from wireline networks, we plan to reveal and identify the possible shift in the way that cyber-criminals launch attacks against smart mobile devices.
Cognitive radio network security has become an important issue affecting the development of cognitive radio technology. Primary user emulation attacks(PUEA) is a common attack in the Cognitive radio network. In the scheme, we choose cooperative spectrum sensing system as our model and soft fusion as our Fusion method. We analyze the impact of PUEA on the system performance in the traditional maximal ratio combining (MRC) method and single-user energy detection scheme. In order to decrease the effect of PUEA on the system performance, We find the proper weight coefficient. Through the simulation, our scheme can effectively improve the system performance.
WiFi has been adopted into enterprise production environment in larger scale, yet the flexibility of WiFi network also exposes more vulnerability to current security defense systems and introduces greater challenges to network security for modern enterprises. In wireless world, there are many dead corners that traditional firewall and intrusion detection system cannot cover. Modern enterprises are calling for more efficient defense approaches to guarantee the safety of the information on their wireless network. Upon probing to the weaknesses of current enterprise WiFi security, this paper proposes a defense strategy with the capacities of intelligent planning and integrated reactions to remedy the weaknesses of conventional enterprise security mechanism of WiFi network. A security defense system is designed to monitor WiFi security on Physical Layer, Data-link Layer and Internet Layer of the enterprise WiFi network, and provide attack defense mechanism to minimize the damage to enterprises when their WiFi network is under attack.
MAVIR,the Hungarian Independent Transmission Operator Company,as the Hungarian transmission network owner and operator is focused on preserving the natural values of Hungary in cooperation with National Parks and the BirdLife Hungary. MAVIR participates in several bird protection programs. These activities are carried out without de-energizing transmission lines,thus with the process of live working or solutions that are used in live working.
This paper is an attempt at formulating a Best Current Practice (BCP) for access security and a baseline for core network security in the 3GPP-based systems. This encompasses the 2G circuit-switched GSM system,the 2.5G packet-switched GPRS system,the 3G UMTS system and the 4G LTE/LTE-A system. The 3GPP have defined several security standards,but many measures are optional and there are several areas deliberately not covered by the 3GPP standards. The present document is therefore an attempt at pointing out the best available options and providing advice on how to achieve an overall system hardening,which is badly needed as the cellular systems have undoubtedly become one of the most critical of all critical infrastructures in our modern society.
The next-generation smart grid will rely highly on telecommunications infrastructure for data transfer between various systems. Anywhere we have data transfer in a system is a potential security threat. When we consider the possibility of smart grid data being at the heart of our critical systems infrastructure it is imperative that we do all we can to ensure the confidentiality, availability and integrity of the data. A discussion on security itself is outside the scope of this paper, but if we assume the network to be as secure as possible we must consider what we can do to detect when that security fails, or when the attacks comes from the inside of the network. One way to do this is to setup a hacker-trap, or honeypot. A honeypot is a device or service on a network which appears legitimate, but is in-fact a trap setup to catch breech attempts. This paper identifies the different types of honeypot and describes where each may be used. The authors have setup a test honeypot system which has been live for some time. The test system has been setup to emulate a device on a utility network. The system has had many hits, which are described in detail by the authors. Finally, the authors discuss how larger-scale systems in utilities may benefit from honeypot placement.
Security in open network is one of the major challenges with concerns such as Internet worms, Botnet, Phishing and Flooding attacks. To address the security problems collaborative network security management system is introduced with collaborative Unified Threat Management (UTM), traffic prober and cloud based security center. The security center can instruct each collaborative UTM and prober to collect raw traffic and this huge traffic is given to the data center which classifies the data in parallel. Security center will deeply analyze the classified data and generates new security rules. These security rules are carried out by collaborative UTM and the feedback events of such rules are given back to the security center. The cloud storage is used to store the huge amount of internet traffic data and then processing it with cloud computing platform to detect the malicious attacks. Security center analyze the data and when any attack is detected it will generate new rules. These rules are given to the network and feedback is evaluated. Then it will remove the invalid rules to make the system more efficient and reliable.
A Mobile Ad hoc NETwork (MANET) is a network that expedites nodes to communicate each other in the absence of a fixed infrastructure. In recent years research issues for securing MANET is burgeoning as it finds use in variety of applications. Security of communication in this type of network is paramount for secure transmission. Routing attacks has drawn sizable consideration since it could cause the most devastating damage to MANET. Even though there prevail many intrusion response techniques to alleviate such critical attacks, existing system usually tries to isolate malicious nodes based on binary or na?ve fuzzy response decisions. Binary responses outcomes in the unexpected network partition, causing additional damages to the network infrastructure and na?ve fuzzy responses could lead to uncertainty in countering routing attacks in MANET. This paper targets on the replica attack in MANET. A serious repercussion of the device capture attack is the node replication attacks in which adversaries deploy a large number of replicas of the compromised/ captured nodes throughout the network. Replicated nodes have all legitimate security credentials and therefore can launch various insider attacks or even take over the network effortlessly. They are undeniably Attack multipliers and therefore are immensely catastrophic to the network. An efficient detection scheme named Sentinel Protocol is proposed for detecting replica attack in MANET. Detailed simulation studies have confirmed the efficiency and effectiveness of the proposed protocol.
This paper demonstrates that the growth in application of corrective actions to enhance network utilization will require a probabilistic treatment of network security for determining efficient levels of investment in network reinforcement. A Benders decomposition based two-stage probabilistic optimization model for the operational and investment problems is proposed. For selecting relevant contingencies (beyond N-1 criteria),a novel filtering technique for efficient elimination of redundant outages is presented and successfully tested. In 2 numerical examples we compare efficiency of network reinforcement propositions under both deterministic and probabilistic frameworks,while optimizing available preventive and corrective control actions,and in particular focusing on the application of generation reserve in combination with Special Protection Schemes (SPS) for network congestion management purposes. We highlight the inadequacies of the deterministic approach with respect to its inherent inability to optimize accurately the portfolio of pre-fault post-fault actions since the impacts of corrective actions (in the form of SPS,demand response) and occurrence of non-credible events require explicit consideration of the likelihood of various outages. We concluded that deterministic approach drives less efficient and potentially more risky system operation that ultimately leads to inefficient network investment.
Intrusion detection is one of the most important problems in network security. Its target is to secure internal networks by identifying unusual access or attacks. Machine learning techniques have been playing a significant role in intrusion detection. Considering the large size of training data and time-consuming labeling task,it is wise to select some informative data to train a classifier. Active learning is a family of approaches selecting samples for labeling to build classifier with maximum prediction accuracy. So it is able to improve the performance of intrusion detection while it is not time-costing and labor-consuming. In this paper,definition and some efficient query strategies of active learning are reviewed and suggested. Some popular algorithms of intrusion detection and the combination of active learning and intrusion detection are also introduced. But existing work of active learning for intrusion detection is very limited. We propose more active learning methods should be developed for intrusion detection.
Attacks against web servers and web-based applications remain a serious global network security threat. Attackers are able to compromise web services,collect confidential information from web data bases,interrupt or completely paralyze web servers. In this study,we consider the analysis of HTTP logs for the detection of network intrusions. First,a training set of HTTP requests which does not contain any attacks is analyzed. When all relevant information has been extracted from the logs,several clustering and anomaly detection algorithms are employed to describe the model of normal users behavior. This model is then used to detect network attacks as deviations from the norms in an online mode. The simulation results presented show that,compared to other data mining algorithms,the method results in a higher accuracy rate.
As a major threat today,how to defense against APT (advanced persistent attack) effectively becomes a major issue for network security. APT is a combination of past attacks,not a new one. It's different from any one of previous attacks. Predicting the attack path of APT exactly would be a breakthrough for the future defense in Internet of things. Firstly,the paper proposes classifications of attack and defense for game model from the perspective of game theory. Then,we present the OAPG model,which uses attack path of APT as the attacker's strategy. Finally,according to the Nash equilibrium,we compute the optimal attack path for the attacker and best-response strategies for the defender.
Taking network security system APPDRR and SAPPDRR model of the whole network as theoretical guidance,this thesis analyzes security risks faced by campus network completely and in full detail,combining with the network system features. The current mainstream of network security technologies such as firewalls,intrusion detection,virtual private network (VPN),certification,and others are carefully studied to illustrate security technologies which shall be focused for the campus network security. Then,taking the Jiangxi Institute of Education campus network as an example,the specific characteristics of campus network are considered to analyze the reality of network security and to design campus network security solutions,which have strong relevance and feasibility.
Recent literature points out the benefits of encrypting signals in the physical layer to improve network security. In particular,a novel all-optical encryption approach uses narrowband optical band-pass filters (OBPF) to split a dense wavelength division multiplexing (DWDM)-compatible signal into several spectral slices which,then,have their physical properties,such as phase and delay,altered;as a result,after multiplexing all slices,an encoded version of the input signal is obtained. The performance of such technique was previously evaluated by assuming that all OBPFs are characterized by ideal rectangular-shaped transfer functions. In this work,we investigate how such performance is affected by the utilization of practical OBPFs with super-Gaussian profiles. Simulation results indicate that there is a trade-off between the filter bandwidth and filter order that may allow for encrypted signals to be properly decoded even after propagation distances up to 400 km,which is typical metropolitan area network distance.
In order to strengthen network security and improve the network's active defense intrusion detection capabilities,this paper presented and established one active defense intrusion detection system which based on the mixed interactive honeypot. The system can help to reduce the false information,enhance the stability and security of the network. Testing and simulation experiments show that: the system improved active defense of the network's security,increase the honeypot decoy capability and strengthen the attack predictive ability. So it has better application and promotion value.
Tunnel is used for network security between two network as it is network communication channel between two network. But tunnel increases header overhead during packet transmission which degrade the quality of network. Thus to improve quality of service in tunnel network one solution is header compression. Nested tunnel is tunnel inside tunnel or more than one tunnels in network. Nested tunnel have more header overhead than the single tunnel in network. Header compression for nested tunneling in network reduces header overhead and save bandwidth . In this paper we have implemented End-to-end Nested Tunnel header compression Protocol with LSB Encoding scheme in wireless network. System is implemented in Microsoft .NET technology platform for all profiles of two tunnels. Results of this implementation gives better compression efficiency as well as better robustness in network. Future work will be design and implement the header compression for nested tunnel with payload compression.
Mobile Adhoc NETwork (MANET) is a wireless network where nodes communicate through other nodes without the aid of a base station. Security is a major challenge in MANET as the packets are prone vulnerability and eavesdropping in wireless environment. Generally MAC layer provides the security in such wireless network through encryption and authentication and the protocol is called WEP. Many authentication and encryption techniques are proposed to increase the security of the MANET. But stronger Security leads to more energy loss as mobiles have less energy and limited processing capability. In this work a Cross layer timestamp based network security technique is developed. The technique reduces the encryption packet overflow which is due to PKE or public key exchange,and derives the public key directly from the neighbor's table which is transmitted using routing information exchange. The simulation is performed with omnet++ simulator. Performance results demonstrate that the energy overhead due to encryption or performance compromise are very low in the proposed system. Further as the protocol is embedded in the network layer it is easily adoptable to any existing architecture without modifying the MAC or Physical layer standard or protocol.
The advancement of internetworking and mobile communication technologies has resulted in both improved network security and at the same time have contributed optimal cryptanalysis techniques. Remote user authentication and mutual authentication among the communicating entities is the critical requirement in remote client-server architecture. Several remote user authentication schemes have been proposed using various combinations like the password (1 Factor Authentication),the smart card (2 Factor Authentication) and biometrics (3 Factor Authentication) by various researchers. Recently,in 2013,Khan et al proposed a biometric based (three factor) remote user authentication scheme and claimed that their scheme is secure against various cryptographic attacks. Unfortunately,in this paper we will show that their scheme is vulnerable to user impersonation attack and the adversary can get the password of a legal user etc.
The past years have shown an increase in the both number and sophistication of cyber-attacks targeting Windows and Linux operating systems. Traditional network security solutions such as firewalls are incapable of detecting and stopping these attacks. In this paper,we describe our distributed firewall solution Distfw and its integration with a sandbox for malware analysis and detection. We demonstrate the effectiveness and shortcomings of such a solution. We use Cuckoo to perform automated analysis of malware samples and compare the results with the ones from manual analysis. We discover that Cuckoo provides similar results in a considerable amount of time.
A microscopic investigation and estimation of rail line capacity are essential in the planning,design,and operation of railway facilities. This research aims at assessing the effects of railway vertical alignment at uphill sections on railway line capacity as well as on capacity loss at such a railway segment. The evaluation of operation and geometry data to estimate rail line capacity and capacity loss due to rail geometric alignment change was conducted using the backup method of semi-simulation analysis tool from THSRC in Taiwan. A Log-linear type regression model was developed to formulate the relationship between rail capacity loss and uphill alignment characteristics. The developed model is capable of analyzing regular rail capacity and capacity changes due to factors of geometric alignment as well as rail operational setting.
Modern network security rests on the Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols. Distributed systems,mobile and desktop applications,embedded devices,and all of secure Web rely on SSL/TLS for protection against network attacks. This protection critically depends on whether SSL/TLS clients correctly validate X.509 certificates presented by servers during the SSL/TLS handshake protocol. We design,implement,and apply the first methodology for large-scale testing of certificate validation logic in SSL/TLS implementations. Our first ingredient is "frankencerts," synthetic certificates that are randomly mutated from parts of real certificates and thus include unusual combinations of extensions and constraints. Our second ingredient is differential testing: if one SSL/TLS implementation accepts a certificate while another rejects the same certificate,we use the discrepancy as an oracle for finding flaws in individual implementations. Differential testing with frankencerts uncovered 208 discrepancies between popular SSL/TLS implementations such as OpenSSL,NSS,CyaSSL,GnuTLS,PolarSSL,MatrixSSL,etc. Many of them are caused by serious security vulnerabilities. For example,any server with a valid X.509 version1 certificate can act as a rogue certificate authority and issue fake certificates for any domain,enabling man-in-the-middle attacks against MatrixSSL and GnuTLS. Several implementations also accept certificate authorities created by unauthorized issuers,as well as certificates not intended for server authentication. We also found serious vulnerabilities in how users are warned about certificate validation errors. When presented with an expired,self-signed certificate,NSS,Safari,and Chrome (on Linux) report that the certificate has expired - a low-risk,often ignored error - but not that the connection is insecure against a man-in-the-middle attack. These results demonstrate that automated adversarial testing - ith frankencerts is a powerful methodology for discovering security flaws in SSL/TLS implementations.
Secondary user data capturing is a fundamental building block for cognitive radio network forensics. It faces great challenges mainly due to the unknown secondary user behavior,wide spectrum,and packet capturing uncertainty. There is a lack of fundamental understanding of the data capturing problem in theory. In this paper,for the first time,we formulate the dynamic sniffer channel assignment problem without the knowledge of users' behavior patterns as a non-stochastic multiarmed bandit (MAB) problem. Moreover,we consider a more practical scenario with the consideration of packet capturing uncertainty and switching cost. We then propose an efficient solution to solve the problem and analyze the regret of our policy. Finally,a simulation study validates the convergence of our method.
This risk of catastrophe from an attack is a consequence of a network's structure formed by the connected individuals,businesses and computer systems. Understanding the likelihood of extreme events,or,more generally,the probability distribution of the number of compromised nodes is an essential requirement to provide risk-mitigation or cyber-insurance. However,previous network security research has not considered features of these distributions beyond their first central moments,while previous cyber-insurance research has not considered the effect of topologies on the supply side. We provide a mathematical basis for bridging this gap: we study the complexity of computing these loss-number distributions,both generally and for special cases of common real-world networks. In the case of scale-free networks,we demonstrate that expected loss alone cannot determine the riskiness of a network,and that this riskiness cannot be naively estimated from smaller samples,which highlights the lack/importance of topological data in security incident reporting.
The anonymous communication protocol Tor constitutes the most widely deployed technology for providing anonymity for user communication over the Internet. Several frameworks have been proposed that show strong anonymity guarantees,none of these,however,are capable of modeling the class of traffic-related timing attacks against Tor,such as traffic correlation and website fingerprinting. In this work,we present TUC: the first framework that allows for establishing strong anonymity guarantees in the presence of time-sensitive adversaries that mount traffic-related timing attacks. TUC incorporates a comprehensive notion of time in an asynchronous communication model with sequential activation,while offering strong compositionality properties for security proofs. We apply TUC to evaluate a novel countermeasure for Tor against website fingerprinting attacks. Our analysis relies on a formalization of the onion routing protocol that underlies Tor and proves rigorous anonymity guarantees in the presence of traffic-related timing attacks.
Fault-tolerant scheduling is an efficient approach to improving the reliability of multiple earth-observing satellites especially in some emergent scenarios such as obtaining photographs on battlefields or earthquake areas. Unfortunately,little work has been done to deal with the fault-tolerant scheduling on satellites. To address this issue,this paper presents a novel dynamic fault-tolerant scheduling model using primary-backup policy to tolerate one satellite's permanent failure at one time instant. On this basis,we propose a novel fault-tolerant satellite scheduling algorithm named FTSS,in which an overlapping technology is adopted to improve the resource utilization. Besides,the FTSS employs the task merging strategies to further enhance the schedulability. To demonstrate the superiority of our FTSS,we conduct extensive experiments by simulations using real-world satellite parameters from STK to compare FTSS with other baseline algorithms. The experimental results indicate that FTSS efficiently improves the scheduling quality of others and is suitable for fault-tolerant satellite scheduling.
In the recent years,web applications have become increasingly popular for delivering security critical services. Because web applications are exposed to various threats and attacks,numerous tools,including commercial tools and open source software,have been developed for detecting web application vulnerabilities,called web vulnerability scanner. Many studies have focused on evaluating web vulnerability scanners by comparing the vulnerability coverage,precision,recall,and time complexity. However,tremendous new attack scenarios and various hacking techniques usually cause erroneous judgement by the scanners and a comprehensive scan often results in redundant vulnerability alerts. Therefore,an efficient detection tools is essential and can be extremely helpful to the users. In this paper,we propose the advanced confusion matrix to estimate the performance of Web vulnerability scanners and then propose a cost-effective approach with three main phases to evaluating vulnerability scanners by additionally considering the reduction of redundant vulnerability alert. We define the redundant alert problem in scanner evaluation based upon two attributes,true duplication (TD) and false duplication (FD). Accordingly,we build up the Web Vulnerability Scanner Testbed,W-VST. Two experiments have been made to evaluate the performance. The experimental results indicate that our evaluation approach can verify the performance of scanners and W-VST is efficient in tool evaluation.
This paper presents new experimental investigations on a newly-proposed Ultra-Wideband (UWB) nanostructured identification and security technique. This technique is dedicated to securing electronic chips by using unique fingerprints created by electromagnetic waves interacting with nanostructured materials. The introduced work focuses on Carbon Nanotubes (CNTs) as a new candidate that is recently proposed by the authors. Physical robustness of this technique is meanwhile investigated with respect to aging effects,temperature variations and group delay measurements. The illustrated results proved the functionality of this new approach for hardware and chip security applications in the UWB range.
Worms are a serious potential threat to computer network security. The high potential speed of propagation of worms and their ability to self-replicate make them highly infectious. Zero-day worms represent a particularly challenging class of such malware,with the cost of a single worm outbreak estimated to be as high as US$2.6 Billion. In this paper,we present a distributed automated worm detection and containment scheme that is based on the correlation of Domain Name System (DNS) queries and the destination IP address of outgoing TCP SYN and UDP datagrams leaving the network boundary. The proposed countermeasure scheme also utilizes cooperation between different communicating scheme members using a custom protocol,which we term Friends. The absence of a DNS lookup action prior to an outgoing TCP SYN or UDP datagram to a new destination IP addresses is used as a behavioral signature for a rate limiting mechanism while the Friends protocol spreads reports of the event to potentially vulnerable uninfected peer networks within the scheme. To our knowledge,this is the first implementation of such a scheme. We conducted empirical experiments across six class C networks by using a Slammer-like pseudo-worm to evaluate the performance of the proposed scheme. The results show a significant reduction in the worm infection,when the countermeasure scheme is invoked.
Wireless sensor networks are usually deployed in harsh environment or a hostile environment,vulnerable to various attacks. A key agreement scheme is proposed in order to solving the problem that traditional key system can not withstand attacks from internal node captured. The way of judgment node credibility is to calculate the trust value by establishing node communication behavior reputation model. Trusted neighbors exchange public key certificate by the base station to generate and signed. A new authenticated key agreement is designed based on each other's Identity information and public key certificates,combined with the elliptic curve cryptosystem. The program can resist the malicious behavior of the internal and external nodes effectively,make network security and survivability enhance significantly compared with other programs,make the network have good scalability,and ensure the key forward security and backward security.
Nigerian or advance fee fraud scams continue to gain prevelance within the world of online classified advertisements. As law enforcement,user training,and website technologies improve to thwart known techniques,scammers continue to evolve their methods of targeting victims and monetizing their scam methods. As our understanding of the underground scammer community and their methods grows,we gain a greater insight about the critical points of disruption to interrupt the scammers ability to succeed. In this paper we extend on previous works about fake payment scams targeting Craigslist. To grow our understanding of scammer methods and how they monetize these scams,we utilize a data collection system posting honeypot advertisements on Craigslist offering products for sale and interact with scammers gathering information on their payment methods. We then conduct an analysis of 75 days worth of data to better understand the scammer's patterns,supporting agents,geolocations,and methods used to perpetuate fraudulent payments. Our analysis shows that 5 groups are responsible for over 50% of the scam payments received. These groups operate primarily out of Nigeria,but use the services of agents within the United States to facilitate the sending and receiving of payments and shipping of products to addresses both in Nigeria and the United States. This small number of scammer organizations combined with the necessity of support agents within the United States indicate areas for potential targeting and disruption of the key scammer groups.
Cross Site Request Forgery (CSRF) is among the most exploited web security vulnerabilities. Yet it has received comparatively less attention. It's a must for web site administrators to protect their web sites from CSRF attacks. In order to let the students master the attack and defense skills of CSRF,it's essential to let them have the opportunity to truly practice the attacks as hackers do,and to practice the defenses as web site administrators do. Yet we haven't seen much research done in the area of developing lab environments to facilitate CSRF education in higher vocational colleges. In this paper,we have shown how teachers in higher vocational colleges can develop a simple lab environment for CSRF attack and defense education. We believe that the developed lab environment will facilitate the education of CSRF attack and defense. And we suggest that a lot more researches need to be done in the area of developments of attack and defense lab environments to improve the outcomes of network security education in higher vocational colleges.
Firewall,as one kind of network security device,is widely used in all kinds of networks and personal computers. This paper focus on the package filter technology of firewall in kernel layer,an intensive study of Windows operating system architecture and NDIS intermediate driver operating principle also has been made. On this basis,an scheme of personal firewall system based on NDIS intermediate driver has been designed,which include filter driver and UI application in kernel mode and user mode respectively.
To increase the security of network system,we propose a network security situation awareness method based on intercepting the threat spread. This method firstly merges security data from different sensors to get the normalization data of network nodes,threats and vulnerability. And secondly,the method intercepts the threat spread by the normalization data. Finally,the method constructs the game analysis among the attacker,defender and neutral. Then the system manager can reinforce the most vulnerable node in real time according to the analysis. Experiment results show that our method can stably improve the security performance of the network system with the system continuously running.
Anonymous communications are important for many of the applications of mobile ad hoc networks (MANETs) deployed in adversary environments. A major requirement on the network is the ability to provide unidentifiability and unlinkability for mobile nodes and their traffic. Although a number of anonymous secure routing protocols have been proposed,the requirement is not fully satisfied. The existing protocols are vulnerable to the attacks of fake routing packets or denial-of-service broadcasting,even the node identities are protected by pseudonyms. In this paper,we propose a new routing protocol,i.e.,authenticated anonymous secure routing (AASR),to satisfy the requirement and defend against the attacks. More specifically,the route request packets are authenticated by a group signature,to defend against potential active attacks without unveiling the node identities. The key-encrypted onion routing with a route secret verification message is designed to prevent intermediate nodes from inferring a real destination. Simulation results have demonstrated the effectiveness of the proposed AASR protocol with improved performance as compared with the existing protocols.
The emerging Internet-of-Things (IoT) are vulnerable to Sybil attacks where attackers can manipulate fake identities or abuse pseudoidentities to compromise the effectiveness of the IoT and even disseminate spam. In this paper,we survey Sybil attacks and defense schemes in IoT. Specifically,we first define three types Sybil attacks: SA-1,SA-2,and SA-3 according to the Sybil attacker's capabilities. We then present some Sybil defense schemes,including social graph-based Sybil detection (SGSD),behavior classification-based Sybil detection (BCSD),and mobile Sybil detection with the comprehensive comparisons. Finally,we discuss the challenging research issues and future directions for Sybil defense in IoT.
For several years,browsers have been able to assure users that they are talking to a specific,identified website,protected from network-based attackers. In email,messaging,and other applications where sites act as intermediaries,there is a need for additional protections to provide end-to-end security. This article describes the approach that WebRTC takes to providing end-to-end security,leveraging both the flexibility of JavaScript and the ability of browsers to create constraints through JavaScript APIs.
As the domain name system (DNS) plays a critical role in malicious services and number of networks, especially small enterprise networks and home networks that are generally and poorly managed, grows rapidly, it is highly desired to outsource the malicious domain detection service to a third-party system that can aggregate information from multiple vantage points to perform detection. To this end, we propose DNSRadar, a system that explores the coexistence of domain cache-footprints distributed in all networks that participate in the outsourcing service. Bootstrapping from a list of prelabeled malicious domains, DNSRadar leverages link analysis techniques to infer maliciousness likelihood of unknown domains based on coexistence information. As DNSRadar only uses the existence of an unknown domain in a network for detection, privacy concerns have been drastically reduced. Both MapReduce and lightweight matrix analysis techniques are employed to implement DNSRadar, making scalability as a built-in feature. Taking advantage of a large number of open recursive DNS servers, we have performed extensive evaluation at scale. Experimental results have demonstrated that DNSRadar can efficiently detect  (sim 90) % malicious domains given a low false positive rate of 1%. Of all these detected malicious domains,  (sim 30) % are on average 6 days earlier than public DNS reputation services, indicating DNSRadars great early detection capability.
The security of computer systems and networks is severely threatened today by the combination of novel attack patterns and high traffic volumes. Together,this often exceeds the capabilities of purely software-based network security systems. As an alternative,hardware acceleration has been employed,e.g.,for performing deep-packet inspection and pattern matching as well as general packet-header processing. While such implementations,capable of handling lower protocol layers,have been extensively studied in research and industry,their extension to higher communication layers has only rarely been addressed. Such capabilities,including the application level (OSI Layer 7),are the focus of this work. We present the NetStage platform,employing reconfigurable computing for high-throughput low-latency network processing,as well as associated development tools that allow networking domain experts to easily customize the system. As a use-case,we consider the realization of high-performance attack-resilient honeypots based on NetStage. To this end,we introduce the Malacoda language,its programming tools,and the generated target microarchitecture. We then evaluate the performance of Malacoda-generated vulnerability emulation handlers running on the NetStage platform.
With the emergence of the Internet of Things (IoT),many devices organized into network,communicate by themselves on the Internet,and send data or private information on the web. It is essential to secure the transmitted data and the identities that may be disclosed to make these new technologies accepted by the largest number of citizens. However,the security mechanisms that are widely used on the Internet are too heavy to be integrated on small constrained objects. This paper describes the current protocols and security solutions that can be deployed in constrained resources. It shows the benefits and the limitations of each scheme-the security extension of IEEE 802.15.4e in time-slotted channel hopping (TSCH) mode,compressed IPsec,datagram transport layer security (DTLS)-embedded at different levels of the OSI model into the 6LoWPAN stack. It opens with the challenge that one must tackle in the coming years. Several use cases are studied to envisage the security integration in cyber physical systems (CPSs) for host-to-host and host-to-network communications. The privacy issue is also addressed and different ways to hide the device identity are discussed.
String matching is a key technique for network security applications such as network intrusion detection systems and antivirus scanners,where the payload of every packet is inspected against thousands of patterns in real time. As the transmission rate of Internet links is getting higher and higher,the speed of matching engines is required to be faster and faster. Existing deterministic finite automaton (DFA)-based approaches achieve high throughput at the expense of extremely expensive memory cost;therefore,they are not suitable for the scenarios where only limited on-chip memory resources are available. To achieve fast matching speed while controlling memory expense,in this paper,we propose Kangaroo,a compact string matching scheme that scans multiple characters each time by running multiple small-sized finite state machines in parallel. Specifically,Kangaroo processes $k$ consecutive characters mostly in one cycle by accessing $k$ different memories in parallel,where $k$ is a predefined factor that can be tuned based on the requirement of applications. Kangaroo is memory efficient. Experimental evaluations on Snort and ClamAV rule sets show that a tenfold increase in speed can be practically achieved by a single Kangaroo matching engine with a reduced memory cost comparing with the state-of-the-art DFA-based approaches.
Around one billion people access the Internet using their mobile phones today,and many of the mobile phones are prone to be compromised by hackers due to their inherited vulnerability. It is critical to identify these compromised mobile phones to effectively eliminate cyber attacks. However,we see few research works in the field. In order to address this desperate situation,we design a practical traceback framework to identify active compromised mobiles in the mobile Internet environment in this letter. In the proposed framework,we creatively use the IMEI number of mobile hardware as unique marks for the traceback purpose. Two-layer traceback tables are designed to collect global attack information and identify local attacking bots,respectively. Our analysis and simulation demonstrate that the proposed traceback method is effective and feasible,and it can identify every possible attacking mobile in the current mobile Internet environment with single packet marking.
Network security requirements based on virtual network technologies in IaaS platforms and corresponding solutions were reviewed. A dynamic network security architecture was proposed, which was built on the technologies of software defined networking, Virtual Machine (VM) traffic redirection, network policy unified management, software defined isolation networks, vulnerability scanning, and software updates. The proposed architecture was able to obtain the capacity for detection and access control for VM traffic by redirecting it to configurable security appliances, and ensured the effectiveness of network policies in the total life cycle of the VM by configuring the policies to the right place at the appropriate time, according to the impacts of VM state transitions. The virtual isolation domains for tenants' VMs could be built flexibly based on VLAN policies or Netfilter/Iptables firewall appliances, and vulnerability scanning as a service and software update as a service were both provided as security supports. Through cooperation with IDS appliances and automatic alarm mechanisms, the proposed architecture could dynamically mitigate a wide range of network-based attacks. The experimental results demonstrate the effectiveness of the proposed architecture.
Secured interpersonal communications should come with asserted user identities and trust between the involved parties. No single trust model exists in Web Real-Time Communications services,so neither is there a single identity architecture. The authors discuss different models for provisioning user identity and their impact on user privacy.
Privacy-preserving data publishing (PPDP) is one of the hot issues in the field of the network security. The existing PPDP technique cannot deal with generality attacks,which explicitly contain the sensitivity attack and the similarity attack. This paper proposes a novel model,(w,,k)-anonymity,to avoid generality attacks on both cases of numeric and categorical attributes. We show that the optimal (w,,k)-anonymity problem is NP-hard and conduct the Top-down Local recoding (TDL) algorithm to implement the model. Our experiments validate the improvement of our model with real data.
The following topics are dealt with: SDN management and orchestration; 5G networks; softwarization; network security; virtual network functions; SLA management; mobile networking; QoE-based bandwith allocation; overlay networks; traffic engineering; protocol stack. Presents the table of contents/splash page of the proceedings record.
In this paper we present a cloud-based research testbed designed to aid network security managers. The testbed enables operators to emulate various network topologies, services, and to analyze attacks threatening these systems. A possibility to test results of network management measures is desired, since testing these measures in a production environment is always not possible. We demonstrate a testbed use case, which aids to scrutinize network behavior under attack. Our use case is based on a large DDoS attack which targeted network infrastructure and web servers in Czech Republic in March, 2013.
The original design of the Internet did not take network security aspects into consideration, instead it aimed to facilitate the process of information exchange between end-hosts. Consequently, many protocols that are part of the Internet infrastructure expose a set of vulnerabilities that can be exploited by attackers. To reduce these vulnerabilities, several security approaches were introduced as a form of add-ons to the existing Internet architecture. However, these approaches have their drawbacks (e.g., lack of centralized control, and automation). In this paper, to address these drawbacks, the features provided by Software Defined Networking (SDN) such as network-visibility, centralized management and control are considered for developing security applications. Although the SDN architecture provides features that can aid in the process of network security, it has some deficiencies when it comes to using SDN for security. To address these deficiencies, several architectural requirements are derived to adapt the SDN architecture for security use cases. For this purpose, OrchSec, an Orchestrator-based architecture that utilizes Network Monitoring and SDN Control functions to develop security applications is proposed. The functionality of the proposed architecture is demonstrated, tested, and validated using a security application.
The following topics are dealt with: information theory; lossy source coding; wiretap channel; wireless communication; caching; ad-hoc networks; network coding; polar coding; quantum channels; entropy; MIMO wiretap channel; message passing; wireless networks; index coding; quantum sensing; f-divergence; degrees of freedom; secrecy; communication theory; network delay; network stability; quantum protocols; hypothesis testing; multiterminal source coding; relay networks; locally repairable codes; Shannon theory; network security; sequential detection; energy harvesting; interference channels; quantum error control codes; Turbo codes; statistics; algebraic coding; multiple access channel; fading channels; channel capacity; optical communications; cellular systems; relay channels; signal processing; Gaussian channel; iterative decoding algorithms; lossless compression; information measures; distributed storage; lattice codes; dirty paper coding; data privacy; distributed information processing; dimensionality reduction; flash memories; compressed sensing; broadcast channels; space-time codes; relaying schemes; finite block length capacity; Reed-Solomon codes; cryptography; biometrics; modulation; demodulation; quantum communication; cognitive radio; bioinformatics; neuroscience; and wireless network protocols.
It is a great pleasure to welcome all of you to the IEEE International Wireless Communications and Mobile Computing Conference (IEEE IWCMC 2014) in the beautiful city of Nicosia, Cyprus! We are indeed delighted that this year's IEEE IWCMC accomplishes its goal under the conference theme Connecting the World, and continues its tradition of providing the premier forum for presentation of research results and experience reporting on the cutting edge research in the general areas of wireless communications and mobile computing. This year, we received more than 595 submissions from 35 countries. Each paper received at least three peer technical reviews, comprised of more than 500 TPC members from academia, government laboratories, and industries. After carefully examining all review reports, the IEEE IWCMC 2014 TPC finally selected about 34% high-quality papers for presentation at the conference and publication in the IEEE IWCMC 2014 proceedings. The conference program starts on Monday August 4th with a full day Tutorials that are free of charge to all our attendees. Then, each day starts with a keynote speaker chosen from renowned world-class leaders in the area-Dr. Mario Gerla, Dr. Fumiyuki Adachi, and Dr. Tarik Taleb, highlighting the latest research trends in the wireless communications, mobile computing, and networks. This year, the technical sessions reflect the continued and growing interests in a wide range of spectrum, including wireless communications and networks, cross-layer design and optimization, mobile computing, wireless sensor networks, network security, and use of wireless technologies in social applications. There are two special sessions composed of invited papers from renowned experts from around the world. Outstanding papers will be selected for possible Special Issues in international journals. Our objective in the future is to reduce the acceptance rate and improve the quality of presentations and discussions furthe- .
TasNetworks own and operate the Network Control System Protection Scheme and provide this service to Basslink Pty. Ltd. NCSPS was designed and implemented to maximise the power transfer capacity on transmission corridor. Due to the evolution of the Tasmanian power system,there is a need to review the network security risks of Temporary Over Voltage (TOV) following certain NCSPS action at Hadspen-George Town 220 kV transmission corridor. Appropriate limit advice and constraint equations are developed and invoked for the secure operation of the power system. This paper describes the system studies and observations used in constructing the TOV limit advice.
With 6.1 trillion text messages sent in 2010 alone,short message service (SMS) is still one of the most popular mobile communication services. Due to its continuing popularity,SMS technology is nowadays used in various fields of application. This also includes security-sensitive fields such as e-banking,or e-government. In these fields,SMS technology is for instance employed to authorize financial transactions or the creation of qualified electronic signatures. Modern smartphone platforms such as Google Android provide application developers with the means to include SMS functionality. This can be beneficial in most cases but also facilitates the implementation of malware that is able to send and receive SMS messages unnoticed by the legitimate end user. In this context,SMS sniffers and SMS catchers have recently attracted attention. This kind of malware intercepts incoming SMS messages either to spy on security-sensitive data transmitted via SMS or to receive SMS-based malware control commands. For security-sensitive SMS-based applications,SMS catchers pose a serious threat. A recent attack on SMS-based e-banking systems has employed SMS catchers on smartphones to steal 36.000.000 Euro from corporate and private bank accounts in Europe. Unfortunately,security software for smartphones is still in the fledging stages and current solutions are not able to reliably detect SMS catchers. To overcome this problem,we introduce different methods to detect SMS sniffers and SMS catchers on smartphones. We discuss benefits and limitations of the proposed methods and show how these methods can be assembled to a comprehensive detection workflow for SMS-based malware. By providing means to detect SMS catchers and sniffers on smartphones,our work contributes to the security of current and future SMS-based applications.
Development of DC power plant controllers in the past has been focused on: ? Introduction and development of industry standard open protocols for remote management and monitoring such as Modbus and SNMP ? Added network security features such as HTTPS and SNMP V3 ? Managing DC power systems with multiple DC voltages +24VDC,?48V DC and 400V DC ? Automated battery discharge and online testing ? Manage new load characteristics introduced by adding server racks in the telecom environment All the above has made the installation and site configuration of a DC power plant more complex and time consuming. The large number of parameters to configure and set up during installation will increase the possibility of mistakes due to human errors. The above also requires coordinated planning for the DC power equipment installation,in many cases Facility,DC power and IT representatives need to be involved before the installation is commissioned and completed. This paper will deal with aspects and procedures on how to design a DC power plant controller based on human-centered design,which involves the following steps: ? How to identify and quantify DC power controller users' pain points and obstacles ? What are the frequency and priority of tasks performed on the DC power plant controller? ? What functions and user interfaces are required to support the installation,operation and maintenance of the DC power plant controller? ? How to deal with backward compatibility The paper describes practical experiences to develop a new DC power plant controller user interface based on human-centered design ideas.
With the increasingly widespread application of computer network,it has become a critical task to detect anomalous behaviors in the field of network security. In this paper we develop an entropy-based statistical approach that determines and reports entropy contents for variables in the Management Information Base. The change of the entropy value indicates that a massive network event or an anomaly may occur. We give the analysis on a real data set provided by a large-size network company. Both our theoretical analysis and experimental results demonstrate that the method is effective and efficient for network anomaly detection.
This paper proposes the concept of application software compatibility and highlights its important implication to high-reliable information application system. Aiming at the issues of typical of software compatibility,an analysis method of software compatibility is presented based on hardware virtualization. Compatibility of two typical information security software is analyzed and the results are evaluated. The proposed analysis method can also be an illumination in other scenarios.
Approximately 25 years ago the ecological interface design approach to developing decision making and problem solving support was formalized in the context of process control work domains. Here we review its sources of inspiration and its subsequent application to two alternative work domains: military command & control and computer network defense. The discussion is organized according to the analytical tools and principles of design that characterize the approach. A representative evaluation is summarized;these results illustrate the potential of this framework for improving overall human-machine system performance.
With the pretty prompt growth in Internet content,future Internet is emerging as the main usage shifting from traditional host-to-host model to content dissemination model,e.g. video makes up more than half of Internet traffic. ISPs,content providers and other third parties have widely deployed content delivery networks (CDNs) to support digital content distribution. Though CDN is an ad-hoc solution to the content dissemination problem,there are still big challenges,such as complicated control plane. By contrast,as a wholly new designed network architecture,named data networking (NDN) incorporates content delivery function in its network layer,its stateful routing and forwarding plane can effectively detect and adapt to the dynamic and ever-changing Internet. In this paper,we try to explore the similarities and differences between CDN and NDN. Hence,we evaluate the distribution efficiency,network security and protocol overhead between CDN and NDN. Especially in the implementation phase,we conduct their testbeds separately with the same topology to derive their performance of content delivery. Finally,summarizing our main results,we gather that: 1) NDN has its own advantage on lots of aspects,including security,scalability and quality of service (QoS);2) NDN make full use of surrounding resources and is more adaptive to the dynamic and ever-changing Internet;3) though CDN is a commercial and mature architecture,in some scenarios,NDN can perform better than CDN under the same topology and caching storage. In a word,NDN is practical to play an even greater role in the evolution of the Internet based on the massive distribution and retrieval in the future.
Conventional database security can employ a wide range of access controls including database roles,fine-grained object access and virtual private databases. Access controls are used to protect against malicious attacks and to ensure that established database privileges are not misused. Intrusion detection systems can augment these controls by alerting the intrusion response team after an attack has occurred. While intrusion detection can assist forensic analysis,a passive response to detection can permit the inflicted damage to go undetected for a long period of time,allowing the damage to potentially propagate. In contrast,we propose an adaptive anomaly-based intrusion prevention system to secure the database from attacks. The approach requires the database to learn the activities considered normal using training data taken from production. The model adapts to stringent variations of the training data while in operation,reducing the potential for normal activities to be misclassified as malicious.
Wireless sensor networks (WSN) are entering the daily life of many. Like wireless LAN before,these are a new playground for developers,but also offer new ways for misuse of this technology. For the improvement of security and to get more valuable data about new attacks,a honeypot for WSNs is in development. This paper presents an effectiveness analysis of a honeypot for WSN and shows detection capabilities in the categories of known and unknown attacks.
The computer network technology is developing rapidly,and the development of Internet technology is more quickly,in this case,people more aware of the importance of network security. Due to the information system in the application of the need for security protection,therefore,the study of computer network security problems are always revolve around the information system. Nowadays,the global computer network security companies and scientific research departments are trying to study and solve the problem of network security,not only developed a variety of maintenance network security hardware and software products,and launched a variety of security of network communication standards and specifications. This article is from the network between the transport layer and application layer,designed a network security system based on Web,and implements a truly safe Internet network.
With the rapid development of computer technology and application of Internet is becoming more and more widely,the Internet plays a more and more important role in people's life. At the same time,all kinds of network security events emerge in endlessly,seriously threaten the application and development of the Internet. With the purpose of safety,network monitoring,have more and more important significance in the maintenance of normal efficiently network run,key facilities,information system security,etc.,. How to realize effective network transmission and efficient online analysis to a huge number of distributed network security monitoring data so as to provide further support for a variety of applications become a major challenge in the field of network security and data processing.
Communication protocol reverse engineering has played an important role in the field of network security. Inferring protocol state machine for unknown protocol is a part of protocol specifications mining. This paper proposed a novel approach in the mining of unknown binary protocol state machine. It allows to automatically generating the state models for binary protocol by listening to network traces. We present a new methodology to align the corresponding fields and extract the state relevant fields from binary protocol communication traces,and then based on the state relevant fields to construct the protocol state model. The experimental results of ARP and TCP show that our approach is effective.
Detailed introduction of the related knowledge of honeypot and intrusion detection technology. From the adaptability,effectiveness and scalability to a detailed analysis of the current intrusion detection system in the existing problems and the honeypot technology is applied to the advantages of the system in Intrusion Detection System,At the same time put forward a detection model and describes the main functions of the model and the system structure based on the intrusion honeypot technology.
According to the underlying IPv6 network of the differential computer system,there are differences in the computer system network,and the security detection has the blind zone. The difference of the underlying IPv6 network in the computer system is analyzed comprehensively. On the basis of this,an improved underlying IPv6 network security detection module design method is proposed for the multi computer system. The detailed module design plan is obtained. In the network security detection system,the new technologies such as receiving thread domain,object forwarding domain and file buffer are taken into consideration. It can solve the security detection difficult problem caused by the IPv6 network differences in underlying computer system. Experimental results show that this system can detect the security problem in differential computer system quickly and accurately. The data packet forwarding success rate has been increased from 77% to 90%,and the acquisition success rate of abnormal data packet in terminal increases from 67% to 88%. It shows good application value in security detection of differential computer system.
Smart grids are the next generation of electrical grids,enabling the better management and leveling of power consumption by suppliers. Via the use of automatic meter reading,smart grid also provides better information to the end-users,making it possible to enhance their energy consumption and adapt it according to the current energy price,availability and other factors. As the grid becomes more and more reliant to ICT and communication networks,risks related to cybersecurity and privacy have to be taken into account. The link between automatic meters and the distribution operator has to be protected from security breaches that may lead to false billing and the transferred data has to be protected as it contains sensitive information about household and business behavior. In this article we present a limited state-of-the-art review as well as a network security monitoring setup for small-scale laboratory that is in essence a small scale smart grid environment. We discuss about the challenges and threats that are possible in the smart grid environment and the feasibility of using network security monitoring techniques that represent our work-in-progress research in this context.
Network forensics is addressed to deal with cybercrime. The main purpose of a network forensics system is reconstructing evidences of network attacks. In order to reconstruct evidence,the network attack is firstly identified. Therefore,network attack detection solutions play an important role in network forensics. There are two main types of network attacks: network level and application level. Network level attack detection solutions focus on the information in the headers of network packets. While,application level attack detection solutions investigate the data fragments carried out in the packet payloads. We propose an approach based on Shannon entropy and machine learning techniques to identify executable content for anomaly-based network attack detection in network forensics systems. Experimental results show that the proposed approach provides very high detection rate.
In response to the urgent needs of current users of network security services,we propose a security routing technology which is based on node trusted degree. By collecting and integrating the evidence of identity and behavior,such as the security,reliability and availability,we build the node trusted measurement model. Based on this model,the safe transmission and diffusion of information is guaranteed by the OSPF routing protocol and CPK security authentication mechanism,and we can generate the security optimal path under the node's trusted degree and bandwidth constraints. Simulation results show that the improved security routing protocol has better effectiveness and dynamic adaptive capacity when the network is attacked.
As Ajax webpage can be built by Javascript function,Ajax makes it possible to send asynchronous requests to the server and handle the response in the background. It is not definite that the current URL reflects the current state of the webpage. The traditional approach of security crawler can not walk through the Ajax web application to collect all the interface information for web security testing. In this page,we will introduce several benefits of Ajax web application and some technical differences about the crawler and propose a new model based on finite-state machine with double filter strategy to make it possible for Ajax web application security test automatically. Additionally,the crawler will be highly compatible with the metaspolit framework.
A new modified mathematics model is proposed based on fuzzy mathematics theory and adaptive weighted algorithm aimed at how to analyze the influence on communication network security. The proposed model is associated with the attention degree on security defense is executed. In the proposed model,the communication network security threat,communication network attack approach,network security character and network security counter character are quantified,and influence on communication network security on the condition of security defense is carried out effectively is analyzed quantificational and is modeled. The effectiveness of the algorithm is proved by the simulation results.
Intrusion detection system (IDS) is becoming an integral part of the network security infrastructure. Data mining tools are widely used for developing IDS. There is a lack of researches in the temporal data mining analysis of the intrusions (intrusions detection over different time periods). Most of researches are focusing on the latest snapshot data mining of intrusion detection systems. This work presented in this paper proposes a new temporal data mining analysis technique of intrusion detection systems based on na?ve Bayes networks. The presented system considered the time dimension and built many different classifier models to obtain an accurate analysis of intruders. The obtained results give more focusing and deep understanding of the intruders' behavior during the different time periods and illustrate the shrinking and expansions of intruders' classes over the time slices (the migrations of intruders from one segment to another),The temporal analysis of intruders can help in taking an appropriate decision against specific type of attacks (decisions must be suitable with the intruder behaviour). The results indicate the reduction of the possible high positive false rate.
The great popularity of the Internet increases the concern for the safety of its users as many malicious Web pages pop up in daily basis. Client honeypots are tools,which are able to detect malicious Web pages,which aim to infect their visitors. These tools are widely used by researchers and anti-virus companies in their attempt to protect Internet users from being infected. Unfortunately,cyber-criminals are becoming aware of this type of detection and create evasion techniques that allow them to behave in a benign way when they feel to be threatened. This bi-faceted behavior enables them to operate for a longer period,which translates in more profit. Hence,these deceptive Web pages pose a significant challenge to existing client honeypot approaches,making them incapable to detect them when exhibit the aforementioned behavior. In this paper,we mitigate this problem by designing and developing a framework that benefits from this bi-faceted behavior. Our main goal is to protect users from being infected. More precisely,we leverage the evasion techniques used by cyber-criminals and implement a prototype,called Scarecrow,which triggers false alarms in the cases of deceptive Web pages. Consequently,the users that use Scarecrow for Web surfing can remain protected,even if they visit a malicious Website. We evaluate our implementation against malicious URLs provided by a large anti-virus company and show that when Scarecrow is deployed,malicious Websites with bi-faceted behavior do not launch their attacks against normal users.
Firstly,analyzes the characteristics of the smart grid network,this paper expounds the importance of information network security in the safe operation of smart grid. Based on the analysis of power information security protection framework,pointing out the importance of establishing the information safety monitoring system for smart grid. Secondly,according to the mainfunctions and characteristics of the smart grid information new elements are analyzed. Various risks as well as malicious attack scenarios to the intelligent power network are analyzed from the aspect of information security,ranging from the information collection,transmission,management and interaction. Finally,some feasible countermeasures and the improvement measures based on modern power system developments are given to enhance the information security of the intelligent power system in key technologies,standards,and policies.
Extreme weather conditions such as hurricanes,heavy thunderstorms,and severe snow/ice storms could have serious impact to the secure operations of electrical distribution network especially in the city/metropolitan areas and may lead to large scale of power outage to the customers. This paper presents a distribution network outage pre-warning modeling and analytic method under the extreme weather conditions. First,the correlation model between the electrical equipment failure and its loading pressure is established based on the improved loading pressure model and the weather forecasting information;Second,the device failure probability models for overhead lines and transformers are built based on the calculation of the device failure probability;third,the distribution network security evaluation model is developed for the current network operation condition with the optimal objective of minimal load shedding;last,individual customer's outage probability are calculated and the risk evaluation are provided. The simulation results using the standard IEEE system shows that the proposed models and analytic method could effectively improve utility's capability of distribution network outage pre-warning and preparation. The proposed methodology could also be applied to the transmission network outage management
Nowadays,the mobile internet security problems are more and more prominent. Network protocol analysis system can eliminate the network faults and optimize network in order to improve the security of the network. This paper briefly introduces the WinPcap technology. Using its powerful programming function,we have designed a network protocol analysis system. When an android mobile phone access to the internet,for example,it can circularly capture packets and analyze the typical network protocols. Experimental results show that the system can provide a basic tool to solve the problems of network security of mobile intelligent terminals.
Honeypots have been extensively used,as a very powerful anti-phishing tool,by IT security experts and financial institutions to gather spurious mails of phishing kind. This has helped the security service providers to detect new phishing sites and quickly shut them down. Honeypots are also deployed to collect critical information about activities of people involved in phishing,helping in generation of statistical data to later aid in security research and forensic investigations. More recently,active feeding of phishers with honeytokens is also proposed as a proactive security mechanism,in line with the taking the war to their home approach. In this research paper,we elaborate certain problems of anti-phishing solutions based on honeypots being used currently. We propose to minimize or overcome these limitations/problems by performing the makeover of real online banking system into a large honeypot armed with honeytokens. This large honeypot will be supported by some additional honeypots,to make it more powerful. A phishing attempt detection algorithm,called PhishDetekt,is used to automatically sense dubious phishers' bids of stealing money from victims' bank accounts. The system asks for the potential victim's reconfirmation for the transaction under suspicion. This results in development of a new honeypot-based anti-phishing framework. As a vital component of the proposed framework,we also propose to use virtual honeypots-emulating agents to mimic behavior of real users to access the Online banking system regularly. The main objective of such agents will be to submit honeytokens to phishing malware and to take the fight against phishers to their own territory.
IEEE 802.15.4/ZigBee Wireless sensor networks are now becoming popular due to rapid increase in the number of very small,integrated low-powered sensors and mobile devices capable of gathering critical,real-time information for remote surveillance,environmental monitoring,distributed target tracking etc. ZigBee is the only IEEE standard uniquely designed for low-power,a low-cost,low-data rate based wireless technology network,that provides network security,and application support services operating on top of the IEEE 802.15.4 Medium Access Control (MAC) and Physical (PHY) Layer wireless standard[1]. The survey on IEEE 802.15.4/ZigBee sensor networks showed that the unreliability is the major limitation of such sensor networks,which is termed as MAC unreliability problem. The studies also highlighted that,this unreliability problem is due to the default MAC parameter settings suggested by the standard itself. Different solutions have come up to improve the reliability problem,such as setting a predefined value for parameters which is suitable only for static conditions,an adaptive technique for dynamic conditions based on mathematical model etc. In this proposed work,the impact of MAC parameters on packet delivery ratio and the effectiveness of ADAPT algorithm is analyzed for a ZigBee sensor network. A comparative study of ZigBee network with and without ADAPT is carried out based on simulated results obtained from NS2,which is an object oriented network simulating tool. As a modification,signature verification of each node on the sensor network is performed for security purposes.
Distinction between humans and web robots,in terms of computer network security,has led to the robot detection problem. An exact solution for this issue can preserve web sites from the intrusion of malicious robots and increase the performance of web servers by prioritizing human users. In this article,we propose a density based method called DBC_WRD (Density Based Clustering for Web Robot Detection) to discover the traffic of web robots on two large real data sets. So,we assume the visitors as the spatial instances and introduce two new features to describe and distinguish them. These attributes are based on the behavioral patterns of web visitors and remain invariant over time. By focusing on one of the disadvantages of DBSCAN as the density based clustering algorithm used in this paper,we just utilize 4 features to reduce the dimensions. According to the supervised evaluations,DBC_WRD can have the 96% of Jaccard metric and produce two clusters which have the entropy and purity rates of 0.0215 and 0.97,respectively. Furthermore,the comparisons show that from the standpoint of clustering quality and accuracy,DBC_WRD performs better than state-of-the-art algorithms. Finally,it can be concluded that some non-malicious popular web robots,through imitating the human's behavior,make it difficult to be identified.
Using a field portable,smartphone fluorometer for assessing water quality,a pH map of drinking water around Sydney is obtained. The work demonstrates a new security concept  network forensics  based on a novel smartgrid approach for the potential detection of water quality disruption.
Software defined networking brings many possibilities to network security,one of the most important security challenge it can help with is the possibility to make network traffic pass through specific security devices,in other words,determine where to deploy these devices logically. However,most researches focus on high level policy and interaction framework but ignored how to translate them to low-level OpenFlow rules with scalability. We analyze different actions used in common security scenarios and resource constraints of physical switch. Based on them,we propose a rule translation implementation which can optimize the resource consumption according to different actions by selecting forward path dynamically.
We introduce the concept of a multi-trapdoor hashing scheme that allows multiple entities to compute a collision with a given hash value. Unlike previous proposals that use of multiple trapdoors to strengthen security of conventional trapdoor hash functions,the proposed use of multiple trapdoors in hash functions is motivated by potential applications in security schemes involving multiple participants. We present two such applications that include preliminary designs of aggregate signatures and sanitizable signatures with support for multiple sanitizers. We also present a discrete log-based instantiation of the proposed concept using a novel key-exposure free trapdoor hash function. Finally,we analyze the proposed instantiation in terms of its security and performance.
Many important network security areas,such as Intrusion Detection System and Next-Generation Firewall,leverage Traffic Classification techniques to reveal application-level protocols. Machine Learning algorithms give us the ability to identify encrypted or complicated traffic. However,classification accuracies of Machine Learning algorithms are always facing challenges and doubts in practical usage. In this paper,we propose a time-varying Logistic Regression model embedded with traffic pattern. The comparison between original Logistic Regression model and time-varying one shows an effective improvement in accuracy. We hope to exploit a new way to implement Machine Learning algorithms in network traffic analysis areas by considering the characteristics of traffic changes in time domain.
The electrical power grid forms the function foundation of our modern societies,but in the near future our grids will reach a limit due to increased demand and aging infrastructures. As a result nations worldwide have started to convert their power grids into modern,dynamic grids with improved communications and control systems. The Smart Grid will thus be better able to incorporate new forms of energy generation,as well as be self-healing and more reliable. This work looks at the security threat to wireless mesh networks (WMN) from the quantum computer attack,more specifically,we argue for the use of Merkle Trees [1] as opposed to public key encryption for authentication of devices in WMN when used on the Smart Grid. This is an ongoing work on our research on wireless network security [5,6].
The Internet protocol version 6 (IPv6) was designed with security in mind. However,from the survey of network security community,the No.1 risk today is the lack of IPv6 security knowledge. The coexistence of IPv4 and IPv6 supported by transition technology complicates the security management. Weak v6 security policies are a direct result of the current deficit in IPv6 security knowledge. To solve the problem,the IP Network Threat Ontology,and the Anti-threat Tools Ontology were proposed to collect and compare various network threats on IPv4 and IPv6. Thus,the ontology-based anti-threat decision support system can be developed to support the decision making of security policies.
The main goal of this paper is to present of security evaluation of block cipher use in wireless network. Information security contributes directly to increase level of trust between users by providing an assurance of confidentiality,integrity,and availability of sensitive information of the network. In this paper,evaluations analysis of RC5,Blowfish and DES block cipher algorithms have been done on the basis of block size,numbers of round,and key size of evaluation modeling based on fuzzy logic tool of MATLAB 2012a. Three metrics are considered for using crypto algorithms for wireless network security. These three algorithms have a variable block size,number of rounds and a variable key size in their structure. Performances of RC5,Blowfish and DES algorithms have been evaluated on variables range of metrics of key size,number of rounds,and block size in this paper. Presents the results of evaluation for security level of crypto have shown the security level depending on the structure of algorithm.
Many network security applications such as Intrusion Detection System (IDS),Firewall and Data Loss Prevention System (DLPS) are based on deep packet inspection,in this packets header as well as payload of the packets are checked with predefined attack signature to identify whether it contains malicious traffic or not. To perform this checking different pattern matching methods are used by NIDS. The most popular method to implement pattern matching is to use of Finite Automata (FA). Generally,regular expressions are used to represent most of the attack signatures defined by NIDS. They are implemented using finite automata,which takes the payload of packet as input string. However,existing approaches of Finite Automata (FA),both deterministic finite automata (DFA) and non-deterministic finite automata (NFA) for pattern matching are having their own advantages and some drawbacks. The DFA based pattern matching methods are fast enough but require more memory. However,NFA based pattern matching methods are comparatively takes less memory but the speed of matching is very slow,to overcome these drawbacks of finite automata there are many approaches have been proposed. This paper discuses comparative study of some Finite Automata (FA) based techniques for pattern matching in network intrusion detection system (NIDS).
Recently cyber hacking incidents and accidents increase significantly so that the damage has spread to businesses,society and national level. It increases the external and internal threats significantly to resources on the corporate network by the malicious software diffusing through the internet. As web security is emerged as the most important security,it has emerged as core topics in security research. We analyze the web sites which are exposed to the particular web vulnerability to highlight the importance of security technologies and to wake the security awareness. Also we build a system to find unencrypted packets through real-time packet monitoring,and furthermore propose to build the applied encryption system.
The frequency and the extent of damages caused by network attacks have been actually increasing greatly in recent years,although many approaches to avoiding and detecting attacks have been proposed in the community of network security. Thus,how to fast detect actual or potential attacks has become an urgent issue. Among the detection strategies,behavior-based ones,which use normal access patterns learned from reference data (e.g.,History traffic) to detect new attacks,have attracted attention from many researchers. In each of all such strategies,a learning algorithm is necessary and plays a key role. Obviously,whether the learning algorithm can extract the normal behavior modes properly or not directly influence the detection result. However,some parameters have to determine in advance in the existing learning algorithms,which is not easy,even not feasible,in many actual applications. For example,even in the newest learning algorithm,which called FHST learning algorithm in this study,two parameters are used and they are difficult to be determined in advance. In this study,we propose a parameter less learning algorithm for the first time,in which no parameters are used. The efficiency of our proposal is verified by experiment. Although the proposed learning algorithm in this study is designed for detecting port scans,it is obviously able to be used to other behavior-based detections.
Smart grid technologies such as synchrophasors using Phasor Measurement Units (PMUs),make real-time monitoring,control and data analysis of the electric power grid possible. The PMU network measures voltage and current phasors across the electrical power grid,and sends reports to control centers. Synchrophasor technology enables reliable and efficient power system operation;but may make the system vulnerable to cyber-attacks. In this paper,security vulnerabilities found in literature,that are relevant to PMUs,are discussed and mapped to four general attack classes. Known network security vulnerabilities are addressed in hopes of exposing gaps where further research needs to be conducted on PMU networks.
Rising penetration of renewable energy sources in present day transmission systems requires increased attention to network security. Existing computation tools in power system operations evaluate individual scenarios for power injection and network configuration but do not fully consider nearby regions in the operating space. Such tools may lead to market transactions,preventive actions,or corrective actions that are nominally efficient but poor in general. In this paper,the interval based ISI method is reformulated to a security oriented form potentially applicable in energy management systems. The presented results include an algorithm defined within a tractable optimization framework that computes maximal power injection sets containing power injection profiles that are necessarily secure in terms of physical constraints and the N-1 security criterion. The method is tested on the IEEE 14 bus test system.
Network forensics is an emerging interdiscipline used to track down cyber crimes and detect network anomalies for a multitude of applications. Efficient capture of data is the basis of network forensics. Compared to traditional networks,data capture faces significant challenges in cognitive radio networks. In traditional wireless networks,usually one monitor is assigned to one channel for traffic capture. This approach will incur very high cost in cognitive radio networks because it typically has a large number of channels. Furthermore,due to the uncertainty of the primary user's behavior,cognitive radio devices change their operating channels dynamically,which makes data capturing more difficult. In this paper,we propose a systematic method to capture data in cognitive radio networks with a small number of monitors. We utilize incremental support vector regression to predict packet arrival time and intelligently switch monitors between channels. We also propose a protocol that schedules multiple monitors to perform channel scanning and packet capturing in an efficient manner. Monitors are reused in the time domain,and geographic coverage is taken into account. The real-world experiments and simulations show that our method is able to achieve the packet capture rate above 70% using a small number of monitors,which outperforms the random scheme by 200%300%.
Malware is pervasive in networks,and poses a critical threat to network security. However,we have very limited understanding of malware behavior in networks to date. In this paper,we investigate how malware propagates in networks from a global perspective. We formulate the problem,and establish a rigorous two layer epidemic model for malware propagation from network to network. Based on the proposed model,our analysis indicates that the distribution of a given malware follows exponential distribution,power law distribution with a short exponential tail,and power law distribution at its early,late and final stages,respectively. Extensive experiments have been performed through two real-world global scale malware data sets,and the results confirm our theoretical findings.
Network intrusion detection and prevention systems commonly use regular expression (RE) signatures to represent individual security threats. While the corresponding deterministic finite state automata (DFA) for any one RE is typically small,the DFA that corresponds to the entire set of REs is usually too large to be constructed or deployed. To address this issue,a variety of alternative automata implementations that compress the size of the final automaton have been proposed such as extended finite automata (XFA) and delayed input DFA (D$^{2}$ FA). The resulting final automata are typically much smaller than the corresponding DFA. However,the previously proposed automata construction algorithms do suffer from some drawbacks. First,most employ a Union then Minimize framework where the automata for each RE are first joined before minimization occurs. This leads to an expensive nondeterministic finite automata (NFA) to DFA subset construction on a relatively large NFA. Second,most construct the corresponding large DFA as an intermediate step. In some cases,this DFA is so large that the final automaton cannot be constructed even though the final automaton is small enough to be deployed. In this paper,we propose a Minimize then Union framework for constructing compact alternative automata focusing on the D $^{2}$FA. We show that we can construct an almost optimal final D$^{2}$ FA with small intermediate parsers. The key to our approach is a space- and time-efficient routine for merging two compact D $^{2}$FA into a compact D $^{2}$FA. In our experiments,our algorithm runs on average 155 times faster and- uses 1500 times less memory than previous algorithms. For example,we are able to construct a D$^{2}$FA with over 80$,$000$,$000 states using only 1 GB of main memory in only 77 min.
The main type of obstacles of practical applications of quantum key distribution (QKD) network are various attacks on detection. Measurement-device-independent QKD (MDIQKD) protocol is immune to all these attacks,and thus,a strong candidate for network security. Recently,several proof-of-principle demonstrations of MDIQKD have been performed. Although novel,those experiments are implemented in the laboratory with secure key rates less than 0.1 b/s. Besides,they need manual calibration frequently to maintain the system performance. These aspects render these demonstrations far from practicability. Thus,justification is extremely crucial for practical deployment into the field environment. Here,by developing an automatic feedback MDIQKD system operated at a high clock rate,we perform a field test via deployed fiber network of 30 km total length achieving a 16.9 b/s secure key rate. The result lays the foundation for a global quantum network,which can shield from all the detection-side attacks.
The computation of the entropy of a high-speed data stream in a one-pass fashion is crucial to many network security applications. Motivated by the work of Lall et al.,this study examines the design trade-off of processing speed and accuracy for estimating the entropy norm. The proposed scheme leverages the Count Sketch with constant memory access on counter update and point query operations. With a bounded relative error and a constant memory access cycle,the design can process incoming traffic with a throughput of 30 Gbit/s.
Software-defined network (SDN) is the next generation of networking architecture that is dynamic,manageable,cost-effective,and adaptable,making it ideal for the high-bandwidth,dynamic nature of todays applications. In SDN,network management is facilitated through software rather than low-level device configurations. However,the centralized control plane introduced by SDN imposes a great challenge for the network security. In this paper,we present a secure SDN structure,in which each device is managed by multiple controllers,not just a single as in a traditional manner,with the dynamic and isolated instance provided by the cloud. It can resist Byzantine attacks on controllers and the communication links between controllers and SDN switches. Furthermore,we study a controller minimization problem with security requirement and propose a cost-efficient controller assignment algorithm with a constant approximation ratio. From the experiment result,the secure SDN structure has little impact on the network latency,provide better security than general distributed controller,and the proposed algorithm performs higher efficiency than random assignment.
As part of this special issue on control systems for the energy sector,guest editors Sean Peisert and Jonathan Margulies put together a roundtable discussion so readers can learn about the security challenges facing the industrial control system/SCADA world from those who are on the front lines. The discussion touches on some of the hard problems of securing mission-critical systems in the real world,including the challenges of securing 20-year-old legacy infrastructures,defining vendors' roles and responsibilities in security,and where research and new technologies are needed to fill today's security gaps.
In general,a smart grid is a modernized electrical grid that uses digital technology for measurement,control,and protection functions to ensure a network security. It tries to solve the problem of weather-dependant fluctuations of renewable energy power supplies (e.g. wind turbines,or photo-voltaic systems) when they are connected to an actual power system. In two papers in this issue,we present some of the challenges raised by Smart Grids in instrumentation and measurement applications,putting emphasis on synchrophasor estimation. In this part 1 article,we describe the problem of identifying a normal condition from a fault condition and between a fault condition and an oscillation using phasor estimations in protective relays. In "Synchrophasor Measurement Challenges in Smart Grids," we discuss a novel synchrophasor- estimation algorithm that improves the accuracy of the estimates under oscillations conditions and serves to identify electromechanical modes in Smart Grids. This algorithm ameliorates protection as well as measurement applications in smart grids.
Software Defined Networks (SDN) are becoming a trending technology in modern Internet. This technology helps to solve a significant number of well-known engineering problems in a effective and elegant way as they provide software-defined centralized network control. An SDN controller can be extended with application that effectively serve for concrete purposes and provide flexible management of network flows. This opens a great number of opportunities for a lot of network security problems such as maintaining of privileges in a proper way,splitting control and data planes,and attacks detection and mitigation. In this work we consider the opportunities of SDN for a survival mitigation during DDoS attacks,the load balancing problem. We propose two-level balancing solution in SDN networks,which includes traditional balancing between servers and load balancing between network devices as well. Experiments show that our solution increase survival time of a system during DDoS attack in times compared to existing balancing solution in SDN networks.
Software Defined Networking,SDN,is the programmable separation of control and forwarding elements of networking that enables software control of network forwarding that can be logically and/or physically separated from physical switches and routers. The following important question is considered in this paper: to what extent can SDN-based networks address network security management problems? The new opportunities for enhancing network security brought by this separation are considered in the paper
Survivability is a key attribute to measure large-scale network security and has become a hot research issue. Large-scale network is a complex system,which has the typical survivability characteristics of complex system. The current network survivability studies usually aim for a specific aspect of network. In this paper,we attempt to study the survivability of large-scale network based on the characteristics of complex system. Firstly,the concepts and features of large-scale network survivability are described. Secondly,the definition and process of large-scale network survivability are proposed based upon the information entropy theory. Thirdly,the distance and time measurement of large-scale network survivability are proposed to measure the degree of large-scale network survivability. Finally,the proposed model are validated to accurately measure large-scale network survivability through case studies.
Recently,with wide use of computer systems,internet,and rapid growth of computer networks,the problem of intrusion detection in network security has become an important issue of concern. In this regard,various intrusion detection systems have been developed for using misuse detection and anomaly detection methodologies. These systems try to improve detection rates of variation in attack types and reduce the false positive rate. In this paper,a new intrusion detection method has been introduced using MinMax K-means clustering algorithm,which overcomes the shortage of sensitivity to initial centers in K-means algorithm,and increases the quality of clustering. The experiments on the NSL-KDD data set indicate that the proposed method is more efficient than that based on K-means clustering algorithm. Also,the method has higher detection rate and lower false positive detection rate.
VoIP or Telephony systems are increasingly replacing PSTN infrastructures for its easy management and low cost. While moving telephony to the public IP platform broadens its service capabilities,some security problems may occur because the amount of threats existing in IP networks and also the implementation weaknesses of IP based telephony systems. In this paper for evolving effective countermeasures for this ample array of attacks and providing available,reliable and qualitics VoIP services to users,a secure architecture was proposed by deploying honeypot idea. A honeypot is an information system resource. It intentionally put in harm's way to be compromised by attackers. If deployed in relevant location,it interacts just like real systems and obtains valuable information from attacker activities. We aim to protect VoIP infrastructure from different types of attacks after analyzing and evaluating this captured data favor other defense systems.
By daily increasing appearance of vulnerabilities and various ways of intruding networks,one of the most important fields in network security will be doing network hardening and this can be possible by patching the vulnerabilities. But this action for all vulnerabilities may cause high cost in the network and so,we should try to eliminate only most perilous vulnerabilities of the network. CVSS itself can score vulnerabilities based on amount of damage they incur in the network but the main problem with CVSS is that,it can only score individual vulnerabilities without considering its relationship with other vulnerabilities of the network. So,in order to help fill this gap,in this paper we have defined some Attack graph and CVSS-based security metrics that can help us to prioritize vulnerabilities in the network by measuring the probability of exploiting them and also the amount of damage they will impose on the network. Proposed security metrics are defined by considering interaction between all vulnerabilities of the network. So our method can rank vulnerabilities based on the network they exist in. Results of applying these security metrics on one well-known network example are also shown that can demonstrates effectiveness of our approach.
IEEE 802.16 based WiMAX is an emerging wireless Internet technology. Salient features of WiMAX such as high speed internet facility over a long distance,quality of service,scalability,security,and mobility proves it better than Wi-Fi Internet access. Security is a vital requirement to prevent WiMAX network from the various attacks and to increase the reliability. This survey paper presents the threats associated with the layers in WiMAX along with possible solutions. The paper reviews the physical layer threats i.e. scrambling and jamming,MAC layer threats i.e. user authentication and data confidentiality,routing layer threats i.e. black-hole attack and other miscellaneous attacks e.g. Man-in-the-Middle (MITM),Denial of Service (DoS) and Bandwidth Spoofing. The paper observes that jamming attack and eavesdropping of management messages are the most destructive attacks for WiMAX network.
The development of computer network technology has changed the single machine oriented calculation model,however,the risks of network intrusion also are increasing greatly. Design security measures to prevent unauthorized access to system resources and data become an important and urgent problem in the network security field. At present,it is not realistic to completely avoid the security incidents. What researchers can do is try to discover the intrusion as soon as possible to take effective measures to plug the loopholes and repair the system,which is called as intrusion detection research. Intrusion detection plays an important role in system security. This paper firstly presents the necessity,definition and principles of intrusion detection. Then we introduce the most popular intrusion detection algorithms: Boyer-Moore (BM) and Aho-Corasic (AC) algorithms. Finally the improved algorithms based on them,Comments-Walter algorithm and IACBM,are presented in this paper.
Reconfigurable computing has grown to become a large and important field of research. Implementing network security algorithms on reconfigurable platform provides major benefits over VLSI and software platforms since they offers high speed similar to VLSI and high flexibility similar to software platforms. The paper presents efficient implementation of DES algorithm on XC25200 FPGA. The synthesis result shows that with this kind of implementation only 2118 slices and 97 number of bonded IOBs are utilized as compared to 2151 slices and 186 number of bonded IOBs with normal implementation.
Analysing forensics evidence is an essential step in proving the malicious intents of an attacker or adversary and the severity of the damages caused to any network. This paper presents how security metrics can be used to sustain a sense of credibility to network evidence gathered as an elaboration and extension to an embedded feature of Network Forensic Readiness (NFR) - Redress that is defined as holding intruders responsible. We apply the Common Vulnerability Scoring System (CVSS) metrics to show that a forensics metrics system could assess the severity of network attacks committed,thus giving a degree of credibility to the evidence gathered. This way,hard evidence could be objectively collected to lend support to the resource-intensive process of investigation and litigation,leading to successful conviction,while reducing effort expended on the process.
Applying of network security situation awareness in smart substations provides a high level view to understand the security situation of substations. After analyzing security threats faced by smart substations,this paper not only proposes a security situation awareness framework from aspects of physical security,network security,system security,protocol security and data security,but also puts forward a network security monitoring solution for substations,which uses a kind of dedicated smart substation network security situation awareness devices to collect security events,analyze them and generate situation analysis results. Further,we focus on several key technologies used in our framework. Finally,we describe a design scheme including hardware and software for the smart substation network security awareness system and its implementation architecture,in which a multi-layer software architecture is adopted by the system,and a combinatory of the Intel? Sandy Bridge,the bus structure and a modular design technology,is adopted in order to meet the need of high performance.
Router security analysis plays a vital role in maintaining network security. However,IOS,which runs in Cisco routers,has been proved carrying serious security risks. And in order to improve security,we need to conduct vulnerability mining on IOS. Currently,Fuzzing,as a simple and effective automated test technology,is widely used in vulnerability discovery. In this paper,we introduce a novel testing framework for Cisco routers. Based on this framework,we first generate test cases with Semi-valid Fuzzing Test Cases Generator (SFTCG),which considerably improves the test effectiveness and code coverage. After that,we develop a new Fuzzer based on SFTCG and then emulate Cisco router in Dynamips,which makes it easy to interact with GDB or IDA Pro for debugging. In order to supervise the Target,we employ a Monitor Module to check the status of the router regularly. Finally,through the experiment on ICMP protocol in IOS,we find the released vulnerabilities of Ping of Death and Denial of Service,which demonstrates the effectiveness of our proposed Fuzzer.
Monitoring communication networks and their traffic is of essential importance for estimating the risk in the Internet,and therefore designing suited protection systems for computer networks. Network and traffic analysis can be done thanks to measurement devices or honeypots. However,analyzing the huge amount of gathered data,and characterizing the anomalies and attacks contained in these traces remain complex and time consuming tasks,done by network and security experts using poorly automatized tools,and are consequently slow and costly. In this paper,we present an unsupervised method for classification and characterization of security related anomalies and attacks occurring in honeypots. This as automatized as possible method does not need any attack signature database,learning phase,or labeled traffic. This corresponds to a major step towards autonomous security systems. This paper also shows how it is possible from anomalies characterization results to infer filtering rules that could serve for automatically configuring network routers,switches or firewalls.
The recent emergence of cloud enabled applications raises security concerns increasingly,since more and more personal and company data is outsourced. The security of single systems and services was broadly treated in the past. Cloud systems and services require a more detailed observation of their security requirements and fulfillment,since a huge amount of services and systems coexist on one virtualization layer without knowing other systems on the same layer. Only the cloud provider has a rare idea of these systems' behavior in his own cloud environment. Therefore this work proposes a network security approach which is aware of all existing systems and services hosted by at least one cloud provider. The main idea is to maintain a logically centralized database that provides latest security related information about each system or service. Using this knowledge base,our approach ponders a systems' security score,security requirements given by the systems' owners and the cloud provider,and reconfigures the network accordingly to meet the security requirements for every system. In addition,the reconfiguration process can be used to redirect traffic to additional security systems,in order to obtain more detailed information about a system and therefore increase the accuracy of the specific systems' security score.
Software-Defined Networking (SDN) is an emerging technology,physically separating data and control planes of network devices. From a security point of view SDN has two sides. First,it enables network security functions by design,because traffic flows can be redirected or filtered based on packet content or application layer state - functionality,which to date requires additional network security devices like fire-walls,intrusion detection systems or spam filters in conventional networks. On the other hand,due to physical separation of planes,SDN possibly offers additional attack vectors compared to traditional network architectures,which may severely impact overall network availability as well as confidentiality,authenticity,integrity and consistency of network traffic and control data. In this paper,we discuss and balance security provided by SDN with security threats of SDN also in respect of traditional networks. We develop an evaluation methodology for both sides and show that from a security point of view SDN is a blessing for today's and future network design and operation.
Access Control (AC) systems are among the most critical of network security components. A system's privacy and security controls are more likely to be compromised due to the misconfiguration of access control policies rather than the failure of cryptographic primitives or protocols. This problem becomes increasingly severe as software systems become more and more complex,such as Big Data (BD) processing systems,which are deployed to manage a large amount of sensitive information and resources organized into a sophisticated BD processing cluster. Basically,BD access control requires the collaboration among cooperating processing domains to be protected as computing environments that consist of computing units under distributed AC managements. Many BD architecture designs were proposed to address BD challenges;however,most of them were focused on the processing capabilities of the ???three Vs??? (Velocity,Volume,and Variety). Considerations for security in protecting BD are mostly ad hoc and patch efforts. Even with some inclusion of security in recent BD systems,a critical security component,AC (Authorization),for protecting BD processing components and their users from the insider attacks,remains elusive. This paper proposes a general purpose AC scheme for distributed BD processing clusters.
The US healthcare Affordable Care Act established the 30 day readmission protection program as one of the base lines of measuring quality of care at hospitals and post discharge. With reduced payment penalties for hospitals with excessive readmissions,hospitals have increased their focus on managing post discharge care. With the emphasis on prevention and proactive care,integrated approaches that have the ability to collect relevant data from patients,process it efficiently and timely and predict risk patterns in advance and enable seamless collaboration between the patients and the care team is required. This allows care teams to proactively manage the care of the patient and limit complications and readmissions. Internet of things enabled collaborative cloud based e-health is evolving as one of the key transformation approaches in helping to address the 30 day readmission avoidance efforts. While the sensors provide critical data,there are significant constraints in terms of processing,power,storage and overall context. The power and capabilities of the cloud can augment the local visibility of sensors by providing capabilities that the sensors lack. In this work,we define these capabilities as the five P's: Provisioning,Policy Management,Processing,Protection and Prediction. We argue that by bringing these elements together,the e-health architecture is able to take the data from sensors securely and transfer it to the cloud and generate insights and actions that help improve healthcare outcomes in a timely manner. The Cloud management plays a critical role in ensuring the integrity and availability of vital information. The blind spots in the unavailability of data or compromised data can result in missed opportunities for proactive care;ours proposed architecture ensures data availability,processing availability and integrity and thus is very important in a 30 day readmission context.
Invisible eavesdropping in this paper means that some applications call the mobile phone's microphone module and gather the user's voice and transmit information illegally when the screen is locked or the users are playing games and completely unaware of the situation. Based on idea of real-time monitoring and active protection,a scheme was proposed in this paper using the access authority of mobile phone's microphone,network access,user trust and etc. This scheme implements the function of alarm and threat position in the open source Android mobile platform. Compared with the domestic and foreign products,the system has the following specific characteristics: (1) it can boot automatically and scan on the backstage without multiple settings after it installed and set up for the first time;(2) compared with some of the existing protection software,when the invisible eavesdropping happened,this application will alarm the users,but not just to protect the files in the phone;(3) the application can navigate to the specific risk of eavesdropping applications;(4) it need not the authority of root or to modify the kernel.
Data replication is an increasingly important topic as databases are more and more deployed over distributed systems,grid community and clustering systems. The performance,reliability and portability of entire database may possible by using replication technique. Replication may be considered as a data backup policy. Replication in homogeneous system is common practice in real life,but replication in heterogeneous system is quite challenging,because of the dissimilar computing environment. Since the computer environment porn to be heterogeneous,hence it's a promising field for researchers to consider replication in heterogeneous environment. In our research a persistence layer has been proposed for replication in heterogeneous systems. This persistence layer work on asynchronous model,hence it may call as asynchronous replication model. The model works implements multi threading technique for creating parallel connection with peer servers. The main server and replicated server are connected with a common interface. The interface is a replication engine,which intelligently holds data and makes decision depending on different factors for sending data to smoothen the replication process. The whole structure follows the rules of SOA (Service oriented architecture) thus,modification of replication servers do not affect the main server. Finally the architecture of this concept builds on different configurable files. These files help us in system up-gradation without shutting down the system. At the end,some experiments have been carried out and the results have been analyzed.
Research on unknown network protocol reverse engineering is of great significance in many network security applications. Currently most of methods are limited in analyzing plain-text protocols,and a few of method can partly analyze the encryption protocol which is powerless for multiple encryption protocol or sectional encryption protocol. This paper proposes a method of encrypted protocol reverse engineering based on dynamic taint analysis. The method uses Pin to record executed instructions,and then conducts off-line analysis of the data dependencies to build two taint propagation graphs on instruction and function level,then recover the decryption process. The decrypted plaintext can be located due to the decryption process feature. And then,the format of protocol can be parsed. Experiments show that the method can accurately locate the decrypted protocol data of the multiple encryption and sectional encryption protocol,and restore the original format.
Packet filter system based on high speed match engine of REGular EXPressions (REGEXP) plays an important role in domain of Intrusion Detection System (IDS),Deep Packet Inspection (DPI) system,network security and traffic monitoring,etc. However,the existing filter schemas suffer from several deficiencies in matching speed and memory footprint,such as traditional DFA matching,single-level signature hash and DFA grouping. To overcome these shortcomings,in this paper,a new packet filter schema based on multilevel signature and DFA grouping is proposed. In particular,an algorithm called "DFA pseudo-split" is presented in our proposal to overcome the shortage of signatures. The experimental results show that our proposal significantly outperforms the traditional filter schemas.
Firewalls are very important elements in network security. Working of firewall rules for enterprise network has become complex,error-prone and time-consuming. Firewall filtering rules have to be carefully written and organized in order to correctly implement the security policy. The main issue is the slow filtering action during heavy load. To reduce the time consumption there is a very urgent need of optimized firewall engine,which runs on GPU's. We mainly focus on the creating parallel algorithms for desktop firewall which reduces the time consumption and at the same time it can allow for strong threat detection,intrusion detection of incoming packets. In our paper we have created parallel optimized algorithms for intrusion detection,threat detection,packet filtering and network address translation which runs on NVIDIA's GPU card and it is based on CUDA programming. For our experimental analysis,we have created test packets and for virus scanning we have used the virus-signatures from Clam-AV.
Security of communication systems has become a crucial issue. A harder problem to crack in the field of Network Security is the identification and prevention of attacks. An effective Intrusion Detection System (IDS) is essential for ensuring network security. Intrusion detection systems include pattern analysis techniques to discover useful patterns of system features. These patterns describe user behavior. Anomalies are computed using the set of relevant system features. The derived patterns comprise inputs of classification systems,which are based on statistical and machine learning pattern recognition techniques. Clustering methods are useful in detection of unknown attack patterns. Elimination of insignificant features is essential for a simplified,faster and more accurate detection of attacks. Genetic algorithm based clustering offers identification of significant reduced input features. We present a conceptual framework for identifying attacks for intrusion detection by applying genetic K-means algorithm.
With the rapid expansion of the Internet,network security has become more and more important. IDS (Intrusion Detection System) is an important technology coping network attacks and is of two main types: network based (NIDS) and host based (HIDS). In this paper,we propose the conception of NFPPB (Network Flow Patterns of Penetrating Behavior) to network vulnerable ports and design a NIDS algorithm to detect infiltration behaviors of attacker. Essentially,NFPPB is a set of metrics calculated by network activities exploiting the vulnerabilities of hosts. The paper investigates choosing,generation and comparison of NFPPB metrics. Experiments show that the method is effective and with high efficiency. At last,the paper addresses the future direction and the points that need to be improved.
In this paper a new approach for pattern mining is done through the image set consists of layers that might be irregular or regular one. Those layers will contain the hulls to be identified by using the onion routing approach. As the onion consists of several layers,if those layers are separated they can be easily mined and later the counting algorithm will be easily applied to find number of layers. This can be helpful in knowing how many floors,rooms etc in a building. In a way it can be applied to a number of applications like traffic,military,etc. The estimation results are easily obtained while going through identifying the sub-regional objects by using the SASK algorithm. The hard clustering and density based clustering process of obtained image rough set is used to recognize the differentiated internal objects,if any. Layered hulls are those having the structured layers inside. These are useful in the Military Services and Traffic to identify the number of vehicles or persons. In this Proposed Paper SASK algorithm is designed which is helpful for identifying the regions and By creating clusters and it will be useful to differentiate the layers and can help to identify them easily.
Multiple e-mail address certificates have been proposed very recently,therefore,neither certification authorities currently issue such a certificate nor does any mail client permit its use for signing or encrypting e-mail messages. Further,no study has been made to evaluate their effectiveness for practical use. This paper studies the usability of multiple e-mail address certificates in secure messaging. It discusses S/MIME,its usefulness in e-mail security,current adoption by users,advantages,and disadvantages. It illustrates procedures in vogue for singing,encrypting,and both signing and encrypting e-mail messages using e-mail address certificates. It further reports the results of study carried out to evaluate user efficiency in sending/receiving S/MIME mail with and without the use of multiple e-mail address certificates.
Mobile Ad hoc Network (MANET) is a collection of self-configuring,infrastructure less mobile nodes that dynamically form a temporary network and capable of communicating each other without the use of network infrastructure. Due to the open medium,dynamic topology,distributed co-operation Ad hoc networks are vulnerable to many type of security attacks. An important concept in network security is trust;it is estimated as a relation among the entities that participate in various protocols. Trust relations are based on evidence related to the previous behaviour of entities within the protocol. In this paper we focus an trusted route selection value between mobile node and secure gateway which is based on trust values of nodes,trust value of route and residual route load capacity and we also focus mobile node and secure gateway authentication process which is based on pre-authentication technique. To provide a host-to-host security,it is important to evaluate secure trusted route towards the secure gateway and authenticate mobile node and secure gateway. It can be achieved in proposed scheme. Our proposed protocol provides better performance as compared to other protocols. The correctness of our proposed scheme is validated through simulation results and performance evaluation matrices.
Because network security has become one of the most serious problems in the world,intrusion detection is an important defence tool of network security. In this paper,A cooperative and adaptive intrusion detection method is proposed and a corresponding intrusion detection model is designed and implemented. The E-CARGO model is used to build the collaborative and adaptive intrusion detection model. The roles,agents and groups based on 2-class Support Vector Machines (SVMs) and Decision Trees (DTs) are described and built,and the adaptive scheduling mechanisms are designed. Finally,the KDD CUP 1999 data set is used to verify the effectiveness of our method. Experimental results show that the collaborative and adaptive intrusion detection method proposed in this paper is superior to the detection of the SVM in the detection accuracy and detection efficiency.
In this paper,we investigate the performance of Nth best relay selection networks with secrecy constraints where several eavesdroppers try to overhear the source message. In order to enhance the network security,a single jammer is introduced to jam the eavesdroppers. In addition,sub-optimal selection strategy is introduced to combat the malicious attempt of eavesdroppers. We derive the exact secrecy performance in terms of probability of the non-zero secrecy capacity and the secrecy outage probability of the proposed relay selection approach. Based on these expressions,the effect of several important network parameters,i.e.,the number of relays and the number of eavesdroppers,as well as the quality of the relay links,jammer links,and eavesdroppers links,on the proposed network are analytically characterized.
Start of the above-titled section of the conference proceedings record.
Reinforcement learning techniques become more popular in computer network security. The same reinforcement learning techniques developed for network security can be applied to software security as well. This research summarizes a work in progress attempt to incorporate Q-learning algorithm in software security. The Q-learning method is embedded as part of the software itself to provide a security mechanism that has ability to learn by itself to develop a temporary repair mechanism. The results of the experiment express that given the right parameters and the right setting the Q-learning approach rapidly learns to block all malicious actions. Data analysis on the Q-values produced by the software can provide security diagnostic as well. A larger scale experiment is expected to be seen in the future work.
An ever increasing impact and amount of network attacks have driven many organizations to deploy various network monitoring and analysis systems such as honeypots,intrusion detection systems,log analyzers and flow monitors. Besides improving these systems a logical next step is to collect and correlate alerts from multiple systems distributed across organizations. The idea is to leverage a joint effect of multiple monitoring systems to build a more robust and efficient system,ideally,lacking the shortcomings of the individual contributing systems. This paper presents an analysis of alert reports gathered from several such detectors deployed in national research and education network (NREN). The analysis focuses on the correlations of reported events in temporal domain as well as on the correlations of different event types.
This paper compares and contrasts the most widely used network security datasets,evaluating their efficacy in providing a benchmark for intrusion and anomaly detection systems. The antiquated nature of some of the most widely used datasets along with their inadequacies is examined and used as a basis for discussion of a new approach to analyzing network traffic data. Live network traffic is collected that consists of real normal traffic and both real and penetration testing attack data. Attack data is then inspected and labeled by means of manual analysis. While network attacks and anomaly features vary widely,they share some commonalities that are examined here. Among these are: self-similarity convergence,periodicity,and repetition. Further,the knowledge inherent in the definition of network boundaries and advertised services can provide crucial context that allows the network analyst to consider self-aware attributes when examining network traffic sessions. To these ends the Session Aggregation for Network Traffic Analysis (SANTA) dataset is proposed. The motivation and the methodology of collection,aggregation and evaluation of the raw data are presented,as well as the conceptualization of the SANTA attributes and advantages provided by this approach.
The tremendous growth in computer network and Internet usage,combined with the growing number of attacks makes network security a topic of serious concern. One of the most prevalent network attacks that can threaten computers connected to the network is brute force attack. In this work we investigate the use of machine learners for detecting brute force attacks (on the SSH protocol) at the network level. We base our approach on applying machine learning algorithms on a newly generated dataset based upon network flow data collected at the network level. Applying detection at the network level makes the detection approach more scalable. It also provides protection for the hosts who do not have their own protection. The new dataset consists of real-world network data collected from a production network. We use four different classifiers to build brute force attack detection models. The use of different classifiers facilitates a relatively comprehensive study on the effectiveness of machine learners in the detection of brute force attack on the SSH protocol at the network level. Empirical results show that the machine learners were quite successful in detecting the brute force attacks with a high detection rate and low false alarms. We also investigate the effectiveness of using ports as features during the learning process. We provide a detailed analysis of how the models built can change as a result of including or excluding port features.
Advanced Technology in the area of intrusion detection is the Honeypot technology that unlike common IDS s tends to provide the attacker with all the necessary resources needed for a successful attack. Honeypot provide a platform for studying the methods and tools used by the intruders,thus deriving their value from the unauthorized use of their resource. To provide scalable,early warning and analysis of new Internet threats like worms or automated attacks,we propose globally distributed,hybrid monitoring model that can capture and analyze new vulnerabilities and exploits as they occur. To achieve this,our Model increases the exposure of high-interaction honeypots to these threats by employing low-interaction honeypots as frontend content filters. Host-based techniques capture relevant details such as packet payload of attacks while network monitoring provides wide coverage for quick detection and assessment. To reduce the load of the backends,we filter prevalent content at the network frontends and use a novel handoff mechanism to enable interactions between network and host components.
A plethora of mobile wireless networking approaches,from multi-hop infrastructure-less networking to mobile offloading and crowd sourcing,relies on establishing communication directly between devices. However,the 802.11 ad-hoc mode,as the designated technological means to realize device-to-device communication,suffers from missing support by vendors and operating system and lacks 802.11 functionality support. If at all,mobile wireless networking approaches reach a very low number of compatible devices and have to tolerate low network performance,deprecated WEP network security,and a lack of energy saving mechanisms. Eventually,this lack of a technological basis prevents the timely and beneficial real-world adoption of mobile networking. We thus propose such a basis in MA-Fi,multi-hop mobile networking using the 802.11 infrastructure mode. Building on comprehensive vendor and device support,MA-Fi realizes 802.11n performance,WPA2 security,and efficient energy saving mechanisms in ubiquitously compatible,mobile 802.11 infrastructure mode networks. Specifically,MA-Fi uses network virtualization to establish a two-tiered network topology that seamlessly incorporates legacy Wi-Fi devices. In comparison to the ad-hoc mode,MA-Fi achieves throughput of up to 340% while reducing the network-wide energy consumption by up to 75%.
Decentralized security software deployment is important to achieve reliable and secure network operations. In this paper,we consider the joint traffic routing and security software deployment problem. In this problem,the network can make decisions about routing,security protection and recovery software to minimize total energy consumption of all nodes in the network. Specifically,the network has to trade off between energy consumption and preventing security damages by running protection software based on uncertainty about attacks. The protection software has to be deployed along the route of traffic flow,and hence the network has to optimize traffic routing. We first formulate this problem as a stochastic programming model. To obtain the solution,we transform the stochastic programming model into the deterministic problem,which can be solved using a standard solver. The performance evaluation reveals that the optimal solution depends largely on energy budget and energy consumption of the nodes for transferring traffic and running security software.
As the popularity of software defined networks (SDN) and OpenFlow increases,policy-driven network management has received more attention. Manual configuration of multiple devices is being replaced by an automated approach where a software-based,network-aware controller handles the configuration of all network devices. Software applications running on top of the network controller provide an abstraction of the topology and facilitate the task of operating the network. We propose OpenSec,an OpenFlow-based security framework that allows a network security operator to create and implement security policies written in human-readable language. Using OpenSec,the user can describe a flow in terms of OpenFlow matching fields,define which security services must be applied to that flow (deep packet inspection,intrusion detection,spam detection,etc) and specify security levels that define how OpenSec reacts if malicious traffic is detected. We implement OpenSec in the GENI testbed to evaluate the flexibility,accuracy and scalability of the framework. The experimental setup includes deep packet inspection,intrusion detection and network quarantining to secure a web server from network scanners. We achieve a constant delay when reacting to security alerts and a detection rate of 98%.
In this paper,we analyze the throughput of a novel mobile access coordinated wireless sensor network architecture (MC-WSN) under single path and multipath routing. The obtained throughput expressions highlight the trade-off between achieving high throughput performance and improving the network security strength. The results reveal the importance of: (i) minimizing the number of hops in maximizing the throughput,and (ii) adopting routing diversity in combating malicious attacks and network failure conditions. We control the number of hops in data transmission through optimal topology design and active network deployment achieved by the mobile access point (MA). To combat routing attacks,we propose a secure routing path selection approach,and show the impact of the proposed approach on improving the throughput performance under malicious attacks.
In this paper a round-the-year based network reinforcement solution generator for the transmission expansion planning problem is proposed. The solution generator makes use of a round-the-year network security analysis which combines zonal market simulations with load flow calculations. This allows a robust evaluation of the reinforcement candidates as all the hours in the year are considered. The solver iterates sequentially over possible reinforcements until no more congestion occurs in the grid,under both normal and N-1 branch outage situations. New criteria for assessing the grid congestion level are defined and used in the evaluation of candidates. The decrease in the proposed "grid severity index" is used to measure the effectiveness of each reinforcement candidate. At the end of the process,an ordered list of reinforcements is given together with the criteria values at each reinforcement step. The New England IEEE test system is used for validating the method.
In recent years,many countries have used e-government to provide high quality services to their citizens. Thus,a number of studies have investigated user acceptance of e-government through the use of adoption models,such as the Unified Theory of Acceptance and Use of Technology (UTAUT) model. However,these models do not focus sufficiently on security. In order to develop a more security-focused adoption model for investigating user acceptance of e-government,it is important to first understand the security challenges that may inhibit the adoption of e-government. This paper considers these security challenges based on the end user's perspective,identifying the extent of the challenges. For example,85.5% of the participants agree that there is a lack of user awareness. In addition,62.4% of the participants believe that culture plays an important role in e-government security. Also,49.8% of the participants are worried about privacy when using e-government services.
Phishing takes advantage of the way humans interact with computers or interpret messages;and also that many online authentication protocols place a disproportional burden on human abilities. A security ceremony is an extension of the concept of network security protocol and includes user interface and human-protocol interaction. It is one way of extending the reach of current methods for social,technical and contextual analysis of security protocols to include humans. In this paper,we propose a Human Factors in Anti-Phishing Authentication Ceremonies (APAC) Framework for investigating phishing attacks in authentication ceremonies,which builds on The Human-in-the-Loop Security Framework of communication processing. We show how to apply the APAC framework to model human-protocol behaviour. The resulting Model for Analysing APAC correlates the framework components and examines how the authentication tasks required to be performed by humans influence their decision-making and consequently their phishing detection.
Distributed Denial of Service (DDoS) attacks are a serious threat to network security. Servers of many companies and/or governments have been victims of such attacks. DDoS attacks jam the network service of the target using multiple bots hijacked by crackers and send numerous packets to the target server. In such an attack,detecting the crackers is extremely difficult,because they only send a command by multiple bots from another network and then leave the bots quickly after command execute. Therefore,we need an intelligent detection system for DDoS attacks to defend network services. To develop the system,we utilized machine learning techniques to study the patterns of DDoS attacks and detect them. We analyzed large numbers of network packets provided by the Center for Applied Internet Data Analysis,and detected some important patterns that affect the accuracy of the detection system. We implemented the detection system using the patterns of DDoS attacks. A support vector machine with the radial basis function (Gaussian) kernel is its core part. The detection system is accurate in detecting DDoS attacks.
Moving target defense is an area of network security research in which machines are moved logically around a network in order to avoid detection. This is done by leveraging the immense size of the IPv6 address space and the statistical improbability of two machines selecting the same IPv6 address. This defensive technique forces a malicious actor to focus on the reconnaissance phase of their attack rather than focusing only on finding holes in a machine's static defenses. We have a current implementation of an IPv6 moving target defense entitled MT6D,which works well although is limited to functioning in a peer to peer scenario. As we push our research forward into client server networks,we must discover what the limits are in reference to the client server ratio. In our current implementation of a simple UDP echo server that binds large numbers of IPv6 addresses to the ethernet interface,we discover limits in both the number of addresses that we can successfully bind to an interface and the speed at which UDP requests can be successfully handled across a large number of bound interfaces.
We study how an underlying network property affects network security when nodes are rational and have four choices - i) invest in protection,ii) purchase (incomplete coverage) insurance,iii) invest in protection and purchase insurance,or iv) do nothing. More specifically,using a population game model,we examine how the degree distribution of nodes influences their choices at Nash equilibria (NEs) and overall security level. We first show that there exists a degree threshold at NEs so that only the populations with degrees greater than or equal to the threshold invest in protection. Second,as the weighted degree distribution of nodes becomes stochastically larger,the risk or threat posed by a neighbor decreases,even though the aforementioned degree threshold tends to rise,hence only nodes with increasingly higher degrees invest in protection,at the same time. Third,we show that the social optimum also possesses similar properties. Finally,we derive an upper bound on the price of anarchy,which is an affine function of the average degree of nodes. This upper bound is tight in that it is achieved in some scenarios.
Applications of high-performance graph analysis range from computational biology to network security and even transportation. These applications often consider graphs under rapid change and are moving beyond HPC platforms into energy-constrained embedded systems. This paper optimizes one successful and demanding analysis kernel,betweenness centrality,for NVIDIA GPU accelerators in both environments. Our algorithm for static analysis is capable of exceeding 2 million traversed edges per second per watt (MTEPS/W). Optimizing the parallel algorithm and treating the dynamic problem directly achieves a 6.9 average speed-up and 83% average reduction in energy consumption.
We created a pixel map for multivariate data based on an analysis of the needs of network security engineers. Parameters of a log record are shown as pixels and these pixels are stacked to represent a record. This allows a broad view of a data set on one screen while staying very close to the raw data and to expose common and rare patterns of user behavior through the visualization itself (the "Carpet"). Visualizations that immediately point to areas of suspicious activity without requiring extensive filtering,help network engineers investigating unknown computer security incidents. Most of them,however,have limited knowledge of advanced visualization techniques,while many designers and data scientists are unfamiliar with computer security topics. To bridge this gap,we developed visualizations together with engineers,following a co-creative process. We will show how we explored the scope of the engineers' tasks and how we jointly developed ideas and designs. Our expert evaluation indicates that this visualization helps to scan large parts of log files quickly and to define areas of interest for closer inspection.
Cybersecurity education does not only confine to the technical studies embodying network security,malware analysis and reverse reengineering,application security,and operating systems security. The increasing numbers of Cyberterrorism and incidents of hacktivism suggest that Cybersecurity pertains to politics,religion,and culture. Drawing on globalization shaped by the economic,legal,political,religion,and social dimensions,extant literature discloses that infusing social science,cultural and political studies into Cybersecurity education could better prepare students for the job market by making students realize the complexity in the real world. This study presents the efforts of integrating cultural,social,and political dimensions into the Cybersecurity curriculum in a public,regional university in the Midwest of the United States. In particular,this study presents the approaches of introducing intelligence analysis coursework that requires students to understand the analysis of competing hypotheses for drawing conclusion related to the possible Cyberattack from a foreign nation,identify the cultural differences across nations for comprehending the hacking motivation of a different nation,and recognize individual's cognitive and cultural biases during the process of evaluating a cultural event.
Secure group communication is an important research area in the field of cryptography and network security,because the group communication like electronic conferences,video chatting,video games etc. are rapidly increasing. Group key agreement protocols allow all members of the group to agree on the same session key which is used later for secure group communication. The design of secure group key agreement protocol can be very critical for achieving security goals. Many group key agreement protocols have been established for wired and wireless group communication. This paper proposes an ECC (Elliptic Curve Cryptography) based authenticated group key agreement protocol for wireless scenario. The proposed protocol uses the concepts of elliptic curve cryptography to minimize the computation overheads and AES (Asymmetric Encryption Standards) to maintain efficiency. The proposed protocol consists of a set of users and a trusted server,where server and the user contribute to create the group key. The performance and security analysis shows that the proposed protocol is secure against passive and active attack and performs better in terms of computation and communication cost from other related protocols.
For Liao Hui and others proposed network terminal security assessment index system exists index weights unreasonable distribution problem,using Delphi method and AHP calculate each index weight and the weights of total ranking of lowest level indexes relative to the highest lever indexes,identified the indicators which have greatest impact to the assessment objectives and apply it to a host of information security evaluation system. Combined with examples to prove the index system after improving its weight distribution can be more scientifically reflect the importance of the indexes in the evaluation system and the result of the host information security evaluation is reasonable and comprehensive.
The paper aims to investigate the statues of digital preservation of Old Persian periodicals especially the Newspapers in Iran. It also discusses strategies and challenges faced by Iranian libraries. The surveyed libraries were National Library of Iran,Malek National Library and Museum,Library of Islamic Consultative Assembly and Central Library of Astan Quds Razavi. The result of the study showed that the most metadata standards used at these libraries were METS and Doblin Core. Additionally,among strategies,the most used was backup strategy. The paper concludes that long term digital preservation of Old Persian periodicals with the help of localization metadata standards and strategies are access door for next generation to transfer historical culture of Iran.
Provides a listing of current committee members and society officers.
Firewalls are a defense mechanism for network security. Today,firewalls have played a pivotal role in a wide spectrum of circumstances,from enterprise networks to home networks. Firewall rules have their execution semantics. Firewalls are often networked to establish perimeters for different parts of an enterprise with differing security policy requirements. Hence,rules in intra-firewall and inter-firewall settings interact,sometimes creating unintended side-effects. The complexity in utilizing firewalls to implement consistent and coherent security policies to safeguard enterprise networks poses great challenges to the network security as a whole. A major challenge is firewall inconsistencies that compromise the effectiveness of firewall protection. In this paper,we propose an approach to detecting and resolving major types of firewall inconsistencies. We describe our detection algorithms and resolution strategies to firewall inconsistencies,and report some initial results of a tool implementing the proposed approach. The main contribution of our work lies in the fact that our approach takes advantage of two necessary conditions in firewall inconsistencies,resulting in a more effective detection method.
Firewalls form an essential element of modern network security,detecting and discarding malicious packets before they can cause harm to the network being protected. However,these firewalls must process a large number of packets very quickly,and so can't always make decisions based on all of the packets' properties (features). Thus,it is important to understand which features are most relevant in determining if a packet is malicious,and whether a simple model built from these features can be as effective as a model which uses all information on each packet. We explore a dataset with real-world firewall data to answer these questions,ranking the features with 22 feature selection techniques and building classification models using four classifiers (learners). Our results show that the top two features are proto and dst (representing the network protocol and destination IP address,respectively),and that models built using these two features in combination with the Naive Bayes learner are highly effective while being minimally computationally expensive. Such models have the potential to replace conventional firewalls while lowering computational needs.
Aiming at the tracking control for the underactuated surface ship at open sea,an adaptive robust neural network method was proposed. At present,most researches would consider the underactuated characteristics and parameters uncertainties and environmental disturbance. The controller structure was designed by nonlinear algorithm,thus ensuring the tracking loop system robustness. Proposed adaptive neural network control method,used to estimate the uncertainty in the external environment and model parameters. Based on Lyapunov stability theory,we designed the adaptive law of neural network weights,thus ensuring the tracking error near zero. The stability and the robustness of the closed-loop system are ensured by Lyapunov stability theory. Simulation results illustrate the effectiveness of the proposed control method.
In research of Optimization of performance of wireless sensor networks to improve anti-attack capability,key management is an important security issue to be solved. Due to the severe restriction on resources and work environment,the classic key distribution algorithms are not suitable for wireless sensor networks. Aiming to improve the capability of security and feasibility of wireless sensor networks,after considering the issues of network security,traffic problems,memory problems and computational complexity problem,this paper proposes a key management scheme based on grouping within cluster to solve the problem of poor salability,low work efficiency and communication times . This scheme makes a big improvement of group key generation method. Cluster heads and group heads in the clusters are respectively used to generate and distribute keys via secure channels in each group. The simulation results show that this scheme further improves network's ability to resist DOS (denial of service) attack,reduces the energy consumption and time consumption in key negotiation and improves the survival time and efficiency of entire network.
The connection between PROFIBUS network and Internet can realize remote real-time monitoring device on the fieldbus. This paper conducts detailed analysis and research advance on network security based on the interconnection methods between PROFIBUS and Internet,and proposes a PROFIBUS industrial network security model. An embedded gateway is developed especially for meeting the security requirement in this research work. With compact structure and low cost,it paves the way to realize real-time monitoring PROFIBUS device safely via Internet.
Many-core accelerator chips are becoming increasingly popular these days for its high performance floating-point performance exceeding 1 Tflops per chip. Aho-Corasick (AC) is a multiple patterns string matching algorithm commonly used in computer and network security,bioinformatics,among others. In order to simultaneously match a number of string patterns with respect to the input text data,a Deterministic Finite Automata (DFA) is constructed from a given set of pattern strings. The DFA is referenced almost randomly,whereas the input data is sequentially accessed. As the number of pattern strings increases,the irregular DFA accesses lead to poor cache locality and low overall performance. In this paper,we present a cache locality optimizing parallelization on the many-core accelerator chip,the Intel Xeon Phi. A given set of pattern strings is partitioned into into multiple sets of a smaller number of patterns so that multiple small DFAs are constructed instead of single large DFA. The accesses to multiple small DFAs lead to significantly smaller cache footprints in each core's cache and result in impressive performance improvements. Experimental results on the Intel Xeon Phi 5110P show that our approach delivers up to 2.00  speedup compared with the previous approach using single large DFA.
Database is an important asset of any leading and emerging industry and this database needs to improved security features from various threats in the network and database repository. Most of an organization's sensitive and proprietary data resides in a Database Management System (DBMS). Data security is a major issue in any web-based application and database repository. Although there are various model implemented in the network for the security of these databases. Real world web databases have information that needs to be securely stored and accessed. In this paper we discussed proposed work classification based approach to create database policy dynamically and using these policy find intrusion in user request and response the request accordingly. The focus of our work is to develop advanced security solutions for protecting the data residing in a DBMS. Our approach is to develop an Intrusion Detection and Response (IDR) system. We introduce a policy-driven intrusion response mechanism that is capable of issuing an appropriate response based on the details of the anomalous request and discuss the implementation details on the same and report experimental results that show that our techniques are feasible and efficient.
Network traffic classification plays an important role in the areas of network security,network monitoring,QoS and traffic engineering. In this paper,we design a network traffic classifier based on the statistical features extracted from network flows. Instead of deriving the statistical characteristics per flow,our model make use of features extracted from the first few seconds of each flows. The first few seconds of each flow is divided into overlapping time-based windows. This approach enables our classifier to classify each flow early. Attribute selection algorithms Chi-Square,CON and CFS are used to obtain the optimal subset of features. We give a comparative analysis of the result on the said approach based on the classification algorithms (Decision tree (C4.5),Naive Bayes,Bayesian Belief Network and SVM). We also present a single class classifier implementation of C4.5 algorithm. The experimental results show that the proposed method can achieve over 99% accuracy for all testing dataset. Using the proposed method,C4.5 algorithm delivers high speed and accuracy. By taking inference from these offline classifiers,we build an online standalone classifier using C/C++. We used the following applications: Skype,Gtalk and Asterisk.
Crypto-biometric system (CBS) is a combination of biometrie with cryptography to enhance network security. Biometrie is the most trustworthy measure to identify a person uniquely using his or her behavioral and physiological characteristics. Cryptography is an effective concern to the security of information. The security of cryptography depends on the strength of cryptographic key and strength of key depends on the length of key. In the traditional cryptography,key is generated randomly and it is very difficult to remember as the key is not linked with user. To address this limitation of cryptography,CBS uses biometrie data of user to bind key with its owner and as the key is linked with user's biometrie data,user does not need to remember the key. As biometrie data is irrevocable,it becomes useless when compromised and as a result the biometrie based key becomes also useless. In this approach,fingerprint features are used to generate key for cryptographic application. The key is revocable and easy to revoke when required. In our experiment,FVC2004 fingerprint database is used to investigate the result.
Key aspect of communication is security. Encryption of information to make it inaccessible to unauthorized recipients is a main area in network security. AES encryption is one of the most secure encryption algorithms. But with improving technology,attacks which can be used for easy cryptanalysis of encrypted information have been developed. One of the main reasons for vulnerability of AES encryption algorithm is the use of static S-Boxes. Also with attempts made to tap the key,AES has become more vulnerable to cryptanalysis attacks. Static S-Boxes make it very easy for reverse engineering which form the basis of Super S-Box attack. Hence there have been a lot of proposals for generation of S-Boxes which are key dependent. Majority of those algorithms involve probabilistic methods which are very complicated for hardware implementation. In this paper,we present a synthesizable algorithm which involves the use of conventional bitwise operations for generation of key dependent S-Boxes. Also a dual key based AES is presented in the paper which is FPGA implementable. The algorithm is reliable in terms of security and also suitable for hardware implementation. Mathematical analyses have been carried out on the algorithm to compute the reliability.
Users currently experience different levels of protection when accessing the Internet via their various personal devices and network connections,due to variable network security conditions and security applications available at each device. The SECURED project addresses these issues by designing an architecture to offload security applications from the end-user devices to a suitable trusted node in the network: the Network Edge Device (NED). Users populate a repository with their security applications and policy,which will then be fetched by the closest NED to protect the user's traffic when he connects to a network. This setting provides uniform protection,independent of the actual user device and network location (e.g. public WiFi hotspot or 3G mobile connection). In other words,a user-centric approach is fostered by this architecture,opposed to the current device- or network-based security schema,with cost and protection benefits and simultaneously enabling new business models for service and network providers.
Deep packet inspection or DPI is now a fast growing application technology in the field of network security,which requires the network security platform has a higher speed to handle a large number of session connections,and track the status of these connections quickly. This paper proposed the MP-DPI,a many-core based network processing platform,which uses the ATCA standard modular design,makes use of the integrated many-core network process accelerate engine,and integrates a popular open source DPI system named SNORT. The experiment result shows that in the same power consumption,the throughput of MP-DPI platform is three times as large as traditional X86 servers.
Based on the review of current prediction algorithms of network security situation,prediction algorithms based on Kaiman filtering are studied. A prediction algorithm of network security situation based on grey correlation entropy Kaiman filtering is presented,hoping to be more helpful to network administrators through providing them information more effectively. First correlation of factors influencing network security situation is analyzed by Grey correlation entropy analysis method,and key influencing factors are selected. Then according to these influencing factors corresponding process equation and prediction equation are established. Finally,network security situation prediction is made recursively by Kaiman filtering. Experiment results show that the prediction by this method is more precise compared to GM(1,1) and general Kaiman algorithm,and its real-time performance is better than RBF algorithm.
Since last few years,users have been paying significant attention and interest towards the WiMAX Internet technology because of the salient features i.e. Higher data rate,broad coverage,quality of service (QoS),scalability,security,and mobility support to the subscribers. Mobile WiMAX (IEEE 802.16e) supports the handover process and mobility of subscriber stations. WiMAX network security is a vital requirement to increase the reliability and to prevent from the various attacks. Implementation of top level security is highly required to lessen the vulnerabilities. Jamming attack,scrambling attack and water-torture attack are among the most severe threats to the physical layer of mobile WiMAX. This paper presents a collaborative attack model based on the combination of scrambling and water-torture attacks. Analysis of the paper shows that the cooperation of two or more attacks/attackers may be more devastating for the network security.
Firstly,this paper analyzes the present research situation of intranet security from different aspects. Then,the main information security issue of campus network of scientific research is proposed: that is share information with limited right and "no tolerant" with the confidentiality of information. Lastly,the paper designs of campus network of scientific research based on security management,virtual resource pool technology and multi-layered network security model. Those technologies provide powerful supports for the safety of intranet with higher confidentiality level.
Intrusion detection is the problem of identifying unauthorized use,misuse,and abuse of computer systems or network resources by both system insiders and external penetrators. The proliferation of heterogeneous computer networks provides additional implications for the intrusion detection problem. The increased connectivity of computer systems gives greater access to outsiders,and makes it easier for intruders to escape from detection. IDS's are based on the belief that an intruder's behavior will be noticeably different from a legitimate user. An intrusion detection system (IDS) monitors network traffic for suspicious activity and alerts the system or network administrator. It detects and identifies unauthorized use,misuse,and abuse of computer systems from both the system insiders and external penetrators. Intrusion detection systems (IDS) are becoming essential components in a secure network environment,allowing for early detection of malicious activities and attacks. By employing information provided by IDS,it is possible to apply appropriate countermeasures techniques and mitigate attacks that would otherwise seriously undermine network security. In this paper we are proposing a techniques,which maintain the small distributed signature database and dynamically update it based on the change in network environment. Overall objective of the research work is to improve the efficiency and self configurability of the IDS in a dynamic and changing environment.
Attackers are most likely to exploit invalidated and unsanitized user input with several attacks such as cross-site scripting (XSS) or SQL injection. Many methods were proposed to prevent those attacks. Some of them were created to learn about pattern and behavior of the attacker. That is honeypot. Honeypot is classified into two types based on the simulation that honeypot can do: low interaction and high interaction.
This study focuses on the application of security metrics to a computer network. Mean Time-to-Compromise (MTTC) metric and VEA-bility metric are selected for this study. MTTC is calculated using a set of equations based on the known vulnerabilities of the system. VEA-bility is selected because it uses CVSS that has a wide coverage of security aspects. The input data for both metrics are obtained from Nessus,a network security tool. Both metrics give numerical results which are simple to comprehend to average clients. The purpose of this study are to calculate MTTC and VEA-bility values of the network,to compare the security level of different network configurations,also to compare the feasibility and convenience of using both metrics. The results of the study can be used as recommendations for network security assessment and references to determine policies relating to computer network management.
Recently,most wireless network security schemes merely based on physical layer characteristics tackle the two fundamental issuesdevice authentication and secret key extraction separately. It remains an open problem to simultaneously achieve device authentication and fast secret key extraction merely using wireless physical layer characteristics,without the help of advanced hardware or out-of-band channel. In this paper,we answer this open problem in the setting of wireless body area networks (BANs). We propose MASK-BAN,a lightweight fast authenticated secret key extraction scheme for intra-BAN communication. Our scheme neither introduces advanced hardware nor relies on out-of-band channels. To perform device authentication and fast secret key extraction at the same time,we exploit the heterogeneous channel characteristics among the collection of on-body channels during body motion. On one hand,MASK-BAN achieves authentication through multihop stable channels,which greatly reduces the false positive rate as compared to existing work. On the other hand,based on dynamic channels,key extraction between two on-body devices with multihop relay nodes is modeled as a max-flow problem,and a novel collaborative secret key generation algorithm is introduced to maximize the key generation rate. Extensive real-world experiments on low-end commercial-off-the-shelf sensor devices validate MASK-BANs great authentication capability and high-secret key generation rate.
Reliable erasing of data from storage devices is a critical component of secure data management and is well understood for magnetic disks. However,flash memory has unusual electronic limitations that make in-place updating impossible. Many secure deletion techniques have been proposed to improve both information security and erasing reliability. This paper investigates secure modification further. First,a formal definition of the levels of reliable erasing over flash-based systems is introduced. Then,an encryption strategy and key (ESK) security module are implemented. This module encrypts confidential files and forces all the generated keys and other relevant information of each file to be stored in the same blocks. Consequently,when securely modifying or deleting files,the ESK module erases as few blocks as possible. Experimental results show that the proposed module can improve the level of information safety and can reduce the number of page copies and block erases due to reliable erasing.
With the growing popularity of Internet applications and the widespread use of mobile Internet,Internet traffic has maintained rapid growth over the past two decades. Internet Traffic Archival Systems (ITAS) for packets or flow records have become more and more widely used in network monitoring,network troubleshooting,and user behavior and experience analysis. Among the three key technologies in ITAS,we focus on bitmap index compression algorithm and give a detailed survey in this paper. The current state-of-the-art bitmap index encoding schemes include: BBC,WAH,PLWAH,EWAH,PWAH,CONCISE,COMPAX,VLC,DF-WAH,and VAL-WAH. Based on differences in segmentation,chunking,merge compress,and Near Identical (NI) features,we provide a thorough categorization of the state-of-the-art bitmap index compression algorithms. We also propose some new bitmap index encoding algorithms,such as SECOMPAX,ICX,MASC,and PLWAH+,and present the state diagrams for their encoding algorithms. We then evaluate their CPU and GPU implementations with a real Internet trace from CAIDA. Finally,we summarize and discuss the future direction of bitmap index compression algorithms. Beyond the application in network security and network forensic,bitmap index compression with faster bitwise-logical operations and reduced search space is widely used in analysis in genome data,geographical information system,graph databases,image retrieval,Internet of things,etc. It is expected that bitmap index compression will thrive and be prosperous again in Big Data era since 1980s.
In the present study,the authors investigate robust secrecy rate optimisation problems for a multiple-input-single-output secrecy channel with multiple device-to-device (D2D) communications. The D2D communication nodes access this secrecy channel by sharing the same spectrum,and help to improve the secrecy communications by confusing the eavesdroppers. In return,the legitimate transmitter ensures that the D2D communication nodes achieve their required rates. In addition,it is assumed that the legitimate transmitter has imperfect channel state information of different nodes. For this secrecy network,the authors solve two robust secrecy rate optimisation problems: (a) robust power minimisation problem,subject to the probability based secrecy rate and the D2D transmission rate constraints;(b) robust secrecy rate maximisation problem with the transmit power,the probabilistic based secrecy rate and the D2D transmission rate constraints. Owing to the non-convexity of robust beamforming design based on two statistical channel uncertainty models,the authors present two conservative approximation approaches based on Bernstein-type inequality and S-Procedure to solve these robust optimisation problems. Simulation results are provided to validate the performance of these two conservative approximation methods,where it is shown that Bernstein-type inequality based approach outperforms the S-Procedure approach in terms of achievable secrecy rates.
All-optical networks (AONs) are emerging as a promising technology for future telecommunication and data networks. AONs provide huge transmission capacities and are mainly characterized by their transparency to the transmitted traffic. The characteristics of AONs are desirable in many respects,but they give rise to a new set of challenging issues in terms of network security that do not exist in traditional communication networks. The aim of this paper is to present a deep analysis of the security issues of AONs that distinguish them from traditional communication networks. In particular,we focus on the physical security aspect that differs significantly from that in electro-optic and electronic networks and that directly affects the physical infrastructure of AONs. The paper provides an analysis of the infrastructure vulnerabilities of AONs and shows how they can be exploited to perform attacks to the proper functioning of the network. Furthermore,it describes a set of scenarios that may be used to perform a service disruption or a tapping attack on an AON. Finally,it investigates the main challenging issues facing the efficient management of security attacks in AONs and presents a brief overview of the main proposed schemes for attack management in AONs.
The paper aims to investigate the statues of digital preservation of Old Persian periodicals especially the Newspapers in Iran. It also discusses strategies and challenges faced by Iranian libraries. The surveyed libraries were National Library of Iran, Malek National Library and Museum, Library of Islamic Consultative Assembly and Central Library of Astan Quds Razavi. The result of the study showed that the most metadata standards used at these libraries were METS and Doblin Core. Additionally, among strategies, the most used was backup strategy. The paper concludes that long term digital preservation of Old Persian periodicals with the help of localization metadata standards and strategies are access door for next generation to transfer historical culture of Iran.
Crypto-biometric system (CBS) is a combination of biometrie with cryptography to enhance network security. Biometrie is the most trustworthy measure to identify a person uniquely using his or her behavioral and physiological characteristics. Cryptography is an effective concern to the security of information. The security of cryptography depends on the strength of cryptographic key and strength of key depends on the length of key. In the traditional cryptography, key is generated randomly and it is very difficult to remember as the key is not linked with user. To address this limitation of cryptography, CBS uses biometrie data of user to bind key with its owner and as the key is linked with user's biometrie data, user does not need to remember the key. As biometrie data is irrevocable, it becomes useless when compromised and as a result the biometrie based key becomes also useless. In this approach, fingerprint features are used to generate key for cryptographic application. The key is revocable and easy to revoke when required. In our experiment, FVC2004 fingerprint database is used to investigate the result.
Improved network security is addressed using device dependent physical-layer (PHY) based fingerprints from Ethernet cards to augment traditional MAC-based ID verification. The investigation uses unintentional Ethernet cable emissions and device fingerprints comprised of Constellation-Based, Distinct Native Attribute (CB-DNA) features. Near-field collection probe derivative effects dictated the need for developing a two-dimensional (2D) binary constellation for demodulation and CB-DNA extraction. Results show that the 2D constellation provides reliable demodulation (bit estimation) and device discrimination using symbol cluster statistics for CB-DNA. Bit Error Rate (BER) and Cross-Manufacturer Discrimination (CMD) results are provided for 16 devices from 4 different manufactures. Device discrimination is assessed using both Nearest Neighbor (NN) and Multiple Discriminant Analysis, Maximum Likelihood (MDA/ML) classifiers. Overall results are promising and include CMD average classification accuracy of %C = 76.73% (NN) and %C = 91.38% (MDA/ML).
Modern society is becoming increasingly dependent on various communication networks such as the Internet and the sensor networks. These networks need strong security measures to keep the traffic secure. If the security fails, many aspects of the society may suffer. In this paper, we address the network security challenge by focusing on one integral part of the network functionality: routing. Using Open Shortest Path First (OSPF) as our Internal Gateway Protocol (IGP), we aim to create a randomization process in which the packets will be sent through less predictable paths. By using this process we expect to increase the difficulty for a hacker to eavesdrop traffic hence improve network security.
The security of e-government systems is a recurring theme among e-government literature and is often cited as the greatest challenge and barrier to e-government initiatives. However, research pertaining to the security needs of e-government systems is very limited and frequently raises more questions than answers. The discussion of security for municipal or city-level e-government systems is even more scarce. This paper presents the findings from a study conducted amongst municipalities in the county of Orange, California and highlights: 1) the need for e-government security at the municipal level, 2) areas of strengths in municipal e-government systems and 3) the greatest challenges in securing municipal e-government systems.
Increasing situational awareness and investigating the cause of a software-induced cyber attack continues to be one of the most difficult yet important endeavors faced by network security professionals. Traditionally, these forensic pursuits are carried out by manually analyzing the malicious software agents at the heart of the incident, and then observing their interactions in a controlled environment. Both these steps are time consuming and difficult to maintain due to the ever changing nature of malicious software. In this paper we introduce a network science based framework which conducts incident analysis on a dataset by constructing and analyzing relational communities. Construction of these communities is based on the connections of topological features formed when actors communicate with each other. We evaluate our framework using a network trace of the Black Energy malware network, captured by our honey net. We have found that our approach is accurate, efficient, and could prove as a viable alternative to the current status quo.
This paper proposes an automated virtual security testing platform for Android mobile apps. The testing platform includes three key components: customizing Android OS to include mobile app trace information, creating a virtual testing platform using the customized OS, and developing static and dynamic analyzing techniques for mobile malware detection. The proposed testing platform is a server-side malware detection solution. It can utilize both static and dynamic analysis and is a good compensation to the client-side mobile security software.
Malware classification and detection process is a very complex process in network security. In current network security scenario various types of malware family are available, some are known family and some are unknown family. The family of knowing malware detection used some well know technique such as signature based technique and rule based technique. In case of an unknown malware family of attack detection is various challenging tasks. In the current trend of malware detection used some data mining technique such as classification and clustering. The process of classification improves the process of detection of malware. In this paper used graph based technique for malware classification and detection. The graph based technique used for a feature collection of different malware data. The proposed algorithm is very efficient in compression of pervious method.
The following topics are dealt with: LALRED for congestion avoidance using automata-like solution;nonexclusive clustering;wireless sensor networks;encrypted and searchable network audit logs;bounded-SVD;scalable honeypot architecture;real-time P2P traffic identification.
Computer security has become an important part of the day today's life. Not only single computer systems but an extensive network of the computer system also requires security. In achieving the safety of the systems, an Intrusion Detection System (IDS) plays a significant role. IDS is a software that monitors the computer network and detects the suspicious activities that occur in the systems or network. The process of intrusion detection includes detecting intrusion. Intrusion is a suspicious activity attempted by the attacker. This paper presents a fuzzy-genetic approach to detecting network intrusion. Paper presents the results of the proposed system in terms of accuracy, execution time, and memory allocation. To implement and measure the performance of the system the KDD99 benchmark dataset is used. The KDD99 dataset is a benchmark dataset that researchers use in various network security researches. Genetic algorithm includes a development and collection that uses a chromosome like data structure and develop the chromosomes using selection, crossover and mutation operators. Fuzzy rule sorts network attack data.
Information security is major obstacle in different areas like military, network application, bank application. File is forward from one location to another location in the network. Many hackers are illegally access the information. To provide solution to this problem many authors has introduced different algorithms and techniques. The different algorithms like AES, DES and triple DES achieve more security but it takes more time for encryption and decryption files. These algorithm increases the complexity of the algorithm. In our algorithm we have investigate parameters of network security. This algorithm provides more security and takes smallest amount of time for file encryption and decryption. This algorithm can apply on different types of files like text, image, audio, video files. In the Byte Rotation Algorithm involve two techniques. One is random key generation technique is used. And second is parallel encryption and decryption is process using multithreading technique. Key size of random key generation technique is 128 bit.128 bit random key generation is difficult for crack to attacker.
Botnets have become major engines for malicious activities in cyberspace nowadays. To sustain their botnets and disguise their malicious actions, botnet owners are mimicking legitimate cyber behavior to fly under the radar. This poses a critical challenge in anomaly detection. In this paper, we use web browsing on popular web sites as an example to tackle this problem. First of all, we establish a semi-Markov model for browsing behavior. Based on this model, we find that it is impossible to detect mimicking attacks based on statistics if the number of active bots of the attacking botnet is sufficiently large (no less than the number of active legitimate users). However, we also find it is hard for botnet owners to satisfy the condition to carry out a mimicking attack most of the time. With this new finding, we conclude that mimicking attacks can be discriminated from genuine flash crowds using second order statistical metrics. We define a new fine correntropy metrics and show its effectiveness compared to others. Our real world data set experiments and simulations confirm our theoretical claims. Furthermore, the findings can be widely applied to similar situations in other research fields.
Malware is pervasive in networks, and poses a critical threat to network security. However, we have very limited understanding of malware behavior in networks to date. In this paper, we investigate how malware propagates in networks from a global perspective. We formulate the problem, and establish a rigorous two layer epidemic model for malware propagation from network to network. Based on the proposed model, our analysis indicates that the distribution of a given malware follows exponential distribution, power law distribution with a short exponential tail, and power law distribution at its early, late and final stages, respectively. Extensive experiments have been performed through two real-world global scale malware data sets, and the results confirm our theoretical findings.
The main type of obstacles of practical applications of quantum key distribution (QKD) network are various attacks on detection. Measurement-device-independent QKD (MDIQKD) protocol is immune to all these attacks, and thus, a strong candidate for network security. Recently, several proof-of-principle demonstrations of MDIQKD have been performed. Although novel, those experiments are implemented in the laboratory with secure key rates less than 0.1 b/s. Besides, they need manual calibration frequently to maintain the system performance. These aspects render these demonstrations far from practicability. Thus, justification is extremely crucial for practical deployment into the field environment. Here, by developing an automatic feedback MDIQKD system operated at a high clock rate, we perform a field test via deployed fiber network of 30 km total length achieving a 16.9 b/s secure key rate. The result lays the foundation for a global quantum network, which can shield from all the detection-side attacks.
We provide a generic framework that, with the help of a preprocessing phase that is independent of the inputs of the users, allows an arbitrary number of users to securely outsource a computation to two non-colluding external servers. Our approach is shown to be provably secure in an adversarial model where one of the servers may arbitrarily deviate from the protocol specification, as well as employ an arbitrary number of dummy users. We use these techniques to implement a secure recommender system based on collaborative filtering that becomes more secure, and significantly more efficient than previously known implementations of such systems, when the preprocessing efforts are excluded. We suggest different alternatives for preprocessing, and discuss their merits and demerits.
The Internet has led to the creation of a digital society, where (almost) everything is connected and is accessible from anywhere. However, despite their widespread adoption, traditional IP networks are complex and very hard to manage. It is both difficult to configure the network according to predefined policies, and to reconfigure it to respond to faults, load, and changes. To make matters even more difficult, current networks are also vertically integrated: the control and data planes are bundled together. Software-defined networking (SDN) is an emerging paradigm that promises to change this state of affairs, by breaking vertical integration, separating the network's control logic from the underlying routers and switches, promoting (logical) centralization of network control, and introducing the ability to program the network. The separation of concerns, introduced between the definition of network policies, their implementation in switching hardware, and the forwarding of traffic, is key to the desired flexibility: by breaking the network control problem into tractable pieces, SDN makes it easier to create and introduce new abstractions in networking, simplifying network management and facilitating network evolution. In this paper, we present a comprehensive survey on SDN. We start by introducing the motivation for SDN, explain its main concepts and how it differs from traditional networking, its roots, and the standardization activities regarding this novel paradigm. Next, we present the key building blocks of an SDN infrastructure using a bottom-up, layered approach. We provide an in-depth analysis of the hardware infrastructure, southbound and northbound application programming interfaces (APIs), network virtualization layers, network operating systems (SDN controllers), network programming languages, and network applications. We also look at cross-layer problems such as debugging and troubleshooting. In an effort to anticipate the future evolution of this - ew paradigm, we discuss the main ongoing research efforts and challenges of SDN. In particular, we address the design of switches and control platforms - with a focus on aspects such as resiliency, scalability, performance, security, and dependability - as well as new opportunities for carrier transport networks and cloud providers. Last but not least, we analyze the position of SDN as a key enabler of a software-defined environment.
Recently, most wireless network security schemes merely based on physical layer characteristics tackle the two fundamental issues-device authentication and secret key extraction separately. It remains an open problem to simultaneously achieve device authentication and fast secret key extraction merely using wireless physical layer characteristics, without the help of advanced hardware or out-of-band channel. In this paper, we answer this open problem in the setting of wireless body area networks (BANs). We propose MASK-BAN, a lightweight fast authenticated secret key extraction scheme for intra-BAN communication. Our scheme neither introduces advanced hardware nor relies on out-of-band channels. To perform device authentication and fast secret key extraction at the same time, we exploit the heterogeneous channel characteristics among the collection of on-body channels during body motion. On one hand, MASK-BAN achieves authentication through multihop stable channels, which greatly reduces the false positive rate as compared to existing work. On the other hand, based on dynamic channels, key extraction between two on-body devices with multihop relay nodes is modeled as a max-flow problem, and a novel collaborative secret key generation algorithm is introduced to maximize the key generation rate. Extensive real-world experiments on low-end commercial-off-the-shelf sensor devices validate MASK-BAN's great authentication capability and high-secret key generation rate.
In general, a smart grid is a modernized electrical grid that uses digital technology for measurement, control, and protection functions to ensure a network security. It tries to solve the problem of weather-dependant fluctuations of renewable energy power supplies (e.g. wind turbines, or photo-voltaic systems) when they are connected to an actual power system. In two papers in this issue, we present some of the challenges raised by Smart Grids in instrumentation and measurement applications, putting emphasis on synchrophasor estimation. In this part 1 article, we describe the problem of identifying a normal condition from a fault condition and between a fault condition and an oscillation using phasor estimations in protective relays. In "Synchrophasor Measurement Challenges in Smart Grids," we discuss a novel synchrophasor- estimation algorithm that improves the accuracy of the estimates under oscillations conditions and serves to identify electromechanical modes in Smart Grids. This algorithm ameliorates protection as well as measurement applications in smart grids.
For ages, people have sought inspiration in nature. Biomimicry has propelled inventions from Velcro tape to "cat's eyes" retroreflective road markers. At the same time, scientists have been developing biologically inspired techniques, including genetic algorithms and neural and sensor networks. Although a first glance shows no direct connection between the Internet's offensive and defensive techniques and patterns present in nature, closer inspection reveals many analogies between these two worlds. Botnets, distributed denial-of-service attacks, intrusion detection/prevention systems, and others techniques use strategies that closely resemble actions undertaken by certain species in the natural kingdom. The authors analyze these analogies and conclude by suggesting that the security community should turn to nature in search of new offensive and defensive techniques for virtual world security. This article is part of a special issue on IT security.
Statistics from security firms, research institutions and government organizations show that the number of data-leak instances have grown rapidly in recent years. Among various data-leak cases, human mistakes are one of the main causes of data loss. There exist solutions detecting inadvertent sensitive data leaks caused by human mistakes and to provide alerts for organizations. A common approach is to screen content in storage and transmission for exposed sensitive information. Such an approach usually requires the detection operation to be conducted in secrecy. However, this secrecy requirement is challenging to satisfy in practice, as detection servers may be compromised or outsourced. In this paper, we present a privacy-preserving data-leak detection (DLD) solution to solve the issue where a special set of sensitive data digests is used in detection. The advantage of our method is that it enables the data owner to safely delegate the detection operation to a semihonest provider without revealing the sensitive data to the provider. We describe how Internet service providers can offer their customers DLD as an add-on service with strong privacy guarantees. The evaluation results show that our method can support accurate detection with very small number of false alarms under various data-leak scenarios.
In the present study, the authors investigate robust secrecy rate optimisation problems for a multiple-input-single-output secrecy channel with multiple device-to-device (D2D) communications. The D2D communication nodes access this secrecy channel by sharing the same spectrum, and help to improve the secrecy communications by confusing the eavesdroppers. In return, the legitimate transmitter ensures that the D2D communication nodes achieve their required rates. In addition, it is assumed that the legitimate transmitter has imperfect channel state information of different nodes. For this secrecy network, the authors solve two robust secrecy rate optimisation problems: (a) robust power minimisation problem, subject to the probability based secrecy rate and the D2D transmission rate constraints;(b) robust secrecy rate maximisation problem with the transmit power, the probabilistic based secrecy rate and the D2D transmission rate constraints. Owing to the non-convexity of robust beamforming design based on two statistical channel uncertainty models, the authors present two conservative approximation approaches based on `Bernstein-type' inequality and `S-Procedure' to solve these robust optimisation problems. Simulation results are provided to validate the performance of these two conservative approximation methods, where it is shown that `Bernstein-type' inequality based approach outperforms the `S-Procedure' approach in terms of achievable secrecy rates.
All-optical networks (AONs) are emerging as a promising technology for future telecommunication and data networks. AONs provide huge transmission capacities and are mainly characterized by their transparency to the transmitted traffic. The characteristics of AONs are desirable in many respects, but they give rise to a new set of challenging issues in terms of network security that do not exist in traditional communication networks. The aim of this paper is to present a deep analysis of the security issues of AONs that distinguish them from traditional communication networks. In particular, we focus on the physical security aspect that differs significantly from that in electro-optic and electronic networks and that directly affects the physical infrastructure of AONs. The paper provides an analysis of the infrastructure vulnerabilities of AONs and shows how they can be exploited to perform attacks to the proper functioning of the network. Furthermore, it describes a set of scenarios that may be used to perform a service disruption or a tapping attack on an AON. Finally, it investigates the main challenging issues facing the efficient management of security attacks in AONs and presents a brief overview of the main proposed schemes for attack management in AONs.
Network forensics is a security infrastructure, and becomes the research focus of forensic investigation. However many challenges still exist in conducting network forensics: network has produced large amounts of data;the comprehensibility of evidence extracting from collected data;the efficiency of evidence analysis methods, etc. To solve these problems, in this paper we develop a network intrusion forensics system based on transductive scheme that can detect and analyze efficiently computer crime in networked environments, and extract digital evidence automatically. At the end of the paper, we evaluate our method on a series of experiments on KDD Cup 1999 dataset. The results demonstrate that our methods are actually effective for real-time network forensics, and can provide comprehensible aid for a forensic expert.
Because network security has become one of the most serious problems in the world, intrusion detection is an important defence tool of network security. In this paper, A cooperative and adaptive intrusion detection method is proposed and a corresponding intrusion detection model is designed and implemented. The E-CARGO model is used to build the collaborative and adaptive intrusion detection model. The roles, agents and groups based on 2-class Support Vector Machines (SVMs) and Decision Trees (DTs) are described and built, and the adaptive scheduling mechanisms are designed. Finally, the KDD CUP 1999 data set is used to verify the effectiveness of our method. Experimental results show that the collaborative and adaptive intrusion detection method proposed in this paper is superior to the detection of the SVM in the detection accuracy and detection efficiency.
In this paper, we investigate the performance of Nth best relay selection networks with secrecy constraints where several eavesdroppers try to overhear the source message. In order to enhance the network security, a single jammer is introduced to jam the eavesdroppers. In addition, sub-optimal selection strategy is introduced to combat the malicious attempt of eavesdroppers. We derive the exact secrecy performance in terms of probability of the non-zero secrecy capacity and the secrecy outage probability of the proposed relay selection approach. Based on these expressions, the effect of several important network parameters, i.e., the number of relays and the number of eavesdroppers, as well as the quality of the relay links, jammer links, and eavesdroppers links, on the proposed network are analytically characterized.
The frequency and the extent of damages caused by network attacks have been actually increasing greatly in recent years, although many approaches to avoiding and detecting attacks have been proposed in the community of network security. Thus, how to fast detect actual or potential attacks has become an urgent issue. Among the detection strategies, behavior-based ones, which use normal access patterns learned from reference data (e.g., History traffic) to detect new attacks, have attracted attention from many researchers. In each of all such strategies, a learning algorithm is necessary and plays a key role. Obviously, whether the learning algorithm can extract the normal behavior modes properly or not directly influence the detection result. However, some parameters have to determine in advance in the existing learning algorithms, which is not easy, even not feasible, in many actual applications. For example, even in the newest learning algorithm, which called FHST learning algorithm in this study, two parameters are used and they are difficult to be determined in advance. In this study, we propose a parameter less learning algorithm for the first time, in which no parameters are used. The efficiency of our proposal is verified by experiment. Although the proposed learning algorithm in this study is designed for detecting port scans, it is obviously able to be used to other behavior-based detections.
The following topics are dealt with: information security;Internet applications;cyber security and network security.
Start of the above-titled section of the conference proceedings record.
Smart grid technologies such as synchrophasors using Phasor Measurement Units (PMUs), make real-time monitoring, control and data analysis of the electric power grid possible. The PMU network measures voltage and current phasors across the electrical power grid, and sends `reports' to control centers. Synchrophasor technology enables reliable and efficient power system operation;but may make the system vulnerable to cyber-attacks. In this paper, security vulnerabilities found in literature, that are relevant to PMUs, are discussed and mapped to four general attack classes. Known network security vulnerabilities are addressed in hopes of exposing gaps where further research needs to be conducted on PMU networks.
Rising penetration of renewable energy sources in present day transmission systems requires increased attention to network security. Existing computation tools in power system operations evaluate individual scenarios for power injection and network configuration but do not fully consider nearby regions in the operating space. Such tools may lead to market transactions, preventive actions, or corrective actions that are nominally efficient but poor in general. In this paper, the interval based ISI method is reformulated to a security oriented form potentially applicable in energy management systems. The presented results include an algorithm defined within a tractable optimization framework that computes maximal power injection sets containing power injection profiles that are necessarily secure in terms of physical constraints and the N-1 security criterion. The method is tested on the IEEE 14 bus test system.
The following topics are dealt with: information security;network security;cloud computing;ubiquitous computing.
Reinforcement learning techniques become more popular in computer network security. The same reinforcement learning techniques developed for network security can be applied to software security as well. This research summarizes a work in progress attempt to incorporate Q-learning algorithm in software security. The Q-learning method is embedded as part of the software itself to provide a security mechanism that has ability to learn by itself to develop a temporary repair mechanism. The results of the experiment express that given the right parameters and the right setting the Q-learning approach rapidly learns to block all malicious actions. Data analysis on the Q-values produced by the software can provide security diagnostic as well. A larger scale experiment is expected to be seen in the future work.
An ever increasing impact and amount of network attacks have driven many organizations to deploy various network monitoring and analysis systems such as honeypots, intrusion detection systems, log analyzers and flow monitors. Besides improving these systems a logical next step is to collect and correlate alerts from multiple systems distributed across organizations. The idea is to leverage a joint effect of multiple monitoring systems to build a more robust and efficient system, ideally, lacking the shortcomings of the individual contributing systems. This paper presents an analysis of alert reports gathered from several such detectors deployed in national research and education network (NREN). The analysis focuses on the correlations of reported events in temporal domain as well as on the correlations of different event types.
This paper compares and contrasts the most widely used network security datasets, evaluating their efficacy in providing a benchmark for intrusion and anomaly detection systems. The antiquated nature of some of the most widely used datasets along with their inadequacies is examined and used as a basis for discussion of a new approach to analyzing network traffic data. Live network traffic is collected that consists of real normal traffic and both real and penetration testing attack data. Attack data is then inspected and labeled by means of manual analysis. While network attacks and anomaly features vary widely, they share some commonalities that are examined here. Among these are: self-similarity convergence, periodicity, and repetition. Further, the knowledge inherent in the definition of network boundaries and advertised services can provide crucial context that allows the network analyst to consider self-aware attributes when examining network traffic sessions. To these ends the Session Aggregation for Network Traffic Analysis (SANTA) dataset is proposed. The motivation and the methodology of collection, aggregation and evaluation of the raw data are presented, as well as the conceptualization of the SANTA attributes and advantages provided by this approach.
The tremendous growth in computer network and Internet usage, combined with the growing number of attacks makes network security a topic of serious concern. One of the most prevalent network attacks that can threaten computers connected to the network is brute force attack. In this work we investigate the use of machine learners for detecting brute force attacks (on the SSH protocol) at the network level. We base our approach on applying machine learning algorithms on a newly generated dataset based upon network flow data collected at the network level. Applying detection at the network level makes the detection approach more scalable. It also provides protection for the hosts who do not have their own protection. The new dataset consists of real-world network data collected from a production network. We use four different classifiers to build brute force attack detection models. The use of different classifiers facilitates a relatively comprehensive study on the effectiveness of machine learners in the detection of brute force attack on the SSH protocol at the network level. Empirical results show that the machine learners were quite successful in detecting the brute force attacks with a high detection rate and low false alarms. We also investigate the effectiveness of using ports as features during the learning process. We provide a detailed analysis of how the models built can change as a result of including or excluding port features.
Advanced Technology in the area of intrusion detection is the Honeypot technology that unlike common IDS s tends to provide the attacker with all the necessary resources needed for a successful attack. Honeypot provide a platform for studying the methods and tools used by the intruders, thus deriving their value from the unauthorized use of their resource. To provide scalable, early warning and analysis of new Internet threats like worms or automated attacks, we propose globally distributed, hybrid monitoring model that can capture and analyze new vulnerabilities and exploits as they occur. To achieve this, our Model increases the exposure of high-interaction honeypots to these threats by employing low-interaction honeypots as frontend content filters. Host-based techniques capture relevant details such as packet payload of attacks while network monitoring provides wide coverage for quick detection and assessment. To reduce the load of the backends, we filter prevalent content at the network frontends and use a novel handoff mechanism to enable interactions between network and host components.
A plethora of mobile wireless networking approaches, from multi-hop infrastructure-less networking to mobile offloading and crowd sourcing, relies on establishing communication directly between devices. However, the 802.11 ad-hoc mode, as the designated technological means to realize device-to-device communication, suffers from missing support by vendors and operating system and lacks 802.11 functionality support. If at all, mobile wireless networking approaches reach a very low number of compatible devices and have to tolerate low network performance, deprecated WEP network security, and a lack of energy saving mechanisms. Eventually, this lack of a technological basis prevents the timely and beneficial real-world adoption of mobile networking. We thus propose such a basis in MA-Fi, multi-hop mobile networking using the 802.11 infrastructure mode. Building on comprehensive vendor and device support, MA-Fi realizes 802.11n performance, WPA2 security, and efficient energy saving mechanisms in ubiquitously compatible, mobile 802.11 infrastructure mode networks. Specifically, MA-Fi uses network virtualization to establish a two-tiered network topology that seamlessly incorporates legacy Wi-Fi devices. In comparison to the ad-hoc mode, MA-Fi achieves throughput of up to 340% while reducing the network-wide energy consumption by up to 75%.
Decentralized security software deployment is important to achieve reliable and secure network operations. In this paper, we consider the joint traffic routing and security software deployment problem. In this problem, the network can make decisions about routing, security protection and recovery software to minimize total energy consumption of all nodes in the network. Specifically, the network has to trade off between energy consumption and preventing security damages by running protection software based on uncertainty about attacks. The protection software has to be deployed along the route of traffic flow, and hence the network has to optimize traffic routing. We first formulate this problem as a stochastic programming model. To obtain the solution, we transform the stochastic programming model into the deterministic problem, which can be solved using a standard solver. The performance evaluation reveals that the optimal solution depends largely on energy budget and energy consumption of the nodes for transferring traffic and running security software.
As the popularity of software defined networks (SDN) and OpenFlow increases, policy-driven network management has received more attention. Manual configuration of multiple devices is being replaced by an automated approach where a software-based, network-aware controller handles the configuration of all network devices. Software applications running on top of the network controller provide an abstraction of the topology and facilitate the task of operating the network. We propose OpenSec, an OpenFlow-based security framework that allows a network security operator to create and implement security policies written in human-readable language. Using OpenSec, the user can describe a flow in terms of OpenFlow matching fields, define which security services must be applied to that flow (deep packet inspection, intrusion detection, spam detection, etc) and specify security levels that define how OpenSec reacts if malicious traffic is detected. We implement OpenSec in the GENI testbed to evaluate the flexibility, accuracy and scalability of the framework. The experimental setup includes deep packet inspection, intrusion detection and network quarantining to secure a web server from network scanners. We achieve a constant delay when reacting to security alerts and a detection rate of 98%.
In this paper, we analyze the throughput of a novel mobile access coordinated wireless sensor network architecture (MC-WSN) under single path and multipath routing. The obtained throughput expressions highlight the trade-off between achieving high throughput performance and improving the network security strength. The results reveal the importance of: (i) minimizing the number of hops in maximizing the throughput, and (ii) adopting routing diversity in combating malicious attacks and network failure conditions. We control the number of hops in data transmission through optimal topology design and active network deployment achieved by the mobile access point (MA). To combat routing attacks, we propose a secure routing path selection approach, and show the impact of the proposed approach on improving the throughput performance under malicious attacks.
In this paper a round-the-year based network reinforcement solution generator for the transmission expansion planning problem is proposed. The solution generator makes use of a round-the-year network security analysis which combines zonal market simulations with load flow calculations. This allows a robust evaluation of the reinforcement candidates as all the hours in the year are considered. The solver iterates sequentially over possible reinforcements until no more congestion occurs in the grid, under both normal and N-1 branch outage situations. New criteria for assessing the grid congestion level are defined and used in the evaluation of candidates. The decrease in the proposed "grid severity index" is used to measure the effectiveness of each reinforcement candidate. At the end of the process, an ordered list of reinforcements is given together with the criteria values at each reinforcement step. The New England IEEE test system is used for validating the method.
In recent years, many countries have used e-government to provide high quality services to their citizens. Thus, a number of studies have investigated user acceptance of e-government through the use of adoption models, such as the Unified Theory of Acceptance and Use of Technology (UTAUT) model. However, these models do not focus sufficiently on security. In order to develop a more security-focused adoption model for investigating user acceptance of e-government, it is important to first understand the security challenges that may inhibit the adoption of e-government. This paper considers these security challenges based on the end user's perspective, identifying the extent of the challenges. For example, 85.5% of the participants agree that there is a lack of user awareness. In addition, 62.4% of the participants believe that culture plays an important role in e-government security. Also, 49.8% of the participants are worried about privacy when using e-government services.
Phishing takes advantage of the way humans interact with computers or interpret messages;and also that many online authentication protocols place a disproportional burden on human abilities. A security ceremony is an extension of the concept of network security protocol and includes user interface and human-protocol interaction. It is one way of extending the reach of current methods for social, technical and contextual analysis of security protocols to include humans. In this paper, we propose a Human Factors in Anti-Phishing Authentication Ceremonies (APAC) Framework for investigating phishing attacks in authentication ceremonies, which builds on The Human-in-the-Loop Security Framework of communication processing. We show how to apply the APAC framework to model human-protocol behaviour. The resulting Model for Analysing APAC correlates the framework components and examines how the authentication tasks required to be performed by humans influence their decision-making and consequently their phishing detection.
Distributed Denial of Service (DDoS) attacks are a serious threat to network security. Servers of many companies and/or governments have been victims of such attacks. DDoS attacks jam the network service of the target using multiple bots hijacked by crackers and send numerous packets to the target server. In such an attack, detecting the crackers is extremely difficult, because they only send a command by multiple bots from another network and then leave the bots quickly after command execute. Therefore, we need an intelligent detection system for DDoS attacks to defend network services. To develop the system, we utilized machine learning techniques to study the patterns of DDoS attacks and detect them. We analyzed large numbers of network packets provided by the Center for Applied Internet Data Analysis, and detected some important patterns that affect the accuracy of the detection system. We implemented the detection system using the patterns of DDoS attacks. A support vector machine with the radial basis function (Gaussian) kernel is its core part. The detection system is accurate in detecting DDoS attacks.
Moving target defense is an area of network security research in which machines are moved logically around a network in order to avoid detection. This is done by leveraging the immense size of the IPv6 address space and the statistical improbability of two machines selecting the same IPv6 address. This defensive technique forces a malicious actor to focus on the reconnaissance phase of their attack rather than focusing only on finding holes in a machine's static defenses. We have a current implementation of an IPv6 moving target defense entitled MT6D, which works well although is limited to functioning in a peer to peer scenario. As we push our research forward into client server networks, we must discover what the limits are in reference to the client server ratio. In our current implementation of a simple UDP echo server that binds large numbers of IPv6 addresses to the ethernet interface, we discover limits in both the number of addresses that we can successfully bind to an interface and the speed at which UDP requests can be successfully handled across a large number of bound interfaces.
We study how an underlying network property affects network security when nodes are rational and have four choices - i) invest in protection, ii) purchase (incomplete coverage) insurance, iii) invest in protection and purchase insurance, or iv) do nothing. More specifically, using a population game model, we examine how the degree distribution of nodes influences their choices at Nash equilibria (NEs) and overall security level. We first show that there exists a degree threshold at NEs so that only the populations with degrees greater than or equal to the threshold invest in protection. Second, as the weighted degree distribution of nodes becomes stochastically larger, the risk or threat posed by a neighbor decreases, even though the aforementioned degree threshold tends to rise, hence only nodes with increasingly higher degrees invest in protection, at the same time. Third, we show that the social optimum also possesses similar properties. Finally, we derive an upper bound on the price of anarchy, which is an affine function of the average degree of nodes. This upper bound is tight in that it is achieved in some scenarios.
Applications of high-performance graph analysis range from computational biology to network security and even transportation. These applications often consider graphs under rapid change and are moving beyond HPC platforms into energy-constrained embedded systems. This paper optimizes one successful and demanding analysis kernel, betweenness centrality, for NVIDIA GPU accelerators in both environments. Our algorithm for static analysis is capable of exceeding 2 million traversed edges per second per watt (MTEPS/W). Optimizing the parallel algorithm and treating the dynamic problem directly achieves a 6.9 average speed-up and 83% average reduction in energy consumption.
We created a pixel map for multivariate data based on an analysis of the needs of network security engineers. Parameters of a log record are shown as pixels and these pixels are stacked to represent a record. This allows a broad view of a data set on one screen while staying very close to the raw data and to expose common and rare patterns of user behavior through the visualization itself (the "Carpet"). Visualizations that immediately point to areas of suspicious activity without requiring extensive filtering, help network engineers investigating unknown computer security incidents. Most of them, however, have limited knowledge of advanced visualization techniques, while many designers and data scientists are unfamiliar with computer security topics. To bridge this gap, we developed visualizations together with engineers, following a co-creative process. We will show how we explored the scope of the engineers' tasks and how we jointly developed ideas and designs. Our expert evaluation indicates that this visualization helps to scan large parts of log files quickly and to define areas of interest for closer inspection.
Cybersecurity education does not only confine to the technical studies embodying network security, malware analysis and reverse reengineering, application security, and operating systems security. The increasing numbers of Cyberterrorism and incidents of hacktivism suggest that Cybersecurity pertains to politics, religion, and culture. Drawing on globalization shaped by the economic, legal, political, religion, and social dimensions, extant literature discloses that infusing social science, cultural and political studies into Cybersecurity education could better prepare students for the job market by making students realize the complexity in the real world. This study presents the efforts of integrating cultural, social, and political dimensions into the Cybersecurity curriculum in a public, regional university in the Midwest of the United States. In particular, this study presents the approaches of introducing intelligence analysis coursework that requires students to understand the analysis of competing hypotheses for drawing conclusion related to the possible Cyberattack from a foreign nation, identify the cultural differences across nations for comprehending the hacking motivation of a different nation, and recognize individual's cognitive and cultural biases during the process of evaluating a cultural event.
Secure group communication is an important research area in the field of cryptography and network security, because the group communication like electronic conferences, video chatting, video games etc. are rapidly increasing. Group key agreement protocols allow all members of the group to agree on the same session key which is used later for secure group communication. The design of secure group key agreement protocol can be very critical for achieving security goals. Many group key agreement protocols have been established for wired and wireless group communication. This paper proposes an ECC (Elliptic Curve Cryptography) based authenticated group key agreement protocol for wireless scenario. The proposed protocol uses the concepts of elliptic curve cryptography to minimize the computation overheads and AES (Asymmetric Encryption Standards) to maintain efficiency. The proposed protocol consists of a set of users and a trusted server, where server and the user contribute to create the group key. The performance and security analysis shows that the proposed protocol is secure against passive and active attack and performs better in terms of computation and communication cost from other related protocols.
For Liao Hui and others proposed network terminal security assessment index system exists index weights unreasonable distribution problem, using Delphi method and AHP calculate each index weight and the weights of total ranking of lowest level indexes relative to the highest lever indexes, identified the indicators which have greatest impact to the assessment objectives and apply it to a host of information security evaluation system. Combined with examples to prove the index system after improving its weight distribution can be more scientifically reflect the importance of the indexes in the evaluation system and the result of the host information security evaluation is reasonable and comprehensive.
The following topics are dealt with: information security & privacy;software design, testing and reuse;social network security;social media, crowdsourcing, and public health;Big Data/networks & cloud computing;information reuse & extraction;multiagent system;data analysis, management & integration;health informatics;data mining and knowledge discovery;AI & decision support systems;formal methods integration;cyber physical systems;and fuzzy systems & heuristic optimization.
Provides a listing of current committee members and society officers.
Firewalls are a defense mechanism for network security. Today, firewalls have played a pivotal role in a wide spectrum of circumstances, from enterprise networks to home networks. Firewall rules have their execution semantics. Firewalls are often networked to establish perimeters for different parts of an enterprise with differing security policy requirements. Hence, rules in intra-firewall and inter-firewall settings interact, sometimes creating unintended side-effects. The complexity in utilizing firewalls to implement consistent and coherent security policies to safeguard enterprise networks poses great challenges to the network security as a whole. A major challenge is firewall inconsistencies that compromise the effectiveness of firewall protection. In this paper, we propose an approach to detecting and resolving major types of firewall inconsistencies. We describe our detection algorithms and resolution strategies to firewall inconsistencies, and report some initial results of a tool implementing the proposed approach. The main contribution of our work lies in the fact that our approach takes advantage of two necessary conditions in firewall inconsistencies, resulting in a more effective detection method.
Firewalls form an essential element of modern network security, detecting and discarding malicious packets before they can cause harm to the network being protected. However, these firewalls must process a large number of packets very quickly, and so can't always make decisions based on all of the packets' properties (features). Thus, it is important to understand which features are most relevant in determining if a packet is malicious, and whether a simple model built from these features can be as effective as a model which uses all information on each packet. We explore a dataset with real-world firewall data to answer these questions, ranking the features with 22 feature selection techniques and building classification models using four classifiers (learners). Our results show that the top two features are proto and dst (representing the network protocol and destination IP address, respectively), and that models built using these two features in combination with the Naive Bayes learner are highly effective while being minimally computationally expensive. Such models have the potential to replace conventional firewalls while lowering computational needs.
Aiming at the tracking control for the underactuated surface ship at open sea, an adaptive robust neural network method was proposed. At present, most researches would consider the underactuated characteristics and parameters uncertainties and environmental disturbance. The controller structure was designed by nonlinear algorithm, thus ensuring the tracking loop system robustness. Proposed adaptive neural network control method, used to estimate the uncertainty in the external environment and model parameters. Based on Lyapunov stability theory, we designed the adaptive law of neural network weights, thus ensuring the tracking error near zero. The stability and the robustness of the closed-loop system are ensured by Lyapunov stability theory. Simulation results illustrate the effectiveness of the proposed control method.
In research of Optimization of performance of wireless sensor networks to improve anti-attack capability, key management is an important security issue to be solved. Due to the severe restriction on resources and work environment, the classic key distribution algorithms are not suitable for wireless sensor networks. Aiming to improve the capability of security and feasibility of wireless sensor networks, after considering the issues of network security, traffic problems, memory problems and computational complexity problem, this paper proposes a key management scheme based on grouping within cluster to solve the problem of poor salability, low work efficiency and communication times . This scheme makes a big improvement of group key generation method. Cluster heads and group heads in the clusters are respectively used to generate and distribute keys via secure channels in each group. The simulation results show that this scheme further improves network's ability to resist DOS (denial of service) attack, reduces the energy consumption and time consumption in key negotiation and improves the survival time and efficiency of entire network.
The connection between PROFIBUS network and Internet can realize remote real-time monitoring device on the fieldbus. This paper conducts detailed analysis and research advance on network security based on the interconnection methods between PROFIBUS and Internet, and proposes a PROFIBUS industrial network security model. An embedded gateway is developed especially for meeting the security requirement in this research work. With compact structure and low cost, it paves the way to realize real-time monitoring PROFIBUS device safely via Internet.
The following topics are dealt with: learning management system;distance learning;social network;wireless sensor network;mobile health technology;wireless power transfer system;unified theory of acceptance and use of technology;mobile learning;mobile network security;and geographic information system.
Many-core accelerator chips are becoming increasingly popular these days for its high performance floating-point performance exceeding 1 Tflops per chip. Aho-Corasick (AC) is a multiple patterns string matching algorithm commonly used in computer and network security, bioinformatics, among others. In order to simultaneously match a number of string patterns with respect to the input text data, a Deterministic Finite Automata (DFA) is constructed from a given set of pattern strings. The DFA is referenced almost randomly, whereas the input data is sequentially accessed. As the number of pattern strings increases, the irregular DFA accesses lead to poor cache locality and low overall performance. In this paper, we present a cache locality optimizing parallelization on the many-core accelerator chip, the Intel Xeon Phi. A given set of pattern strings is partitioned into into multiple sets of a smaller number of patterns so that multiple small DFAs are constructed instead of single large DFA. The accesses to multiple small DFAs lead to significantly smaller cache footprints in each core's cache and result in impressive performance improvements. Experimental results on the Intel Xeon Phi 5110P show that our approach delivers up to 2.00  speedup compared with the previous approach using single large DFA.
Database is an important asset of any leading and emerging industry and this database needs to improved security features from various threats in the network and database repository. Most of an organization's sensitive and proprietary data resides in a Database Management System (DBMS). Data security is a major issue in any web-based application and database repository. Although there are various model implemented in the network for the security of these databases. Real world web databases have information that needs to be securely stored and accessed. In this paper we discussed proposed work classification based approach to create database policy dynamically and using these policy find intrusion in user request and response the request accordingly. The focus of our work is to develop advanced security solutions for protecting the data residing in a DBMS. Our approach is to develop an Intrusion Detection and Response (IDR) system. We introduce a policy-driven intrusion response mechanism that is capable of issuing an appropriate response based on the details of the anomalous request and discuss the implementation details on the same and report experimental results that show that our techniques are feasible and efficient.
The following topics are dealt with: broadband wireless networks;optical networks;wireless sensor networks;cognitive radio networks;wireless LAN;network security;software defined networks;transport networks.
Network traffic classification plays an important role in the areas of network security, network monitoring, QoS and traffic engineering. In this paper, we design a network traffic classifier based on the statistical features extracted from network flows. Instead of deriving the statistical characteristics per flow, our model make use of features extracted from the first few seconds of each flows. The first few seconds of each flow is divided into overlapping time-based windows. This approach enables our classifier to classify each flow early. Attribute selection algorithms Chi-Square, CON and CFS are used to obtain the optimal subset of features. We give a comparative analysis of the result on the said approach based on the classification algorithms (Decision tree (C4.5), Naive Bayes, Bayesian Belief Network and SVM). We also present a single class classifier implementation of C4.5 algorithm. The experimental results show that the proposed method can achieve over 99% accuracy for all testing dataset. Using the proposed method, C4.5 algorithm delivers high speed and accuracy. By taking inference from these offline classifiers, we build an online standalone classifier using C/C++. We used the following applications: Skype, Gtalk and Asterisk.
Key aspect of communication is security. Encryption of information to make it inaccessible to unauthorized recipients is a main area in network security. AES encryption is one of the most secure encryption algorithms. But with improving technology, attacks which can be used for easy cryptanalysis of encrypted information have been developed. One of the main reasons for vulnerability of AES encryption algorithm is the use of static S-Boxes. Also with attempts made to tap the key, AES has become more vulnerable to cryptanalysis attacks. Static S-Boxes make it very easy for reverse engineering which form the basis of Super S-Box attack. Hence there have been a lot of proposals for generation of S-Boxes which are key dependent. Majority of those algorithms involve probabilistic methods which are very complicated for hardware implementation. In this paper, we present a synthesizable algorithm which involves the use of conventional bitwise operations for generation of key dependent S-Boxes. Also a dual key based AES is presented in the paper which is FPGA implementable. The algorithm is reliable in terms of security and also suitable for hardware implementation. Mathematical analyses have been carried out on the algorithm to compute the reliability.
Users currently experience different levels of protection when accessing the Internet via their various personal devices and network connections, due to variable network security conditions and security applications available at each device. The SECURED project addresses these issues by designing an architecture to offload security applications from the end-user devices to a suitable trusted node in the network: the Network Edge Device (NED). Users populate a repository with their security applications and policy, which will then be fetched by the closest NED to protect the user's traffic when he connects to a network. This setting provides uniform protection, independent of the actual user device and network location (e.g. public WiFi hotspot or 3G mobile connection). In other words, a user-centric approach is fostered by this architecture, opposed to the current device- or network-based security schema, with cost and protection benefits and simultaneously enabling new business models for service and network providers.
Deep packet inspection or DPI is now a fast growing application technology in the field of network security, which requires the network security platform has a higher speed to handle a large number of session connections, and track the status of these connections quickly. This paper proposed the MP-DPI, a many-core based network processing platform, which uses the ATCA standard modular design, makes use of the integrated many-core network process accelerate engine, and integrates a popular open source DPI system named SNORT. The experiment result shows that in the same power consumption, the throughput of MP-DPI platform is three times as large as traditional X86 servers.
Based on the review of current prediction algorithms of network security situation, prediction algorithms based on Kaiman filtering are studied. A prediction algorithm of network security situation based on grey correlation entropy Kaiman filtering is presented, hoping to be more helpful to network administrators through providing them information more effectively. First correlation of factors influencing network security situation is analyzed by Grey correlation entropy analysis method, and key influencing factors are selected. Then according to these influencing factors corresponding process equation and prediction equation are established. Finally, network security situation prediction is made recursively by Kaiman filtering. Experiment results show that the prediction by this method is more precise compared to GM(1, 1) and general Kaiman algorithm, and its real-time performance is better than RBF algorithm.
Since last few years, users have been paying significant attention and interest towards the WiMAX Internet technology because of the salient features i.e. Higher data rate, broad coverage, quality of service (QoS), scalability, security, and mobility support to the subscribers. Mobile WiMAX (IEEE 802.16e) supports the handover process and mobility of subscriber stations. WiMAX network security is a vital requirement to increase the reliability and to prevent from the various attacks. Implementation of top level security is highly required to lessen the vulnerabilities. Jamming attack, scrambling attack and water-torture attack are among the most severe threats to the physical layer of mobile WiMAX. This paper presents a collaborative attack model based on the combination of scrambling and water-torture attacks. Analysis of the paper shows that the cooperation of two or more attacks/attackers may be more devastating for the network security.
Firstly, this paper analyzes the present research situation of intranet security from different aspects. Then, the main information security issue of campus network of scientific research is proposed: that is share information with limited right and "no tolerant" with the confidentiality of information. Lastly, the paper designs of campus network of scientific research based on security management, virtual resource pool technology and multi-layered network security model. Those technologies provide powerful supports for the safety of intranet with higher confidentiality level.
Intrusion detection is the problem of identifying unauthorized use, misuse, and abuse of computer systems or network resources by both system insiders and external penetrators. The proliferation of heterogeneous computer networks provides additional implications for the intrusion detection problem. The increased connectivity of computer systems gives greater access to outsiders, and makes it easier for intruders to escape from detection. IDS's are based on the belief that an intruder's behavior will be noticeably different from a legitimate user. An intrusion detection system (IDS) monitors network traffic for suspicious activity and alerts the system or network administrator. It detects and identifies unauthorized use, misuse, and abuse of computer systems from both the system insiders and external penetrators. Intrusion detection systems (IDS) are becoming essential components in a secure network environment, allowing for early detection of malicious activities and attacks. By employing information provided by IDS, it is possible to apply appropriate countermeasures techniques and mitigate attacks that would otherwise seriously undermine network security. In this paper we are proposing a techniques, which maintain the small distributed signature database and dynamically update it based on the change in network environment. Overall objective of the research work is to improve the efficiency and self configurability of the IDS in a dynamic and changing environment.
Attackers are most likely to exploit invalidated and unsanitized user input with several attacks such as cross-site scripting (XSS) or SQLinjection. Many methods were proposed to prevent those attacks. Some of them were created to learn about pattern and behavior of the attacker. That is honeypot. Honeypot is classified into two types based on the simulation that honeypot can do : low interaction and high interaction. In this paper, we propose a low-interaction honeypot for emulating vulnerabilities that can be exploited using XSS and SQL injection attacks. But this honeypot not only records attacker's request, but also try to expose attacker identity by using some browser exploitation techniques. Some attackers would use techniques to hide their identity, thus they couldn't be tracked. Our proposed honeypot was trying to overcome this problem by giving them malicious JavaScript codes. The malicious JavaScript codes will be run when an attacker open the honeypot's website. We have conducted several test to see how our honeypot's performance. Our honeypot could catch more useful information about the HTTP request than popular web-based honeypot, Glastopf. Moreover, there were attacker's social media accounts caught by using LikeJacking technique although they might have used proxy or TOR to hide their identity.
This study focuses on the application of security metrics to a computer network. Mean Time-to-Compromise (MTTC) metric and VEA-bility metric are selected for this study. MTTC is calculated using a set of equations based on the known vulnerabilities of the system. VEA-bility is selected because it uses CVSS that has a wide coverage of security aspects. The input data for both metrics are obtained from Nessus, a network security tool. Both metrics give numerical results which are simple to comprehend to average clients. The purpose of this study are to calculate MTTC and VEA-bility values of the network, to compare the security level of different network configurations, also to compare the feasibility and convenience of using both metrics. The results of the study can be used as recommendations for network security assessment and references to determine policies relating to computer network management.
One of the most important fields in discrete mathematics is graph theory. Graph theory is discrete structures, consisting of vertices and edges that connect these vertices. Problems in almost every conceivable discipline can be solved using graph models. The field graph theory started its journey from the problem of Konigsberg Bridges in 1735. This paper is a guide for the applied mathematician who would like to know more about network security, cryptography and cyber security based of graph theory. The paper gives a brief overview of the subject and the applications of graph theory in computer security, and provides pointers to key research and recent survey papers in the area.
As current research can not solve the problem of making multi-source heterogeneous network security situation elements uniformly described, a network security situation elements fusion method based on ontology is proposed. Firstly, a fusion model is constructed which contains network environment, network vulnerability, network attack, network security incident and sensor as key class. Secondly, three fusion rules which contain alert aggregation, alert verification and attack session reconstruction are formulated by using Semantic Query-enhanced Web Rule Language. Finally, the application example shows that the method can make situation elements uniformly described. Security situation information has higher complementary and lower redundancy, which achieves better fusion effect.
Honeynets have now become a standard part of security measures within the organization. Their purpose is to protect critical information systems and information;this is complemented by acquisition of information about the network threats, attackers and attacks. It is very important to consider issues affecting the deployment and usage of the honeypots and honeynets. This paper discusses the legal issues of honeynets considering their generations. Paper focuses on legal issues of core elements of honeynets, especially data control, data capture and data collection. Paper also draws attention on the issues pertaining to privacy and liability. The analysis of legal issues is based on EU law and it is supplemented by a review of the research literature, related to legal aspects of honeypots and honeynets.
Providing security for Dynamic Cluster based Mobile Ad-hoc networks (MANET) is a vital task to improve network security with less compromising mobility and with reduced dedicated Data occupancy. Key generation, distribution and authentication are major tasks of a key management system. A flexible and strong model is required to handle key management since dynamic ad-hoc networks are more prone to many kinds of hacking activity. In this paper a new procedure is implemented using Chinese remainder Theorem based Key-management (CRT-KM) and it is compared with one existing ID-Based Multiple Key Management system (IMKM). The performances are compared interms of parameters like mobility, Overhead message;security and power consumption.
Secure file transmission from one android user to another android user is important issue in network security. Byte rotation algorithm (BRA) and AES algorithm solve the problem of secure file transmission using encryption and decryption. In this paper we have introduced BRA for file encryption and decryption within minim delay. These algorithms improves the security and reduce time for process of file encryption and decryption. To provide security for different types of file like image, text, audio and video using BRA. Results are taken using Net Bean java complier for AES and BRA. Compare result for various parameters like BRA and AES encryption time as well as decryption time. To improve file security using random key generation (128bit) is used. User can set time and frequency during file transmission. When these conditions are satisfied file and key used for encryption will be deleted.
In this paper, we examine a novel approach to network security against passive eavesdroppers. By configuring antenna array beam patterns to transmit data to specific regions, it is possible to create defined regions of coverage for targeted users. By adapting the antenna configuration according to the intended user's channel state information, this allows the vulnerability to eavesdropping to be reduced. We present the application of our concept to 802.11n networks where an antenna array is employed at the access point. A range of array configurations is examined in a ray-tracing model and realized using the Wireless Open-Access Research Platform (WARP).
Publish/subscribe pattern is a reliable way of message service. Compared with Store-and-forward mode and Web Service request/response mode, the way of asynchronous message delivery is better in flexibility and scalability. The message is pushed by message server to the subscriber so that the message consumer can get message without request. Access control is managed by the message server in the traditional message service model, but it is not suitable in complex SWIM (System Wide Information Management). SWIM is a very complex and huge system, the message sender and the message server are probably not controlled by a same department, so it is difficult to guarantee the fairness and security of the access control in the message service. In order to improve the credibility of information security transmission between different departments, the message service security model based on the sender control is proposed in this paper according to taking JMS publish/subscribe model as an example.
A trap set to detect attempts at unauthorized use of information systems. But setting up these honeypots and keep these guzzling electricity 24X7 is rather expensive. Plus there is always a risk of a skillful hacker or a deadly malware may break through this and compromise the whole system. Honeypot name suggest, a pot that contents full of honey to allure beers, but in networks Scenario honeypot is valuable tool that helps to allure attackers. It helps to detect and analyze malicious activity over your network. However honeypots used for commercial organization do not share data and large honeypot gives read only data. We propose an Arm based device having all capability of honeypots to allure attackers. Current honeypots are based on large Network but we are trying to make s device which have the capabilities to establish in small network and cost effective. This research helps us to make a device based on arm board and CCFIS Software to allure attackers which is easy to install and cost effective. CCFIS Sensor helps us to Capture malware and Analysis the attack. In this we did reverse Engineering of honeypots to know about how it captures malware. During reverse engineering we know about pros and cons of honeypots that are mitigated in CCFIS Sensor. After Completion of device we compared honeypots and CCFIS Sensor to check the effectiveness of device.
It is very important to maintain a high level security to ensure safe and trusted communication of information between various organizations. As the growth of computer networks is increasing day by day in modern society, therefore network security is one of the hottest issue to be solved. This paper proposes application of Genetic Algorithm (GA) for network intrusion detection system. Any action that tries to compromise the confidentially, integrity or availability of resources is termed as intrusion and the detection of such intrusion by the help of a system is called as intrusion detection system (IDS). The proposed GA is a hybrid evolutionary technique specially developed for intrusion detection. The technique developed has been implemented and the experiments have shown the promising results. The evolutionary process of this technique including its parameters and the experimental outcomes are discussed at length in this paper. In this paper we have made the comparison of our technique with the GNN classifier technique and the outcomes from our experiment have given promising results towards applying genetic algorithm for network intrusion detection.
Power distribution communication network is an important part of smart grid. Through the study of the communication network security isolation key theory and technology. This paper analyses the anomaly detection method for the network service and presents a kind of network security isolation model based on the anomaly detection. The paper describes the overall concept, network structure, operation flow and key technical modules of the model, which provides a feasible solution to the network information security.
Aiming at the shortcomings of the information security under the environment of complex network, intrusion tolerance focus on remaining and providing continual network service even in the presence of attacks or intrusions and is the central issue of network security in recent years. The related theory of intrusion tolerance, the status and the applications of intrusion tolerance are studied in this paper. As the core technology of the 3rd-generation information security, the intrusion tolerance which has a wide prospect can prevent the intrusion effectively and ensure the network operate properly with the existence of intrusion.
Security testing of web applications remains a major problem of software engineering. In order to reveal vulnerabilities, testing approaches use different strategies for detection of certain kinds of inputs that might lead to a security breach. Such approaches depend on the corresponding test case generation technique that are executed against the system under test. In this work we examine how two of the most popular algorithms for combinatorial test case generation, namely the IPOG and IPOG-F algorithms, perform in web security testing. For generating comprehensive and sophisticated testing inputs we have used input parameter modelling which includes also constraints between the different parameter values. To handle the test execution, we make use of a recently introduced methodology which is based on model-based testing. Our evaluation indicates that both algorithms generate test inputs that succeed in revealing security leaks in web applications with IPOG-F giving overall slightly better results w.r.t. the test quality of the generated inputs. In addition, using constraints during the modelling of the attack grammars results in an increase on the number of test inputs that cause security breaches. Last but not least, a detailed analysis of our evaluation results confirms that combinatorial testing is an efficient test case generation method for web security testing as the security leaks are mainly due to the interaction of a few parameters. This statement is further supported by some combinatorial coverage measurement experiments on the successful test inputs.
A wireless sensor network (WSN) is a distributed network that facilitates wireless information gathering within a region of interest. The information collected by sensors is aggregated at a central node know as the sink node. Two challenges in the deployment of WSNs are limited battery power of each sensor node and sink node anonymity. The role played by the sink node raises its profile as a high value target for attack, thus its anonymity is crucial to the security of a WSN. In order to improve network security, we must implement a protocol that conceals the sink node's location while being cognizant of energy resource constraints. In this paper we develop a routing algorithm based on node clustering to improve sink node anonymity while simultaneously limiting node energy depletion. Via MATLAB simulations, we analyze the effectiveness of this algorithm in obfuscating the sink node's location in the WSN while preserving node energy. We show that the anonymity of the sink node is independent of traffic volume and that the average energy consumed by a node remains consistent across topological variations.
Cyber attacking is one of the most concerning threats in military operation nowadays. In Cyber warfare concept, cyberspace is very logical comparing to the other 4 battlefields including land, water, air, and space. It is almost impossible to identify physical location of hosts in Latitude/Longitude, not even direction or distance from one to another. Traditional boundaries disappear within cyberspace. Moreover, separated activities can be done from many locations via computer network. In this information age, information superiority is translated into combat power. Robust and secure network is significantly needed as a core media for information operation e.g. censoring and sharing. However, Cyber risk environment in the battlefield is not yet clarified. Vulnerability detection, penetration testing, and risk evaluation are parts of network security. Therefore, this paper proposes a cyber-risk evaluation framework that integrates Network Risk Metric and Risk Environment of military operation together in order to get a better understanding of cyber risk in the battlefield.
The global spread revolution of information and information technology is playing a decisive role to social change. Internet has become the most effective way for information transmission, whose role is network security. How to guarantee network security has become a serious and worrying problem. Biometric identification technology has some advantages including universality, uniqueness, stability and hard to be stolen. Through comparing with the other methods of biometric identification technology, such as fingerprint recognition, palm recognition, facial recognition, signature recognition, iris recognition and retina recognition, gene recognition has advantages of exclusiveness, never change, convenience and a large amount of information, which is thought to be the most important method of biometric identification technology. With the development of modern technology, the fusion of biological technology and information technology has become an inevitable trend. Biometric identification technology will necessarily replace the traditional identification technology and greatly change the life-style of people in the near future.
For any software organization, understanding the software quality is desirable in order to increase user experience of the software. When we talk about security software this factor becomes even more important. This paper aims to develop models for predicting the change proneness for object oriented system. The developed models may be used to predict the change prone classes at early phase of software development. Rigorous testing and allocation of some extra resources to those change prone classes may lead to better quality and it may also reduce our work at the maintenance phase. We apply one statistical and 10 machine learning techniques to predict the models. The results are analyzed from Receiver Operating Characteristics (ROC) analysis using Area under the Curve (AUC) obtained from ROC. Adaboost and Random forest method have shown the best result and hence, based on these results we can claim that quality models have a good relevance with Object Oriented systems.
Firewall, as a mechanism of compulsory access control between the network or system, is an important means to ensure the network security. Firewall can be a very simple filter, but also it can be a carefully targeted gateway. But the principle is the same, which is monitoring and filtering all the information exchanged in internal and external networks. Linux as an open source operating system, is famous for it's stability and security.netfilter/iptables is a firewall system based on Linux which has a great function. This thesis first analysed the working principle of pintables, then introduced pintables rule set, and last proposed an effective algorithm to optimize the rules set which is implemented based on Linux system. In the part of implementation, some key code of the algorithm are given.
Intro: Computer network defense has models for attacks and incidents comprised of multiple attacks after the fact. However, we lack an evidence-based model the likelihood and intensity of attacks and incidents. Purpose: We propose a model of global capability advancement, the adversarial capability chain (ACC), to fit this need. The model enables cyber risk analysis to better understand the costs for an adversary to attack a system, which directly influences the cost to defend it. Method: The model is based on four historical studies of adversarial capabilities: capability to exploit Windows XP, to exploit the Android API, to exploit Apache, and to administer compromised industrial control systems. Result: We propose the ACC with five phases: Discovery, Validation, Escalation, Democratization, and Ubiquity. We use the four case studies as examples as to how the ACC can be applied and used to predict attack likelihood and intensity.
We've used several terms for security from electronic attacks on networks and systems: network security, information security, systems security, and cyber security These terms are often used interchangeably, which may cause confusion as to their intended meaning.
? The ERAU report presents several interesting network security threat scenarios. ? There may be others  ? Threat Scenarios 1 thru 11 do not represent either Hazardous or Catastrophic risk severity from an operational perspective. ? We are continuing to evaluate this important issue.
A set of design methodologies and experiments related to enabling hardware systems to utilize on-the-fly configuration of reconfigurable logic to recover system operation from unexpected loss of system function. Methods included programming using locally stored configuration bit stream as well as using configuration bit stream transmitted from a remote site. A specific ways of utilizing reconfigurable logic to regenerate system function, as well as the effectiveness of this approach as a function of the type of attack, and various architectural attributes of the system. Based on this analysis, proposed architectural features of System-on-Chip (SOC) that can minimize performance degradation and maximize the likelihood of seamless system operation despite the function replacement. This approach is highly feasible in that it is not required to specially manage system software and other normal system hardware functions for the replacement.
Peer-to-Peer (P2P) botnets have emerged as a serious threat against the network security. They are used to carry out various illicit activities like click fraud, DDOS attacks and for information exfiltration. These botnets use distributed concept for command dissemination. These botnets are resilient to dynamic churn and to take-down attempts. Earlier P2P botnet detection techniques have some shortcomings such as they have less accuracy, unable to detect stealthy botnets and advanced botnets using fast-flux networks. In this paper, we list recent P2P botnet detection techniques that overcome the weaknesses of previous techniques with higher detection accuracy. We also discuss various such techniques, their advantages, accuracy and the weaknesses they too are having. However, two or more techniques can be used together to have more accurate and robust P2P botnet detection.
A Computer network or data communication network is a telecommunication network that allows computers to exchange data. Computer networks are typically built from a large number of network devices such as routers, switches and numerous types of middle boxes with many complex protocols implemented on them. They need to accomplish very complex tasks with access to very limited tools. As a result, network management and performance tuning is quite challenging. Software-Defined Networking (SDN) is an emerging architecture purporting to be adaptable, cost-effective, dynamic and manageable pursuing to be suitable for the high-bandwidth, changing nature of today's applications. SDN architectures decouples network control and forwarding functions, making network control to become directly programmable and the underlying infrastructure to be abstracted from applications and network services. The network security is a prominent feature of the network ensuring accountability, confidentiality, integrity, and protection against many external and internal threats. An Intrusion Detection System (IDS) is a type of security software designed to automatically alert administrators when someone or something is trying to compromise information system through malicious activities or through security policy violations. Security violation in SDN environment needs to be identified to prevent the system from an attack. The proposed work aims to detect the attacks on SDN environment. Detecting anomalies on SDN environment will be more manageable and efficient.
This paper presents a Lyapunov function based neural network tracking control strategy for single-input-single-output nonlinear dynamic systems. The proposed architecture is composed of two feed-forward neural networks operating as controller and estimator in a unified framework. The network parameters are tuned online with a Lyapunov function based backpropagation learning algorithm. The closed-loop error convergence and stability are analyzed with Lyapunov stability theory. Two simulation case studies are included that successfully validate the proposed controller performance.
This paper presents an approach to shellcode recognition directly from network traffic data using a multi-layer perceptron with back-propagation learning algorithm. Using raw network data composed of a mixture of shellcode, image files, and DLL-Dynamic Link Library files, our proposed design was able to classify the three types of data with high accuracy and high precision with neither false positives nor false negatives. The proposed method comprises simple and fast pre-processing of raw data of a fixed length for each network data package and yields perfect results with 100% accuracy for the three data types considered. The research is significant in the context of network security and intrusion detection systems. Work is under way for real time recognition and fine-tuning the differentiation between various shellcodes.
The cloud offers a new environment for achieving Denial of Service (DoS) conditions on targeted infrastructure. Once confined to the network, they are now conducted over the hypercall interface. These attacks are initiated by malicious, unprivileged guests with a goal of incapacitating hosting hypervisors. Because they are not packet-based, they cannot be detected or prevented using network security measures. The present study systematically explores this risk and develops a taxonomy of hypercall-based DoS attacks. For purpose of illustration, a denial of service is attempted against a Xen hypervisor. This scenario demonstrates that even a relatively simple attack could have significant implications for system stability. Finally, system for defending hypervisors against hypercall attacks is introduced. This mitigation observes N-grams and calculates the conditional probability of a sequence of hypercalls. The assumption is that exploits will be manifested as previously-unobserved sequences of hypercalls. The early results of testing are provided.
The following topics are dealt with: information security;network security;image watermarking;content-based audio watermarking method;mobile cloud storage services;wireless sensor network;firewall;identity-based encryption scheme;P2P network;MP3 steganography algorithm;cryptography protocol;multilevel security access control policy processing method;and vehicular network.
In a distributed network, there are multiple routing paths between source node and destination node. In order to meet certain demands of network security, selecting required firewalls in multiple routing paths and deploying appropriate rules on them are indispensable steps. Thus, in this paper, we carry on a further research on it and propose permit and deny algorithms which serve to improve the performance of network security. Then, we give theoretical proof of our algorithms. Finally, we test the effectiveness of algorithms based on a network simulation platform named mininet. The analysis of time efficiency of our algorithms and previous algorithms, as well as the comparison between them prove the high efficiency of our algorithms.
Recent advances in electronics and wireless communication technologies have enabled the development of large-scale wireless sensor networks that consist of many low-power, low-cost, and small-size sensor nodes. Sensor networks hold the promise of facilitating large-scale and real-time data processing in complex environments. Security is critical for many sensor network applications, such as military target tracking and security monitoring. To provide security and privacy to small sensor nodes is challenging, due to the limited capabilities of sensor nodes in terms of computation, communication, memory/storage, and energy supply. In this article we survey the state of the art in research on sensor network security.
This paper presents the application of a mixed-integer programming (MIP) approach for solving stochastic security-constrained daily hydrothermal generation scheduling (SCDHGS). Power system uncertainties including generating units and branch contingencies and load uncertainty are explicitly considered in the stochastic programming of SCDHGS. The roulette wheel mechanism and lattice Monte Carlo simulation (LMCS) are first employed for random scenario generation wherein the stochastic SCDHGS procedure is converted into its respective deterministic equivalents (scenarios). Then, the generating units are scheduled through MIP over the set of deterministic scenarios for the purpose of minimizing the cost of supplying energy and ancillary services over the optimization horizon (24 h) while satisfying all the operating and network security constraints. To a more realistic modeling of the DHGS problem, in the proposed MIP formulation, the nonlinear valve loading effect, cost, and emission function are modeled in linear form, and prohibited operating zones (POZs) of thermal units are considered. Furthermore, a dynamic ramp rate of thermal units is used, and for the hydro plants, the multiperformance curve with spillage and time delay between reservoirs is considered. To assess the efficiency and powerful performance of the aforementioned method, a typical case study based on the standard IEEE-118 bus system is investigated, and the results are compared to each other in different test systems.
Anomaly detection (AD) use within the network intrusion detection field of research, or network intrusion AD (NIAD), is dependent on the proper use of similarity and distance measures, but the measures used are often not documented in published research. As a result, while the body of NIAD research has grown extensively, knowledge of the utility of similarity and distance measures within the field has not grown correspondingly. NIAD research covers a myriad of domains and employs a diverse array of techniques from simple k-means clustering through advanced multiagent distributed AD systems. This review presents an overview of the use of similarity and distance measures within NIAD research. The analysis provides a theoretical background in distance measures and a discussion of various types of distance measures and their uses. Exemplary uses of distance measures in published research are presented, as is the overall state of the distance measure rigor in the field. Finally, areas that require further focus on improving the distance measure rigor in the NIAD field are presented.
An adaptive neural network tracking control is studied for a class of multiple-input multiple-output (MIMO) nonlinear systems. The studied systems are in discrete-time form and the discretized dead-zone inputs are considered. In addition, the studied MIMO systems are composed of $N$ subsystems, and each subsystem contains unknown functions and external disturbance. Due to the complicated framework of the discrete-time systems, the existence of the dead zone and the noncausal problem in discrete-time, it brings about difficulties for controlling such a class of systems. To overcome the noncausal problem, by defining the coordinate transformations, the studied systems are transformed into a special form, which is suitable for the backstepping design. The radial basis functions NNs are utilized to approximate the unknown functions of the systems. The adaptation laws and the controllers are designed based on the transformed systems. By using the Lyapunov method, it is proved that the closed-loop system is stable in the sense that the semiglobally uniformly ultimately bounded of all the signals and the tracking errors converge to a bounded compact set. The simulation examples and the comparisons with previous approaches are provided to illustrate the effectiveness of the proposed control algorithm.
In ad hoc networks, selfish nodes deviating from the standard MAC (Medium Access Control) protocol can significantly degrade normal nodes' performance and are usually difficult to detect. In this paper, we propose detection and defense schemes to identify and defend against MAC-layer selfish misbehavior, respectively, in IEEE 802.11 multi-hop ad hoc networks. Specifically, the non-deterministic nature of the IEEE 802.11 MAC protocol imposes great challenges to distinguishing selfish nodes from well-behaved nodes. Most traditional selfish misbehavior detection approaches are for wireless local area networks (WLANs) only. They either rely on a large amount of historical data to perform statistical detection, or employ throughput or delay models that are only valid in WLANs for detection. In contrast, we propose a realtime selfish misbehavior detection scheme for multi-hop ad hoc networks. It requires only several samples, and hence is more efficient and can adapt to channel dynamics more quickly. Then, based on the proposed detection scheme, we design three selfish misbehavior defense schemes against three typical kinds of smart selfish nodes. We find that the smart selfish nodes cannot degrade normal nodes' performance much without getting detected. Extensive simulation results are finally presented to validate the proposed detection and defense schemes.
This paper addresses the problem of rumor source detection with multiple observations, from a statistical point of view of a spreading over a network, based on the susceptible-infectious model. For tree networks, multiple independent observations can dramatically improve the detection probability. For the case of a single rumor source, we propose a unified inference framework based on the joint rumor centrality, and provide explicit detection performance for degree-regular tree networks. Surprisingly, even with merely two observations, the detection probability at least doubles that of a single observation, and further approaches one, i.e., reliable detection, with increasing degree. This indicates that a richer diversity enhances detectability. Furthermore, we consider the case of multiple connected sources and investigate the effect of diversity. For general graphs, a detection algorithm using a breadth-first search strategy is also proposed and evaluated. Besides rumor source detection, our results can be used in network forensics to combat recurring epidemic-like information spreading such as online anomaly and fraudulent email spams.
Recommender systems, medical diagnosis, network security, etc., require on-going learning and decision-making in real time. These-and many others-represent perfect examples of the opportunities and difficulties presented by Big Data: the available information often arrives from a variety of sources and has diverse features so that learning from all the sources may be valuable but integrating what is learned is subject to the curse of dimensionality. This paper develops and analyzes algorithms that allow efficient learning and decision-making while avoiding the curse of dimensionality. We formalize the information available to the learner/decision-maker at a particular time as a context vector which the learner should consider when taking actions. In general the context vector is very high dimensional, but in many settings, the most relevant information is embedded into only a few relevant dimensions. If these relevant dimensions were known in advance, the problem would be simple-but they are not. Moreover, the relevant dimensions may be different for different actions. Our algorithm learns the relevant dimensions for each action, and makes decisions based in what it has learned. Formally, we build on the structure of a contextual multi-armed bandit by adding and exploiting a relevance relation. We prove a general regret bound for our algorithm whose time order depends only on the maximum number of relevant dimensions among all the actions, which in the special case where the relevance relation is single-valued (a function), reduces to mathtildeO(T2(2-1));in the absence of a relevance relation, the best known contextual bandit algorithms achieve regret mathtildeO(T(D+1)/(D+2)), where D is the full dimension of the context vector. Our algorithm alternates between exploring and exploiting and does not require observing outcomes during exploitation (so allows for active learning). Moreover, during exploitation, suboptimal actions are c- osen with arbitrarily low probability. Our algorithm is tested on datasets arising from network security and online news article recommendations.
With the growing popularity of Internet applications and the widespread use of mobile Internet, Internet traffic has maintained rapid growth over the past two decades. Internet Traffic Archival Systems (ITAS) for packets or flow records have become more and more widely used in network monitoring, network troubleshooting, and user behavior and experience analysis. Among the three key technologies in ITAS, we focus on bitmap index compression algorithm and give a detailed survey in this paper. The current state-of-the-art bitmap index encoding schemes include: BBC,WAH, PLWAH, EWAH, PWAH, CONCISE, COMPAX, VLC, DF-WAH, and VAL-WAH. Based on differences in segmentation, chunking, merge compress, and Near Identical (NI) features, we provide a thorough categorization of the state-of-the-art bitmap index compression algorithms. We also propose some new bitmap index encoding algorithms, such as SECOMPAX, ICX, MASC, and PLWAH+, and present the state diagrams for their encoding algorithms. We then evaluate their CPU and GPU implementations with a real Internet trace from CAIDA. Finally, we summarize and discuss the future direction of bitmap index compression algorithms. Beyond the application in network security and network forensic, bitmap index compression with faster bitwise-logical operations and reduced search space is widely used in analysis in genome data, geographical information system, graph databases, image retrieval, Internet of things, etc. It is expected that bitmap index compression will thrive and be prosperous again in Big Data era since 1980s.
Electronic elements of a substation control system have been recognized as critical cyberassets due to the increased complexity of the automation system that is further integrated with physical facilities. Since this can be executed by unauthorized users, the security investment of cybersystems remains one of the most important factors for substation planning and maintenance. As a result of these integrated systems, intrusion attacks can impact operations. This work systematically investigates the intrusion resilience of the ten architectures between a substation network and others. In this paper, two network architectures comparing computer-based boundary protection and firewall-dedicated virtual local-area networks are detailed, that is, architectures one and ten. A comparison on the remaining eight architecture models was performed. Mean time to compromise is used to determine the system operational period. Simulation cases have been set up with the metrics based on different levels of attackers' strength. These results as well as sensitivity analysis show that implementing certain architectures would enhance substation network security.
The requirement for accurate one-way delay (OWD) estimation led to the recent introduction of an algorithm enabling a server to estimate OWDs between itself and a client by cooperating with two other servers, requiring neither client-clock synchronization nor client trustworthiness in reporting one-way delays. We evaluate the algorithm by deriving the probability distribution of its absolute error and compare its accuracy with the well-known round-trip halving algorithm. While neither algorithm requires client trustworthiness nor client-clock synchronization, the analysis shows that the new algorithm is more accurate in many situations.
How can standards working groups protect Internet users from pervasive monitoring, where users' data are being massively collected, aggregated, and analyzed?
Web services work over dynamic connections among distributed systems. This technology was specifically designed to easily pass SOAP message through firewalls using open ports. These benefits involve a number of security challenges, such as Injection Attacks, phishing, Denial-of-Services (DoS) attacks, and so on. The difficulty to detect vulnerabilities,before they are exploited, encourages developers to use security testing like penetration testing to reduce the potential attacks. Given a black-box approach, this research use the penetration testing to emulate a series of attacks, such as Cross-site Scripting (XSS), Fuzzing Scan, Invalid Types, Malformed XML, SQL Injection, XPath Injection and XML Bomb. In this way, was used the soapUI vulnerability scanner in order to emulate these attacks and insert malicious scripts in the requests of the web services tested. Furthermore, was developed a set of rules to analyze the responses in order to reduce false positives and negatives. The results suggest that 97.1% of web services have at least one vulnerability of these attacks. We also determined a ranking of these attacks against web services.
The number of users of VoIP services is increasing every year. Consequently, VoIP systems get more attractive for attackers. This paper describes the implementation of a low interaction honeypot for monitoring illegal activities in VoIP environments. The honeypot operated during 92 days and collected 3502 events related to the SIP protocol. The analysis of the results allows understanding the modus operandi of the attacks targeted to VoIP infrastructures. These results may be used to improve defense mechanisms such as firewalls and intrusion detection systems.
The emergence of SDNs promises to dramatically simplify network management and enable innovation through network programmability. Despite all the hype surrounding SDNs, exploiting its full potential is demanding. Security is still the key concern and is an equally striking challenge that reduces the growth of SDNs. Moreover, the deployment of novel entities and the introduction of several architectural components of SDNs pose new security threats and vulnerabilities. Besides, the landscape of digital threats and cyber-attacks is evolving tremendously, considering SDNs as a potential target to have even more devastating effects than using simple networks. Security is not considered as part of the initial SDN design;therefore, it must be raised on the agenda. This article discusses the state-of-the-art security solutions proposed to secure SDNs. We classify the security solutions in the literature by presenting a thematic taxonomy based on SDN layers/interfaces, security measures, simulation environments, and security objectives. Moreover, the article points out the possible attacks and threat vectors targeting different layers/interfaces of SDNs. The potential requirements and their key enablers for securing SDNs are also identified and presented. Also, the article gives great guidance for secure and dependable SDNs. Finally, we discuss open issues and challenges of SDN security that may be deemed appropriate to be tackled by researchers and professionals in the future.
The past few years have witnessed revolutionary advances in network technology. Along with new techniques such as SDN come lots of new network security challenges. Conventional network security mechanisms are incompetent to overcome these challenges, since they are built on a static network configuration that facilitates attackers in finding the weaknesses of a network. In this article, we conceive a novel conceptual network security mechanism, the evolving defense mechanism (EDM), to resolve current and future security problems. EDM is based on a bio-inspired idea of network configuration variations. According to the security requirements of the system, the user, and the network security state, EDM selects an efficient network configuration variation strategy to prevent corresponding security threats. Combined with SDN implementation, EDM resolves security problems from a new angle and is capable of evolving with new network security technology. We sketch a way to implement EDM and present its reference framework, which serves as an ecosystem and coexisting environment for various kinds of network configuration variations. The proposed mechanism avoids the deficiency of conventional mechanisms and has potential to cope with emerging security threats.
To fulfill global requirements for network security and privacy, anonymous communication systems have been extensively investigated and deployed over the world to provide anonymous communication services for users. Nonetheless, diverse de-anonymizing techniques have been proposed to compromise anonymity and impose a severe threat to anonymous communication systems. In this article, we classify the existing de-anonymizing techniques and provide an overview of these techniques. In addition, corresponding countermeasures are studied to mitigate the risks posed by these de-anonymizing techniques.
The current device-centric protection model against security threats has serious limitations. On one hand, the proliferation of user terminals such as smartphones, tablets, notebooks, smart TVs, game consoles, and desktop computers makes it extremely difficult to achieve the same level of protection regardless of the device used. On the other hand, when various users share devices (e.g., parents and kids using the same devices at home), the setup of distinct security profiles, policies, and protection rules for the different users of a terminal is far from trivial. In light of this, this article advocates for a paradigm shift in user protection. In our model, protection is decoupled from users' terminals, and it is provided by the access network through a trusted virtual domain. Each trusted virtual domain provides unified and homogeneous security for a single user irrespective of the terminal employed. We describe a user-centric model where nontechnically savvy users can define their own profiles and protection rules in an intuitive way. We show that our model can harness the virtualization power offered by next-generation access networks, especially from network functions virtualization in the points of presence at the edge of telecom operators. We also analyze the distinctive features of our model, and the challenges faced based on the experience gained in the development of a proof of concept.
The Internet of Things (IoT) demands tailor-made security solutions. Today, there are a number of proposals able to meet IoT's demands in the context of attacks from outsiders. In the context of insiders, however, this does not hold true. Existing solutions to deal with this class of attacks not always take into consideration the IoT's idiosyncrasies and, therefore, they do not produce the best results. This work aims at coming up with tailor-made security schemes for thwarting attacks from insiders in the context of IoT systems. Our solution makes use of a pioneering solution to pinpoint vulnerabilities: we crosscheck data from communicating nodes. It provides the same guarantees as traditional security mechanisms, but it is about 83% more efficient, according to the experiments that this article describes.
Device classification is important in many applications such as industrial quality control, through-wall imaging and network security. A novel approach has been proposed to use a digital noise radar (DNR) to actively interrogate microwave devices and classify defective units using `radio frequency distinct native attribute (RF-DNA)' fingerprinting and various classifier algorithms. RF-DNA has previously demonstrated `serial number' discrimination of numerous passive radio frequency signals, achieving classification accuracies above 80% using multiple discriminant analysis/maximum likelihood (MDA/ML) and generalised relevance learning vector quantisation-improved (GRLVQI) classifiers. It has also demonstrated above 80% classification of limited active interrogation responses with a DNR signal using these classifiers. The performance capabilities of the two different classifiers, MDA/ML and GRLVQI, on RF-DNA fingerprints produced from the ultra-wideband noise radar correlation response is expanded.
In the second half of the 1970s, established computer firms and new IT start-ups chose alternative paths to offer commercial access control systems to organizational mainframe computer users. These developments in effect launched the computer security software products industry with IBM's Resource Access Control Facility (RACF) and SKK's Access Control Facility 2 (ACF2).
The design tension between security and surveillance has existed for decades. This article specifically examines the protocol design tension between national security interests in surveillance versus network security in the early decades of the Internet and its predecessor networks. Using archival research and protocol-specific case studies, this article describes episodes in which the Internet Engineering Task Force (IETF) assessed whether to build wiretapping capability into protocols, made specific engineering decisions regarding security and surveillance, and entered broader global debates about encryption strength and government policies to institute cryptographic controls that facilitate surveillance. This study reveals that, even as protocol design has continuously evolved and adapted to changing political, socioeconomic, and technological contexts, the Internet engineering community has consistently staked out a consensus position pushing back against technologically based indiscriminate government surveillance.
Software-defined networking (SDN) is a new networking paradigm that decouples the forwarding and control planes, traditionally coupled with one another, while adopting a logically centralized architecture aiming to increase network agility and programability. While many efforts are currently being made to standardize this emerging paradigm, careful attention needs to be paid to security at this early design stage too, rather than waiting until the technology becomes mature, thereby potentially avoiding previous pitfalls made when designing the Internet in the 1980s. This article focuses on the security aspects of SDN networks. We begin by discussing the new security advantages that SDN brings and by showing how some of the long-lasting issues in network security can be addressed by exploiting SDN capabilities. Then we describe the new security threats that SDN is faced with and discuss possible techniques that can be used to prevent and mitigate such threats.
The optional use of Cipher Suites that make use of a 64-bit (PN) to allow more than 232 MACsec protected frames to be sent with a single Secure Association Key are specified by this amendment.
This standard defines the functions and features to be provided in substation intelligent electronic devices (IEDs) to accommodate critical infrastructure protection programs. The standard addresses security regarding the access, operation, configuration,
This standard defines the functions and features to be provided in substation intelligent electronic devices (IEDs) to accommodate critical infrastructure protection programs. The standard addresses security regarding the access, operation, configuration,
The enhanced security management function for the protocol defined in IEEE 1888(TM), Ubiquitous Green Community Control Network Protocol, is described in this standard. Securityrequirements, system security architecture definitions, and a standardized des
Port-based network access control allows a network administrator to restrict the use of IEEE 802(R) LAN service access points (ports) to secure communication between authenticated and authorized devices. This standard specifies a common architecture, func
The functions and features to be provided in intelligent electronic devices (IEDs) to accommodate critical infrastructure protection programs are defined in this standard. Security regarding the access, operation, configuration, firmware revision and data
Media Access Control security (MACsec) Key Agreement protocol (MKA) data elements and procedures that provide additional security and manageability capabilities, including the ability to maintain secure communication while the operation of MKA is suspende
This standard defines system engineering and management requirements for the life cycle of websites, including strategy, design, engineering, testing and validation, and management and sustainment for Intranet and Extranet environments. The goal of this s
We present a novel approach to network security against passive eavesdroppers by employing a configurable beam-forming technique to create tightly defined regions of coverage for targeted users. In contrast to conventional encryption methods, our security scheme is developed at the physical layer by configuring antenna array beam patterns to transmit the data to specific regions. It is shown that this technique can effectively reduce vulnerability of the physical regions to eavesdropping by adapting the antenna configuration according to the intended user's channel state information. In this paper we present the application of our concept to 802.11n networks where an antenna array is employed at the access point, and consider the issue of minimizing the coverage area of the region surrounding the targeted user. A metric termed the exposure region is formally defined and used to evaluate the level of security offered by this technique. A range of antenna array configurations are examined through analysis and simulation, and these are subsequently used to obtain the optimum array configuration for a user traversing a coverage area.
Traditional enterprise network security is based on the deployment of security appliances placed on some specific locations filtering, monitoring the traffic going through them. In this perspective, security appliances are chained in specific order to perform different security functions on the traffic. In the cloud, the same approach is often adopted using virtual security appliances to protect traffic for different virtual applications with the challenge of dealing with the flexible and elastic nature of the cloud. In this paper, we investigate the problem of placing virtual security appliances within the data center in order to minimize network latency and computing costs for security functions while maintaining the required sequential order of traversing virtual security appliances. We propose a new algorithm computing the best place to deploy these virtual security appliances in the data center. We further integrated our placement algorithm in an open source cloud framework, i.e. Openstack, in our test laboratory. The preliminary results show that we are placing the virtual security appliances in the required sequential order while improving the efficiency compared to the current default placement algorithm in Openstack.
Programmable Logic Controller (PLC) technology plays an important role in the automation architectures of several critical infrastructures such as Industrial Control Systems (ICS), controlling equipment in contexts such as chemical processes, factory lines, power production plants or power distribution grids, just to mention a few examples. Despite their importance, PLCs constitute one of the weakest links in ICS security, frequently due to reasons such as the absence of secure communication mechanisms, authenticated access or system integrity checks. While events such as the Stuxnet worm have raised awareness for this problem, industry has slowly reacted, either due to reliability or cost concerns. This paper introduces the Shadow Security Unit, a low-cost device deployed in parallel with a PLC or Remote Terminal Unit (RTU), being capable of transparently intercepting its communications control channels and physical process I/O lines to continuously assess its security and operational status. The proposed device does not require significant changes to the existing control network, being able to work in standalone or integrated within an ICS protection framework.
The following topics are dealt with: wireless sensor network;caching;queue management and scheduling;content centric networking;multimedia communication;routing;data offloading;software defined networking;traffic analysis;network security;sensors localization;peer to peer system;and cognitive radio.
Intrusion Detection Systems (IDSs) are an essential component of any network security architecture. Their importance is emphasized in today's heterogeneous and complex networks, where a variety of network assets are constantly subject to a large number of attacks. As the network traffic increases, the importance of proper IDS configuration is reinforced. For instance, the larger the number of detection libraries are, the larger number of attacks is expected to be detected. A larger number of libraries implies that the computational complexity is increased, which may reduce system performance. There is always a tradeoff between security enforcement level and system performance. Many papers in the literature have exploited Game theory to address this problem by including different factors in their proposed models. In this paper, we propose a game theoretic approach to determine the networked IDS configuration in heterogeneous networks. We utilize a more efficient way to tune IDS configuration, including library selection, based on the type and value of protected network assets;the interdependencies between assets are considered in the model. Unlike most existing methods, in the proposed game model the impact of each particular attack is considered to be different for each asset. The problem has been modeled as a non-cooperative multi-person nonzero-sum stochastic game. The existence of stationary Nash equilibrium for this game has been demonstrated.
The paper describes an approach to modeling telecommunication systems and processes of data transmission based on Boolean-valued networks. Models to address two specific problems of network security are offered. It is possible to adapt existing algorithms of graph theory to apply them in this approach. Thus, it becomes possible to take into account not only quantitative but also qualitative measure of information in solving urgent network problems. A definition of a maximum flow in Boolean-valued networks and an algorithm to find this flow were proposed. Two examples to demonstrate the described models and algorithms are also presented in the paper.
The Smart Grid, the next generation power grid, comes with promises of widely distributed automated energy delivery, self-monitoring, self-healing, energy efficiency, utility and cost optimization. However, as attacks on the current power grid and similar systems indicate, the Smart Grid will be vulnerable to all kinds of attacks and will even raise new security challenges, due to its complex nature. In this paper we analyze this complexity of the Smart Grid as a System of Systems, and the specific security challenges it raises. To address these challenges we propose a vision/framework based on principles of Software Engineering. This framework structures and brings together the research on Smart Grid security.
SQL Injection (SQLI) is a quotidian phenomenon in the field of network security. It is a potent and effective way of intruding into secured databases thereby jeopardizing the confidentiality, integrity and availability of information in them. SQL Injection works by inserting malicious queries into legal queries thereby rendering it increasingly arduous for most detection systems to be able to discern its occurrence. Hence, the need of the hour is to build a coherent and a smart SQL Injection detection system to make web applications safer and thus, more reliable. Unlike a great majority of current detection tools and systems that are deployed at a region between the web server and the database server, the proposed system is deployed between client and the web server, thereby shielding the web server from the inimical impacts of the attack. This approach is nascent and efficient in terms of detection, ranking and notification of the attack designed using pattern matching algorithm based on the concept of hashing.
The following topics are dealt with: next generation wireless communications;modulation and coding techniques;signal processing;computer networks;data security;network security;intelligent communication systems;ICT convergence;optical communication systems;software development;big data;and data analytics.
Advancements in information technology has increased the use of internet. With the pervasiveness of internet, network security has become critical issue in every organization. Network attacks results in massive amount of loss in terms of money, reputation and data confidentiality. Reducing or eliminating the negative effects of any intrusion is a fundamental issue of network security. The network security problem can be represented as a game between the attacker or intruder and the network administrator where both the players try to attain maximum outcome. The network administrator tries to defend the attack and the attacker tries to overcome it and attack the system. Thus network security can be enforced using game theoretic approach. This paper presents a review of game theoretic solutions developed for network security.
Computing Industry is moving towards Service Oriented Architecture at a fast pace. This brings network security to the forefront of major concerns to an organization. Ensuring round the clock uninterrupted service to the clients becomes a top priority to any organization. Flooding Attack is one of the costliest attacks that use a host to overwhelm a server, causing complete system crash. Flash Crowd is an unexpected rise in traffic caused by legitimate users. Both flooding which is one of the types of DoS attack and Flash crowd creates abnormal traffic condition, but in order to improve good put, the server must be deployed with the mechanism that should classify legitimate and malicious call requests. The on-going attacks usually similar to each other compared to the flows of flash crowd so the provocation is to recognize flooding attacks from flash crowd. The recognition of flooding attack is done by, using the Entropy based detection method.
Network forensics is a branch of digital forensics that focuses on monitoring, capturing, recording, and analysis of network traffic. More accurately, it is the use of scientifically proved techniques to collect and analyse network packets and events for investigative purposes. Network forensics is an extension of the network security model which traditionally emphasizes prevention and detection of network attacks. Current network forensics approaches are costly and time consuming. However, unlike other areas of digital forensics, network forensics deals with volatile and dynamic data. It helps organizations to investigate attacks that originated from outside and inside of the company. It's also important for law enforcement agencies when solving crimes. Paper presents different challenges that are facing investigators due to the rapid growth of network and attacker's skill, and possible framework solutions that would help to solve or minimize problems.
The focus of this paper is to present observations related to information assurance (IA) in rural and urban populations. Based on our experience teaching college students in these environments, we have noted that on entering school, generally, individuals demonstrate limited background knowledge of a variety of computer related technologies. Students begin with a technical disadvantage that represents a readily exploitable attack vector for identity thieves. Because of the experience deficit, the hazard of identity theft is significant with possible severe detrimental outcomes for the student victim. In addition, the negative impact on the society as a whole is substantial. Methods that reach beyond traditional formal computer or network security instruction in the classroom and extend information assurance education across disciplines are needed. We have explored this direction at the university and have worked on strategies to educate students about identity theft. The interdisciplinary programs outlined here span the curriculum. In addition, we suggest community outreach programs that extend the scope of influence of information assurance education beyond the university to include surrounding at risk populations.
The following topics are dealt with: hardware-aided security;cryptocurrencies;cyber crime;network security;cryptographic protocols;ORAM;secure multiparty computation;side channel attack;program analysis;malware analysis;memory integrity;and Android security.
Vulnerability exploits remain an important mechanism for malware delivery, despite efforts to speed up the creation of patches and improvements in software updating mechanisms. Vulnerabilities in client applications (e.g., Browsers, multimedia players, document readers and editors) are often exploited in spear phishing attacks and are difficult to characterize using network vulnerability scanners. Analyzing their lifecycle requires observing the deployment of patches on hosts around the world. Using data collected over 5 years on 8.4 million hosts, available through Symantec's WINE platform, we present the first systematic study of patch deployment in client-side vulnerabilities. We analyze the patch deployment process of 1,593 vulnerabilities from 10 popular client applications, and we identify several new threats presented by multiple installations of the same program and by shared libraries distributed with several applications. For the 80 vulnerabilities in our dataset that affect code shared by two applications, the time between patch releases in the different applications is up to 118 days (with a median of 11 days). Furthermore, as the patching rates differ considerably among applications, many hosts patch the vulnerability in one application but not in the other one. We demonstrate two novel attacks that enable exploitation by invoking old versions of applications that are used infrequently, but remain installed. We also find that the median fraction of vulnerable hosts patched when exploits are released is at most 14%. Finally, we show that the patching rate is affected by user-specific and application-specific factors, for example, hosts belonging to security analysts and applications with an automated updating mechanism have significantly lower median times to patch.
The inherent weaknesses of existing notice-and-consent paradigms of data privacy are becoming clear, not just to privacy practitioners but to ordinary online users as well. The corporate privacy function is a maturing discipline, but greater maturity often equates just to greater regulatory compliance. At a time when many users are disturbed by the status quo, new trends in web security and data sharing are demonstrating useful new consent paradigms. Benefiting from these trends, the emerging standard User-Managed Access (UMA) allows apps to extend the power of consent. UMA corrects a power imbalance that favors companies over individuals, enabling privacy solutions that move beyond compliance.
Packet Classification is a core function used in an internet router, firewall, network security and quality of services. A flow of packets is decided by the header fields of incoming packets. Software solutions for packet classification are not suitable for wire-speed processing and are not secure. For wirespeed and secure network access, hardware solutions for packet classification are mandatory which can also sustain high throughput at low latency. Memory required for hardware architecture is also a crucial problem. In this paper, we have performed classification of packets using basic XNOR gate. We compare our proposed design with StrideBV which is one of efficient decomposition based technique for packet classification. The results obtained by synthesis and stimulation using XilinxISE Design tool 13.1 are presented in this paper. From the results, we have concluded that our proposed technique is memory efficient as well as has low latency than StrideBV.
Honeypots act as security resources that are used to catch malign activities, so they may be anatomized and watched. During the past few years, they are called as a safeguard of assets of an organization. They are used to acquire information on interrupters in a network. This paper gives an introduction to the honeypots, their classification, detailed study of commercial as well as open source honeypots tools and comparison between them. This paper may be helpful for readers to secure their resources from intruders by using the freely available honeypots tools.
Software vulnerabilities are the weaknesses in the software that inadvertently allow dangerous operations. If the vulnerability is in a network service, it poses serious security threats because a cyber-attacker can exploit it to gain unauthorized access to the system. Hence, rapid discovery and remediation of network vulnerabilities is critical issues in network security. In today's dynamic IT environment, it is common practice that an organization prioritizes the mitigation of discovered vulnerabilities according to their risk levels. Currently available technologies, however, associate each vulnerability to the static risk level which does not take the unique characteristics of the target network into account. This often leads to inaccurate risk prioritization and less-than-optimal resource allocation. In this research, we introduce a novel way of quantifying the risk of network vulnerability by augmenting the static risk level with conditions specific to the target network. The method calculates the risk value of each vulnerability by measuring the proximity to the untrusted network and risk of the neighboring hosts. The resulting risk value, RCR is a composite index of the individual risk, network location and neighborhood risk conditions. Thus, it can be effectively used for prioritization, comparison and trending. We tested the methodology through the network intrusion simulation. The results shows average 88.9% the correlation between RCR and number of successful attacks on each vulnerability.
Malware or virus is one of the most significant security threats in internet. There are mainly two types of successful (partially) solutions available. One is anti-virus and other is back listing. This kind of detection generally depends on the existing malware or virus signature database. Cyber-criminals bypass defenses by generating variants of their malware program. Traditional approach has limitations such as unable to detect zero day threats or generate so many false alerts et al. To overcome these difficulties, a system is built based on Atanassov's intuitionistic fuzzy set (AIFS) theory based clustering method that takes care of these problems in a robust way. It not only raises an alert for new kind of malware but also decreases the number of false alerts. This is done by giving it decision making intelligence. There is not much work done in the field of network forensics using AIFS theory. This method clusters the malwares/viruses with high accuracy on the basis of severity. Experiments are performed on several pcap files with malware traffic to assess the performance and accuracy of the method.
The increasing network speeds, number of attacks, and need for energy efficiency are pushing software-based network security to the limit. A common kind of threat is probing attacks, in which an attacker tries to find vulnerabilities by sending many probe packets to a target machine. In this paper, we evaluate three machine learning classifiers (Decision Tree, Naive Bayes, and k-Nearest Neighbors), implemented in hardware and software, for the detection of probing attacks. We present detailed results showing the tradeoffs between energy consumption, throughput, and accuracy of the three classifiers. The fastest hardware implementation is 926 times as fast as its software counterpart, and its energy consumption per classification is 0.05% that of the software version.
This paper addresses the issues of security and vulnerability of links in the stabilization of networked control systems from robustness viewpoint. As is done in recent research, we view network security as a robustness issue. However, in this paper we shed considerable new insight into this topic and offer a new and differing perspective. We argue that `robustness' aspect is a common theme related to both vulnerability and security. This paper puts forth the viewpoint that vulnerability of a networked system is a manifestation of the combination of two types of robustness, namely `qualitative robustness' and `quantitative robustness'. In other words, the entire robustness concept is treated as a combination of qualitative robustness and quantitative robustness, wherein qualitative robustness is linked to the system's nature of interactions and interconnections i.e. system's structure while quantitative robustness is linked to the system dynamics. Put it another way, qualitative robustness is independent of magnitudes and depends only on the signs of the system dynamics matrix whereas quantitative robustness is purely a function of the quantitative information (both magnitudes and signs) of the entries of the system dynamics matrix. In that sense, these two concepts are inter-related, each influencing and complementing the other. Applying these notions to the networked control systems represented by `dynamical structure functions', it is shown that any specific dynamical structure function originated by a state space represenation, is a function of both qualitative and quantitative robustness. In other words, vulnerability of links in that network is determined by both the signs and magnitudes of that state space matrix of that dynamical structure function. Thus the notion in the recent literature that `vulnerability depends on the system structure, not the dynamics and the robustness, which depends on the dynamics, and not on the system structure' is disput- d and clear justification for our viewpoint is provided by newly introduced notions of `qualitative robustness' and `quantitative robustness'. This paper then presents few specific dynamical structure functions that possess a large number of non-vulnerable links which is desirable for a secure network. The proposed concepts are illustrated with many useful examples.
The Information Centric Networking (ICN) model relies on the ubiquitous use of caching to improve performance and reduce bandwidth requirements. ICN also makes it possible for routers to fetch content from downstream nodes, such as when a content for a home user is fetched from a neighbor's home router, with significant performance improvement. This paper shows how an attacker using compromised hosts can easily gather a massive amount of low-cost, low-latency storage for malware, junk, and other attacker-controlled content. We conclude by considering a possible countermeasure, a blacklist fed by a honeypot, which we show to be effective.
In-depth understanding of network traffic is important for a variety of applications, such as network management and network security. In this paper, we propose a novel protocol identification system PSKS, which relies on the statistical signatures of network packet payloads. The proposed approach is based on the key insight that message segmentation patterns can be leveraged for accurate application identification. Specifically, the segmentation possibility for every position of protocol messages exhibits highly skewed frequency distribution due to the reason that different protocols have different message formats (i.e., Distinct message segmentation patterns). Motivated by this observation, we want to extract statistical application fingerprints by exploiting the message segmentation patterns. In PSKS, we first extract the message segmentation patterns by scoring the segmentation possibility scale for each position of messages, and then extract statistical signatures by Kolmogorov-Smirnov test and feed the signatures to tri-training, a collaborative learning algorithm. The tri-training can improve the generalization ability of our final classifier. We implemented and evaluated PSKS, and the experimental results show that PSKS achieves an average precision and recall of approximately 98%.
To found security events from web logs has become an important aspect of network security. This paper proposes a website anomaly detection model based on security-log-analysis. After creating a anomaly feature sets of the model, C4.5 algorithm was used to improve feature sets, making the abnormal records in feature sets store hierarchically. Compared logs in website with the treated feature stes, the model ultimately achieves the purpose of checking website's security event fast and accurately.
This paper focuses on an important research problem of Big Data classification in intrusion detection system. Deep Belief Networks is introduced to the field of intrusion detection, and an intrusion detection model based on Deep Belief Networks is proposed to apply in intrusion recognition domain. The deep hierarchical model is a deep neural network classifier of a combination of multilayer unsupervised learning networks, which is called as Restricted Boltzmann Machine, and a supervised learning network, which is called as Back-propagation network. The experimental results on KDD CUP 1999 dataset demonstrate that the performance of Deep Belief Networks model is better than that of SVM and ANN.
Honeynet represents a new strategy in defending the computer networks and systems against unauthorized access or hacking attempts. Not only it can detect and display the attack pattern or the tools utilized, it can also help in eliminating access to real systems by representing an emulation of the physical systems and services present within the network, thus delaying or confusing the intruder. In this paper we provide an overview of a lightweight container based deployment that emulates popular Linux and Windows services to unsuspecting intruders. Results show the real world attacks against the deployed system.
Persistent network communication can be found in many instances of malware. In this paper, we analyse the possibility of leveraging low variability of persistent malware communication for its detection. We propose a new method for capturing statistical fingerprints of connections and employ outlier detection to identify the malicious ones. Emphasis is put on using minimal information possible to make our method very lightweight and easy to deploy. Anomaly detection is commonly used in network security, yet to our best knowledge, there are not many works focusing on the persistent communication itself, without making further assumptions about its purpose.
Despite the vibrant advantages of Free Space Optics (FSO) technology and the variety of its applications, its widespread adoption has been hampered by rather disappointing link reliability for long-range links due to atmospheric turbulence-induced fading and sensitivity to detrimental climate conditions. A major challenge of such hybrid systems is to provide a strong backup system with soft-switching capabilities when the FSO link becomes down. The specific objective of this work is to study for the first time in Qatar and the Gulf Cooperation Council (GCC) the link capacity, link availability, and link outage of an FSO system with Radio Frequency (RF) back up (i.e. hybrid FSO/RF). In order to analyze the two transport media, we have designed a network sniffer application in Linux and embedded it in FPGA. We have installed new FSO/RF terminals and configure and align them successively. In the reporting period, we carry out measurements and relate them to real-time weather conditions. The relative humidity, which is due to high temperature and does not generally form fogs, was found to be the main factor degrading the FSO link throughput rather than the fog as reported in previous studies in Europe and North America.
Distributed Denial of Service (DDoS) attacks is the most challenging problems for network security. The attacker uses large number of compromised hosts to launch attack on victim. Various DDoS defense mechanisms aim at detecting and preventing the attack traffic. The effectiveness depends on the point of deployment. The purpose of this paper is to study various detection and defense mechanisms, their performance and deployment characteristics. This helps in understanding which defense should be deployed under what circumstances and at what locations.
The ever-growing demand for optical network security can be addressed by data encryption at different network layers. In this work, we consider all-optical cryptography technique that applies a spectral phase change and delay encoding on spectrum slices of a specified WDM channel. In this case we have investigated a novel approach where signals are double encrypted to achieve an enhanced degree of security. Simulation results indicate that such double cyphering scheme can be applied to high data-rate M-QAM signals propagating in metro/regional networks.
This paper presents Ariadne, a tool for engineering topology aware adaptive security for cyber-physical systems. It allows security software engineers to model security requirements together with the topology of the operational environment. This model is then used at runtime to perform speculative threat analysis to reason about the consequences that topological changes arising from the movement of agents and assets can have on the satisfaction of security requirements. Our tool also identifies an adaptation strategy that applies security controls when necessary to prevent potential security requirements violations.
As the Internet of Things becomes a reality, proliferation of wireless devices such as ZigBee nodes has accelerated. Their presence is now wide spread in sensitive areas such as home automation, industrial control systems, medical devices and security systems. As their popularity has increased, so has the urgency to protect and defend these networks. The decentralized nature of ZigBee ad-hoc networks creates unique security challenges for maintaining network security and intrusion detection. RF-fingerprinting provides a unique physical (PHY) layer defense against node counterfeiting by identifying rogue devices through their RF-emissions. In previous work, feature generation from received RF signals was accomplished with parametric variables such as standard deviation, variance, skewness, and kurtosis, all based on the assumption of a normal distribution of a particular region of interest. We show in this work that most collected signals are either multi-modal or non-parametric. Use of non-parametric methods for feature generation such as mean, median, mode, and trend represented by linear model coefficient estimates are shown to be much more applicable to the non-parametric distribution of the collected ZigBee preamble, resulting in improved classification performance of devices. Non-parametric classifier Random Forest is used with both parametric and non-parametric features to provide a classification performance comparison. Performance improvements of upto 9% in correct classification rates have been achieved and an effective gain of 4 dB SNR was realized.1
Cloud computing is increasingly changing the landscape of computing, however, one of the main issues that is refraining potential customers from adopting the cloud is the security. Network functions virtualization together with software-defined networking can be used to efficiently coordinate different network security functionality in the network. To squeeze the best out of network capabilities, there is need for algorithms for optimal placement of the security functionality in the cloud infrastructure. However, due to the large number of flows to be considered and complexity of interactions in these networks, the classical placement algorithms are not scalable. To address this issue, we elaborate an optimization framework, namely OCDO, that provides adequate and scalable network security provisioning and deployment in the cloud. Our approach is based on an innovative multistage approach that combines together decomposition and segmentation techniques to the problem of security functions placement while coping with the complexity and the scalability of such an optimization problem. We present the results of multiple scenarios to assess the efficiency and the adequacy of our framework. We also describe our prototype implementation of the framework integrated into an open source cloud framework, i.e. Open stack.
Internet traffic has increased dramatically in recent years due to the popularization of the Internet and the appearance of wireless Internet mobile devices such as smart-phones and tablets. The explosive growth of Internet traffic has introduced a practical example that demonstrates the concept of Big Data. Accurate identification and classification of large network traffic data plays an important role in network management including capacity planning, network forensics, QoS and intrusion detection. However, the state-of-the-art solutions, which rely on a dedicated server, are not scalable for analyzing high volume network traffic data. In this paper, we implement a distributed Support Vector Machines (SVMs) framework for classifying network traffic using Hadoop, an open-source distributed computing framework for Big Data processing. We design a global parameter store that maintains the global shared parameters between SVM training nodes. The distributed SVMs have been deployed on a 20 node cluster to analyze real network traffic trace. The results demonstrate that with 19 Mapper nodes the system is around 30% faster than Cloud SVM solution and outperforms the standalone SVM with nearly 9 times faster in training process and 15 times in the classifying process. In addition, the distributed SVMs architecture is designed to analyze large scale datasets. Therefore, it can be used not only for processing network traffic dataset, but also other large scale datasets such as Web data.
In cyber physical system (CPS), computational resources and physical resources are strongly correlated and mutually dependent. Cascading failures occur between coupled networks, cause the system more fragile than single network. Besides widely used metric giant component, we study small cluster (small component) in interdependent networks after cascading failures occur. We first introduce an overview on how small clusters distribute in various single networks. Then we propose a percolation theory based mathematical method to study how small clusters be affected by the interdependence between two coupled networks. We prove that the upper bounds exist for both the fraction and the number of operating small clusters. Without loss of generality, we apply both synthetic network and real network data in simulation to study small clusters under different interdependence models and network topologies. The extensive simulations highlight our findings: except the giant component, considerable proportion of small clusters exists, with the remaining part fragmenting to very tiny pieces or even massive isolated single vertex;no matter how the two networks are tightly coupled, an upper bound exists for the size of small clusters. We also discover that the interdependent small-world networks generally have the highest fractions of operating small clusters. Three attack strategies are compared: Inter Degree Priority Attack, Intra Degree Priority Attack and Random Attack. We observe that the fraction of functioning small clusters keeps stable and is independent from the attack strategies.
In communication networks, cyber attacks, such as resource depleting attacks, can cause failure of nodes and can damage or significantly slow down the convergence of the average consensus algorithm. In particular, if the network topology information is learned, an intelligent adversary can attack the most critical node in the sense that deactivating it causes the largest destruction, among all the network nodes, to the convergence speed of the average consensus algorithm. Although a centralized method can undoubtedly identify such a critical node, it requires global information and is computationally intensive and, hence, is not scalable. In this paper, we aim to identify the most critical node in a distributed manner. The network algebraic connectivity is used to assess the destruction caused by node removal and further the importance of a node. We propose three low-complexity algorithms to estimate the descent of the algebraic connectivity due to node removal and theoretically analyze the corresponding estimation errors. Based on these estimation algorithms, distributed power iteration, and maximum-consensus, we propose a fully distributed algorithm for the nodes to iteratively find the most critical one. Extensive simulation results demonstrate the effectiveness of the proposed methods.
In most of the current IPv6 addressing protocols, a node's IP address remains unchanged during its lifetime. Since addresses are a fundamental requirement of communications and cannot easily be hidden from eavesdroppers, eavesdroppers may track a node's activities via checking its IPv6 address. Consequently, the use of a constant address is of special concern, and changing an address regularly is desirable to lessen privacy concerns. This paper proposes an IPv6 over low-power wireless personal area network addressing scheme with privacy support. In this scheme, a node regularly changes its address without extra overheads and the size of an address set is randomly generated, the address privacy is achieved. This scheme is analyzed and evaluated, and the data show that this scheme achieves the address privacy and improves the addressing performance.
The following topics are dealt with: wireless network security;wireless network management;optical network;telecommunication traffic;ubiquitous application;and resilient network.
Recently, machine-learning based vulnerability prediction models are gaining popularity in web security space, as these models provide a simple and efficient way to handle web application security issues. Existing state-of-art Cross-Site Scripting (XSS) vulnerability prediction approaches do not consider the context of the user-input in output-statement, which is very important to identify context-sensitive security vulnerabilities. In this paper, we propose a novel feature extraction algorithm to extract basic and context features from the source code of web applications. Our approach uses these features to build various machine-learning models for predicting context-sensitive Cross-Site Scripting (XSS) security vulnerabilities. Experimental results show that the proposed features based prediction models can discriminate vulnerable code from non-vulnerable code at a very low false rate.
This paper starts with the presentation of results from an IPv6-darknet experiment that we conducted during summer 2012. The experiment indicates that attackers are gaining interest in IPv6 networks and appropriate security tools need to be readied. Therefore, we propose HoneydV6, a low-interaction IPv6 honeypot that can simulate entire IPv6 networks and which may be utilized to detect and analyze IPv6 network attacks. Our implementation extends the well-known low-interaction honeypot Honeyd. To the best of our knowledge, this is the first low-interaction honeypot which is able to simulate entire IPv6 networks on a single host. The huge IPv6 address spaces requires new approaches and concepts in order to enable attackers to find and exploit a honeypot. We increase the chance for an attacker to find a target host in our IPv6 honeypot by reacting to the attacker's requests with the dynamic generation of new IPv6 host instances in the honeynet.
Intrusion detection is one of the challenging problems encountered by the modern network security industry. A network has to be continuously monitored for detecting policy violation or suspicious traffic. So an intrusion detection system needs to be developed which can monitor network for any harmful activities and generate results to the management authority. Data mining can play a massive role in the development of a system which can detect network intrusion. Data mining is a technique through which important information can be extracted from huge data repositories. In order to spot intrusion, the traffic created in the network can be broadly categorized into following two categories- normal and anomalous. In our proposed paper, several classification techniques and machine learning algorithms have been considered to categorize the network traffic. Out of the classification techniques, we have found nine suitable classifiers like BayesNet, Logistic, IBK, J48, PART, JRip, Random Tree, Random Forest and REPTree. Out of the several machine learning algorithms, we have worked on Boosting, Bagging and Blending (Stacking) and compared their accuracies as well. The comparison of these algorithms has been performed using WEKA tool and listed below according to certain performance metrics. Simulation of these classification models has been performed using 10-fold cross validation. NSL-KDD based data set has been used for this simulation in WEKA.
Computer security has become a major problem in our society. Specifically, computer network security is concerned with preventing the intrusion of an unauthorized person into a network of computers. An intrusion detection system (IDS) is a tool to monitor the network traffic and users activity with the aim of distinguishing between hostile and non-hostile traffic. Most of current networks implement Misuse detection or Anomaly detection techniques for Intrusion detection. By deploying misuse based IDS it cannot detect unknown intrusions and anomaly based IDS have high false positive rate for detection. To overcome this, proposed system uses combination of both network based and host based IDPS as Hybrid Intrusion Detection and Prevention System which will be helpful for detecting maximum attacks on networks.
Computing Industry is moving towards Service Oriented Architecture at a fast pace. This brings network security to the forefront of major concerns to an organization. Ensuring round the clock uninterrupted service to the clients becomes a top priority to any organization. Flooding Attack is one of the costliest attacks that use a host to overwhelm a server, causing complete system crash. Flash Crowd is an unexpected rise in traffic caused by legitimate users. Both flooding which is one of the types of DoS attack and Flash crowd creates abnormal traffic condition, but in order to improve good put, the server must be deployed with the mechanism that should classify legitimate and malicious call requests. The on-going attacks usually similar to each other compared to the flows of flash crowd so the provocation is to recognize flooding attacks from flash crowd. The recognition of flooding attack is done by, using the Entropy based detection method.
People expect some technologies to help access, share and enjoy the human knowledge and resources via the Internet as the deepening of Internet globalization. Deep packet inspection is a packet sniffing technology on the network traffic, enabling operators to monitor what is happening in real time. It could be applied to management bandwidth, lawful surveillance, copyright enforcement, network security and so forth. However, DPI deployment should be concerned its black boxing results such as ISPs unilateral measure, privacy infringement, advertisement implantation. When ISPs deploy the applications of DPI popularly, it is lack of sufficient attention from users, policy-makers, and researchers to rethink its social adverse impact. This paper seeks to examine the DPI deployment by ISPs in China, and be aware of the unbalanced gap between DPI deployment and social public policy. It is a brief tale of gap between DPI deployment and social public policy in China, hoping more attention could be paid to this domain.
With continuous development of network technology, more and more attacks on system from Internet have become more automated and caused greater damage and loss. It is important to improve network security and protect transmission information. General speaking, it is able to protect information, to some extent, with file encryption and digital signature. This article explores ElGamal public key encryption algorithm that is put practice into information transmission. Base on it, it also aims to achieve the text encryption and decryption, image encryption and decryption so as to guarantee information transmission.
Wind energy is beneficial both economically and environmentally, while its utilization levels are still far from satisfactory and has plenty of room to be improved. One way to achieve this is to make use of the heat storage in some of the thermal generators to obtain fast ramping capabilities, so that they can catch up with the fluctuating and intermittent wind energy. Output characteristics of such thermal units are distinguished from conventional ones in that their ramping rates are dynamic and with memory, thus new models of system dispatch need to be established. In this paper, we present a mixed integer linear formulation with decisions of thermal unit heat usage for the dispatch of wind-thermal power systems containing heat storage units, and network security constraints of power balance and transmission limits are considered. Numerical tests are performed for a 3-bus system and the IEEE 30-bus system by using CPLEX, and results show that additional ramping capabilities from heat storage units lead to substantial improvements in wind power utilization.
Swissgrid (the Swiss TSO) introduced a new framework for real-time network security management supported by three main pillars: a) a revised operational planning procedure;b) a near real-time decision support tool performing an unambiguous optimal redispatch of power plants based on an AC security-constrained optimal power flow (AC-SCOPF);and c) a financial incentive for power plants involved in these redispatch procedures. This paper outlines these main concepts and describes in detail the practical aspects of the used AC-SCOPF tool. These new procedures and tool are in operation since early 2013. The paper also describes the main challenges faced during the AC-SCOPF tool integration into the Swissgrid operations.
It poses high requirements for the calculation speed and the precision of the solving method when we consider the large-scale transmission expansion planning (TEP) problems. Therefore, combined with the respective characteristics of EDA (Distribution of Estimation Algorithm) and DE (Differential Evolution algorithm), this paper puts forward a new hybrid EDA/DE algorithm for large-scale TEP problems. Meanwhile, it improves the updating mechanism of probabilistic model of EDA based on the characteristics of the TEP problems. Considering the investments of grid company, the new energy incentive politics and network security constraints, this paper proposes a multi-objective static planning model for the TEP considering wind power penetration, which takes the comprehensive cost, the wind curtailment and the risk value into consideration. Finally, a specific example is applied in this paper to verify the applicability and effectiveness of the proposed model and algorithm.
Voice over Internet Protocol (VoIP) represents considerable cost savings in comparison with traditional phone lines. VoIP, operates in the IP network, therefore it inherits all network security and safety problems. Consequently, implementing VoIP without any security mechanisms produces many risks and it does not provide any guaranty of privacy of information and calls. It is possible to use the encryption in order to secure the calls. However, VoIP is a real time application and using encryption degrades the quality of service (QoS) by increasing the delay and the jitter. In this paper, we propose the use of the Blowfish encryption algorithm to encrypt the audio communication between the VoIP clients as the used algorithm was previously compared to the Advanced Encryption Standard (AES) algorithm, and it was shown that it is faster and it offers a better throughput. In this study, we developed VoIP clients and we performed many experimental tests on these clients with the implemented algorithm in order to provide security without degradation of the quality of calls. The obtained results show an acceptable compromise among all performance parameters and they are considered motivating and very encouraging.
Honeypots are an indispensable tool for network and system security as well as for computer forensic investigations. They can be helpful for detecting possible intrusions, as well as for gathering information about their source, attack patterns, final target and purpose. Highly interactive honeypots, are probably the most useful and enlightening ones, since they reveal many information about intruders' behavior and skills, even though the implementation and setup of such tools might require considerable efforts and computational resources. Accordingly we present an architecture for highly interactive honeypots aiming at detecting password-cracking attacks by means of honeywords and leveraging container-based virtualization to provide persistent sessions needed to capture attacker activities.
Wireless network security schemes are characterized by parameters such as processing time and the avalanche property. These parameters tend to adversely affect the efficiency of the wireless network namely;throughput of network and lost/ retransmitted packets. The undesirable effects of processing time and avalanche property are due to the fact that, existing implementation of wireless security schemes is based on symmetric cryptography. The avalanche property makes a block cipher secure but in turn reduces throughput since it causes them to be sensitive to bit errors. In addition, processing time through the many rounds required to establish a session key increases the round trip time (RTT) for a message significantly. Hence there is need to implement a wireless security scheme which could minimize both the processing time and the avalanche property. The paper introduces a new algorithm for wireless security based on RSA public-key cryptography, convolutional codes and subband coding. It describes implementation using small integer key lengths thereby minimizing processing time and avalanche property since it is based on asymmetric cryptography. Future works in this study can show that, the implementation can fit in a single FPGA device which is close to a wireless transmitter and receiver at access points (APs).
The large number of event logs generated in a typical network is increasingly becoming an obstacle for forensic investigators to analyze and use to detect and verify malicious activities. Research in the area of network forensic is trying to address the challenge of using network logs to reconstruct attack scenarios by proposing event correlation models. In this paper we introduce a new network forensics model that makes network event-logs admissible in the court of law. Our model collects available logs from connected network devices, applies decision tree algorithm in order to filter anomaly intrusion, then re-route the logs to a central repository where event-logs management functions are applied.
Power cable insulation damage affect the power network security. Blackout can be avoided by sensitively detecting the insulation damage. Interdigital capacitive sensors are widely used to measure moisture, porosity and viscosity with the advantages of single-side access, adjustable signal strength and tomography capability. In this paper, a PCB type interdigital capacitive sensor was designed for detecting cross-linked polyethylene (XLPE) power cable insulation damage. The structure parameters are optimized and Sensitive field distribution are improved. The results show different damage can be detected effectively. Therefore, the interdigital capacitive sensor has a potential application in cable insulation damage detection.
Key management is one of the most challenging security problems in wireless sensor network (WSN). A complete key management scheme should include key generation, key distribution, and key updating. Random key predistribution scheme is widely considered as the most suitable for WSN. However, network security connectivity contradicts resilience, and related research lacks key updating. To address these problems, a random key management scheme for heterogeneous WSN is proposed. Deployment knowledge is introduced into q-composite random key predistribution, and key updating mechanism is designed. Theoretical analysis and simulation show that the scheme can effectively improve network connectivity, support scalability, and ensure key effective without affecting security connectivity and resilience.
Packet classification is a key network function enabling a variety of network applications, such as network security, Quality of Service (QoS) routing, and other value-added services. Routers perform packet classification based on a predefined rule set. Packet classification faces two challenges: (1) the data rate of the network traffic keeps increasing, and (2) the size of the rule sets are becoming very large. In this paper, we propose an FPGA-based packet classification engine for large rule sets. We present a decomposition-based approach, where each field of the packet header is searched separately. Then we merge the partial search results from all the fields using a merging network. Experimental results show that our design can achieve a throughput of 147 Million Packets Per Second (MPPS), while supporting upto 256K rules on a state-of-the-art FPGA. Compared to the prior works on FPGA or multi-core processors, our design demonstrates significant performance improvements.
The visualization of mobile network data can be of significant value to the network security administrator in order to detect anomalies in the normal traffic, caused by malicious attacks. Although several visualization types of the network structure and traffic already exist, the literature around visualizing behavioral aspects of users or network components, in order to distinguish the normal from the abnormal ones, is limited. In this paper, a behavior-based approach for visualizing the users of the network, with respect to specific aspects of their behavior, is proposed. The approach introduces the extraction of behavior-related descriptors from the raw network traffic data, which can be used to visualize behavioral similarities, so that users with similar behavior are depicted as points close to each other. Multiple descriptors are extracted from each user and are used as the multiple modalities in a state-of-the-art multi-objective visualization method. The outcome of the multi-objective method is a visualization of the behavioral similarities of users, according to the selection of a trade-off among the multiple descriptors. This allows the analyst to visually detect anomalies and analyze their evolution in time. Experimental evaluation of the proposed approach with several datasets in various application scenarios verify its efficiency.
The vast interconnectivity of devices in the Internet of Things (IoT) and Cyber-Physical systems under wireless environments facilitates information exchange, but challenges network security. In this paper, cooperative beamforming for secure communications is studied in wireless cyber-physical systems, where two legitimate devices communicate with the help of amplify-and-forward (AF) relays, and the eavesdropping devices around purpose to intercept the information. To achieve high secrecy sum rate, a secrecy sum rate maximization problem via cooperative beamforming with or without artificial noise is formulated under individual relay power constraint. The problem is shown to be non-convex. We then reformulate it into a difference of convex programming problem, relax it by a tight semidefinite relaxation (SDR), and approximate it into sequential convex problems by first-order Taylor expansion, which eventually leads to a high-rate beamforming solution. The simulations show that our developed beamforming scheme can achieve high secrecy sum rate and fast convergence.
Traditional two-dimensional (2D) and three-dimensional (3D) visualization tools for network security applications often employ a desktop, mouse, and keyboard setup of WIMP (Windows, Icons, Menus, and a Pointer) interfaces, which use a serial set of command inputs (e.g., click, rotate, zoom). However, research has shown that multiple inputs (e.g., Microsoft Kinect [8] and multi-touch monitors) could reduce the selection time of objects, resulting in a quicker response time than its traditional counterparts. In this work, we investigate these alternative user interfaces that are natural to the user for multiple inputs that reduce response time as a user navigates within a complex three-dimensional (3D) visualization for network security applications. Specifically, we introduce a visualization tool called InterSec, an interaction system prototype for interacting with 3D network security visualizations. InterSec helps developers build and manage gestures that require the coordination of multiple inputs across multiple interaction technologies. To our knowledge, InterSec is the first tool that proposes a system to reduce number of interactions within 3D visualizations for network security tools. Through our evaluation of live Honeynet data and a user study, the results reveal InterSec's ability to reduce the number of interactions to aid in 3D navigation in comparison to the mouse user interface.
Logging is a critical component of Linux auditing. The experiments indicate that the logging overhead can be significant. The paper aims to leverage the performance overhead introduced by Linux audit framework under various usage patterns. The study on the problem leads an adaptive audit logging mechanism. The adaptive auditing mechanism reduces the overall system overhead and achieves a similar level of protection on the system and network security.
Effective network security may targets a plenty of threats and also avoids them from entering or opening out on a network. An attack in Mobile Ad hoc NETwork (MANET) is due to unreliability, unfixed topology, limited battery power and lack of centralized control. The first line of defense solutions is Encryption and authentication which are not adequate to protect MANETs from packet dropping attacks. Existing IDSs for MANETS depend on the Watchdog technique. In existing system Researchers main attraction is on designing new prevention, detection and response mechanism for MANETs. The scheme will identify, supervise and observe the malicious nodes without adjusting the performances in the network. The motivation is to overcome the issues such as limited transmission power, packet dropping, receiver collision and false misbehavior reports generation of the Watchdog system. In this paper, we proposed a new the Modified Version of EAACK based IDS that is used to overcome the MANET attacks. Elliptic Curve Digital signature Algorithm (ECDSA) is use to authenticate the acknowledgment packets used in the propose work to overcome drawbacks in the security level.
Complex networks rely on their structural robustness for their function and performance. Considering that redundancy backup is frequently used to enhance the robustness of complex networks, we try to find better redundancy backup strategy by using optimization methods. In this paper, our contributions are twofold. First, we prove that natural connectivity is suitable for measuring network robustness. Second, a robustness optimization algorithm is proposed based on GA, while it is different from traditional GA. The method of coding, crossover and mutation operations are all improved in this research. Extensive experiments on real-world datasets demonstrate that the effectiveness of our methods is better than the classical rich-rich redundancy backup strategy.
Wireless sensor network (WSNs) with a mobile sink node (MS) has been widely concerned. In view of the low security in the basic random key pre-distribution scheme, and the vital role of MS in the research on key management, this paper proposed a new scheme (PPBR scheme) based on a composite key management schemes with polynomial pool-based key pre-distribution and basic random key pre-distribution. The scheme uses polynomial t-degree property to increase the difficulty of cracking the key by enemy and enhance the network resilience to node capture, meanwhile, improves the storage efficiency as heterogeneous features between MS and sensor nodes. The low connectivity is solved by introducing path key tree-based establishment method. Theoretical analysis and simulation experiments show that the proposed scheme has advantages in terms of network security, connectivity and storage effectiveness under the comprehensive consideration on different performance evaluation.
The current network security technologies are reviewed, the present situation and the demand of network security is analyzed, the key network security technology is investigated. Starting from the enterprise information safety, the network virus, access control and information system stability is analyzed. On this basis, the enterprise network security scheme is designed. This system can provide information security management with a more comprehensive, convenient, intuitive, accurate virus monitoring platform, reduce damage and risk caused by growing virus on network, and provide security for the enterprise's informatization construction.
The rapid development of computer network system brings both a great convenience and new security threats for users. Network security problem generally includes network system security and data security. Specifically, it refers to the reliability of network system, confidentiality, integrity and availability of data information in the system. Network security problem exists through all the layers of the computer network, and the network security objective is to maintain the confidentiality, authenticity, integrity, dependability, availability and audit-ability of the network. This paper introduces the network security technologies mainly in detail, including authentication, data encryption technology, firewall technology, intrusion detection system (IDS), antivirus technology and virtual private network (VPN). Network security problem is related to every network user, so we should put a high value upon network security, try to prevent hostile attacks and ensure the network security.
According to the security problems that may exist when the users transport the sensitive data on the network, this paper tries to optimize and specify the user's encryption processing for the sensitive data. Then we propose a strategy using double encryption technology, and combined with the check of messages technology, to ensure that the users' data transport safe and effective. The client generates symmetric key and message digest, and set them add behind the message encrypted transmission together. It ensures that the user data be security and integrity. It also uses the double encryption technology to encrypt symmetric key once again, which ensures the security of key transmission. Experiments show that improvement program technology not only ensures the message security of the transmission and the security distribution of keys, but also it doesn't reduce the efficiency of encryption and decryption data.
Intrusion Detection System (IDS) is an essential component of the network security infrastructure. It detects malicious activities by monitoring network traffic. There are two main classes of IDS: the anomaly-based IDS and signature-based IDS. An important challenge, for signature-based IDS, is automating attack signature writing from traffic logs, which can be very hard to be established for human administrator. In this paper, we propose a solution addressing this challenge. We propose cloud-based signature learning service using Inductive Logic Programming (ILP). Learning service generates rule describing properties shared by packets labelled as malicious and that do not cover normal packets. The system uses a background knowledge composed of predicates used to describe network attack signature. The cloud architecture of our IDS enables it to have specialized nodes. Preliminary experimentations show that the proposed system is able to reproduce automatically SNORT signature.
In the current enterprise data enter networking environment, a major hurdle in the development of network security is the lack of an orchestrated and resilient defensive mechanism that uses well-established quantifiable metrics, models, and evaluation methods. In this position paper, we describe an emerging Secure and Resilient Networking (SeReNe) service model to establish a programmable and dynamic defensive mechanism that can adjust the system's networking resources such as topology, bandwidth allocation, and traffic/flow forwarding policies, according to the network security situations. We posit that this requires addressing two interdependent technical areas: (a) a Moving Target Defense (MTD) framework both at networking and software levels, and (b) an Adaptive Security-enabled Traffic Engineering (ASeTE) approach to select optimal countermeasures by considering the effectiveness of countermeasures and network bandwidth allocations while minimizing the intrusiveness to the applications and the cost of deploying the countermeasures. We believe that our position can greatly benefit the virtual networking system established in data Centerior enterprise virtual networking systems that have adopted latest Open Flow technologies.
For a past few decades, there has been quick progress in internet based applications and technology in the area of computer networks. Data is most important asset of any organization and they require proper protection and management of private and highly sensitive information. Nowadays cyber-attacks have become very common and network security can be provided with Detection Systems. An intrusion detection system analyzes and gathers information from various areas within a network or computer to identify possible security breaches, which include both misuse and intrusion. Researchers are interested in intrusion detection system using data mining techniques as a deceitful skill. This paper aims to give an intrusion detection system using Bagging Ensemble Selection. Bagging Ensemble Selection implementation is fairly straightforward, and it gives an excellent predictive performance on practical problems.
A botnet is group of compromised computers that are controlled by a botmaster, who uses them to perform illegal activities. Centralized and P2P (Peer-to-Peer) botnets are the most commonly used botnet types. Honeypots have been used in many systems as computer defense. They are used to attract botmasters to add them in their botnets;to become spies in exposing botnet attacker behaviors. In recent research works, improved mechanisms for honeypot detection have been proposed. Such mechanisms would enable bot masters to distinguish honeypots from real bots, making it more difficult for honeypots to join botnets. This paper presents a new method that can be used by security defenders to overcome the authentication procedure used by the advanced two-stage reconnaissance worm (ATSRW). The presented method utilizes the peer list information sent by an infected host during the ATSRW authentication process and uses a combination of IP address spoofing and fake TCP three-way handshake. The paper provides an analytical study on the performance and the success probability of the presented method. We show that the presented method provide a higher chance for honeypots to join botnets despite security measures taken by botmasters.
The SSL/TLS, one of the most popular encryption protocol, was developed as a solution of various network security problem while the network traffic has become complex and diverse. But the SSL/TLS traffic has been identified as its protocol name, not its used services, which is required for the effective network traffic management. This paper proposes a new method to generate service signatures automatically from SSL/TLS payload data and to classify network traffic in accordance with their application services. We utilize the certificate publication information field in the certificate exchanging record of SSL/TLS traffic for the service signatures, which occurs when SSL/TLS performs Handshaking before encrypt transmission. We proved the performance and feasibility of the proposed method by experimental result that classify about 95% SSL/TLS traffic with about 90% accuracy for every SSL/TLS services.
In previous few years, an incredible growth is witnessed in the popularity and pervasiveness of smart phones. It has also seen that new mobile applications are built day by day. These applications provide users functionality like social networking applications, games, and many more. Some of the mobile applications might be having a direct purchasing cost or be free but having an ad-support for revenue and in return these applications provide users' private data to ad provider with or without users' consent. Worryingly, some of ad libraries ask for permissions beyond the requirement and additional ones listed in their documentation. Some applications also track users by a network sniffer across ad providers and its applications. It is often ineffective at conveying meaningful, useful information on how a user's privacy might be impacted by using an application. Here in this paper, we have examined the effect on user privacy of some grossing Android applications that transmit private data of user without their permission. Using third party connections that an app makes, we defined the legitimacy of application. Also we observed some other parameter to check whether an app is stealing users' private information.
Currently critical infrastructures such as SCADA systems are increasingly under threat, they often go unreported. There is a great need in addressing them. Today majority of the industries use these SCADA systems, so it is very critical to protect these systems. Attack on these systems could cause serious damage to the infrastructure and sometimes a threat to human life as well. As per date there are very few solutions to address SCADA security. So, it is important to take countermeasures against the attacks on these systems. In this paper we will analyze the use of honeypot systems in detecting the network attack vectors on SCADA systems. We will start by analyzing and testing various honeypot features which can help in providing additional security for SCADA systems. A Honeypot is built to mimic the services of an ICS;exposing them to the Internet, making them attractive for attackers and monitor the attackers activities. The goal is to model the attacking methodologies and suggest recommendations to make SCADA system secure.
Honeypot is a recent technology in the area of computer network security. Production systems that are connected to the Internet are the main target for various cyber attacks. This paper presents a deployment of honeypot system in campus network. The system implements high-interaction honeypot with Secure Shell installed to study common SSH attacks in Linux environment. This system records usernames and passwords that are attempted by intruder from the Internet. It also captures detail activities of the attackers while they are interacting inside the target honeypot. Intruders attack SSH servers through dictionary and brute-force mechanism followed by intrusion. This paper covers both dictionary attack and intrusion.
New botnet and bots using P2P protocols have become the increasing threat to network security because P2P botnet and bots do not have a centralized point to trace back or shut down, thus detecting the P2P bots is very difficult. In order to deal with these threats, the model in terms of the dendritic cells algorithm (DCA) is presented to detect P2P bots on an individual host. The detailed approach to detect P2P bots is also described. The raw data for P2P bots detection are obtained via APITrace tool. The processes ID are mapped into the antigens, and the behavioral data created by the processes are mapped into the signals, which are the time series input data of DCA. These data as the input data of the algorithm are used to implement data fusion and correlation. Through related experiments, the systems using the proposed method in this paper can detect p2p bots. The method should outperform the other existing P2P detection techniques due to its linear computation in the process of detection and analysis, and no training phrase.
Global darknet monitoring provides an effective way to observe cyber-attacks that are significantly threatening network security and management. In this paper, we present a study on characterization of cyberattacks in the big stream data collected in a large scale distributed darknet using association rule learning. The experiment shows that association rule learning in the darknet stream data can support strategic cyberattack countermeasure in the following ways. First, statistics computed from malware-specific rules can lead to better understanding of the global trend of cyberattacks in the Internet. Second, strong association rules can lead to further insights into the nature of the attacking tools and hence expedite the diagnosis. Then, the discovery of emerging new attacks may lead to early detection and prompt prevention of pandemic incidents, preventing damage to the IT infrastructure and extensive financial loss. Finally, exploring the knowledge in the frequent attacking patterns can enable accurate prediction of future attacks from analyzed hosts, which could improve the performance of honeypot systems to collect more pertinent malware information using limited system and network resources.
Due to the active defense of benign worms against the damage imposed by worms, benign worms have been paid enough attention by network security researchers. This paper presents patching structured benign worms, and we designed their deployment and work mechanism. Furthermore, the process of patching structured benign worms countering against worms is modeled based on the infectious model. Finally, the model was simulated. From the simulation, two factors (detection rate of the sensor and delay time), which affect the process of patching structured benign worms countering against worms, were summarized. The model of patching structured benign worms leads to a better understanding and prediction of the scale and speed of benign worms countering against the propagation of worms.
Along with the development of Internet applications, many kinds of network security issues become highlights. Customer confidentiality should be its highest priority for every Internet company. The network intrusion detection system as one of the key technology while auditing safely, is the important component of network safe protection. In this paper, we present an intelligent framework to detect network intrusions. We design two intrusion detection engines in the framework. One is the rule-based that depends on the programmed rules to detect intrusions, and the other is the anomaly-based that depends on machine learning to detect intrusions. They have a complementary effect to avoid missing some attacks. The ultimate trait of our proposed framework is that it is flexible enough for users to do some changes and improvements. Users just need to take surprisingly little effort to customize the framework to fit for their needs. We have designed an experiment to test the framework's ability to protect the simulated Web application against the brute force attack. The experimental results show that our intelligent framework has good performance and is able to detect the brute force attack timely.
Vulnerability testing platform may be time consuming and inconvenient to establish for that it greatly depends on tester's testing ability and knowledge of network security architecture. Virtual vulnerability validation platform (VVVP) is proposed in this paper to deal with this dilemma. Key technologies such as OS image customization and KVM are analyzed. VVVP model has two main parts, the OS image customization technology which is utilized to have those needed software files packed into the OS image, and the KVM (Kernel-based Virtual Machine) which is used for improving operation efficiency. The results show that VVVP is very efficient for building an environment.
The large number of internet traffic has highlighted the importance of traffic detection. Anomaly detection is playing an increasingly important role in network security. Feature matching, statistics rules and data mining are widely used in traditional anomaly detection systems, but they have numerous disadvantages, such as low accuracy, over consumption of processing resources. For the complexities of irregular situation, we propose a new model for anomaly traffic detection in this paper. This study combine feature matching module, statistics rules module and data mining module under fully considering the advantages and disadvantages of these three detection methods. Moreover, a multi-layer detection scheme was introduced to enhance system accuracy and reduce the complexity at the same time. Data mining module is the core of the model, Naive Bayes, decision tree and clustering algorithms are used in this module. The results of this system are produced by integrating the detection results of multi detection modules and proved that it has more accuracy than separate module.
Cloud computing is the trend of information resources sharing and also an important tool for enterprises or organizations to enhance competitiveness. Users have the flexibility to adjust request and configuration. However, network security and resource sharing issues have continued to threaten the operation of cloud computing, making cloud computing security encounter serious test. How to evaluate cloud computing security has become a topic worthy of further exploration. Combining system, management and technique three levels security factors, this paper proposes a Security Threats Measurement Model (STMM). Applying the STMM, security of cloud computing system environment can be effectively evaluated and cloud computing security defects and problems can be concretely identified. Based on the evaluation results, the user can choice the higher security service provider or request the service provider security improvement actions.
Due to the proliferation of new threats from spammers, attackers, and criminal enterprises, Anomaly-based Intrusion Detection Systems have emerged as a key element in network security and different statistical approaches have been considered in the literature. To cope with scalability issues, random aggregation through the use of sketches seems to be a powerful prefiltering stage that can be applied to backbone data traffic. In this paper we compare two different statistical methods to detect the presence of anomalies from such aggregated data. In more detail, histogram cloning (with different distance measurements) and CuSum algorithm (at the bucket level) are tested over A well-known publicly available data set. The performance analysis, presented in this paper, demonstrates the effectiveness of the CuSum when a proper definition of the algorithm, which takes into account the standard deviation of the underlying variables, is chosen.
This paper is to look at who are continuing to attack industry control system (ICS) and how. It also features a honeynet that we developed and deployed worldwide as a network of honeypots of ICS. This paper describes series of attack methods and attack statistics against the honeynet.
Over the last few years, there has been a significant increase in the number of IP cameras used in various places including markets, malls, pharmacies, movie theatres and schools. Recent products on the market are cloud-based and upload the captured video to a cloud server. With widespread use, security of these IP cameras emerges as an important issue. However, there is still very little work done on the security of these devices. In this paper, we investigate security of cloud-based wireless IP cameras. Security of these devices spans multiple research areas including secure multimedia, network security and cloud security. We have investigated the traffic generated by a low-end, easy-to-setup, off-the-shelf wireless IP camera for average home user. We explored the security precautions taken by the manufacturers of IP cameras and evaluated the access control mechanisms in place. We used a variety of open source and non-commercial tools in our investigation. We identified many security and privacy issues in using these devices ranging from minor to severe issues.We showed that if a malicious person can sniff the IP camera's network traffic anywhere in between the mobile device-cloud servers-IP camera path, he would be able to reconstruct the JPEG images, which is a serious a privacy issue.
Regular expressions have become a fixture in network security systems such as Network Intrusion Detection, Spam email filtering, and Antivirus. Unfortunately, regular expressions require considerably more resources in matching over fixed binary or character strings. Much research has focused on improving matching architectures or hardware support to create more efficient regular expression matching. This research, however, investigated whether or not the regular expression set itself contained any lever that might make for creating more efficient automata prior to moving such automata to any specific matching architecture or hardware. We found that typical Non-deterministic Finite Automata (NFA) construction methodologies create redundant paths in the NFA when used with the complex rule-sets employed in network security. This stems directly from the fact that creating optimized NFA is a hard problem. As such, we created REduce, a tool that uses shared prefixes among regular expressions as a heuristic to eliminate redundant paths among shared prefixes within constructed NFA. The end result is smaller matching automata (between 4-50% depending on the rule-set) and a 4-900% improvement in throughput due to reductions in active state. More importantly, REduce only targets NFA construction, thus the generated NFA can be converted to any specific matching architecture or hardware for cumulative improvement.
It is a great pleasure welcoming all of you to the IEEE International Wireless Communications and Mobile Computing Conference (IEEE IWCMC 2015) in the beautiful Dubrovnik, Croatia! We are indeed delighted that this year's IEEE IWCMC lived up to its goal under the conference theme Communications for the 21st Century, and continues its tradition of providing the premier forum for presentation of research results and experience reporting on the cutting edge research in the general areas of wireless communications and mobile computing. This year, we received more than 700 submissions from 42 countries. Each paper received at least three peer technical reviews, comprised of more than 450 TPC members from academia, government laboratories, and industries. After carefully examining all review reports, the IEEE IWCMC 2015 TPC finally selected about 36% high-quality papers for presentation at the conference and publication in the IEEE IWCMC 2015 proceedings. The conference program starts on Monday August 24th with a full day of Tutorials that are free of charge to all our attendees. Then, each day starts with a keynote speaker chosen from renowned world-class leaders in the area-Dr. Giuseppe Bianchi from University of Roma Tor Vergata, Italy, Dr. Mario Gerla from UCLA, and Dr. Slim Alouini from KAUST, highlighting the latest research trends in the wireless communications, mobile computing, and networks. This year, the technical sessions reflect the continued and growing interests in a wide range of spectrum, including wireless communications and networks, cross-layer design and optimization, mobile computing, wireless sensor networks, network security, and use of wireless technologies in social emergency applications. There are two special sessions composed of invited papers from renowned experts from around the world. Outstanding papers will be selected for four Special Issues in well known international journals. Our objective in t- e future is to reduce the acceptance rate further.
Wireless network security has received tremendous attention due to the vulnerabilities exposed in the open communication medium. The most common wireless Medium Access Control (MAC) protocol is IEEE 802.11, which assumes all the nodes in the network are cooperative. However, nodes may purposefully misbehave in order to disrupt network performance, obtain extra bandwidth and conserve resources. These MAC layer misbehaviours can lead to Denial of Service (DoS) attacks which can disrupt the network operation. There is a lack of comprehensive analysis of MAC layer misbehaviour driven DoS attacks for the IEEE 802.11 protocol in MANETs. This research studied possible MAC layer DoS attack strategies that are driven by the MAC layer malicious/selfish nodes on IEEE 802.11 protocol. Such DoS attacks could caused by malicious and selfish nodes by violating their backoff timers associated with the MAC protocol. Therefore, these attacks are experimentally analysed and evaluated the impact on the network performance and stability in MANETs. The simulation results show that introducing DoS attacks at MAC layer could significantly affect the network throughput and data packet collision rate. This paper concludes that DoS attacks with selfish/malicious intend can obtain at least 50% larger throughput by denying well-behaved nodes to obtain deserved throughput, also DoS attacks with the intend of complete destruction which allow only less than 5% throughput for well-behaved nodes which eventually led to shut-down of the network.
The security of virtual environments is a major issue for the deployment of the use of the Cloud. Unfortunately, these environments are composed of a set of already existing technologies used in a new way, many security solutions are only traditional reconditioned solutions to solve the Cloud and virtual networks security issues. The work done in this article is a response to the resource limitations of physical security devices such as firewalls and propose new security architectures consist of management of network security in the cloud-based services following Security as a Service model and propose novel architectures for managing these services. We took the initiative to propose a completely Cloud-Based architecture. The latter allows a cloud provider to provide firewalling service to its customers. It asks them to subscribe to the offer by guaranteeing treatment (analysis) with a capacity of bandwidth traffic with functional filtering rules and other proposed by the subscriber.
Software-Defined Networking (SDN) is an emerging architecture that is ideal for today's high-bandwidth, dynamic network environments. In this architecture, the control and data planes are decoupled from each other. Although much research has been performed into how SDN can resolve some of the most-glaring security issues of traditional networking, less research has addressed cloud security threats, and, in particular, botnet/malware detection and in-cloud attacks. This work proposes an intrusion prevention system for cloud networking with SDN solutions. To realize collaborative defense, mechanisms of botnet/malware blocking, scan filtering and honeypot are implemented. Malicious traffic is isolated because bot-infected VMs are removed effectively and efficiently from the private cloud. The scanning behavior can be filtered at a very early stage of prevention, making the VMs less exploitable. A honeypot mechanism is also deployed to trap attackers. Experimental results show the high detection rate, high prevention accuracy and low vulnerability of the proposed system.
The common means of defense for network security systems is to block the intrusions by matching the signatures. Intrusion-signature matching is the critical operation. However, small and medium-sized enterprise (SME) or Small Office Home Office (SOHO) network security systems may not have sufficient resources to maintain good matching performance with full-set rules. Code generation is a technique used to convert data structures or instruction to other forms to obtain greater benefits within execution environments. This study analyzes intrusion detection system (IDS) signatures and discovers character occurrence to be significantly uneven. Based on this property, this study designs a method to generate a string matching source code according to the state table of AC algorithm for embedded network intrusion detection platforms. The generated source code requires less memory and relies not only on table lookup, but also on the ability of processor. This method can upgrade the performance by compiling optimization and contribute to the application of network processors and DSP-like based platforms. From evaluation, this method requires use of only 20% memory and can achieve 86% performance in clean traffic compared to the original Aho-Corasick algorithm (AC).
SQL attack prevention measures are a hotspot of network security research in recent years. In this paper, the principle and characteristics was introduced firstly, then SQL attacks and preventive measures were given. After the concept of prevention model structure and model built, the prevention work process of the model was summarized.
Packet classification is one of the key functionalities provided by network devices for QoS and network security purposes. Recently the rapid growth of classification ruleset size and ruleset complexity has caused memory performance woes when applying traditional packet classification algorithms. Inheriting the divide-and-conquer idea of pre-partitioning the original rules into several groups for significant reduction of memory overhead, this paper proposes Swin Top, a new ruleset partitioning approach based on swarm intelligent optimization algorithms, to seek for the global optimum grouping of rules. To enhance convergence accuracy and speed up the iterative process, Swin Top employs several novel ideas, such as the introduction of grouping penalty, the combination of PSO and GA, and a new memory usage estimation method. On the publicly available rulesets from Class Bench, SwinTop is shown to achieve 1 to 4 orders of magnitude lower memory consumption than simply applying a traditional packet classification algorithm without ruleset partitioning, and outperform the state-of-the-art partitioning algorithms EffiCuts and ParaSplit on all kinds of large-sized rulesets.
The information technology has made wireless sensor widely used. But however, the security problem of wireless sensor network becomes more and more serious. Due to limited resource, the security mechanism of traditional network can't be applied to wireless sensor network. This paper proposes a trusted measurement scheme to ensure wireless sensor network running well and securely. This scheme is based on static measurement and combines active measurement with passive measurement. The real-time measurement is the running security assurance for wireless sensor network. It can still run normally under the network security attack. The simulation proves that this scheme can ensure the wireless sensor network running well and has little effect on the network life cycle.
SQL attack prevention measures are a hotspot of network security research in recent years. In this paper, the principle and characteristics was introduced firstly, then SQL attacks and preventive measures were given. After the concept of prevention model structure and model built, the prevention work process of the model was summarized.
The growing share of encrypted network traffic complicates network traffic analysis and network forensics. In this paper, we present real-time lightweight identification of HTTPS clients based on network monitoring and SSL/TLS fingerprinting. Our experiment shows that it is possible to estimate the User-Agent of a client in HTTPS communication via the analysis of the SSL/TLS handshake. The fingerprints of SSL/TLS handshakes, including a list of supported cipher suites, differ among clients and correlate to User-Agent values from a HTTP header. We built up a dictionary of SSL/TLS cipher suite lists and HTTP User-Agents and assigned the User-Agents to the observed SSL/TLS connections to identify communicating clients. We discuss host-based and network-based methods of dictionary retrieval and estimate the quality of the data. The usability of the proposed method is demonstrated on two case studies of network forensics.
Honey pots and honey nets are popular tools in the area of network security and network forensics. The deployment and usage of these tools are influenced by a number of technical and legal issues, which need to be carefully considered together. In this paper, we outline privacy issues of honey pots and honey nets with respect to technical aspects. The paper discusses the legal framework of privacy, legal ground to data processing, and data collection. The analysis of legal issues is based on EU law and is supported by discussions on privacy and related issues. This paper is one of the first papers which discuss in detail privacy issues of honey pots and honey nets in accordance with EU law.
To increase robustness of network nodes and their communication sessions, we propose convoluted multiaddress networking architecture. This approach prevents malicious packets from getting into the incoming traffic of a network terminal. Usually, traffic analyzers and filtering solutions should be installed in the network to isolate a victim node from packet streams created by malefactor terminals. Our network security technique is built on a different approach. The principles of convoluted multiaddress networks are based on the idea that we can protect nodes by hiding their network location from illegitimate clients. In our study, we show how to create dynamic addressing policies for preventing DDoS attacks and traffic eavesdropping. These policies randomize address space and communication data streams, therefore a malefactor cannot acquire access to data streams or destination terminals. In this paper, we discuss IP Fast Hopping, an application of convoluted multiaddress networking in TCP/IP networks. We consider basic implementation of this architecture, its major practical constraints and initial experimental results. The presented approach aims to ensure security of future generation communication technologies. In this study, we suggest Thing Lakes architecture of the Internet of Things, which is based on IP Fast Hopping approach and intended to protect the IoT environment against several major security issues in such networks.
Nowadays the telecommunications industry focuses on two user requirements: increased data-rates at high speed mobility and robust network security. The emerging technology for these milestones is the 3GPP Long Term Evolution standard and the LTE-Advanced improvement of this standard. The paper describes the main features of these modern networks and conducts an analysis on the LTE EPS-AKA proposing an improvement to address the Denial of Service attacks pointed out by researchers in the last years.
The quality of the software has an increasing impact on the performance, operation and ultimately the cost of industrial automation systems. Software metrics are necessary to assess the quality and to identify modules that impact cost of testing and maintenance. There is a number of software metrics available within a software engineering domain. However, there is a gap when it comes to evaluating IEC 61499 automation software. It is a challenging task too. IEC 61499 architecture embraces several software development techniques such as imperative languages, state machines and object oriented programming. This paper proposes the method for calculating metrics for IEC 61499 function blocks. These metrics were used to analyze power system protection software. Metrics confirmed expected results and reasonably described quality of software modules. With further improvement and expansion of the proposed set of metrics, it will be possible to accurately analyze and therefore improve automation software design and reduce cost.
Video surveillance, as an effective technical mean in remote live monitoring and on-site monitoring, is widely used in each link of the smart grid. With the deepening of the continuous development and application of video surveillance technology, it plays an increasingly important role in ensuring network security. This paper, combined with the characteristics of the softswitch system, leads the softswitch technology into the design of video monitoring platform. This method realizes the separation of the video control signaling, video streaming media information makes centralized traffic control convenient relatively, so as to realize the integration of the professional video monitoring system distributed in different places. As well as;it simplified the video network hierarchy and structure across different network configuration, and improves the utilization rate of video resources. Practical application shows, the platform has great advantage in reliability, flexibility, openness to the traditional video monitoring system.
Pattern matching is used in most of the network security devices in order to detect attacks, threats and malicious network traffic. Many hardware architectures have been designed to accelerate this time-critical operation in order to increase processing speed and achieve multi-gigabit throughput. Recently introduced automata processor is an powerful architecture which represents a new class of field programmable circuits called Field Programmable Pattern Matching Array (FPPMA). In the paper, we propose to improve the FPPMA architecture by Deterministic Units (DU), which have been originally introduced in NFA Split and significantly reduce the amount of required resources for mapping NFA to hardware. Dual position automaton is used to create data structure for the new FPPMA architecture from set of regular expressions. Moreover, we investigate efficiency of DU based on a dual position automation. Since the DU can implement a finite automaton without structural restrictions, we propose to use unrestricted finite automaton for deterministic unit and the dual position automaton for parts mapped to basic FPPMA elements. The results show that the DU based on the unrestricted finite automaton utilizes up to 42.43% less State-Transition elements than the DU based on the dual position automaton. Utilization of DUs provides significant reduction of FPPMA resources. For example, State-transition elements were reduced by more than 71% for the Snort module spyware-put.
Security and privacy in distributed systems from various attacks and un-authorized users can be implemented from various security techniques. These techniques enable protection of secure data over distributed system especially in web security applications including online transactions. Techniques which provides privacy and security for various online transaction using one factor or two factor or three factor authentication works efficiently and provides high security but here a new and efficient techniques is implemented which is based on two factor authentication using E-Smart cards and Image based authentication is implemented. The proposed technique implemented provides security from various attacks as well as minimizes the communication overhead in distributed systems in E-commerce.
The convenience of torus automorphism is widely used in network security, data encryption, image watermarking and pattern recognition. This paper presents an implementation and synthesis of FPGA based multimedia image encryption technique by 2D torus automorphism. The proposed computational algorithm of 2D torus automorphism is simple as only three addition and two multiplication logic operations is used for generating the positions of next incident matrices. Therefore this simple system of equation of torus automorphism can be implemented on field programmable gate array (FPGA). The designs were realized in Verilog HDL and synthesized on Vertex-5 FPGA. The encrypted implementations were completely parameterized with respect to the input image dimension and the recurrence number which is dependent on the value of torus coefficient. All exhaustive obtained results regarding the logic block uses, memory requirements and timing report associated with the proposed architecture are investigated. The results consummate the veracity of 2D torus automorphism. This applied work can be useful for real time image encryption applications.
With the development of the Internet, email has become a part of our life to communicate with each other. While bringing convenience for users, it also threats the users' network security. For example, if the user clicks on a malicious URL link in the email, the user's host is likely to be injected by virus, resulting in subsequent network security problems. We call the Internet users' operations on email "users' email behaviors". Through simulating the user's behavior on email, we can find the hidden dangers and threats of network security caused by user's operation on email. In this paper, BP-BDI model is proposed to simulate the user's behavior on email. The behavior learning module with BP neural network is constructed on the basis of BDI (Belief Desire Intention) model to learn the user's past behavior of email. As a result, the accuracy of simulating the user's email behavior will be improved. Finally, the experiment proves that the BP-BDI model can simulate the user's email behavior accurately and effectively.
Energy efficiency of storage system is an important research topic as it largely affects the energy consumption of data centers. Although the technology of deduplication can save resources in storage systems, it cannot always achieve good energy efficiency. In this paper, we propose an energy consumption model for storage systems. In addition, an energy-aware backup strategy has been designed for backup tasks to achieve the balance of the process in terms of energy consumption and deduplication efficiency. To validate the model and the strategy, we introduce an energy-aware disk simulation framework and implement it by extending Disk Sim that is widely used for storage research.
Changes due to increasing use of equipment with communication capability in electricity distribution systems, development of microgrids, government-imposed electricity-market open access competitions, etc., have let electricity utilities in a greater reliance on communication networks for smart-grid operations that include monitoring, protection, control, and time-of-use metering. This paper presents a new smart-grid network access control strategy and a new operation-based access model in order to increase the grid-access security and grid-operation efficiency. The new access model extends the network access control from a traditional single security domain to multiple domains specifically designed for interconnected microgrids. A security policy to simplify power-grid network security administrations is proposed, the authorization is independently defined and separated from policy representations as well as implementation mechanisms, and digital credential is introduced to establish trust and role assignments for users in different microgrid domains. The proposed smart-grid operation-based network access control has significant advantages over the standard role-based access control for application on smart-grid operations. This paper presents case studies for illustrating this new smart-grid operation-based network access controls.
Cyber-attack defense requires network security situation awareness through distributed collaborative monitoring, detection, and mitigation. An issue of developing and demonstrating innovative and effective situational awareness techniques for avionics has increased in importance in the last decade. In this paper, we first conducted a game theoretical based modeling and analysis to study the interaction between an adversary and a defender. We then introduced the implementation of game-theoretic analysis on an Avionics Sensor-based Defense System (ASDS), which consists of distributed passive and active network sensors. A trade-off between defense and attack strategy was studied via existing tools for game theory (Gambit). To further enhance the defense and mitigate attacks, we designed and implemented a multi-functional web display to integrate the game theocratic analysis. Our simulation validates that the game theoretical modeling and analysis can help the Avionics Sensor-based Defense System (ASDS) adapt detection and response strategies to efficiently and dynamically deal with various cyber threats.
This article will explain and will act as a guide to implement a cloud based honeypot using the Kippo honeypot application suite. It will also underline the importance of having an honeypot and will illustrate statistical and real data collected during the implemented system used for this article.
Network security management is becoming more and more complicated in recent years, considering the need of deploying more and more network security devices/middle-boxes at various locations inside the already complicated networks. A grand challenge in this situation is that current management is inflexible and the security resource utilization is not efficient. The flexible deployment and utilization of proper security devices at reasonable places at needed time with low management cost is extremely difficult. In this paper, we present a new concept of network security virtualization, which virtualizes security resources/functions to network administrators/users, and thus maximally utilizing existing security devices/middle-boxes. In addition, it enables security protection to desirable networks with minimal management cost. To verify this concept, we further design and implement a prototype system, NETSECVISOR, which can utilize existing pre-installed (fixed-location) security devices and leverage software-defined networking technology to virtualize network security functions. At its core, NETSECVISOR contains: 1) a simple script language to register security services and policies;2) a set of routing algorithms to determine optimal routing paths for different security policies based on different needs;and 3) a set of security response functions/strategies to handle security incidents. We deploy NETSECVISOR in both virtual test networks and a commercial switch environment to evaluate its performance and feasibility. The evaluation results show that our prototype only adds a very small overhead while providing desired network security virtualization to network users/administrators.
CR is a promising technology for next-generation wireless networks in order to efficiently utilize the limited spectrum resources and satisfy the rapidly increasing demand for wireless applications and services. Security is a very important but not well addressed issue in CR networks. In this article we focus on security problems arising from PUE attacks in CR networks. We present a comprehensive introduction to PUE attacks, from the attacking rationale and its impact on CR networks, to detection and defense approaches. In order to secure CR networks against PUE attacks, a two-level database-assisted detection approach is proposed to detect such attacks. Energy detection and location verification are combined for fast and reliable detection. An admission control based defense approach is proposed to mitigate the performance degradation of a CR network under a PUE attack. Illustrative results are presented to demonstrate the effectiveness of the proposed detection and defense approaches.
Long-term evolution-advanced (LTE-A) networks exploit low-power relay nodes, picocells and femtocells to boost throughput, enhance coverage, decrease latency, and reduce cost. End users in a relay-based LTE-A network can recruit relay nodes to cooperate as virtual antenna arrays, thereby reaping the benefits offered by multiple input single output (MIMO) techniques. Although the relay-based cooperative MIMO (coop MIMO) implementation in LTE-A networks improves performance, security issues are often overlooked. This paper introduces a physical layer security scheme for point-to-point networks, and extends this scheme to MIMO networks. Two practical relay-based coop MIMO architectures and corresponding secret key generation (SKG) schemes are presented. For both the MIMO and coop MIMO networks, the impact of proposed power allocation on SKG rate (SKGR) is quantified via the theoretical and numerical analysis. Results indicate that the proposed power allocation scheme can offer 15%-30% increase in SKGR relative to MIMO/coop MIMO networks with equal power allocation at low-power region, thereby improving network security.
There exists a way that attackers can identify software defined networks (SDNs). Knowing the vulnerabilities of a SDN, the attackers can mount a saturation attack on the SDN controller with the aim of incapacitating the entire SDN. Therefore, the controller should have an architecture to weather out such an attack while continuing operation. A scheduling-based architecture is proposed for the SDN controller that leads to effective attack confinement and network protection during denial of service (DoS) attacks.
UASNs are widely used in many applications, and many studies have been conducted. However, most current research projects have not taken network security into consideration, despite the fact that a UASN is typically vulnerable to malicious attacks due to the unique characteristics of an underwater acoustic communication channel (e.g., low communication bandwidth, long propagation delays, and high bit error rates). In addition, the significant differences between UASNs and terrestrial wireless sensor networks entail the urgent and rapid development of secure communication mechanisms for underwater sensor nodes. For the above mentioned reasons, this article aims to present a somewhat comprehensive survey of the emerging topics arising from secure communications in UASNs, which naturally lead to a great number of open research issues outlined afterward.
Cyber security is moving from traditional infrastructure to sophisticated mobile infrastructureless threats. A major concern is that such imminent transition is happening at a rate far exceeding the evolution of security solutions. In fact, the transformation of mobile devices into highly capable computing platforms makes the possibility of security attacks originating from within the mobile network a reality. Today, mobile devices are capable of initiating sophisticated cyberattacks, especially when they coordinate together to form what we call a MobiBot. MobiBots differ from classical botnets in that they exploit mobile operating system vulnerabilities and the advantages of device-to-device communication to mask malicious code propagation.
The mobile crowdsourcing network (MCN) is a promising network architecture that applies the principles of crowdsourcing to perform tasks with human involvement and powerful mobile devices. However, it also raises some critical security and privacy issues that impede the application of MCNs. In this article, in order to better understand these critical security and privacy challenges, we first propose a general architecture for a mobile crowdsourcing network comprising both crowdsourcing sensing and crowdsourcing computing. After that, we set forth several critical security and privacy challenges that essentially capture the characteristics of MCNs. We also formulate some research problems leading to possible research directions. We expect this work will bring more attention to further investigation on security and privacy solutions for mobile crowdsourcing networks.
Space information networks were proposed to broaden the observation area and realize continuous information acquisition using satellites and high altitude platform stations. Space information networks are able to enhance detection and transmission capabilities compared to the current single Earth observation satellite. Although lots of technical work has been done concerning the space network architecture and protocols, the security issues have not been investigated well. In this article, we focus on the security problems in space information networks from four perspectives, that is, secure handoff, secure transmission control, key management, and secure routing. Existing works, together with their challenges and open problems, are discussed, and our proposed scheme is introduced. Overall, this article aims to help readers understand the motivation, problem formulation, and solutions regarding security issues on space information networks.
Along with the rapid development of social networks, social network worms have constituted one of the major internet security problems. The root of worm is the inevitable software vulnerability during the design and implementation process of software. So it is hard to completely avoid worms in the existing software engineering systems. Due to lots of bandwidth consumption, the patch cannot be transmitted simultaneously by the network administrator to all hosts. This paper studies how to prevent the propagation of social network worms through the immunization of key nodes. Unlike existing containment models for worm propagation, a novel immunization strategy is proposed based on network vertex influence. The strategy selects the critical vertices in the whole network. Then the immunization is applied on the selected vertices to achieve the maximal effect of worm containment with minimal cost. Different algorithms are implemented to select vertices. Simulation experiments are presented to analyze and evaluate the performance of different algorithms.
Discusses the ramifications from a privacy and network security point of view given today's systems that offer access to huge amounts of data and database systems. These information and data sharing interactions capture varied aspects of our lives, such as our shopping behavior, our mental and physical health, our likes and dislikes, political and religious opinions, products we shop for, how we drive our cars, how much energy we use, and even more nebulous things such as our hopes and aspirations. Credit card companies record every purchase we make, with the time and place of purchase;companies like Amazon and Netflix not only record our purchases but also our interest in things we merely peruse on their websites. Loyalty cards and digital profiles of our purchases even in brick-and mortar stores such as Target and Walmart and our local supermarkets create a record of all our purchases, easily being able to infer our shopping patterns.
Modern network security devices employ packet classification and pattern matching algorithms to inspect packets. Due to the complexity and heterogeneity of different search data structures, it is difficult for existing algorithms to leverage modern hardware platforms to achieve high performance. This paper presents a Structural Compression (SC) method that optimizes the data structures of both algorithms. It reviews both algorithms under the model of search space decomposition, and homogenizes their search data structures. This approach not only guarantees deterministic lookup speed but also optimizes the data structure for efficient implementation on many-core platforms. The performance evaluation reveals that the homogeneous data structure achieves 10Gbps line-rate 64byte packet classification throughput and multi-Gbps deep inspection speed.
This paper presents a novel framework for network forensics in cloud computing (CC). The framework investigates malicious activities performed by an intruder while affecting virtual machine on same or another cloud resource (CR). Moreover, it investigate malicious activities of intruders by determining its source while keeps privacy for cloud users with out losing their data confidentiality. Our proposed framework provides initial foundations to create real network forensics model for CC in a right essence.
Accompany with the growth of the speed in network, people's living standard has greatly improved, but the threaten to its information's security has also appeared, as a important branch of information security, steganography is a useful method to protect the secret information safety. In view of the high complexity of current video steganographic algorithms, and combining with the trailing coefficient produced in the process of quantization of H.264 encoding standard, we put forward a kind of algorithm based on trailing coefficients. The algorithm firstly conducts DCT transform on the frame, and then obtains the trailing coefficient for each quantized DCT blocks, last embedded by changing its values. The experimental result indicates that, this algorithm has little influence on video quality and has a few changes to the DCT coefficients after embedding, and has large capacity of steganography, and has high robustness. Above all, the information security has been protected.
Most cyber security research is focused on detecting network intrusions or anomalies through the use of automated methods, exploratory visual analytics systems, or real-time monitoring using dynamic visual representations. However, there has been minimal investigation of effective decision support systems for cyber analysts. This paper describes the user-centered design and development of a decision support visualization for active network defense. Ocelot helps the cyber analyst assess threats to a network and quarantine affected computers from the healthy parts of a network. The described web-based, functional visualization prototype integrates and visualizes multiple data sources through the use of a hybrid space partitioning tree and node link diagram. We describe our design process for requirements gathering and design feedback which included expert interviews, iterative design, and a user study.
Situational awareness is a key concept in cyber-defence. Its goal is to make the user aware of different and complex aspects of the network he or she is monitoring. This paper proposes PERCIVAL, a novel visual analytics environment that contributes to situational awareness by allowing the user to understand the network security status and to monitor security events that are happening on the system. The proposed visualization allows for comparing the proactive security analysis with the actual attack progress, providing insights on the effectiveness of the mitigation actions the system has triggered against the attack and giving an overview of the possible attack's evolution. Moreover, the same visualization can be fruitfully used in the proactive analysis since it allows for getting details on computed attack paths and evaluating the mitigation actions that have been proactively computed by the system. A preliminary user study provided a positive feedback on the prototype implementation of the system. A video of the system is available at: https://youtu.be/uMpYCJCX95k.
Network security analysis and ensemble data visualization are two active research areas. Although they are treated as separate domains, they share many common challenges and characteristics. Both focus on scalability, time-dependent data analytics, and exploration of patterns and unusual behaviors in large datasets. These overlaps provide an opportunity to apply ensemble visualization research to improve network security analysis. To study this goal, we propose methods to interpret network security alerts and flow traffic as ensemble members. We can then apply ensemble visualization techniques in a network analysis environment to produce a network ensemble visualization system. Including ensemble representations provide new, in-depth insights into relationships between alerts and flow traffic. Analysts can cluster traffic with similar behavior and identify traffic with unusual patterns, something that is difficult to achieve with high-level overviews of large network datasets. Furthermore, our ensemble approach facilitates analysis of relationships between alerts and flow traffic, improves scalability, maintains accessibility and configurability, and is designed to fit our analysts' working environment, mental models, and problem solving strategies.
Honeypots and honeynets are unconventional security tools to study techniques, methods, tools, and goals of attackers. Analysis of data collected by these security tools is important for network security. In this paper, authors focus on time-oriented data as well as they outline visualization of this kind of data in honeypots and honeynets. Authors also provide results from honeynet based on special visualization - heatmaps.
The following topics are dealt with: computer science and technology;control technologies;communication technologies;intelligent transportation system;large-scale network security;future communication networks;smart grid;sensor networks;network computing;and data management.
Faced with the growing demand of highway message propagation, network security, as well as the great challenges of global energy crisis, mobile interconnection and energy conservation are considered to be the key factors to promote the development of automotive electronics innovation in Intelligent Transportation System (ITS). Existing network planning is mainly performance-oriented and often neglects the radiation concerns, which have direct impacts not only on the environment but also on the network operators' long-term profitability. In this paper, we investigate a novel optimization framework for multiple Roadside unit (RSU) deployment in eco-sustainable communication networks for intelligent transportation (IT). The term "eco-sustainable" refers to low carbon, low exposure to radiation, low energy consumption and energy cost. We propose a generic integer linear programming (ILP) model which optimally and simultaneously: (i) minimizes the total cost, (ii) identifies the locations of RSUs, (iii) takes radiation constraints into consideration, (iv) meets the minimum information rate, (v) satisfies the coverage requirements so as to allow the users' accesses through the heterogeneous RSUs. We solve the model using Gurobi, which is the newest ILP solver by now. In addition, a new algorithm is proposed. Computational results show that compared with Gurobi, although there is a gap between the results, a shorter running time stands out in the new algorithm. Besides, we use AIMMS to get an interface of the final result intuitively, which makes improvements in visualization. We can also find when low exposure to radiation is concerned under the "Eco-Sustainable" network planning strategy, more dispersed RSUs to be distributed along with greater changes in the resulting network configuration, and thus the corresponding RSUs energy reduction, carbon emissions reduction as well as operational cost savings can be achieved.
Spam messages are used to disseminate malware, make phishing attacks, and advertise illegal products. Spam generates costs to users, e.g., victims of phishing, and to network administrators, e.g., who provision and pay for the traffic. Recent proposals aim to identify and filter spam messages at the origin, restraining message propagation and reducing wasted bandwidth on the route from the spammer to the destination. In this work we analyze spam traffic costs for network operators. We measure the routes traversed by real spam messages colected at five honeypots, and estimate spam traffic costs according to the business relationships between networks traversed on each route. We show that stub networks are systematically encumbered by high spam traffic costs but can cooperate to filter up to 70% of spam messages at the origin. Our results also indicate that transit networks that send a lot of spam may employ traffic engineering to reduce their transit costs.
The wireless sensor networks are always deployed in hostile and pervasive environment. They are prone to security threats and they do have a wide range of applications like military, environmental monitoring, health care, etc... traditional network security methods are not up to the mark due to limited resources. Several key management schemes have been proposed security in HSN. In this paper, we propose a key distribution scheme based on random key pre-distribution for heterogeneous sensor networks to achieve better security and performance compared to homogeneous networks, which is suffer from high communication overhead,computation overhead and high storage requirements. A combination of symmetric and asymmetric keys were tried (hybrid), where the cluster head and BS use public key encryption based on ECC, while using symmetric key encryption between the adjacent nodes in the cluster.
In our digital world internet is a widespread channel for transmission of information. Information that is transmitted can be in form of messages, images, audios and videos. Due to this escalating use of digital data exchange cryptography and network security has now become very important in modern digital communication network. Cryptography is a method of storing and transmitting data in a particular form so that only those for whom it is intended can read and process it. The term cryptography is most often associated with scrambling plaintext into ciphertext. This process is called as encryption. Today in industrial processes images are very frequently used, so it has become essential for us to protect the confidential image data from unauthorized access. In this paper Advanced Encryption Standard (AES) which is a symmetric algorithm is used for encryption and decryption of image. Performance of Advanced Encryption Standard algorithm is further enhanced by adding a key stream generator W7. NIOS II soft core processor is used for implementation of encryption and decryption algorithm. A system is designed with the help of SOPC (System on programmable chip) builder tool which is available in QUARTUS II (Version 10.1) environment using NIOS II soft core processor. Developed single core system is implemented using Altera DE2 FPGA board (Cyclone II EP2C35F672). Using MATLAB the image is read and then by using DWT (Discrete Wavelet Transform) the image is compressed. The image obtained after compression is now given as input to proposed AES encryption algorithm. The output of encryption algorithm is given as input to decryption algorithm in order to get back the original image. The implementation of which is done on the developed single core platform using NIOS II processor. Finally the output is analyzed in MATLAB by plotting histogram of original and encrypted image.
In today's digital world, data security is most of the common issue in network security domain. The National Institute of Standards and Technology (NIST) proposes a publication of encryption standards by considering Federal Information Processing Standards (FIPS) publications. The presented paper describes AES (Advanced Encryption Standard)-128 algorithm in optimized manner. This paper produces a design of AES-128 system which produces area optimized design by reducing number of slices per area in CLB(Configurable Logic Blocks).This paper produces a design which uses Block RAM (BRAM) as stored memory for S-BOX(Substitution Box).This reduces the array storing capacity of CLBs.
Several languages have been proposed for the task of describing networks of systems, either to help on managing, simulate or deploy testbeds for testing purposes. However, there is no one specifically designed to describe the honeynets, covering the specific characteristics in terms of applications and tools included in the honeypot systems that make the honeynet. In this paper, the requirements of honeynet description are studied and a survey of existing description languages is presented, concluding that a CIM (Common Information Model) match the basic requirements. Thus, a CIM like technology independent honeynet description language (TIHDL) is proposed. The language is defined being independent of the platform where the honeynet will be deployed later, and it can be translated, either using model-driven techniques or other translation mechanisms, into the description languages of honeynet deployment platforms and tools. This approach gives flexibility to allow the use of a combination of heterogeneous deployment platforms. Besides, a flexible virtual honeynet generation tool (HoneyGen) based on the approach and description language proposed and capable of deploying honeynets over VNX (Virtual Networks over LinuX) and Honeyd platforms is presented for validation purposes.
Throughout the last couple of years network forensics has gained higher importance due to the ever-growing quantity and quality of attacks. In contrast to conventional network forensics which relies on a central approach, both legal as well as technical guidelines nowadays favor a decentralized approach since aspects like privacy, limited data manipulation possibilities and scalability are addressed superiorly there. In this regard, however, present (decentralized) solutions are all in the need of an improvement especially in the area of protection against manipulation, i.e., falsification of relevant forensics data particularly in case of sophisticated attacks. Following the idea of strategic pre-incident preparation, this publication presents a decentralized approach, which, in advance, selectively collects data based on the suspiciousness of the connection to facilitate a (possible) investigation. To this end, we present an agent-based framework including prototype and evaluation that particularly uses Geolocation to fulfill this task.
The evolution of digital communication era includes both devices and applications running on different platforms. Specifics applications are needed for a safeguard communication ensuring protection and trust services to the users. The growing need to address privacy concerns when social network data is released for mining purposes has recently led to considerable interest in various network security-based applications. This paper presents a privacy preservation security protocol-based that enabling mutual authentication procedure between involved entities. This scheme provides specific security standards such as data integrity, message confidentiality and session key establishment to ensure security and privacy of both data and network. An extensive analysis shows that the proposed scheme achieves efficiency and can be safeguard to real wireless communication applications. Finally, a performance evaluation and comparison shows that our proposed framework is lightweight and resilient to various kinds of attacks.
Network security protocol design is important aspectof network security research. DoS/DDoS is very seriousattack in wired and wireless network. DoS/DDoS attack depletes memory/cpu of service provider, so legitimate user can't gain normal service. According to anti-DoS attack strategy of network security protocols, we give and discuss three mechanisms (stateless connection, Fail-together and Subset Sum Client-Puzzle) on design of a key exchange protocol against denial of service attack for ISO/IEC1170-3 key exchange protocol. Subset SumClient-Puzzle has simple structure, Non-Parallelizable speciality and fast verification. N Subset Sum Client-Puzzles' difficulties are sum of n Subset Sum Client-Puzzle's difficulty. Based on analysis of new key exchange protocol, we compare initiator and responder for computation resource, memory depletion and anti-DoS/DDoS. ISO/IEC1170-3 key exchange protocol on Subset Sum Client Puzzle, which is non-parallelizable, easy construction and verification, has the good property against DoS/DDoS attack. It provides a very good reference for network security protocol design with anti-DoS/DDoS attack.
In the past decades, the Internet usage has increased dramatically. For the network security, the network packet filtering is an important strategy to identify malicious network packets. However, malicious attacks spread much faster than network administrators can respond. The software-only implementations of filter are unlikely to meet the performance goals. Therefore, we develop a novel GPGPU-based parallel packet classification approach by adopting bloom filter to inspect the packet payload by leveraging the computation power of GPGPU. The experiment results present that the proposed algorithm can be significantly enhanced the performance of filtering packets. According to the experimental results, the proposed method can achieve over 5.4 times speed up over the sequential bloom filter on single CPU.
As an important content of chip security, Hardware Trojan detection has attracted a lot of attention. The existing Hardware Trojan detection methods based on side-channel power consumption are effective. The main existing Hardware Trojan detection methods based on side-channel power consumption analysis mainly concentrate on linear features but ignore the features of Hardware Trojan in nonlinear space. This paper proposes a nonlinear detection method based on side-channel power consumption information, it extracts and magnifies features of Hardware Trojan in nonlinear space and achieves Hardware Trojan detection. The results of experiment show that the nonlinear analysis method proposed in this paper can detect the Hardware Trojan effectively in nonlinear space.
Network management plays an important role in improving the utilization ratio of network unit, positioning and troubleshooting fault, optimizing the network performance, ensuring network security and service quality. SDN (Software Defined Networking) has brought new chances and challenges for the development of network management technology. Thus, this paper has designed a network performance management system based on SDN. SNMP4J class library is used to develop network management system of the underlying application in this system. The data forwarding and controlling is separated through the SDN technology, which realizes functions: the collection and analysis of data, alarm and automatic control. Some test cases indicated that this system has many advantages, such as intelligence, high reliability and security, and so on.
Because the information security risk assessment have the problem of less training data and slow convergence rate, we put forward a information security risk assessment model based on support vector machine (SVM) using artificial fish swarm algorithm (AFSA). In this paper, we used weekly security report of the government network security situation from China National Internet Emergency Center(CNCERT) as the source data [1]. We adopted the RBF function as the kernel function SVM, then optimized the penalty coefficient C and kernel function parameter 8 of artificial fish swarm algorithm. At the end of this paper, we established the optimal evaluation model for simulation. Our results showed that the information security risk assessment model based on AFSA SVM has higher accuracy and faster convergence rate than the one of cross-validation.
Software vulnerabilities are one of the root causes of network security issues. Software security testing is an essential part of secure software development. Fuzzing has been proven to be an effective dynamic software security testing method. In this paper we present a guided fuzzing approach based on dynamic taint analysis for security testing of network protocol software. This approach identifies the security sensitive functions of target application affected by network packets via dynamic taint analysis, and generates testcases by mutating these packets with the elements of a fuzz library. Due to the guidance of taint information, it to some extend overcomes the blindness of traditional fuzzing methods and improves efficiency. The approach integrates several successive steps, we currently focus on the taint analysis step and have received interesting preliminary experimental results.
The identification of Web servers has great significance for assessing security of Web system, researching market share and forecasting the scope of influence on a network security incident. This paper introduces the principle of the Web server detection technology based on HTTP, and analyzes how to identify Web servers from aspects of the Banner information, HTTP response characteristics and special HTTP requests. Finally, we summarize their respective advantages and disadvantages.
In a restructured power system, one of the key objectives of transmission network expansion is to provide nondiscriminatory access to cheaper generation for all consumers. However, the robustness of the transmission network against all uncertainties such as credible contingencies should equally be maintained. Thus the development of effective methodologies and strategies to satisfy this objective is a major challenge faced by the transmission expansion planners in the new environment. In this paper a static transmission expansion model with energy storage is proposed. The energy storage is coordinated with transmission expansion planning in a system with base load generators and expensive peaking power plants. The energy storage is modelled to store the cheaper generation from the base load power plants at off peak periods and discharges to meet the demand at peak periods. Four distinct transmission expansion models with and without energy storage system (ESS) and N-1 network security constraint are developed for the comparative analysis. The whole problem is formulated as a mixed integer linear programming (MILP) problem with the objective of minimizing the operational cost of the generators as well as the transmission line and storage investment costs over several demand levels. The proposed methodology is demonstrated on the IEEE 24 bus reliability test system (RTS). The overall results from the model showed that energy storage benefits the system by deferring investment in transmission lines. The model also confirmed that the marginal benefit of storage diminishes as the investment in ESS increases. In addition, the economic viability of corrective planning over preventive planning of transmission network is also justified.
The paper proposes a new methodology for optimal planning of medium voltage distribution networks. The overall model is presented as a two-stage optimization model, where the first stage deals with the investment part and the second stage focuses on the operation part. The novelty of the proposed model is demonstrated through the explicit incorporation of network security constraints in the mathematical model, in line with the UK planning standards, while taking into consideration different operating regimes with changing load and generation profiles, integration of new distributed generation units, construction of circuits on new corridors and optimal network reconfiguration. The highly complex mixed-integer nonlinear optimization model is applied to a few practical medium-voltage test systems of up to 120 nodes using a dedicated commercial software package. Some of the characteristics results are examined and highlighted in this paper. Additionally, sensitivity analyses are done to determine critical parameters affecting the development and performance of medium voltage distribution networks.
In recent yeas a lot of web applications have been released in the world. At the same time, cyber attacks against web application vulnerabilities have also increased. In such a situation, it is necessary to make web applications more secure. However checking all web vulnerabilities by hand is very difficult and time-consuming. Therefore, we need a web application vulnerability scanner. In this work, we evaluate two open source vulnerability scanners OWASP Zed Attack Proxy (OWASP ZAP) and Skipfish using vulnerable web applications Damn Vulnerable Web Application (DVWA) and The Web Application Vulnerability Scanner Evaluation Project (WAVSEP).
The paper proposes a technique and a software tool for generation of attack scenarios - random sequences of attack patterns and appropriate sequences of security events. The technique suggested is based on the application of open standards for representation of attack patterns and vulnerabilities. The tool was developed in scope of the integrated system of network security analysis, risk assessment and countermeasure generation. It is intended to test effectiveness of this system by simulation of the input data - random attacks against computer networks.
Today network security is major problem faced not only by small companies but big giants like Samsung and Google. Traditional architecture of internet is vulnerable to DDoS attacks and it provides an opportunity to an attacker to get access to a great amount of compromised hosts by using their vulnerabilities to create attack networks or Botnets. Although a number of defense techniques and countermeasures against DDoS attacks are developed, the number of such attacks is still increasing. The main reason for that is that scientists and law enforcement agencies are still not able to answer a important question: where is the real source of DDoS attack? In this paper, we tend to survey different types of traceback techniques and methodologies.
Nowadays there is an increasing demand for network security, because of the information processing, communications and revolutions. Cryptography offers high security for communication and networking. Elliptic curve scalar multiplication is the most important operation in elliptic curve cryptosystems. This paper focus on secure elliptic curve scalar multiplication development using Karatsuba multiplier and also generate secret key pair for encryption and decryption in elliptic curve cryptography. The implementation of the elliptic curve point multiplication is achieved by using a dedicated Galois Field arithmetic simulated on ModelSim.
Network based intrusions and information theft events are becoming more and more popular today. To bypass the network security devices such as firewall, intrusion detection/prevention system (IDS/IPS) and web application firewall, attackers use evasive techniques to circumvent them, of which protocol mimicry is a very useful approach. The technique camouflages malicious communications as common protocols or generally innocent applications to avoid network security audit, which has been widely used in advanced Trojans, botnets, as well as anonymous communication systems, bringing a great challenge to current network management and security. To this end, we propose a general network protocol mimicry behavior discovery framework named MimicHunter to detect such evasive masquerade behaviors, which exploits protocol structure and state transition verifications, as well as primary protocol behavior elements. Experiment results on several datasets demonstrate the effectiveness of our method in practice. Besides, MimicHunter is flexible in deployment and can be easily implemented in passive detection systems with only a little cost compared with the active methods.
Most of the existing trust evaluation methods of wireless sensor networks focus on communication behaviors in building paths without considering transmission behaviors in sensing environment. Therefore, a potential threat tothe network security exists in wireless sensor networks. Wepropose a dynamic trust evaluation method based on multi-factor including direct trust factor and indirect trust factor in this paper. Firstly, direct trust factor is composed of communication trust factor and transmission trust factor, both of which can be got from observing the interaction process. Secondly, indirect trust factorcomes from other nodes' recommendations, which are classified into certainand uncertain ones. Such two kinds of recommendation trust values are computed respectively. Finally, nodes' trustworthinessis measured by combining direct trust with indirect trust dynamically. Besides, both the involved classification standard and dynamic weight assignment are dependent on the interaction times between nodes, which are put forward under the background of Hoeffding's Inequality in Probability Theory. Experiments on NS-2 platform show that the proposed method is effective in increasing the network throughput and the network packet delivery ratio.
Fingerprinting techniques aim to identify objects such as devices, data, users, or even attacks, based on distinctive characteristics. In the context of computer network security, these techniques have been proposed and successfully applied in different application scenarios and with different goals in mind, e.g., to detect attacks or even to perform attacks. However, the related works in this field do not follow a consistent definition and notation of fingerprinting. Hence, central concepts such as uniqueness are mistakenly confused. In this paper, we tackle this issue by proposing a novel formalization approach of fingerprinting techniques. Our formal model is based on clear mathematical definitions and centered around the concept of a fingerprinting feature as a distinctive characteristic. We apply our formal model in two different application scenarios of remarkable research interest to illustrate its validity, flexibility and universality.
The sensor network coverage optimization process is vulnerable to be attacked or invaded. Therefore, in the case of wireless sensor network under attacks but also able to ensure secure communications and efficient and reliable coverage is a major problem. In this paper, we through the combination of trust management model and heuristic optimization Particle Swarm Optimization and Cuckoo Search, proposed a sensor network security coverage method based on trust management of intrusion tolerance. This method evaluate the trust value of the nodes through their behavior at first, and then adjust the perception radius and decision-making radius. Finally, combine PSO and CS serial optimization in order to achieve the intrusion tolerance for efficient adaptive coverage. By comparing the simulation with a range WSN covering mechanism, this method has certain advantages over the performance of the algorithm, and in the case of the invasion can effectively protect the safety of the overlay network. The simulation results show the effectiveness of the algorithm.
The internet is riddled with numerous malware and other threats. This puts the limited resources of network security devices, such as firewalls and intrusion detection systems, under growing stress. They have to cope with increasing network traffic and manage numerous detection rules for threatening traffic. Creating covering set of detection rules manually is a slow and tedious process. In this paper, we present a method to automatically create detection rules for an intrusion detection system from interaction signatures of known malware. Our method maintains information integrity and reports potential issues during the derivation process. The method was tested with HTTP traffic generated from known malware signatures using Snort as the IDS rule-engine.
In today's scenario, cyber security is one of the major concerns in network security and malware pose a serious threat to cyber security. The foremost step to guard the cyber system is to have an in-depth knowledge of the existing malware, various types of malware, methods of detecting and bypassing the adverse effects of malware. In this work, machine learning approach to the fore-going static and dynamic analysis techniques is investigated and reported to discuss the most recent trends in cyber security. The study captures a wide variety of samples from various online sources. The peculiar details about the malware such as file details, signatures, and hosts involved, affected files, registry keys, mutexes, section details, imports, strings and results from different antivirus have been deeply analyzed to conclude origin and functionality of malware. This approach contributes to vital cyber situation awareness by combining different malware discovery techniques, for example, static examination, to alter the session of malware triage for cyber defense and decreases the count of false alarms. Current trends in warfare have been determined.
Covert channels threaten conventional network security paradigms by exploiting existing system resources never intended to facilitate communication. By doing so, they can evade detection by conventional network security mechanisms such as firewalls. In order to improve network security, it is necessary to detect and disrupt covert communications. Due to the sheer number and variety of covert channel algorithms, it becomes impossible to deal with them on a case-by-case basis. A complete applicable covert channel detector necessitates the use of a common modeling framework. However a generic model is still lacking and the published models apply to only a few covert channel algorithms. To remedy this problem we present an event-based framework that models the covert communication process as a set of discrete events separated by a finite duration of time. This framework will allow behavioral analysis of the covert communications process in a generic way, which can be used to develop generalized detection mechanisms. Using this model, we derive the error performance of covert channels in different conditions of network delay jitter and packet losses. We then calculate the error performance of these algorithms by implementing them over a test-bed real network traffic and MATLAB simulations and compare the results to verify our model.
Spear phishing is a widespread concern in the modern network security landscape, but there are few metrics that measure the extent to which reconnaissance is performed on phishing targets. Spear phishing emails closely match the expectations of the recipient, based on details of their experiences and interests, making them a popular propagation vector for harmful malware. In this work we use Natural Language Processing techniques to investigate a specific real-world phishing campaign and quantify attributes that indicate a targeted spear phishing attack. Our phishing campaign data sample comprises 596 emails - all containing a web bug and a Curriculum Vitae (CV) PDF attachment - sent to our institution by a foreign IP space. The campaign was found to exclusively target specific demographics within our institution. Performing a semantic similarity analysis between the senders' CV attachments and the recipients' LinkedIn profiles, we conclude with high statistical certainty (p < 10???4) that the attachments contain targeted rather than randomly selected material. Latent Semantic Analysis further demonstrates that individuals who were a primary focus of the campaign received CVs that are highly topically clustered. These findings differentiate this campaign from one that leverages random spam.
The emerge of sophisticated attackers and malware that target Industrial Control System (ICS) suggests that novel security mechanisms are required. Honeypots, can act as an additional line of defense, by providing early warnings for such attacks. We present a mobile ICS honeypot, that can be placed in various network positions to provide security administrators an on-the-go security status of their network. We discuss our system, its merits in comparison to other honeypots, and provide preliminary results towards a large-scale evaluation.
With countless smartphones and mobile devices being widely used, users can have community interaction, recreation, and can even make a purchase through their mobile devices. Therefore, the mobile network service is becoming increasing irreplaceable and essential to modern people's daily life. That makes the network security of mobile Apps become relatively important. And it is every App designer's biggest challenge to guarantee the security of their Apps in mobile gadgets. Moreover, cross platform mobile app (CPMA) combines the features of Web applications and "native" mobile Apps. On the one hand, like Web applications, CPMA is implemented in portable, platform-independent languages such as HTML and JavaScript. On the other hand, like native Apps, CPMA has direct access to local device resources -- file system, location, camera, contacts, etc. Owing to the above advantages, CPMA has gaining more and more popularity. To sum up, how to develop a secured cross platform mobile Apps to prevent users' personal information from being invaded becomes the most interesting and critical issue. In this paper, we present a security CPMA to provide IT network services developers with related idea and model case study.
Investigating the existing approaches of tackling various network security challenges can never be totally exhausted because a perfect and completely secured system is yet unborn! Hence by so doing, their weaknesses would be highlighted with a view to introducing other methods which offer better and more efficient benefits. Membrane computing (MC) is a versatile, non-deterministic and maximally parallel computing device which was inspired by the functioning of living biological cells. The aim of this article therefore is to take a deeper look at concepts within Network security systems (NSS) and especially that of membrane system. Furthermore, based on the inherent benefits of MC and areas where it has been applied successfully, we (i) project a link between it and NSS and (ii) propose MC as an alternative veritable tool towards solving some of the problems of NSS such as packet dropping in Intrusion Detection System (IDS) which usually emanates from systems' inability to handle large traffic data. This was experimented using a model built on recognizer Tissue P system on GPU and the initial results obtained show that our method yields better detection speedup.
In this paper, the real-time wireless transmission system of a medical image by using the RTP / RTSP protocol adaptive streaming transfer is developed. To prepare for network bandwidth change we were utilizing a QoS for real-time services and also to strengthen network security, Secure RTP was used.
This paper proposes a framework for security services using Software-Defined Networking (SDN) and Interface to Network Security Functions (I2NSF). It specifies requirements for such a framework for security services based on network virtualization. It describes two representative security systems, such as (i) centralized firewall system and (ii) DDoS-attack mitigation system. For each service, this paper discusses the limitations of existing systems and presents a possible SDN-based system to protect network resources by controlling suspicious and dangerous network traffic.
As the number, complexity and diversity of cyber threats continue to increase in network infrastructures, anomaly detection techniques constitute a crucial alternative towards enhancing network security. Principal Component Analysis (PCA) is a widely used network anomaly detection statistical methodology. Despite its ability in detecting traffic anomalies, relevant research has highlighted certain drawbacks of this technique. In our work we develop the Iterative PCA (IPCA) method to address those shortcomings. We aim at providing a useful tool that will enable a network administrator to identify network anomalies. The results of our experimentation are encouraging. They indicate that IPCA possesses promising capabilities in efficiently detecting anomalies while mitigating the limitations of the classical PCA approach.
Enhanced solution of topical scientific and practical problem associated with the development of flow-based model of multipath routing with paths overlapping by nodes in Telecommunication Network (TCN) was presented. This model is further extension of the well known multipath routing model based on the introduction to the structure of non-linear constraints responsible for the calculation of routes that intersect just at nodes. It is possible to obtain the order of multipath routing by overlapping by nodes paths in the solution of nonlinear programming problem with given objective function, linear constraints and nonlinear terms. Search for a compromise on the providing fault-tolerance and network security, on the one hand, and quality of service, on the other, led to the fact that in some important cases the requirements for overlapping paths can be slightly reduced and it is allowable to use paths overlapping only by network nodes. Simulation results confirmed the efficiency of the proposed model. Among the advantages of the proposed model of multipath routing with overlapping by nodes paths, one can distinguish the fact that with the same parameters of security and fault-tolerance can be achieved higher performance and quality of service parameters in TCN in a whole.
In this paper, we investigate the secondary network physical layer security under the outage constraint of the primary user (PU) receiver and interference from the PU transmitter to the secondary network. In particular, a secondary user (SU) transmitter sends confidential messages to trusted multiple SU receivers (SU-Rxs) in the presence of multiple eavesdroppers (EAVs). Further, we exploit multiuser diversity where the SU-Rx with best channel condition is scheduled for transmission. Then, analytical expressions of the probability of existence of non-zero secrecy capacity and secrecy outage probability are obtained. Analytical and simulation results are provided to evaluate the effect of the number of SU-Rxs and number of EAVs on the secondary system. Interestingly, the numerical results show that the interference from the primary network to the secondary network is an important parameter to improve the secondary network security.
Detecting network anomalies means a lot to network security. In respect of detection precision and range, network-wide anomaly detection approaches on the basis of traffic flows have distinctive advantages over the methods of the traditional host computer, single link and single path. However, these approaches face actual problems of performance reduction or being unavailable when noise interference or data loss take place. In order to solve these problems, anomaly detection algorithm based on robust multivariate probabilistic calibration model is proposed. This algorithm establishes a normal traffic model of traffic matrix based on the latent variable probability model of multivariate t-distribution, and implements network anomaly detection by judging if the sample's Mahalanobis distance exceeds the threshold. Both theoretical analysis and experimental results demonstrate its robustness and wider use. The algorithm is applicable when dealing with both data integrity and loss. It also has a stronger resistance over noise interference and lower sensitivity to the change of parameters, all of which indicate its performance stability.
DDoS attack has been a threat to network security since a decade and it will continue to be so in the near future also. Now a days application layer DDoS attack poses a major challenge to webservers. The main objective of web server is to offer an uninterrupted application layer services to its benign users. But, the application layer ddos attack blocks the services of the web server to its legitimate clients which can cause immense financial losses. Moreover, it requires very less amount of resources to perform the application layer ddos attack. The solutions available to detect application layer ddos attack, detect only limited number of application layer ddos attacks. The solutions that detect all types of application layer ddos attacks have huge complexity. To find an effective solution for the detection of application layer ddos attack the normal user browsing behavior has to be modeled in such a way that normal user and attacker can be differentiated. In this paper, we propose a method using feature construction and logistic regression to model normal web user browsing behavior to detect application layer ddos attacks. The performance of the proposed method was evaluated in terms of the metrics such as total accuracy, false positive rate, and detection rate. Comparison of the proposed solution with the existing methods reveals that the proposed method performs better than the existing methods.
Rapid growth and popularity of internet has re-emphasized the significance of the intrusion detection system in network security. To overcome the vulnerabilities of network security researchers have come up with different frameworks of intrusion detection system using data mining. Feature selection is a significant method to develop a time and cost effective intrusion detection system. The time consumption in building up the classifiers model enhances the efficiency of the system. This work conducted on the analysis of some approaches of intrusion detection using some machine learning methods with wrapper approaches, which is a type of feature selection methodology. Our paper mainly focuses on the classification preciseness of 3 different classifiers using the minimal amount of features selected by three different wrapper search methods on the well-known public type NSL-KDD dataset and showing the comparisons among them. The 3 basic classifiers are Bayesian Network, Naive Bayes and J48. Best First, Genetic Search and Rank Search have been used as the wrapper search methods. The study proposed an ensemble type of a classification model with a hybrid feature selection method based on the research framework. By using the hybrid feature selection method 12 critical features are chosen and with the combination of basic classifiers, a reliable model is developed to differentiate normal and anomaly. Moreover, the result shows a convenient false positive rate of 0.021. Experiment showed that our proposed ensemble approach showed better result than Naive Bayes, Bayesian Network and J48 classifier. Experiments have been conducted on the NSL-KDD dataset using WEKA 3.6 library functions.
Honeynet research has become more important as a way to overcome the limitations imposed by the use of individual honeypots. A honeynet can be defined as a network of honeypots following certain topology. Although there are at present many existing honeynet solutions, no taxonomies have been proposed in order to classify them. In this paper, we propose such taxonomy, identifying the main criteria used for its classification and applying the classification scheme to some of the existing honeynet solutions, in order to quickly get a clear outline of the honeynet architecture and gain insight of the honeynet technology. The analysis of the classification scheme of the taxonomy allows getting an overview of the advantages and disadvantages of each criterion value. We later use this analysis to explore the design space of honeynet solutions for the proposal of a future optimized honeynet solution.
Fault-tolerance plays an important role in improving the reliability of multiple earth-observing satellites, especially in emergent scenarios such as obtaining photographs on battlefields or earthquake areas. Fault tolerance can be implemented through scheduling approaches. Unfortunately, little attention has been paid to fault-tolerant scheduling on satellites. To address this issue, we propose a novel dynamic fault-tolerant scheduling model for real-time tasks running on multiple observation satellites. In this model, the primary-backup policy is employed to tolerate one satellite's permanent failure at one time instant. In the light of the fault-tolerant model, we develop a novel fault-tolerant satellite scheduling algorithm named FTSS. To improve the resource utilization, we apply the overlapping technology that includes primary-backup copy overlapping (i.e., PB overlapping) and backup-backup copy overlapping (i.e., BB overlapping). According to the satellites characterized with time windows for observations, we extensively analyze the overlapping mechanism on satellites. We integrate the overlapping mechanism with FTSS, which employs the task merging strategies including primary-backup copy merging (i.e., PB merging), backup-backup copy merging (i.e., BB merging) and primary-primary copy merging (i.e., PP merging). These merging strategies are used to decrease the number of tasks required to be executed, thereby enhancing system schedulability. To demonstrate the superiority of our FTSS, we conduct extensive experiments using the real-world satellite parameters supplied from the satellite tool kit or STK;we compare FTSS with the three baseline algorithms, namely, NMFTSS, NOFTSS, and NMNOFTSS. The experimental results indicate that FTSS efficiently improves the scheduling quality of others and is suitable for fault-tolerant satellite scheduling.
Wireless networks are particularly vulnerable to spoofing and route poisoning attacks due to the contested transmission medium. Recent works investigate physical layer features such as received signal strength or radio frequency fingerprints to localize and identify malicious devices. In this paper we demonstrate a novel and complementary approach to exploiting physical layer differences among wireless devices that is more energy efficient and invariant with respect to the environment. Specifically, we exploit subtle design differences among transceiver hardware types. Transceivers fulfill the physical-layer aspects of wireless networking protocols, yet specific hardware implementations vary among manufacturers and device types. In this paper we demonstrate that precise manipulation of the physical layer header prevents a subset of transceiver types from receiving the manipulated packet. By soliciting acknowledgments from wireless devices using a small number of packets with manipulated preambles and frame lengths, a response pattern identifies the true transceiver class of the device under test. Herein we demonstrate a transceiver taxonomy of six classes with greater than 99 percent accuracy, irrespective of environment. We successfully demonstrate wireless multi-factor authentication, intrusion detection, and transceiver type fingerprinting through preamble manipulation.
The Internet of Things (IoT) introduces a vision of a future Internet where users, computing systems, and everyday objects possessing sensing and actuating capabilities cooperate with unprecedented convenience and economical benefits. As with the current Internet architecture, IP-based communication protocols will play a key role in enabling the ubiquitous connectivity of devices in the context of IoT applications. Such communication technologies are being developed in line with the constraints of the sensing platforms likely to be employed by IoT applications, forming a communications stack able to provide the required power-efficiency, reliability, and Internet connectivity. As security will be a fundamental enabling factor of most IoT applications, mechanisms must also be designed to protect communications enabled by such technologies. This survey analyzes existing protocols and mechanisms to secure communications in the IoT, as well as open research issues. We analyze how existing approaches ensure fundamental security requirements and protect communications on the IoT, together with the open challenges and strategies for future research work in the area. This is, as far as our knowledge goes, the first survey with such goals.
Security of Cyber-Physical Systems (CPS) has gained increasing attention in recent years. Most existing works mainly investigate the system performance given some attacking patterns. In this technical note, we investigate how an attacker should schedule its Denial-of-Service (DoS) attacks to degrade the system performance. Specifically, we consider the scenario where a sensor sends its data to a remote estimator through a wireless channel, while an energy-constrained attacker decides whether to jam the channel at each sampling time. We construct optimal attack schedules to maximize the expected average estimation error at the remote estimator. We also provide the optimal attack schedules when a special intrusion detection system (IDS) at the estimator is given. We further discuss the optimal attack schedules when the sensor has energy constraint. Numerical examples are presented to demonstrate the effectiveness of the proposed optimal attack schedules.
Software Defined Networking (SDN) is rapidly emerging as a new paradigm for managing and controlling the operation of networks ranging from the data center to the core, enterprise, and home. The logical centralization of network intelligence presents exciting challenges and opportunities to enhance security in such networks, including new ways to prevent, detect, and react to threats, as well as innovative security services and applications that are built upon SDN capabilities. In this paper, we undertake a comprehensive survey of recent works that apply SDN to security, and identify promising future directions that can be addressed by such research.
Multipath TCP (MPTCP) is an experimental TCP extension designed to add functionality to TCP while remaining backwards-compatible with most networks and devices. MPTCP changes TCP's behavior from how it's commonly understood in ways that go beyond the security of MPTCP itself, with ancillary implications challenging how network security is practiced and implemented. Here, the authors investigate the implications for network security -- both in the transitional state, where MPTCP is partially supported, and in a future where every device supports MPTCP. They find that while MPTCP isn't widely supported, increasing support will stimulate changes to common network security rationales and paradigms. In particular, when a connection's identifiers become abstracted from network addresses, or when traffic is fragmented across paths, many current security approaches aren't prepared to recognize this kind of traffic, let alone act appropriately.
The effectiveness of data loss prevention (DLP) solutions remains obscure, especially with respect to the types of attacks they claim to protect against. A systematic analysis of HTTP data leakage vectors and a test of three content-based DLP solutions give insight into these solutions' vulnerabilities.
Many recent malware implementations employ virtual machines to carry out their malicious activities. These are hard to detect because their states can't be accessed by antivirus software running in the native OS. An approach for OS fingerprinting using TCP SYN packets in an enterprise environment can detect the presence of unauthorized OSs.
Network reconnaissance of addresses and ports is prerequisite to a vast majority of cyber attacks. Meanwhile, the static address configuration of networks and hosts simplifies adversarial reconnaissance for target discovery. Although the randomization of host addresses has been suggested as a proactive disruption mechanism against such reconnaissance, the proposed approaches do not exploit the full potentials of address randomization in provision of unpredictability and attack adaptability. Moreover, these approaches do not provide thorough analysis on effectiveness and limitations of address randomization against relevant threat models, including stealthy scanning and worms. In this paper, we present an effective address randomization technique, called random host address mutation (RHM), that turns end-hosts into untraceable moving targets. This technique achieves maximum efficacy by allowing address randomization to be highly unpredictable and fast, and adaptive to adversarial behavior, while incurring low operational and reconfiguration overhead. Our approach achieves the following objectives: (1) it achieves high uncertainty in adversary scanning by modeling address mutation randomization as a multi-level satisfiability problem;(2) it adapts the mutation scheme by fast characterization of adversarial reconnaissance patterns;(3) it achieves high mutation rate by separating mutation from end-hosts and managing it via network appliances;and (4) it preserves network integrity, manageability and performance by bounding the size of routing tables, preserving end-to-end reachability, and efficient handling of reconfiguration updates. Our extensive analyses and simulation show that the RHM distorts adversarial reconnaissance, slows down (deters) the attack, and increases its detectability. Consequently, the RHM is effective in countering a significant number of sophisticated threat models, including reconnaissance, stealthy/evasive scanning methods, and targeted att- cks. We also address limitations of our approach in terms of effectiveness and applicability.
Edge networks connected to the Internet need effective monitoring techniques to inform routing decisions and detect violations of Service Level Agreements (SLAs). However, existing measurement tools, like ping, traceroute, and trajectory sampling, are vulnerable to attacks that can make a path look better than it really is. Here, we design and analyze a lightweight path-quality monitoring protocol that reliably raises an alarm when the packet-loss rate exceed a threshold, even when an adversary tries to bias monitoring results by selectively delaying, dropping, modifying, injecting, or preferentially treating packets. Our protocol is based on sublinear algorithms for sketching the second moment of stream of items and can monitor billions of packets using only 250-600 B of storage and the periodic transmission of a comparably sized IP packet. We also show how this protocol can be used to construct a more sophisticated protocol that allows the sender to localize the link responsible for the dropped packets. We prove that our protocols satisfy a precise definition of security, analyze their performance using numerical experiments, and derive analytic expressions for the tradeoff between statistical accuracy and system overhead. This paper contains a deeper treatment of results from earlier conference papers and several new results.
Mastering user's behavior character is important for efficient network management and security monitoring. In this paper, we develop a novel framework named as multilevel user cluster mining (MUCM) to measure user's behavior similarity under different network prefix levels. Focusing on aggregated traffic behavior under different network prefixes cannot only reduce the number of traffic flows but also reveal detailed patterns for a group of users sharing similar behaviors. First, we employ the bidirectional flow and bipartite graphs to model network traffic characteristics in large-scale networks. Four traffic features are then extracted to characterize the user's behavior profiles. Second, an efficient method with adjustable weight factors is employed to calculate the user's behavior similarity, and entropy gain is applied to select the weight factor adaptively. Using the behavior similarity metrics, a simple clustering algorithm based on -means is employed to perform user clustering based on behavior profiles. Finally, we examine the applications of behavior clustering in profiling network traffic patterns and detecting anomalous behaviors. The efficiency of our methods is verified with extensive experiments using actual traffic traces collected from the northwest region center of China Education and Research Network (CERNET), and the cluster results can be used for flow control and traffic security monitoring.
Networks with homogeneous routing nodes are constantly at risk as any vulnerability found against a node could be used to compromise all nodes. Introducing diversity among nodes can be used to address this problem. With few variants, the choice of assignment of variants to nodes is critical to the overall network resiliency. We present the Diversity Assignment Problem (DAP), the assignment of variants to nodes in a network, and we show how to compute the optimal solution in medium-size networks. We also present a greedy approximation to DAP that scales well to large networks. Our solution shows that a high level of overall network resiliency can be obtained even from variants that are weak on their own. We provide a variation of our problem that matches the specific communication requirements of applications run over the network (e.g., Paxos and BFT). Also, we analyze the loss in resiliency when optimally assigning variants based on inaccurate information about compromises.
With the main focus of research in routing protocols for Mobile Ad-Hoc Networks (MANET) geared towards routing efficiency, the resulting protocols tend to be vulnerable to various attacks. Over the years, emphasis has also been placed on improving the security of these networks. Different solutions have been proposed for different types of attacks, however, these solutions often compromise routing efficiency or network overload. One major DOS attack against the Optimized Link State Routing protocol (OLSR) known as the node isolation attack occurs when topological knowledge of the network is exploited by an attacker who is able to isolate the victim from the rest of the network and subsequently deny communication services to the victim. In this paper, we suggest a novel solution to defend the OLSR protocol from node isolation attack by employing the same tactics used by the attack itself. Through extensive experimentation, we demonstrate that 1) the proposed protection prevents more than 95 percent of attacks, and 2) the overhead required drastically decreases as the network size increases until it is non-discernable. Last, we suggest that this type of solution can be extended to other similar DOS attacks on OLSR.
Device classification is important for many applications such as industrial quality controls, through-wall imaging, and network security. A novel approach to detection is proposed using a random noise radar (RNR), coupled with Radio Frequency Distinct Native Attribute (RF-DNA) fingerprinting processing algorithms to non-destructively interrogate microwave devices. RF-DNA has previously demonstrated serial number discrimination of passive Radio Frequency (RF) emissions such as Orthogonal Frequency Division Multiplexed (OFDM) signals, Worldwide Interoperability for Microwave Access (WiMAX) signals and others with classification accuracies above 80% using a Multiple Discriminant Analysis/Maximum Likelihood (MDAML) classifier. This approach proposes to couple the classification successes of the RF-DNA fingerprint processing with a non-destructive active interrogation waveform. An Ultra Wideband (UWB) noise waveform is uniquely suitable as an active interrogation method since it will not cause damage to sensitive microwave components and multiple RNRs can operate simultaneously in close proximity, allowing for significant parallelization of detection systems.
Defining an optimal protection strategy against viruses, spam propagation, or any other kind of contamination process is an important feature for designing new networks and architectures. In this paper, we consider decentralized optimal protection strategies when a virus is propagating over a network through an SIS epidemic process. We assume that each node in the network can fully protect itself from infection at a constant cost, or the node can use recovery software, once it is infected. We model our system using a game-theoretic framework and find pure, mixed equilibria, and the Price of Anarchy in several network topologies. Further, we propose a decentralized algorithm and an iterative procedure to compute a pure equilibrium in the general case of a multiple communities network. Finally, we evaluate the algorithms and give numerical illustrations of all our results.
We propose and analyze adaptive network defense management for countering smart attack and selective capture which aim to cripple the basic data delivery functionality of a base station based wireless sensor network. With selective capture, the adversaries strategically capture sensors and turn them into inside attackers. With smart attack, an inside attacker is capable of performing random, opportunistic, and insidious attacks to evade detection and maximize their chance of success. We develop a model-based analysis methodology with simulation validation to identify the best defense protocol settings under which the sensor network lifetime is maximized against selective capture and smart attack.
Communication privacy has been a growing concern, particularly with the Internet becoming a major hub of our daily interactions. Revelations of government tracking and corporate profiling have resulted in increasing interest in anonymous communication mechanisms. Several systems have been developed with the aim of preserving communication privacy via unlinkability within a public network environment such as Tor and I2P. As the anonymity networks cannot guarantee perfect anonymity, it is important for users to understand the risks they might face when utilizing such technologies. In this paper, we discuss potential attacks on the anonymity networks that can compromise user identities and communication links. We also summarize protection mechanisms against such attacks. Many attacks against anonymity networks are well studied, and most of the modern systems have built-in mechanisms to prevent these attacks. Additionally, some of the attacks require considerable resources to be effective and hence are very unlikely to succeed against modern anonymity networks.
Threats of distributed denial of service (DDoS) attacks have been increasing day-by-day due to rapid development of computer networks and associated infrastructure, and millions of software applications, large and small, addressing all varieties of tasks. Botnets pose a major threat to network security as they are widely used for many Internet crimes such as DDoS attacks, identity theft, email spamming, and click fraud. Botnet based DDoS attacks are catastrophic to the victim network as they can exhaust both network bandwidth and resources of the victim machine. This survey presents a comprehensive overview of DDoS attacks, their causes, types with a taxonomy, and technical details of various attack launching tools. A detailed discussion of several botnet architectures, tools developed using botnet architectures, and pros and cons analysis are also included. Furthermore, a list of important issues and research challenges is also reported.
The U.S. National Academies of Science's Board on Science, Technology and Economic Policy estimates that the Internet and voice-over-IP (VoIP) communications infrastructure generates 10% of U.S. economic growth. As market forces move increasingly towards Internet and VoIP communications, there is proportional increase in telephony denial of service (TDoS) attacks. Like denial of service (DoS) attacks, TDoS attacks seek to disrupt business and commerce by directing a flood of anomalous traffic towards key communication servers. In this work, we focus on a new class of anomalous traffic that exhibits a mimicry TDoS attack. Such an attack can be launched by crafting malformed messages with small changes from normal ones. We show that such malicious messages easily bypass intrusion detection systems (IDS) and degrade the goodput of the server drastically by forcing it to parse the message looking for the needed token. Our approach is not to parse at all;instead, we use multiple classifier systems (MCS) to exploit the strength of multiple learners to predict the true class of a message with high probability (98.50%  p  99.12%). We proceed systematically by first formulating an optimization problem of picking the minimum number of classifiers such that their combination yields the optimal classification performance. Next, we analytically bound the maximum performance of such a system and empirically demonstrate that it is possible to attain close to the maximum theoretical performance across varied datasets. Finally, guided by our analysis we construct an MCS appliance that demonstrates superior classification accuracy with O(1) runtime complexity across varied datasets.
On-chip security is an emerging challenge in the design of embedded systems with intellectual property (IP) cores. Traditionally this challenge is addressed using ad hoc design techniques with separate design objectives of secure design for testability (DfT), and IP core protection. However, in this paper, we will argue that such design approaches can incur high costs. Underpinning this argument, we propose a novel design methodology, called Secure TEst and IP core Protection (STEP), which aims to address the joint objective of IP core protection and secure testing. To ensure that this objective is achieved at a low cost, the STEP design methodology employs common key integrated hardware. This hardware is incorporated in the system through an automated design conversion technique, which can be easily merged into the electronic design automation (EDA) tool chain. We evaluate the effectiveness of our proposed design methodology considering various implementations of advanced encryption standard (AES) systems as case studies. We show that our proposed design methodology benefits from design automation with high security, and protection at the cost of low area, and power consumption overheads, when compared with traditional design methodologies.
Software defined networking (SDN) decouples the network control and data planes. The network intelligence and state are logically centralized and the underlying network infrastructure is abstracted from applications. SDN enhances network security by means of global visibility of the network state where a conflict can be easily resolved from the logically centralized control plane. Hence, the SDN architecture empowers networks to actively monitor traffic and diagnose threats to facilitates network forensics, security policy alteration, and security service insertion. The separation of the control and data planes, however, opens security challenges, such as man-in-the middle attacks, denial of service (DoS) attacks, and saturation attacks. In this paper, we analyze security threats to application, control, and data planes of SDN. The security platforms that secure each of the planes are described followed by various security approaches for network-wide security in SDN. SDN security is analyzed according to security dimensions of the ITU-T recommendation, as well as, by the costs of security solutions. In a nutshell, this paper highlights the present and future security challenges in SDN and future directions for secure SDN.
Efficiently processing massive data is a big issue in high-speed network intrusion detection, as network traffic has become increasingly large and complex. In this work, instead of constructing a large number of features from massive network traffic, the authors aim to select the most important features and use them to detect intrusions in a fast and effective manner. The authors first employed several techniques, that is, information gain (IG), wrapper with Bayesian networks (BN) and Decision trees (C4.5), to select important subsets of features for network intrusion detection based on KDD'99 data. The authors then validate the feature selection schemes in a real network test bed to detect distributed denial-of-service attacks. The feature selection schemes are extensively evaluated based on the two data sets. The empirical results demonstrate that with only the most important 10 features selected from all the original 41 features, the attack detection accuracy almost remains the same or even becomes better based on both BN and C4.5 classifiers. Constructing fewer features can also improve the efficiency of network intrusion detection.
This study presents an analytical model for rule-based network security middleboxes as those of network firewalls, intrusion detection systems and email spam filters. In these systems, incoming packets carrying requests arrive at the middlebox and obtain queued for processing in multiple stages. The stages consist of first a main stage for packet processing and then subsequent stages of rulebase interrogation in which rules or conditions are checked sequentially until a match is triggered. The service at these stages is characterised to be mutually exclusive;that is, only one stage is active at any time. The authors derive useful formulas that can predict the middlebox performance, taking into account its incoming request rate, the queue size and the processing capacity of the middlebox, and thereby proper engineering capacity of the middlebox can be achieved.
The various responsibilities in intelligent systems design--responsibilities to acquire certain knowledge and skills--suggest strongly that effective interaction design is inherently interdisciplinary. As a consequence, gaps in knowledge acquisition and creativity on the part of design teams are bound to emerge as long as responsibilities are abrogated because they are tied to roles (such as cognitive systems engineers versus systems engineers). Communication, collaboration, and knowledge sharing are essential components of success, and a language that cuts across discipline-specific jargon is likely to make the development effort much more effective. This essay describes similarities between two related approaches to interaction design that resonate with each other and share principles: ecological interface design and human-centered computing. These similarities are emphasized by noting similarities between two different interfaces for computer network defense that were independently developed from each approach.
Cyber attacks are an unfortunate part of society as an increasing amount of critical infrastructure is managed and controlled via the Internet. In order to protect legitimate users, it is critical for us to obtain an accurate and timely understanding of our cyber opponents. However, at the moment we lack effective tools to do this. In this article we summarize the work on modeling malicious activities from various perspectives, discuss the pros and cons of current models, and present promising directions for possible efforts in the near future.
The severity and number of attacks on computer networks are rapidly increasing. Most existing security solutions concentrate on resisting one or several attacks. We propose an integrated router security framework, called TrustR. It is able to defend against various types of attacks. TrustR integrates collaborating security primitives including cryptography based security mechanisms, trust management system, and trusted platform module. A simple but efficient method for detecting deceptive routing messages is also proposed. The deployment of TrustR is introduced. Simulation results show that TrustR is effective in resisting attacks and improving network performance.
Taking into account the security problems of 802.16 and according to the 802.16's specification, in this research work we have taken a novel approach to these issues by proposing and designing a 802.16-based intrusion detection system. Further on it has been investigated what is the performance of the offered system like. The performance and behavior of the proposed system has been tested and challenged using the NS2 network simulator so that the performance is analyzed by running a DoS attack via executing 50 FTP requests. Eventually the system's performance and operation can be observed in case of 802.16 downlink and uplink with IDS presence and without it.
If the warden Willie attempting to detect the transmission knows the statistics of his receiver noise, Alice can transmit no more thancovert bits reliably to Bob inchannel uses of an AWGN channel. However, if Willie does not know his noise statistics exactly, Alice can covertly transmitbits. In this letter, Willie lacks knowledge of his channel statistics but observeslength-codeword slots, where some number may be used by Alice to attempt covert transmission. We show that Willie can often limit Alice to the same performance scaling as when he knows his channel statistics.
The proliferation of the Internet of Things (IoT) in several application domains requires a well-defined infrastructure of systems that provides services for device abstraction and data management, and also supports the development of applications. Middleware for IoT has been recognized as the system that can provide this necessary infrastructure of services and has become increasingly important for IoT in recent years. The architecture of an IoT middleware is usually based on an SOA (service-oriented architecture) standard and has security requirement as one of its main challenges. The large amount of data that flows in this kind of system demands a security architecture that ensures the protection of the entire system. However, none of the existing SOAbased IoT middleware systems have defined a security standard that can be used as a reference architecture. In this sense, this article discusses the importance of defining a standard security architecture for SOA-based IoT middleware, analyzes concepts and existing work, and makes considerations about a set of security services that can be used to define a security architecture to mitigate the security threats in SOA-based IoT middleware systems.
In this study, the authors consider the detection and identification problems of distributed domain name system (DNS) cache poisoning attack. In the considered distributed attack, multiple cache servers are invaded simultaneously and the attack intensity for each cache server is slight. It is difficult to detect and identify the distributed attack by the existing local information-based detection methods, as the abnormal features for each cache server are indistinctive under distributed attack. To handle this problem, they propose an information fusion-based detection and identification methods. They find that the entropies of the query Internet protocol (IP) addresses for all cache servers are approximately stationary and statistically independent under normal cases. When distributed attack happens, they show the fact that the correlation of the entropies among all cache servers could increase dramatically. On the basis of this feature, they make use of principal component analysis to design the detection and identification methods. Specifically, attack is true when the maximum eigenvalue of the normalised entropies matrix exceeds a threshold, and the attacked servers are identified by the main loading vector. At last, they take a large-scale DNS in China and a simulation as two examples to show the effectiveness of their methods.
With rapidly growing Internet traffic, energy efficient operation of IP over WDM networks with sleep enabled routers is of increasing interest. However, for network security and to provide guaranteed communications, it would still be desirable to ensure that a certain fraction of the bandwidth is assured through routers which cannot be put to sleep using software control. This paper presents an energy-minimized IP over WDM network using a mixture of sleep-enabled and non-sleep-enabled router cards where a certain percentage of the network bandwidth is guaranteed to the offered traffic. Such a mixed configuration is also motivated by the fact that there will always be some traffic demand between each node pair at any time even though the traffic between node pairs may fluctuate to very low levels. This implies a need for some non-sleeping router cards at any time. Another motivation for this mixed configuration is because in the course of migration from today's networks with non-sleep-enabled cards to future networks with sleep-enabled cards, the non-sleep-enabled network devices will not be quickly abandoned but will be gradually replaced. This also causes a network situation with the mixed router card types. To design an IP over WDM network where both sleep-enabled and non-sleep-enabled router cards are used, we propose a mixed integer linear programming (MILP) model which jointly minimizes the energy consumption of all the router cards while guaranteeing a secured fractional bandwidth for all the node pairs. Modified MILPs with subsequent port-channel association are also proposed along with efficient heuristic algorithms which perform almost as well as the joint MILP approaches. The performance of these approaches is studied through simulations on a wide variety of networks.
The fact that the security facilities within a system are closely coupled and the security facilities between systems are unconnected results in an isolated protection structure for systems, and gives rise to a serious challenge to system security integrations and system controls. Also, the need for diversified services and flexible extensions of network security asks for more considerations and contributions from the perspective of software engineering in the process of designing and constructing security systems. Based on the essence of the virtualization technique and the idea of software-defined networks, we in this paper propose a novel software-defined security architecture for systems. By abstracting the traditional security facilities and techniques, the proposed security architecture provides a new, simple, effective, and programmable framework in which security operations and security controls can be decoupled, and thereby reduces the software module sizes, decreases the intensity of software developments, and improves the security extensibility of systems.
This paper presents a novel framework for network forensics in cloud computing (CC). The framework investigates malicious activities performed by an intruder while affecting virtual machine on same or another cloud resource (CR). Moreover, it investigate malicious activities of intruders by determining its source while keeps privacy for cloud users with out losing their data confidentiality. Our proposed framework provides initial foundations to create real network forensics model for CC in a right essence.
With the development of computer network technology, network security issues become increasingly more serious, firewall technology is one of the most effective methods to protect network security. Stream filter technology is a new firewall technology, which not only can supervise the network layer as packet filtering firewall, but also can supervise transport layer and application layer as the application proxy firewall. Hash table algorithm has been often used to manage the TCP stream table in firewall. However, the 4-tuple of TCP stream is not uniform distribution, it may lead to the worst case when searching in the hash table. When the worst case happens on the firewall, the quality of service may become very bad in the network. In order to control the worst case, we propose two-level hash tables algorithm based on bloom filter counter algorithm and multi-level hash tables algorithm. We search in the small hash table firstly, if it's fail, then search in the big hash table. The algorithm we proposed can not only decline the probability of worst case, but also reduce the number of memory access in worst case. And our theoretical research and simulation experiments proof that.
Accompany with the growth of the speed in network, people's living standard has greatly improved, but the threaten to its information's security has also appeared, as a important branch of information security, steganography is a useful method to protect the secret information safety. In view of the high complexity of current video steganographic algorithms, and combining with the trailing coefficient produced in the process of quantization of H.264 encoding standard, we put forward a kind of algorithm based on trailing coefficients. The algorithm firstly conducts DCT transform on the frame, and then obtains the trailing coefficient for each quantized DCT blocks, last embedded by changing its values. The experimental result indicates that, this algorithm has little influence on video quality and has a few changes to the DCT coefficients after embedding, and has large capacity of steganography, and has high robustness. Above all, the information security has been protected.
The following topics are dealt with: fault management;cloud management;network function virtualization;software defined network security;mobile network management and wireless network management.
Access to ground-truth data is limited in network security research, especially at large-scale. If data is available, sharing is typically not possible due to privacy concerns and contractual requirements. Hence, reproducibility of research and comparability of results is difficult. For a prevailing empirical domain of research, the resulting lack of transparency is a methodological problem which especially affects network security management in practice. To address this problem, in this paper we propose a research process that ensures reproducibility by embodying both, synthetic and real-world data. Our motivation for this is to combine best of both worlds: synthetic data is used to establish ground-truth and real-world data to assure validity of results. To the best of our knowledge, no such process has been formulated until today.
The rapid development of the embedded systems and the wide use of them in many sensitive fields require safeguarding their communications. Internet Protocol Security (IPsec) is widely used to solve network security problems by providing confidentiality and integrity for the communications in the network, but it introduces communication overhead. This overhead becomes a critical factor with embedded systems because of their low computing power and limited resources. In this research, we studied the overhead of using embedded IPsec in constrained resource systems, which run microkernel operating system (OS), in terms of the network latency and throughput. To conduct our experiment first, we ran the test with an unmodified network stack, and then we ran the same test with the modified network stack which contains the IPsec implementation. Later, we compared the results obtained from these two sets of experiments to examine the overhead. Our research demonstrated that the overhead imposed by IPsec protocols is small and well within the capabilities of even low cost microcontrollers such as the one used in the Raspberry Pi computer.
The following topics are dealt with: information security;information science;network security;image processing;software reliability;optimisation;medical computing;data mining;Internet of Things and mobile computing.
Among the current Wi-Fi two security models (Enterprise and Personal), while the Enterprise model (802.1X) offers an effective framework for authenticating and controlling the user traffic to a protected network, the Personal model (802.11) offers the cheapest and the easiest to setup solution. However, the drawback of the personal model implementation is that all access points and client radio NIC on the wireless LAN should use the same encryption key. A major underlying problem of the 802.11 standard is that the pre-shared keys are cumbersome to change. So if those keys are not updated frequently, unauthorized users with some resources and within a short timeframe can crack the key and breach the network security. The purpose of this paper is to propose and implement an effective method for the system administrator to manage the users connected to a router, update the keys and further distribute them for the trusted clients using the Freescale embedded system, Infrared and Bluetooth modules.
A Mobile Ad-hoc Network (MANET) has been studied in order to realize a ubiquitous society. However, MANET has many issues. One of the issues is security. For example, MANET is vulnerable to network sniffing because it is using a wireless communication that can be heard by others and existing security approaches are difficult to be applied to it. For the purpose, we have already proposed a novel secure communication method using secret sharing schemes over MANET. In this paper, we study how to enhance the security of the original proposal and propose an improvement by which restoring the secret by malicious user is very hard to be executed. We evaluate the improved method from the viewpoints of difficulty in restoration of key shares and show its effectiveness.
The research on network security concentrates mainly on securing the communication channels between two endpoints, which is insufficient if the authenticity of one of the endpoints cannot be determined with certainty. Previously presented methods that allow one endpoint, the authentication authority, to authenticate another remote machine. These methods are inadequate for modern machines that have multiple processors, introduce virtualization extensions, have a greater variety of side effects, and suffer from nondeterminism. This paper addresses the advances of modern machines with respect to the method presented by Kennell. The authors describe how a remote attestation procedure, involving a challenge, needs to be structured in order to provide correct attestation of a remote modern target system.
The firewall is a critical component of network security and is one of the most commonly used techniques to protect a network. Being based on a set of filtering rules, the accuracy and reliability of firewall protection heavily depend on the quality of the employed rule set. In this context, any mis configurations that arise between rules create ambiguity in classification of new traffic, not only affecting the performance of the firewall, but also putting the system in a vulnerable position. Manual management of this problem can be overwhelming and potentially inaccurate. Therefore, there is a need of automated methods to analyze, detect and fix mis configurations. Given these issues, algorithms and techniques have been proposed. Though these methods are useful for discovering and classifying anomalies, they still have limitations in term of the absence of the distinction between real mis configurations and intentional anomalies and in term of automatic correction of discovered mis configurations. In this paper, we present (1) a new classification of anomalies bringing out real mis configurations using a data structure (FDD) which facilitates mis configurations identification and resolution, (2) Optimal and totally automatic method to fix discovered mis configurations and (3) formal specification of proposed techniques using inference systems. The first results we obtained are very promising.
Anomaly detection is one of the crucial issues of network security. Many techniques have been developed for certain application domains, and recent studies show that machine learning technique contains several advantages to detect anomalies in network traffic. One of the issues applying this technique to real network is to understand how the learning algorithm contains more bias on new traffic than old traffic. In this paper, we investigate the dependency of the time period for learning on the performance of anomaly detection in Internet traffic. For this, we introduce a weighting technique that controls influence of recent and past traffic data in an anomaly detection system. Experimental results show that the weighting technique improves detection performance between 2.7-112% for several learning algorithms, such as multivariate normal distribution, knearest neighbor, and one-class support vector machine.
Peer-to-Peer (P2P) botnets have emerged as a significant threat against network security because of their distributed platform. The decentralized nature of these botnets makes their detection very challenging and the situation gets aggravated if an existing P2P network is exploited for botnet creation (parasite botnets). In this paper, we present a two-tier detection scheme to detect parasite P2P botnets. Our approach detects botnets in their waiting stage itself, without any requirement of seed information about bots and bots' signature. We have considered two basic behavior of botnets for detection: (i) long-living peers and (ii) search requests' intensity. The approach is able to detect bots from a monitored network with accuracy above 99% at the same time addressing several shortcomings of previous detection approaches.
Real-time network danger evaluation is the crucial technology of network security. To effectively evaluate the real-time network danger, a novel real-time network risk evaluation paradigm, inspired by immune (referred to as Reapai) is proposed. The intrusion detection sub-model is given firstly by using the definitions of non-self, self, and detector, which is composed by memory detectors, mature detectors, and immature detectors. Then, we propose the immune danger theory based on a sub-model of real-time network risk evaluation. For the proposed model, each class of network attack, including holistic risks of the host and network, can be calculated in real time and quantification ally. The theoretical analysis and the results of experiments show that the proposed Reapai provides a novel and effective approach for network risk evaluation.
Faced with high dimensional and large amount of data, network intrusion detection is always the focus of current research in the network security field. With the advantages of nonlinear, distributed storage and easily computing, Artificial Neural Networks (ANNs) are widely used in machine learning and pattern recognition fields. In this paper, we adopt a feature selection algorithm based on Fisher to select feature subsets, and three typical neural network algorithms for classification in order to improve the results of the intrusion detection. Experiments adopt KDD'99 as the data set, and use the accuracy, false positive rate and false negative rate, to evaluate the feasibility and effectiveness of the three neural networks. And as a result, the experiments show that the algorithms have acceptable performance in intrusion detection.
Intrusion detection plays an important role in solving network security problems. Artificial Neural Network (ANN) is one of the widely used intrusion detection techniques. However, many ANN-based methods are faced with unsatisfactory results and low detection precision. A new intrusion detection method by using Probabilistic Neural Network (PNN) is proposed. PNN divides inputs into two groups, normal and abnormal. Then different neurons are used to process these two different grouped inputs. Handling separately ensures that normal ones deviate from abnormal ones as far as possible, so as to obtain satisfactory detection results. PNN only needs one feed forward process and does not have any back propagation, thus greatly reducing the training time. Experimental results on KDDCUP99 dataset show that our PNN-based method yields average better performance than other well-knowns such as Decision tree, Naive Bayes and BPNN respecting precision, recall and F-value.
Anonymity networks like Tor harbor many underground markets and discussion forums dedicated to the trade of illegal goods and services. As they are gaining in popularity, the analysis of their content and users is becoming increasingly urgent for many different parties, ranging from law enforcement and security agencies to financial institutions. A major issue in cyber forensics is that anonymization techniques like Tor's onion routing have made it very difficult to trace the identities of suspects. In this paper we propose classification set-ups for two tasks related to user identification, namely alias classification and authorship attribution. We apply our techniques to data from a Tor discussion forum mainly dedicated to drug trafficking, and show that for both tasks we achieve high accuracy using a combination of character-level n-grams, stylometric features and timestamp features of the user posts.
SIP-based Voice Over IP(VoIP) network is becoming predominant in current and future communications. Distributed Denial of service attacks pose a serious threat to VOIP network security. SIP servers are victims of DDos attacks. The major aim of the DDos attacks is to avoid legitimate users to access resources of SIP servers. Distributed Denial of service attacks target the VOIP network by deploying bots at different locations by injecting malformed packets and even they halt the entire VOIP service causes degradation of QoS(Quality of Service). DDos attacks are easy to launch and quickly drain computational resources of VOIP network and nodes. Detecting DDos attacks is a challenging and extremely difficult due to its varying strategy and scope of attackers. Many DDos detection and prevention schemes are deployed in VOIP networks but they are not complete working in both realtime and offline modes. They are inefficient in detecting dynamic and low-rate DDos attacks and even fail when the attack is launched by simultaneously manipulating multiple SIP attributes. In this paper we propose a novel scheme based on Hellinger distance(HD) to detect low-rate and multi-attribute DDos attacks. Usually DDos detection and mitigations schemes are implemented in SIP proxy. But we leverage the SIP load balancer to fight against DDos by using existing load balancing features. We have implemented the proposed scheme by modifying leading open source kamailio SIP proxy server. We have evaluated our scheme by experimental test setup and found results are outperforming the existing DDos prevention schemes in terms of detection rate, system overhead and false-positive alarms.
In recent years, Botnets have become an important security problem on the Internet. Botnets have been used for many attacks, such as banking information theft, spam, distributed denial-of-service, identity theft and phishing. Then, they have been proposed as a major research topic in the network security. Although there are several surveys on the Botnets, they usually do not include a complete review on Botnet phenomenon. This paper is a survey on Botnets and provides a brief of classifications, techniques and algorithms of Botnet detection and defense. In this survey, we provide a comparison on existing research. We present an overview of studies on Botnet and discuss in detail including topologies, architectures, communication protocols, infection mechanisms, attacks, purposes, prevention techniques, detection sources and data, detection techniques and algorithms, and response techniques.
Network Services are confronted with a growing amount and diversity of attacks. The detection of such intrusion attempts however is getting more complex. This is mainly a result of more sophisticated attacks and a consequence of the more ubiquitous and overall more complex IT ecosystem. The resulting rapidly increasing network traffic makes it extremely hard to detect and prevent attacks in traditional ways. This paper proposes Security Information Management (SIM) enhancements considering Big Data Analysis principles. In the context of Cyber- Security, the blueprint and implementation presented can be adopted in organizations or Smart City contexts. After devising a blueprint for Big Data enhanced SIM based on the latest research, the system architecture and the resulting implementation are presented. The blueprint and implementation have been field- tested in a real world SIM large scale environment and evaluated with real network security logs. Our research is timely, since the application of Big Data principles to SIM environments has been rarely investigated so far, and there exists the need for a general concept of enhancement possibilities.
In the computer and network communication technology rapid development, the use of remote control system and its security technology also obtained the very big development. This article mainly analysis the structure of the computer network remote control system, analyze the current computer network environment, security problems faced by remote control. And emphasis on computer remote control system of the concrete application of network security technology.
Build a security of computer information network environment, the use of computer information processing technology is very important. In the current information security technology to improve environment, enhance the computer network users understanding of computer information security technology is very necessary. This article first elaborated under the network environment, the necessity of computer information security technology, then further analyze the actual process of information security technology to meet various problems, and puts forward practical wider related computer information security technology, to enhance the general computer user information security awareness to provide certain reference basis.
Traffic classification plays the significant role in the network security and management. However, accurate classification is challenging if the training data is contaminated with unclean traffic. Recent researches often assume clean training data, and hence performance reduced on real-time network traffic. To meet this challenge, in this paper, we propose a robust method, Unclean Traffic Classification (UTC), which incorporates noise elimination and suspected noise reweighting. Firstly, UTC eliminates strong noisy training data identified by a consensus filtering with multiple classifiers. Furthermore, UTC estimates the relevance of remaining training data and learns a robust traffic classifier. Through a number of experiments on a real-world traffic dataset, we show that the new method outperforms existing state-of-the-art traffic classification methods, under the extremely difficult circumstance with unclean training data.
A new methodology for optimal development of distribution networks with low carbon technologies is proposed in this paper. The complete planning model is developed as a two-stage optimization process, where the first stage deals with the investment problem and the second stage focuses on the operational aspects. The novelty of the proposed investment model is explicit incorporation of network security constraints in the optimization formulation in line with the UK planning standards, while taking into consideration different operating regimes with changing load and generation profiles. Connection of new distributed generation units and new types of demands, construction of circuits on new corridors and optimal network reconfiguration, as well as real-life restoration rules are also included in the model. The investment problem is defined as the complex mixed-integer nonlinear optimization model and solved using a dedicated commercial software package. It is applied to a few practical medium-voltage test systems. Additionally, sensitivity analyses are done to determine critical parameters affecting the development and performance of medium voltage distribution networks.
Network protocol classification plays an important role in modern network security and fine-grained management architectures. The state-of-the-art network protocol classification methods aim to take the advantages of flow statistical features and machine learning techniques. However the classification performance is severely affected by limited supervised information and unknown network protocols. In this paper, a novel semi-supervised learning method is proposed to solve the problem of unknown protocols in the crucial situation of a small labeled training sample set. The proposed method possesses the superior capability of detecting unknown samples generated by unknown protocols with the help of flow correlation information and semi-supervised clustering ensemble learning to boost the classification performance. A theoretical analysis is provided to confirm the effectiveness of the proposed method. Moreover, the comprehensive performance evaluation conducted on real-world network protocols datasets shows that the proposed method is significantly better than the existing methods in the critical network environment.
In this paper, a neural-network-based control scheme is developed for the tracking control problem of a class of disturbed nonlinear switched strict-feedback systems with input delay. First, the auxiliary signals are obtained by ingeniously constructing a filter and a virtual observer. Then the backstepping technique and neural networks are employed to construct a common Lyapunov function (CLF) and a state feedback controller for all subsystems. It is proved all signals of the closedloop system are semi-globally uniformly ultimately bounded (SGUUB), and that the tracking error ultimately converges to an adequately small compact set.
With network getting progressed, it is very crucial for us to guard the information that we have. One of these methods is the honeypot which is also a very powerful component for security analysts to collect malicious data for a long time. We need to let attackers intrude into a honeypot, so that we can analyze the malicious data we get, and find a method to prevent related attacks. Because it is important to prevent attackers to attack another computer through a honeypot, almost all of the honeypots block outgoing traffic. This may create a serious problem. Some assailants would test whether the computer which they attack is a honeypot by creating some simple external connections. If they know the computer they are attacking is a honeypot, they will not do further malicious behavior. If a honeypot cannot collect attack patterns anymore, it becomes useless. In this paper, we introduce a new design of honeypot, DEH (Dynamic Extensible Two-way Honeypot), to fix this serious problem with a bilateral communication mechanism. DEH based on the bilateral communication allows not only incoming traffic but outgoing traffic. If the outgoing traffic includes malicious shellcode, we can hold this traffic and copy the shellcode, and then DEH replace it with our own code to set up the bilateral communication and protective mechanism of the computer that the attacker wants to intrude into. After we set up the mechanism, we let the attacker intrude into a victim, but he is monitored by our protective mechanism. When attacker wants to send traffic out of the victim, DEH can extend the protective mechanism to other computers or redirected the connections back to the honeypot. Therefore, the mechanism can efficiently not only protect the honeypot from being detected but also prevent the attack from being spread, in the same time we could also get more information from attackers.
The number of devices in an operation room (OR) and the complexity of the components and the overall system increases continuously. Today's vendor-dependent integrated ORs are expensive and not able to handle this complexity because they can only form isolated solutions. Thus a device communication for medical devices among each other and to medical information systems has to be based on open and vendor-independent standards. In this paper we will present new standards for networked Point-of-Care medical devices that will be part of the IEEE 11073 family of standards. A service-oriented device communication is defined by means of an architecture definition, a transport specification called Medical Devices Profile for Web Services (MDPWS), and a Domain Information & Service Model. The new system will make the complexity of a comprehensive OR integration manageable and thereby improve patient's safety. The focus of this paper is on MDPWS that enables a device communication for medical requirements and safety issues, like safe data transmission that will typically be used for safe remote control (dual channel and safety context), data streaming, and compact transmission. The suitability of the concept has been shown by a demonstrator with over 20 real world OR devices from more than 10 vendors.
Industrial networks have a mix of devices with different security properties. If a mix of devices with various degrees of security features and capabilities communicate, the overall network dynamics with respect to device trust and security of message exchange will be complex. Therefore, there is a need to understand the trust and risk probabilities of devices in a heterogeneous network. This is required for heterogeneous network where the network configuration has to be made based on how trustworthy they are. In this work we focus on assessing security risks for devices and message exchanges. We define the term assurance value to denote the resilience of a device to security attacks. We study the behavior of a communication network when devices with various degrees of security features exchange messages. We aim to identify the network security properties based on the network architecture. From the study, we propose a model to estimate and predict network security properties in a heterogeneous communication network.
Recently, sophisticated attacks are increased against specific business companies, organizations and various facilities and the attackers are trying to remove attack traces such as system logs and related information on the victim systems. Therefore, it is getting more difficult to collect the information for attack analysis. In order to overcome this situations, companies and organizations have started to collect the network traffic as secondary information for attack analysis. However, most of them are focusing on gathering the network packets. But one of the most important parts is to extract the useful information for attack analysis from the collected data. In this paper, we suggest a network forensics system, Cyber Blackbox, which is focused on the traffic analysis.
Recently, sophisticated attacks are increased against specific business companies, organizations and various facilities and the attackers are trying to remove attack traces such as system logs and related information on the victim systems. Therefore, it is getting more difficult to collect the information for attack analysis. In order to overcome this situations, companies and organizations have started to collect the network traffic as secondary information for attack analysis. However, most of them are focusing on gathering the network packets. But one of the most important parts is to extract the useful information for attack analysis from the collected data. In this paper, we suggest a network forensics system, Cyber Blackbox, which is focused on the traffic analysis.
The development of IT (Information Technology) has made access to control systems easier. However, because such advancement of control systems gave rise to many security vulnerabilities, the threat of cyber-attack is increasing as well. In order to respond these threats, we discusses the mechanism for protecting ICS (Industrial Control System) network. Most of all, since availability is the most critical factor in a control system, independent network security technology is required. In this viewpoint, this paper presents our industrial firewall system, named the IndusCAP-Gate (Industrial Cyber Attack Prevention-Gate) system, that fundamentally prevents unauthorized access to a control system. Our system applies access control filters of various levels. Most of all, the proposed system has an abnormal traffic filtering mechanism about Modbus and DNP3 protocol of the most widely used protocols in ICS network. Therefore, it facilitates the provision of security policy specific to each zone of the control system intranet.
The development of IT (Information Technology) has made access to control systems easier. However, because such advancement of control systems gave rise to many security vulnerabilities, the threat of cyber-attack is increasing as well. In order to respond these threats, we discusses the mechanism for protecting ICS (Industrial Control System) network. Most of all, since availability is the most critical factor in a control system, independent network security technology is required. In this viewpoint, this paper presents our industrial firewall system, named the IndusCAP-Gate (Industrial Cyber Attack Prevention-Gate) system, that fundamentally prevents unauthorized access to a control system. Our system applies access control filters of various levels. Most of all, the proposed system has an abnormal traffic filtering mechanism about Modbus and DNP3 protocol of the most widely used protocols in ICS network. Therefore, it facilitates the provision of security policy specific to each zone of the control system intranet.
Routing detours raise concerns both on network security of nations and efficiency of resource consumption. Geo-optimal routing can be viewed as a long term goal of Internet routing, and a careful designed Internet together with geo-optimal routing can produce the best performance for the Internet. In this paper, we focus on some representative intercontinental circuitous paths summarized from our measurement data set. With the help of PeerDB and CAIDA, we try to find possible reasons for the detour routing and propose some suggestions to improve interconnection and routing for ISPs.
Covert channels are communication channels to transmit information utilizing existing system resources without being detected by network security elements, such as firewalls. Thus, they can be utilized to leak confidential governmental, military, and corporate information. Malicious users, like terrorists, can use covert channels to exchange information without being detected by cyber-intelligence services. Therefore, covert channels can be a grave security concern, and it is important to detect, eliminate, and disrupt covert communications. Active network wardens can attempt to eliminate such channels by traffic modification, but such an implementation will also hamper innocuous traffic, which is not always acceptable. Owing to a large number of covert channel algorithms, it is not possible to deal with them on a case-by-case basis. Therefore, it necessitates a unified system model that can represent them. In this paper, we present an event-based model to represent timing covert channels. Based on our model, we calculate the capacity of various covert channels and evaluate their essential features, such as the impact of network jitter noise and packet losses. We also used simulations to obtain these parameters to verify its accuracy and applicability.
Recently, Juels and Rivest proposed honeywords (decoy passwords) to detect attacks against hashed password databases. For each user account, the legitimate password is stored with several honeywords in order to sense impersonation. If honeywords are selected properly, a cyber-attacker who steals a file of hashed passwords cannot be sure if it is the real password or a honeyword for any account. Moreover, entering with a honeyword to login will trigger an alarm notifying the administrator about a password file breach. At the expense of increasing the storage requirement by 20 times, the authors introduce a simple and effective solution to the detection of password file disclosure events. In this study, we scrutinize the honeyword system and present some remarks to highlight possible weak points. Also, we suggest an alternative approach that selects the honeywords from existing user passwords in the system in order to provide realistic honeywordsa perfectly flat honeyword generation methodand also to reduce storage cost of the honeyword scheme.
This paper presents a network security laboratory project for teaching network traffic anomaly detection methods to electrical engineering students. The project design follows a research-oriented teaching principle, enabling students to make their own discoveries in real network traffic, using data captured from a large IP darkspace monitor operated at the University of California, San Diego (UCSD). Although darkspace traffic does not include bidirectional conversations (only attempts to initiate them), it contains traffic related to or actually perpetrating a variety of network attacks originating from millions of Internet addresses around the world. This breadth of coverage makes this darkspace data an excellent choice for a hands-on study of Internet attack detection techniques. In addition, darkspace data is less privacy-critical than other network traces, because it contains only unwanted network traffic and no legitimate communication. In the lab exercises presented, students learn about network security challenges, search for suspicious anomalies in network traffic, and gain experience in presenting and interpreting their own findings. They acquire not only security-specific technical skills but also general knowledge in statistical data analysis and data mining techniques. They are also encouraged to discover new phenomena in the data, which helps to ignite their general interest in science and engineering research. The Vienna University of Technology, Austria, first implemented this laboratory during the summer semester 2014, with a class of 41 students. With the help of the Center for Applied Internet Data Analysis (CAIDA) at UCSD, all exercises and IP darkspace data are publicly available.
The proposition of increased innovation in network applications and reduced cost for network operators has won over the networking world to the vision of software-defined networking (SDN). With the excitement of holistic visibility across the network and the ability to program network devices, developers have rushed to present a range of new SDN-compliant hardware, software, and services. However, amidst this frenzy of activity, one key element has only recently entered the debate: Network Security. In this paper, security in SDN is surveyed presenting both the research community and industry advances in this area. The challenges to securing the network from the persistent attacker are discussed, and the holistic approach to the security architecture that is required for SDN is described. Future research directions that will be key to providing network security in SDN are identified.
In the existing DAD solutions, a new node constructs a target address and broadcasts the complete target address in the network. The target address becomes invalid as long as another node claims that the target address is occupied. An attacker usually takes advantage of this feature and repeatedly makes such a claim in order to prevent a new node from obtaining an address. In order to protect DAD from such an attack, this letter proposes an addressing scheme with an improved DAD for 6LoWPAN. In this scheme, only a part of a target address, instead of a complete address, is broadcasted in the network. In this way, an attacker is unable to see the complete target address, so it cannot claim that the target address is occupied. Moreover, only a part of a target address is broadcasted in the network, so the addressing cost is reduced.
Software defined wireless networking (SDWN) is a new paradigm of wireless networking, physically separating the data and control planes of various elements in the wireless infrastructure. Similar to its wired counterpart, SDWN is expected to introduce a wide range of benefits to the operation and management of wireless networks. Security is always important to any network. On one hand, SDWN enables new security mechanisms. On the other hand, some new threats are introduced due to the separation of the control and data planes and the introduction of the logically centralized controller. In this article, we discuss its security threat vectors as well as design issues in making it secure. Also, we analyze the security requirements of SDWN, and then summarize the security attacks and countermeasures in this area and suggest some future research directions.
As the popularity of software-defined networks (SDN) and OpenFlow increases, policy-driven network management has received more attention. Manual configuration of multiple devices is being replaced by an automated approach where a software-based, network-aware controller handles the configuration of all network devices. Software applications running on top of the network controller provide an abstraction of the topology and facilitate the task of operating the network. We propose OpenSec, an OpenFlow-based security framework that allows a network security operator to create and implement security policies written in human-readable language. Using OpenSec, the user can describe a flow in terms of OpenFlow matching fields, define which security services must be applied to that flow (deep packet inspection, intrusion detection, spam detection, etc.) and specify security levels that define how OpenSec reacts if malicious traffic is detected. In this paper, we first provide a more detailed explanation of how OpenSec converts security policies into a series of OpenFlow messages needed to implement such a policy. Second, we describe how the framework automatically reacts to security alerts as specified by the policies. Third, we perform additional experiments on the GENI testbed to evaluate the scalability of the proposed framework using existing datasets of campus networks. Our results show that up to 95% of attacks in an existing data set can be detected and 99% of malicious source nodes can be blocked automatically. Furthermore, we show that our policy specification language is simpler while offering fast translation times compared to existing solutions.
With the rapid growth in the number of mobile phone users, mobile payments have become an important part of mobile e-commerce applications. Secure payment systems directly affect the security of e-commerce systems. This study proposes an anomaly detection mechanism supported by an information entropy method combined with neural network to improve mobile payments security. As the entropy value is sensitive and have much difference between normal and abnormal traffic flow in the mobile payment system, the abnormal traffic data will be detected. The simulation result shows that it can realise the effective monitoring of abnormal flow in the mobile payment system.
Public-key cryptography (PKC) operations are heavy to resource-constrained sensor nodes. Therefore, attackers can cripple a sensor node by forcing it to perform a large number of false PKC operations. In this letter, we propose a fully distributed and effective scheme that randomly drops extra PKC request messages beyond its processing capability. Our scheme is not only resistant to PKC-based denial-of-service attacks, but also energy-efficient.
A popular choice for anonymous Internet communication, the Tor network uses entry, relay, and exit nodes to hide the traffic's origin. However, an investigation that involved running real applications and website requests through Tor revealed numerous agglomerations of exiting traffic that an attacker could exploit.
The following topics are dealt with: access control;e-society;cloud security;information security;biometrics;network security;digital forensics;and cyber security.
The following topics are dealt with: access control;e-society;cloud security;information security;biometrics;network security;digital forensics;and cyber security.
This paper presents a new hybrid honeypot architecture which focuses on the coverage of large IPv6 address spaces. Results from a 15-months darknet experiment verify that attackers and researchers utilise various approaches to scan wide and unforeseeable IPv6 address ranges which cannot be managed with current honeypot solutions. The huge IPv6 address space not only makes it hard for attackers to find target hosts, it also makes it difficult for a honeypot to get found by an attacker. We solve this challenge through the use of dynamically configured high-interaction honeypots that can cover large chunks of the IPv6 address space. A new proxy mechanism is used to transparently handover and forward traffic from low-to high-interaction honeypots on demand to provide the best possible service granularity. Measurements with our prototype implementation show that the proposed approach performs well on off-the-shelf hardware and has low maintenance costs.
When hackers try to attack a target system, their first goal is to install a malware to the target system. It is because hackers can do anything what they want if a malware is installed. In the past, most of the malwares were Microsoft PE files, however they have been changed to various file formats such as pdf, jpg, doc, jar and so on. Under this circumstances some network security systems such as network forensics systems have to reconstruct those malwares from network packets to analyze the malwares. For that, we propose a file type signature and network protocol analysis based transmitted file reconstruction technique which can reconstruct various file types from network packets. In this paper, we show the implementation and file reconstruction results.
Various Cloud layers have to work in concert in order to manage and deploy complex multi-cloud applications, executing sophisticated workflows for Cloud resource deployment, activation, adjustment, interaction, and monitoring. While there are ample solutions for managing individual Cloud aspects (e.g. network controllers, deployment tools, and application security software), there are no well-integrated suites for managing an entire multi cloud environment with multiple providers and deployment models. This paper presents the CYCLONE architecture that integrates a number of existing solutions to create an open, unified, holistic Cloud management platform for multicloud applications, tailored to the needs of research organizations and SMEs. It discusses major challenges in providing a network and security infrastructure for the Intercloud and concludes with the demonstration how the architecture is implemented in a real life bioinformatics use case.
Honeypot is a common method of attack capture, can maximize the reduction of cyber-attacks. However, its limited application layer simulation makes it impossible to use effectively in power system. Through research on sandboxing technology, this article implements the simulated power manager applications by packaging real power manager applications, in order to expand the honeypot applied range.
In order to find out the network security problem of the IP Multimedia Subsystem (IMS) in Diameter protocol, first of all, this paper builds a security test model of classification entity according to different entities in IMS network. Then owing to the fact that test messages are difficult to pass the protocol specification, we present a limited distance method for generating test message based on equivalence class partition, which abstracts each Diameter signal massage to the point in the multidimensional space. We calculate Euclidean distance D(X, Y) between the test messages and standard message and then allow test messages to pass protocol specification by limiting D(X, Y) in certain extent. In the monitoring content and method, we propose information disclosure as the core of the monitoring methods according to the Diameter protocol security requirements, we set the IMS network verification environment, and then demonstrate the rationality of the model and effectiveness of massage generation method.
Intrusion detection and prevention systems (IDS/IPS) are a critical component of computer network security. This paper presents the results of an experiment comparing two open source IDS - Snort IDS and Bro IDS on a multi-purpose and low-cost computer called Raspberry Pi 2 (Model B), with a specific objective of determining their performance, efficiency and efficacy for use in computer network environments, where cost is a determining factor. Such environments could include SOHO and educational institution computer networks.
Undoubtedly, it is highly essential to have tools in order to evaluate the sustainability of computer networks in case of unauthorized intrusions. In this paper, a network security metric is presented which is based on attack duration. This metric initially models the attack perquisites in form of a tree according to the network configuration and in the next step, calculates the value of metric with gamma distribution. It was concluded that the calculated metric was not accurate but it could be used to compare security status of similar computer networks and also different configurations of the same network. This criterion is not enough to judge about total security of network but it covers one of the important aspects of attack, i.e. time. It is expected that this method will help researchers contrive more accurate metrics for network security.
Cyber forensics deals with the collection of digital artifacts from digital devices that can be presented in the court as evidence. There are many areas of cyber forensics. Some of these are Disk Forensics, Network Forensics, Mobile Forensics, Database Forensics, etc. This paper mainly focuses on E-mail Forensics coming under network forensics. Emails are highly vulnerable to attacks. Email spoofing is the major one among many forms of email-based attacks. Detecting email spoofing is an important challenge in email forensic investigation. In this paper, different methods to detect spoofing by analyzing the mail header are discussed.
Today, as Network environments become more complex and cyber and Network threats increase, Organizations use wide variety of security solutions against today's threats. For proper and centralized control and management, range of security features need to be integrated into unified security package. Unified threat management (UTM) as a comprehensive network security solution, integrates all of security services such as firewall, URL filtering, virtual private networking, etc. in a single appliance. PfSense is a variant of UTM, and a customized FreeBSD (Unix-like operating system). Specially is used as a router and statefull firewall. It has many packages extend it's capabilities such as Squid3 package as a as a proxy server that cache data and SquidGuard, redirector and access controller plugin for squid3 proxy server. In this paper, with implementing UTM based on PfSense platform we use Squid3 proxy server and SquidGuard proxy filter to avoid extreme amount of unwanted uploading/ downloading over the internet by users in order to optimize our organization's bandwidth consumption. We begin by defining UTM and types of it, PfSense platform with it's key services and introduce a simple and operational solution for security stability and reducing the cost. Finally, results and statistics derived from this approach compared with the prior condition without PfSense platform.
While cyber-physical systems are widely deployed and known to be difficult to analyze due to their increasing complexity, the number of sophisticated attacks against them have been constantly growing. This necessitates semi-automated intrusion response and recovery capabilities for timely termination of ongoing attacks and effective recovery of the infrastructural normal and safe operations. In this paper, we present CPR, a cyber-physical response system to protect power grid critical infrastructures, and discuss major challenges in theoretical formulation and practical deployment of fully-automated tolerance capabilities in settings where continuous physical dynamics continuously interact with cyber-side discrete computation logic. Our evaluation results show that CPR leverages its hybrid cyber-physical formulation, and efficiently selects optimal joint response strategies in both physical and cyber networks.
With the advancement of nanometer scale design technology, transient faults often occur in circuits and thus require low power dissipation design. Reversible logic has become one of the recent emerging research interests contributing to the field of low power dissipating circuit design in the past few years. Few of its application areas include CMOS low power design, quantum computing, network security, digital signal and image processing, optical electronics and nanotechnology. This paper presents a comparative study of efficacy of various reversible full adder and reversible full subtractor gate and their quantum cost values to obtain energy efficient logic design. A novel design approach for implementing reversible combinational circuits using the existing Reversible logic gates and new improved Reversible logic gates is discussed in this paper. The reversible logic circuits are designed and implemented using Verilog code. The simulation results are obtained in Xilinx ISE version 9.2i.
Distributed Denial of Service (DDoS) attack is a major security threat for networks and Internet services. The complexity and frequency of occurrence of DDoS attacks are growing in parallel with rapid developments of the Internet and associated computer networks. A significant number of network security tools are available on the Internet to generate network attacks as well as to defend and analyze network attacks. Attackers can generate attack traffic similar to normal network traffic using sophisticated attacking tools. In such a situation, many defense solutions fail to identify DDoS attacks in real-time. DDoS attack traffic typically behaves differently from legitimate network traffic in terms of traffic features. Statistical properties of various features can be analyzed to distinguish the attack traffic from legitimate traffic. In this paper, we introduce a statistical measure called Feature Feature Score (FFSc) for multivariate data analysis to distinguish DDoS attack traffic from normal traffic. We extract three features of network traffic, viz., entropy of source IPs, variation of source IPs and packet rate to analyze the behavior of network traffic for attack detection. The method is validated using CAIDA DDoS 2007 and MIT DARPA datasets.
Botnets compose a major source of malicious activity over a network and their early identification and detection is considered as a top priority by security experts. The majority of botmasters rely heavily on a scan procedure in order to detect vulnerable hosts and establish their botnets via a command and control (C&C) server. In this paper we examine the statistical characteristics of the scan process invoked by the Mariposa and Zeus botnets and demonstrate the applicability of conditional entropy as a robust metric for profiling it using real pre-captured operational data. Our analysis conducted on real datasets demonstrates that the distributional behaviour of conditional entropy for Mariposa and Zeus-related scan flows differs significantly from flows manifested by the commonly used NMAP scans. In contrast with the typically used by attackers Stealth and Connect NMAP scans, we show that consecutive scanning flows initiated by the C&C servers of the examined botnets exhibit a high dependency between themselves in regards of their conditional entropy. Thus, we argue that the observation of such scan flows under our proposed scheme can sufficiently aid network security experts towards the adequate profiling and early identification of botnet activity.
Due to the widespread research on Software Defined Networks (SDNs), its security has received much attention recently. But most of those attempts consider SDN security from the OpenFlow perspective. To the best of our knowledge, none so far has paid attention to the security analysis and modeling of Forwarding and Control planes Separation Network Structure (FCSNS) in SDN. Therefore, this paper provides a different approach to network security based on Petri net and Attack tree models. Our objective is to analyze the FCSNS security via the combination of model and state. This method represents the network structure and state transferring by way of Petri net. In addition, it introduces the security analysis method of STRIDE to build up the Attack tree model. Finally, we analyze FCSNS via the combination of Petri net and Attack tree model and present the results. Our results are very promising in using such models to achieve such security objectives.
As the number of users connected with internet has increased many folds, Cyber attacks have become a major problem in today's world. The conventional security devices such as IDS, IPS, and Firewalls are good enough to counter and log the known attacks but in case of the unknown attacks theses security devices fails. As a solution to the zero day attacks honeynets provides a proactive approach toward detection and further mitigation of these zero day attacks. Work present in this paper explores the possibility of using honeynets as active security devices complementing the conventional security measures such as firewalls and IDS. We have used honeyd a low interaction honeypot as capturing devices in the organizational network to capture the malicious data. The captured data is further characterised and being segregated in to three major classes 1) legitimate traffic, 2) traffic due to system misconfiguration and 3) traffic due to worm propagation or infection. The knows malicious traffic if filtered using SNORT IDS rule sets and malicious traffic which is not detected is used as an input to the IDS signature generation engine. From the experimental analysis various prons and cons are brought about in this paper.
Haruspex is a suite of tools to assess and manage the risk posed by an information and communication technology system. The suite is built around the application of a Monte Carlo method to a scenario where intelligent agents implement chains of attacks to reach their goals. Some tools build a description of the agents, the target system, its vulnerabilities and the resulting attacks. Another tool applies a Monte Carlo method to this description, simulates the building of attack chains by the agents and it returns a database with samples it collects in the simulations. Further tools analyze this database to select countermeasures. To validate the suite and verify it truthfully models attackers, it has been adopted in Locked Shield 2014, a network defense exercise with participants from 17 nations. The results of this exercise validate the designs of the tools.
Nowadays, network security not only is a technical issue, but also permeates our lives and work, and affects the socioeconomic and national security issues. This paper carried out in-depth research on the detection of network attacks, and introduced the design and implement of a network attack detection module. To imitate the real conditions, we use one computer as an attacker, one computer as a detector, and another computer as a victim. In this experiment, the detector can detect a lot of network attacks, such like ARP attacks, IP address attacks, UDP flooding attacks, TCP SYN flooding attacks, ICMP flooding attacks. Moreover, the detector alerts when it detect attacks.
Nowadays, network security defense based on game theory mostly used complete information or static game model. In order to get closer to the actual network and defend actively, we proposed a network attack-defense game model based on signaling game, which was modeled in a dynamic and incomplete way. We improved the attack-defense strategies quantitative method of the completed static game model to meet the needs of the network signaling game model. Moreover, we put forward an active defense strategy selecting algorithm to select optimal defense strategy for different characters of defender. At last, we analyzed and proved the feasibility and validity of the model and method through a network example.
In order to assess network security threats better, we model the attacker's and defender's behavior based on attack-defense graph from the perspective of both attacker and defender, which lays the foundation of the further research for the scientific and precise information security threat assessment. Due to the existing problems of generating traditional attack-defense graph (e.g. explosion of state space, and the large-scale and complex generation of attack-defense graph), we in this paper propose a method for generating attack-defense graph based on state reduction, then we model the behavior of attacker and defender in network security. Finally, the method and the model proposed in this paper are proved to be scientific and effective within a typical network scenario.
Nowadays, the network defense based on incomplete information game model ignored the type of the defender, used simple cost quantitative method, and chosen defense strategies improperly. Aiming at the problem, this paper proposed an active defense strategy selection based on static Bayesian game theory, constructed static Bayesian game model. The model considered the types of the attacker and the defender, and improved the classical strategies taxonomy and cost quantitative method with considering the strike back act of the defender and the success attack rate. Then, this paper calculated and comprehensively analyzed the Bayesian equilibrium of the game. Taking mixed strategies Bayesian equilibrium of attacker as the defender's prediction of attacker's action, this paper calculated the defense effectiveness of defense strategies and performed a defense strategies selection algorithm. At last, an example was provided to analyze and demonstrate the effectiveness of the model and algorithm.
This paper aims to solve the shortage of the current network security technology and to provide Bayesian-oriented network security situational awareness model on the base of the comprehensive research on network security situational awareness at home and abroad. Multi-angle analyzing and building hierarchical network structure model make the model higher efficient and real-time performance.
Currently network security is treated within a deterministic and preventive control framework where network constraints and balancing costs are not fully optimised by applying a range of advanced, technically effective and economically efficient corrective (or post-fault) actions from flexible network technologies such as phase shifter transformers, reactive compensation and HVDC links. This may be inefficient and lead to increased levels of congestion in the short term and over-investment in network capacity in the long term, potentially increasing cost of integration associated with low-carbon generation. In this context, this paper assesses the effects of coordinating flexible AC and HVDC transmission systems under both deterministic and probabilistic security frameworks and assuming an array of available preventive and corrective control actions. To do so, we use an Optimal Power Flow (OPF) based deterministic/probabilistic cost-benefit model that has been developed for assessing the economic and reliability performance of alternative control strategies enabled by coordination of AC and HVDC technology. This involves co-optimisation of a number of control variables associated with generating units, HVDC links, phase shifters and reactive compensation. We quantify the benefits of alternative control and security strategies applied to the GB transmission system, considering future development scenario with increased penetration of wind generation, demonstrating that probabilistic security with corrective control for AC/DC coordination is significantly more beneficial since it can lead to lower operating costs, increased utilisation of the network infrastructure and improved reserve allocation across transmission network.
The existence of intrusion detection systems in computer network as a part of network security tool is very important, with this tool a computer system can detect the intrusion action before it makes more damage. Traditional intrusion detection system using the rule that created by expert for detecting the intrusion, but because of the increasing of internet activity the data to be analyzed in order to establish that rules become large and create the possibility the new intrusion technique cannot detect. Using data mining techniques to find intrusion pattern from network packet data was success to detect intrusion in offline environment, but the effective intrusion detection system must able to detect the intrusion in online environment. Therefore needed a method that can be used to perform online processing of network packets data. This paper discusses the data processing network packets to establish the connection records are complete or incomplete in an effort to enabling the intrusion detection system detecting the intrusion online and based on the test result this method was success to detect the intrusion in online environment.
In this paper the concept of external appliance as component of honeypot in wireless network is suggested as based on particular computing instance. Also paper suggests approach to implementation of complex Intrusion Detection System (IDS) - Router - Honeypot. Requirements to IDS rules design are formulated by authors.
Web and machine frameworks have raised various security issues because of unsafe utilization of networks. The massive usage of internet contains the risks of network attack. Thus intrusion detection is one of the major research problems in a network security. Today's researcher's goal is to look for unusual accessing of network for secure internal network. Distinctive metaheuristic strategies have been utilized for anomaly locator generation. The very few reported writing has considered the utilization of the multi-start metaheuristic technique for detector generation. This paper describes a mixture approach for anomaly network intrusion detection systems (ANIDS) in vast scale datasets utilizing detectors produced, focus around machine learning techniques using different datasets. The most of ANIDS worked on KDD Cup 99 dataset but very few ANIDS utilizing NSL-KDD dataset which is an altered adaptation of the broadly utilized KDD Cup 99 dataset. This is observed that NSL-KDD dataset is better than KDD99 dataset.
Security has a significant influence in network management. One of the most common way to secure information in the computer from malicious use is IDS Intrusion detection system(IDS) is most prominent to secure a computer and network against intrusion. IDSs primarily intended to preserve the availability, confidentiality and Integrity(CAI)of network and computer. IDS can be broadly classified in two categories: Network intrusion detection system (NIDS) and Host intrusion detection system(HIDS). NIDS is main part of any network security architecture, which monitors network traffic for predefined suspicious activity or patterns and alert system administrators. Nowadays, many IDSs tools are available such as commercial as well as open source tools. Open source tools promotes a global access through free license. In paper we found study of three popular NIDS tools : Snort, Suricata, Bro.
Today's Internet makes hosts and individual networks inherently insecure because permanent addresses turn destinations into permanent attack targets. This paper describes an Evasive Internet Protocol (EIP), a change to the data plane of the Internet that: 1) prevents senders from forging their identities while preserving the current Internet privacy paradigm;2) gives recipients full control over who can communicate with them and for how long;3) achieves the above features without requiring global signaling protocols;and 4) allows coexistence with and graceful introduction into the current Internet. We motivate our approach, present the architectural design, and evaluate it through trace-driven and synthetic simulations as well as prototype testing.
Network reconnaissance of addresses and ports is prerequisite to a vast majority of cyber attacks. Meanwhile, the static address configuration of networks and hosts simplifies adversarial reconnaissance for target discovery. Although the randomization of host addresses has been suggested as a proactive disruption mechanism against such reconnaissance, the proposed approaches do not exploit the full potentials of address randomization in provision of unpredictability and attack adaptability. Moreover, these approaches do not provide thorough analysis on effectiveness and limitations of address randomization against relevant threat models, including stealthy scanning and worms. In this paper, we present an effective address randomization technique, called random host address mutation (RHM), that turns end-hosts into untraceable moving targets. This technique achieves maximum efficacy by allowing address randomization to be highly unpredictable and fast, and adaptive to adversarial behavior, while incurring low operational and reconfiguration overhead. Our approach achieves the following objectives: (1) it achieves high uncertainty in adversary scanning by modeling address mutation randomization as a multi-level satisfiability problem;(2) it adapts the mutation scheme by fast characterization of adversarial reconnaissance patterns;(3) it achieves high mutation rate by separating mutation from end-hosts and managing it via network appliances;and (4) it preserves network integrity, manageability and performance by bounding the size of routing tables, preserving end-to-end reachability, and efficient handling of reconfiguration updates. Our extensive analyses and simulation show that the RHM distorts adversarial reconnaissance, slows down (deters) the attack, and increases its detectability. Consequently, the RHM is effective in countering a significant number of sophisticated threat models, including reconnaissance, stealthy/evasive scanning methods, and targeted att- cks. We also address limitations of our approach in terms of effectiveness and applicability.
Recent studies reveal that an adversary might trace the apparently insignificant traffic rate of source nodes over the net and turn such data to invaluable information so as to breach the privacy of the victim sources. Inhibiting the adversary of being able to extract information from the traffic rate of source nodes is a complicated task unless taking into consideration the flow conservation law effect of the transmitter queue. A reliable method of preserving the rate privacy that copes with the flow conservation law is to transmit original packets augmented with probabilistically dummy ones so as to change the observable aggregated traffic rate. Augmenting dummy packets, however, bears redundancy, and hence, requires extra resources in terms of bandwidth and buffer requirements, and more importantly suggests higher transmitting energy consumption. Grounded on the queueing and information theories, in this paper, we present an efficient method that minimally augments dummy packets to preserve the source rate privacy at a given degree while preserving the delay distribution of the original packets intact, and thus does not affect the quality of service parameters of the transmitted data in terms of delay and jitter. The presented method models the original packets and dummy ones with a preemptive resume 2-priority queueing system and then using information theory attempts to maximize the Fano lower bound of the best estimation of the adversary's speculation. All of the theoretically obtained results have been validated by conducting simulation experiments.
This letter deals with the problem of detecting DoS and DDoS attacks. First of all, two features including number of packets and number of source IP addresses are extracted from network traffics as detection metrics in every minute. Hence, a time series based on the number of packets is built and normalized using a Box-Cox transformation. An ARIMA model is also employed to predict the number of packets in every following minute. Then, the chaotic behavior of prediction error time series is examined by computing the maximum Lyapunov exponent. The local Lyapunov exponent is also calculated as a suitable indicator for chaotic and nonchaotic errors. Finally, a set of rules are proposed based on repeatability of chaotic behavior and enormous growth in the ratio of number of packets to number of source IP addresses during attack times to classify normal and attack traffics from each other. Simulation results show that the proposed algorithm can accurately classify 99.5% of traffic states.
Authentication protocol plays an important role in the short-range wireless communications for the Near Field Communication (NFC) technology. Due to the shared nature of wireless communication networks, there are several kinds of security vulnerabilities. Recently, a pseudonym-based NFC protocol (PBNFCP) has been proposed to withstand the security pitfalls found in the existing conditional privacy preserving security protocol (CPPNFC). However, this paper further analyzes PBNFCP and shows that it still fails to prevent the claimed security properties, such as impersonation attacks against an adversary, who is a malicious registered user having a valid pseudonym and corresponding private key. In order to overcome these security drawbacks, this paper proposes a secure and efficient authentication protocol (SEAP) for NFC applications using lifetime-based pseudonyms. The proposed SEAP is simulated for the formal security verification using the widely-accepted AVISPA (Automated Validation of Internet Security Protocols and Applications) tool. The simulation results show that SEAP is secure. The rigorous security and performance analysis shows that the proposed SEAP is secure and efficient as compared to the related existing authentication protocols for NFC applications.
Significant changes in traffic patterns often indicate network anomalies. Detecting these changes rapidly and accurately is a critical task for network security. Due to the large number of network users and the high throughput requirement of today's networks, traditional per-item-state techniques are either too expensive when implemented using fast storage devices (such as SRAM) or too slow when implemented using storage devices with massive capacity (such as DRAM). Sketch, as a highly accurate data stream summarization technique, significantly reduces the memory requirements while supporting a large number of items. Sketch based techniques are attractive for exploiting the fast on-chip storage of state-of-the-art computing platforms to achieve high throughput. In this work, we propose a fully pipelined Sketch based architecture on FPGA for online heavy change detection. Our architecture forecasts the activity of the network entities based on their history, then reports the entities whose difference between their observed activities and the forecast activities exceed a given threshold. The post place-and-route results on a state-of-the-art FPGA show that our architecture sustains high throughput of 96 - 103 Gbps using various configurations of online heavy change detection.
We report in this paper on research in progress related to network functions virtualization (NFV) and their use in network security. Our objective is to design and develop a virtual Security Appliance (vSA) capable of detecting various network attacks while offering an acceptable level of performance. In this document, we introduce the vSA under construction and show some testing results we have recently obtained.
To increase network security and mitigate identity theft attacks, much of the research is focused on traditional bit-level algorithmic. In conventional wireless networks, security issues are primarily considered above the physical layer and are usually based on bit-level algorithms to establish the identity of a legitimate wireless device. Physical layer security is a new paradigm in which features extracted from an analog signal can be used to establish the unique identity of a transmitter. Our previous research work into Radiometric fingerprinting has shown that every transmitter has a unique fingerprint owing to imperfections in the analog components present in the RF front end. However, to the best of the author's knowledge, no such example is available in the literature in which the effect of radio channel on Radiometric fingerprint is evaluated. This paper presents the simulation and experimental results for radiometric fingerprinting under an indoor varying radio channel. Contrary to popular assumption, it was found that the fingerprinting accuracy is little affected in an indoor channel environment.
Microservice architecture allows different parts of an application to be developed, deployed and scaled independently, therefore becoming a trend for developing cloud applications. However, it comes with challenging security issues. First, the network complexity introduced by the large number of microservices greatly increases the difficulty in monitoring the security of the entire application. Second, microservices are often designed to completely trust each other, therefore compromise of a single microservice may bring down the entire application. The problems are only exacerbated by the cloud, since applications no longer have complete control over their networks. In this paper, we propose a design for security-as-a-service for microservices-based cloud applications. By adding a new API primitive FlowTap for the network hypervisor, we build a flexible monitoring and policy enforcement infrastructure for network traffic to secure cloud applications. We demonstrate the effectiveness of our solution by deploying the Bro network monitor using FlowTap. Results show that our solution is flexible enough to support various kinds of monitoring scenarios and policies and it incurs minimal overhead (~6%) for real world usage. As a result, cloud applications can leverage our solution to deploy network security monitors to flexibly detect and block threats both external and internal to their network.
Intrusion detection can decide whether there are dangerous user behaviors. It can also timely detect network security vulnerabilities of inner attacks, and it can make quick response and improve network security. Therefore, it is necessary to research about the intrusion detection in land and resources information systems. For the security status of land and resources network, we place the security architecture platform and select different types of attacks to test the systems. The test results verify the stability, reliability and scalability of the systems, achieving the needed results.
We present a new analysis of multi-hop network security in the form of a zero-sum game played between an attacker who tries to disrupt a network by disabling one or more nodes, and the nodes of the network who must allocate limited resources in defense of the network. The payoffs in the zero-sum game can be one of several performance metrics that correspond to node centrality measures. In the case of single-node attacks, we use a monotonicity property of the mixed attack strategies to construct a simple and very fast algorithm to compute saddle-point equilibrium strategies for both single-node and multiple-node defense. For simultaneous multiple-node attacks on large networks, the computational complexity becomes quite high, so we present a method to approximate the equilibrium strategies based on a sequential simplification, which performs well in simulations.
The evolution of digital communication includes both applications and devices running them. In this context, specific applications are needed to enhance a safeguard communication ensuring protection and trust services to the users. The growing need to address privacy concerns when social network data is released for mining purposes has recently led to considerable interest in various network security-based applications. In this paper, we develop an efficiency and adaptive network security-based application to ensure the privacy and data integrity across channel communications. This approach is designed on a model of clustering configuration of the involved members. The cluster members interact with the cluster leader for data exchange and sends to the base station. This scheme provides a strong mutual authentication framework that suits for real heterogeneous wireless applications. In addition, we contribute with a mathematical analysis to the delay and optimization analysis in a clustering topology node-based, also we compared the scheme with existing ones using security standards services. Finally, performance of this scheme is evaluated in term of computation and communication overhead;results show the present framework is efficiency and can be safeguard for Network Security-based applications.
As VANET can be considered as a subclass of wireless ad hoc networks, they inherit the security problems. In this work, we focus on network security. We present some examples of attacks on these networks and we propose a method to optimize the security against three of these attacks. To improve the method of attacks' corroboration, we propose an innovative probabilistic model based on Logistic regression. This method will estimate the occurrence of an event (in this case, an attack). The method is based on a history of a knowledge base that estimates the attack's occurrences. When the regression model is validated, it will be used to estimate the probability of an attack and if it exceeds the threshold defined in advance, the attack is then confirmed.
Today's enterprise networks almost ubiquitously deploy middlebox services to improve in-network security and performance. Although virtualization of middleboxes attracts a significant attention, studies show that such implementations are still proprietary and deployed in a static manner at the boundaries of organisations, hindering open innovation. In this paper, we present an open framework to create, deploy and manage virtual network functions (NF)s in OpenFlow-enabled networks. We exploit container-based NFs to achieve low performance overhead, fast deployment and high reusability missing from today's NFV deployments. Through an SDN northbound API, NFs can be instantiated, traffic can be steered through the desired policy chain and applications can raise notifications. We demonstrate the systems operation through the development of exemplar NFs from common Operating System utility binaries, and we show that container-based NFV improves function instantiation time by up to 68% over existing hypervisor-based alternatives, and scales to one hundred co-located NFs while incurring sub-millisecond latency.
Modern web users are exposed to a browser security threat called drive-by-download attacks that occur by simply visiting a malicious Uniform Resource Locator (URL) that embeds code to exploit web browser vulnerabilities. Many web users tend to click such URLs without considering the underlying threats. URL blacklists are an effective countermeasure to such browser-targeted attacks. URLs are frequently updated;therefore, collecting fresh malicious URLs is essential to ensure the effectiveness of a URL blacklist. We propose a framework called automatic blacklist generator (AutoBLG) that automatically identifies new malicious URLs using a given existing URL blacklist. The key idea of AutoBLG is expanding the search space of web pages while reducing the amount of URLs to be analyzed by applying several pre-filters to accelerate the process of generating blacklists. Auto-BLG comprises three primary primitives: URL expansion, URL filtration, and URL verification. Through extensive analysis using a high-performance web client honeypot, we demonstrate that AutoBLG can successfully extract new and previously unknown drive-by-download URLs.
In this work, a node architecture based on wavelength selective switches with feedback loop attenuators that can be used as power equalizers, as well as an optimization algorithm are utilized in order to minimize the impact of physical layer jamming attacks in optical networks, while also minimizing the capital expenditure (Capex) of the network. Due to crosstalk-induced interactions among different connections, malicious high-power jamming signals can potentially spread widely in the network. Therefore, it is necessary to design an optical network in a way that the spread of an attack is minimized. This is achieved in this work by the design of appropriate power equalizer placement and attack-aware Routing and Wavelength Assignment (RWA) algorithms in the form of Integer Linear Program (ILP) formulations. Performance results indicate that the proposed algorithms minimize the number of required equalizers for zero lightpath interactions without the need of additional wavelength resources.
Intrusion Detection System (IDS) is a tool for anomaly detection in network that can help to protect network security. At present, intrusion detection systems have been developed to prevent attacks with accuracy. In this paper, we concentrate on ensemble learning for detecting network intrusion data, which are difficult to detect. In addition, correlation-based algorithm is used for reducing some redundant features. Adaboost algorithm is adopted to create the ensemble of weak learners in order to create the model that can protect the security and improve the performance of classifiers. The U2R and R2L attacks in KDD Cup'99 intrusion detection dataset are used to train and test the ensemble classifiers. The experimental results show that reducing features can improve efficiency in attack detection of classifiers in many weak leaners.
The main objective of this research is to integrate honeypot and IDS, which can generate and activate snort rule automatically based on the data sending by honeypot server. The new technic is present in this paper, honeypot will collect the data, send the data to IDS, and then IDS will evaluate and generate the rules automatically. Rule that has been made will be active to filter packets sent by the user on the network. We compare rule generated automatically with default rule in snort system for the same pattern. The performance of the proposed technique was evaluated by measuring the effectiveness of IDS server from the attacking.
Modern network security applications, such as network-based intrusion detection systems (NIDS) and firewalls, routinely employ deep packet inspection to identify malicious traffic. In deep packet inspection, the contents of network traffic are matched against patterns of malicious traffic to identify attack-carrying packets. The pattern matching algorithms employed for deep packet inspection must be fast, as the algorithms are often implemented on middle-boxes residing on high-speed gigabits per second links. The majority of patterns employed in network security applications are regular languages. However, regular language-based patterns have limited expressive power and are not capable of describing some complex features in network payload. Back reference is an important feature provided by many pattern matching tools, e.g., PCRE, the regular expression libraries of Java, Perl, and Python. Back references are used to identify repeated patterns in input strings. Patterns containing back-references are non-regular languages. Very little work has been done to improve the time-efficiency of back reference-based pattern matching. The de facto algorithm to implement back reference is recursive backtracking, but it is vulnerable to algorithmic complexity attacks. In this paper, we present a novel approach to implement back references. The basic idea of our approach is to transform a back reference problem to a conditional submatch problem, and represent it with a Non-deterministic Finite Automata (NFA)-like machine subject to some constraints. Our experimental results show that our approach resists known algorithmic complexity attacks, and is faster than PCRE by up to three orders of magnitude for certain types of patterns.
Anomaly-based Intrusion Detection is a key research topic in network security due to its ability to face unknown attacks and new security threats. Moreover, new solutions should cope with scalability issues derived from the growth of the Internet traffic. To this aim random aggregation through the use of sketches represents a powerful prefiltering stage that can be applied to backbone data traffic with a performance improvement wrt traditional static aggregations at subnet level. In the paper we apply the CUSUM algorithm at the bucket level to reveal the presence of anomalies in the current data and, in order to improve the detection rate, we correlate the data corresponding to traffic flows aggregation based on different fields of the network and transport level headers. As a side effect, the correlation procedure gives some hints on the typology of the intrusions since different attacks determine the variability of the statistics associated to specific header fields. The performance analysis, presented in this paper, demonstrates the effectiveness of the proposed approach, confirming the goodness of CUSUM as a change-point detection algorithm.
Attacks against websites are increasing rapidly with the expansion of web services. More and more diversified web services make it difficult to prevent such attacks due to many known vulnerabilities in websites. To overcome this problem, it is necessary to collect latest attacks using decoy web honeypots and to implement countermeasures against malicious threats. Web honeypots collect not only malicious accesses by attackers but also benign accesses such as those by web search crawlers. Thus, it is essential to develop a means of identifying malicious accesses automatically from mixed collected data including both malicious and benign accesses. In this study, we have focused on detection of crawlers whose accesses has been increasing rapidly. A related study proposed a crawler detection scheme in which crawlers are identified based on the features of well-known crawlers such as Google crawlers. However, the diversity of crawler accesses has been increasing rapidly, and adapting to that diversity is a challenging task. Therefore, we have adapted AntTree, a bio-inspired clustering scheme that has high scalability and adaptability, for crawler detection. Through our evaluations using data collected in a real network, we show that AntTree can detect crawlers more precisely than a conventional scheme.
The number of devices connected to computer networks is increasing daily, and so is the number of network-based attacks. A honeypot is a system trap that is set to act against unauthorised use of information systems. The objective of this study was to survey the emergent trends in extant honeypot research with the aims of contributing to the knowledge gaps in the honeypot environment. The relevant literature was identified from a myriad of sources, such as books, journal articles, reports, et cetera. The findings suggest that honeypots are attracting the interest of researchers as a valuable security technique that can be implemented to mitigate network attacks and provides an opportunity to learn more about the nature of these attacks. Consequently a honeypot can be used as a research tool to gather data about network attacks.
Software Defined Mobile Network enhances the user services based on network traffic, handoff latency, rate of call drops and packet re-transmissions. The principal components of Software Defined Network are configuration management, radio resource management, mobility management, network security and application services. This article proposes a Co-Operative Radial-basis Neighborhood (CORN) system for distributed mobility management in Software Defined Mobile Networks. CORN protocol creates a virtual layer of Mobile Host nodes to establish end-to-end connectivity in the network. Radial basis functions (neural network) are used to optimize the positions of mobile host nodes in CORN virtual layer. This approach reduces the control overhead, handoff latency and rate of packet loss in the network. Simulation results indicate that the proposed CORN protocol achieves an improved throughput rate with reduced latency as compared to the existing open-flow and software-defined virtual network techniques.
For the question of information security vulnerabilities discovery, the parallel vulnerabilities discovery method is given based on the CAPEC, CWE, CVE and other open source database and text mining. Firstly, we can extract the association vulnerability CWE under the same attack mode, then from CWE associated with CVE based on open source database. That can help us to analyze the potential parallel relationship of the multiple vulnerabilities. Secondly, the vulnerability description information will be vectorized, so that the software system is able to intelligent processing to vulnerability data. That is different from the query based on keyword matching, analyzes the similarity between the multiple vulnerabilities according to the threshold from the training set, and computes the parallel relationship between the multiple vulnerabilities and discovery the parallel vulnerabilities. Finally, this method is correct and effective by the experimental verification and in practice. According to this method, we are able to repair other parallel vulnerabilities when finding a vulnerability is exploited. An advantage of our method is that is applied to network defense.
Intrusion detection occupies a decision position in solving the network security problems. Support Vector Machines (SVMs) are one of the widely used intrusion detection techniques. However, the commonly used two-class SVM algorithms are facing difficulties of constructing the training dataset. That is because in many real application scenarios, normal connection records are easy to be obtained, but attack records are not so. We propose an anomaly detection model based on One-class SVM to detect network intrusions. The one-class SVM adopts only normal network connection records as the training dataset. But after being trained, it is able to recognize normal from various attacks. This just meets the requirements of the anomaly detection. Experimental results on KDDCUP99 dataset show that compared to Probabilistic Neural Network (PNN) and C-SVM, our anomaly detection model based on One-class SVM achieves higher detection rates and yields average better performance in terms of precision, recall and F-value.
Recently, sophisticated attacks are increased against specific business companies, organizations and various facilities and the attackers are trying to remove attack traces such as system logs and related information on the victim systems. Therefore, it is getting more difficult to collect the information for attack analysis. In order to overcome this situations, companies and organizations have started to collect the network traffic as secondary information for attack analysis. However, most of them are focusing on gathering the network packets. But one of the most important parts is to extract the useful information for attack analysis from the collected data. In this paper, we suggest a network forensics system, Cyber Blackbox, which is focused on the traffic analysis.
Recently, sophisticated attacks are increased against specific business companies, organizations and various facilities and the attackers are trying to remove attack traces such as system logs and related information on the victim systems. Therefore, it is getting more difficult to collect the information for attack analysis. In order to overcome this situations, companies and organizations have started to collect the network traffic as secondary information for attack analysis. However, most of them are focusing on gathering the network packets. But one of the most important parts is to extract the useful information for attack analysis from the collected data. In this paper, we suggest a network forensics system, Cyber Blackbox, which is focused on the traffic analysis.
The development of IT (Information Technology) has made access to control systems easier. However, because such advancement of control systems gave rise to many security vulnerabilities, the threat of cyber-attack is increasing as well. In order to respond these threats, we discusses the mechanism for protecting ICS (Industrial Control System) network. Most of all, since availability is the most critical factor in a control system, independent network security technology is required. In this viewpoint, this paper presents our industrial firewall system, named the IndusCAP-Gate (Industrial Cyber Attack Prevention-Gate) system, that fundamentally prevents unauthorized access to a control system. Our system applies access control filters of various levels. Most of all, the proposed system has an abnormal traffic filtering mechanism about Modbus and DNP3 protocol of the most widely used protocols in ICS network. Therefore, it facilitates the provision of security policy specific to each zone of the control system intranet.
The development of IT (Information Technology) has made access to control systems easier. However, because such advancement of control systems gave rise to many security vulnerabilities, the threat of cyber-attack is increasing as well. In order to respond these threats, we discusses the mechanism for protecting ICS (Industrial Control System) network. Most of all, since availability is the most critical factor in a control system, independent network security technology is required. In this viewpoint, this paper presents our industrial firewall system, named the IndusCAP-Gate (Industrial Cyber Attack Prevention-Gate) system, that fundamentally prevents unauthorized access to a control system. Our system applies access control filters of various levels. Most of all, the proposed system has an abnormal traffic filtering mechanism about Modbus and DNP3 protocol of the most widely used protocols in ICS network. Therefore, it facilitates the provision of security policy specific to each zone of the control system intranet.
We can apply psychological methods and behavioral science to understand the practices, techniques, processes, and skillset cyber-criminals are using nowadays for cyberattacks. We can setup honeypots to observe the techniques and methods used for attacks through logs and security settings. However, setting up a honeypot is very expensive and time consuming. So we have to work with our running systems and need to go through the logs and security settings. This way we can build a description of mind set of the cybercriminals behind the cyberattack. This description could be used as a new vector in finding infiltration method. The specific infiltration could signify a tenacious threat or just one time incident. This vector, if applied correctly, could lead to finding threats and risks relatively easily. This vector could also reduce the time required to investigate the incident. Security incident response is centered on detection, response, and resolution of the incident. Once you know the intent behind the incident, incident response becomes much easier.
As the mobile communications environment has undergone drastic changes in the wake of applicable technology development, mobile traffic is rapidly increasing around the world. To cope with such a rapid traffic increase, Korean mobile carriers chose to deploy their 4G networks early rather than upgrading the conventional 3G networks. However, as they were bent on deploying and advancing the Long Term Evolution (LTE) network faster than others, the mobile carriers did not make sufficient considerations for LTE network security. In addition, the LTE network is exposed to all security threats that can occur in any IP-based network such as falsification/alteration of information, eavesdropping, etc. as the LTE network is an all-IP based network providing data and Voice-over-LTE (VoLTE) services. This study attempts to describe the security threats associated with the tracking of location information of VoLTE phones.
Unknown protocol's hidden behavior is becoming a new challenge in network security. This paper takes the captured messages and the binary code that implement the protocol both as the studied object. Dynamic Taint Analysis combined with Static Analysis is used for protocol analyzing. Firstly, monitor and analyze the process of protocol program parses the message in the virtual platform HiddenDisc prototype system developed by ourselves, record the protocol's public behavior, then based on our proposed Hidden Behavior Perception and Mining algorithm, static analyze the protocol's hidden behavior trigger conditions and hidden behavior instruction sequences. According to the hidden behavior trigger conditions, new protocol messages with the sensitive information are generated, and the hidden behaviors are executed by dynamic triggering. HiddenDisc prototype system can sense, trigger and analysis the protocol's hidden behaviors. According to the statistical analysis results, we propose the evaluation method of Protocol Execution Security. The experimental results show that the present method can accurately mining the protocol's hidden behaviors, and can evaluate unknown protocol's execution security.
Protocol's abnormal behavior analysis is an important task in protocol reverse analysis. Traditional protocol reverse analysis focus on the protocol message format, but protocol behavior especially the abnormal behavior is rare studied. In this paper, protocol behavior is represented by the labeled behavior instruction sequences. Similar behavior instruction sequences mean the similar protocol behavior. Using our developed virtual analysis platform HiddenDisc, we can capture a variety of known or unknown protocols' behavior instruction sequences. All kinds of executed or unexecuted instruction sequences can automatic clustering by our designed instruction clustering algorithm. Thereby we can distinguish and mine the unknown protocols' potential abnormal behavior. The mined potential abnormal behavior instruction sequences are executed, monitored and analyzed on HiddenDisc to determine whether it is an abnormal behavior and the behavior nature. Using the instruction clustering algorithm, we have analyzed 1297 protocol samples, mined 193 potential abnormal instruction sequences, and determined 187 malicious abnormal behaviors by regression testing. Experimental results show that our proposed instruction clustering algorithm has high efficiency and accuracy, can effectively mine unknown protocols' abnormal behaviors, and enhance the initiative defense capability of network security.
In recent years, the web security events emerge in endlessly, web security has been widely concerned. Among these web security flaws, defects of unchecked input occupy the majority. The unchecked input defect refers to not reasonable verification of input from users and the environment, which will cause the system to be used by attackers. This paper mainly focuses on the XSS defect and SQL injection defects in Java language. XSS refers to the attacker writing malicious data to the program. When other users access to the page, the malicious data will be executed in the browser, and then get to other permissions. SQL injection refers to that an attacker constructs malicious input to access the database. This may result to changes to the database contents, or access to sensitive data in the case of unauthorized. Common defect detection methods include static analysis and dynamic analysis. The dynamic analysis process is mainly concerned with the behavior of the program, that is, the change of the program's behavior and output caused by the program's input. It is analysis of "taking the input as the center". And the static analysis process is mainly concerned with the structure of the program, that is, through the abstract program structure to analyze the procedural defects. It is "the program as the center" of the analysis. In comparison, the results of dynamic analysis are more accurate, and the static analysis results has higher false positive rate. This is because the static analysis abstracts the program information in order to ensure the efficiency and effectiveness of the program, which does reduce the load of the program, but increases the false alarm rate. In this paper, we put forward aspect-oriented dynamic analysis technique based on static analysis. Aspect-oriented dynamic analysis is a method of modular cross point. First of all, we perform static analysis for Java source code, to find the Source-sink. Then we use the AspectJ tool to carry out the aspec- -oriented dynamic analysis by tracking program.
With the increase in the number of satellites, the security time synchronization protocol between satellites and servers on the ground increasingly important. But, the time protocol that existing in space lack of security protections. Also, SEIEEE1588's author has gathered the advantage of both NTP and IEEE1588 protocol raise a security enhanced IEEE1588 protocol for deep-space environment (SEIEEE1588). The protocol just using symmetric encryption algorithm, does not consider the strong identity authentication, reasonable key update and other security measures. This paper on the basis of IEEE1588 and SEIEEE1588, research the time synchronization mechanism of IEEE1588, the security measures of SEIEEE1588 and safe data transmission protocols. At last, redesign the time synchronization protocol that contains identity authentication that use digital certificate to verify the identity of both sides, asymmetric encryption algorithm and reasonable key updates, to suitable the deep-space that is poor link quality, limited node computing power.
Trust management mechanism is a hot spot in the research of mobile Ad hoc network security. In view of the many problems of trust management mechanism in mobile hoc Ad networks, combining with the characteristics of mobile Ad hoc network, we present a mobile Ad hoc network trust management mechanism based on grey theory in this paper and apply it to the mobile Ad hoc network management in order to improve the availability and effectiveness of trust management mechanism and safeguard the security of mobile Ad hoc networks.
Routing detours raise concerns both on network security of nations and efficiency of resource consumption. Geo-optimal routing can be viewed as a long term goal of Internet routing, and a careful designed Internet together with geo-optimal routing can produce the best performance for the Internet. In this paper, we focus on some representative intercontinental circuitous paths summarized from our measurement data set. With the help of PeerDB and CAIDA, we try to find possible reasons for the detour routing and propose some suggestions to improve interconnection and routing for ISPs.
The research of trustworthiness measurement of user behavior is the hotpot in the network security. According to the existing problems of the user behavior trust evaluation method in the subjective weight and dynamic adaptability, in this paper, the calculation method of indirect credibility has been improved, and combined with the previously proposed user behavior evaluation method based on Analytic Hierarchy Process, the method of user behavior evaluation is more effectively and accurately. In this method, user behavior activity and reward and punishment factor, and the improved calculation method of indirect credibility are combined to evaluate the user's behavior, and the feasibility of the method is demonstrated by an example. The results show that the proposed method can adapt to the dynamic changes of user behavior trust, and can accurately evaluate the credibility of user behaviors.
Fiber optical network offers higher data rate and larger capacity transmission, however the security issues are becoming a big concern. Digital chaos is capable of providing a huge key space for data encryption in physical layer, to enhance network security during high-speed data transmission. We will review the recent progress on security enhancement via the implementation of digital chaos for high-speed signal encryption, especially in orthogonal frequency-division multiplexing passive optical network (OFDM-PON). As examples, we will focus on the recently-proposed two novel security schemes using chaotic signal scrambling: partial transmit sequence (PTS) and selected mapping (SLM). It has been shown that, the digital chaotic schemes not only guarantee the network data security against malicious eavesdropping or exhaustive attacks, but also simultaneously improve the transmission performance. This is due to the significantly reduction in the peak-to-average power ratio (PAPR) of the OFDM signals via PTS or SLM. Experimentally, encrypted data transmissions of 8.9Gb/s 16QAM OFDM signals are demonstrated over 20km standard singlemode fiber (SSMF) using chaotic PTS and SLM approaches.
Wireless Mesh Networks (WMNs) are being considered as most adequate for deployment in the Neighborhood Area Network (NAN) domain of the smart grid infrastructure because their features such as self-organizing, scalability and cost-efficiency complement the NAN requirements. To enhance the security of the WMNs, the key refreshment strategy for the Simultaneous Authentication of Equals (SAE) or the Efficient Mesh Security Association (EMSA) protocols is an efficient way to make the network more resilient against the cyber-attacks. However, a security vulnerability is discovered in the EMSA protocol when using the key refreshment strategy. The first message of the Mesh Key Holder Security Handshake (MKHSH) can be forged and replayed back in the next cycles of the key refreshment leading to a Denial of Service (DoS) attack. In this paper, a simple one-way hash function based scheme is proposed to prevent the unprotected message from being replayed together with an enhancement to the key refreshment scheme to improve the resilience of the MKHSH. The Protocol Composition Logic (PCL) is used to verify the logical correctness of the proposed scheme, while the Process Analysis Toolkit (PAT) is used to evaluate the security functionality against the malicious attacks.
The remarkable growth in digital data is changing what and how the defense against the unknown will take place. Big data is a technical term used today to represent this massive growth of digital data that's being created from many sources. Organizations have turned their attentions to the deployment of Big Data analytics to gain valuable insights that benefit their businesses within protected and secure environments. Hence, network security protocols, especially authentication protocols, are being re-designed to protect and to deliver the real benefits of this data growth. Contrary to the traditional perspective, in which researchers are focusing on identifying users' identity to protect Big Data-based environments, we have an opposite perspective that the Big Data itself would be the fuel for the next generation authentication. In other word, the main goal of this work is to propose a new framework for user authentication that leverages Big Data analytics. The core idea of this framework is to find out unique patterns of the users' dynamic behaviors. The proposed framework comprised of three main components. Data Security-based Analytics (DSA);describing the best utilization of the high velocity data streams, which is capable for distinguishing data that has security/identification potentials. Human dynamics measure engine;that develops an ambitious transformation from the Big Data characteristics into the relevant human dynamics measures. Big data-driven authentication service;describes the required engines to design software as a service-based authentication model. Our investigation shows that this new approach will help create a highly distributed authentication model, minimizing the storage of secrets, and lesser secret management overhead.
Network worm is a common computer virus. Because it has characteristics of spread rapidly, intelligent attack and bigger destructiveness etc., this is an important issue of how to detect the worm virus intrusion, early warning and effective defense in network security of the future. In this paper through the analysis of worm detection technology and indexes, we propose a detection algorithm based on statistical classification which using heavy-tailed abnormal detection features of worm attack traffic in "the first connection", implement the algorithm's performance analysis and comparison on the network security detection platform Bro.
Information and communication systems have several security vulnerabilities. In addition, conventional security software requires tuning effort and may not be able to detect many web attacks. For this reasons, security becomes a worldwide objective to many technological systems including Learning Management Systems (LMSs) such as Moodle. In this work, we propose a secured multi agent system (MAS) with an interceptor IIS module (IIISM) which denies malicious user access by adding a security layer to any web application including the LMS Moodle environment. The IIISM extends the IIS Server and listens to any http request targeting the web application and the LMS Moodle. While, the analysis and the design of the MAS architecture are based on the Gaia methodology, the agents were implemented using the JADE agents' middleware. This security layer overcomes the hacker attack based on the SQL injection and the cross site scripting for Moodle environment. In a first scenario, a student is requesting a course, the MAS validate this request, and the web page is displayed normally. A second scenario presents the URL and form based for the SQL injection attack by a hacker. The MAS agents detect the attack and and they respond by a denial of access to this malicious user. Moreover, this proposed security layer can be easily extended to include other security detection modules which correspond to common web vulnerabilities.
Internet traffic monitoring is a crucial task for network security. Self-similarity, a key property for a relevant description of internet traffic statistics, has already been massively and successfully involved in anomaly detection. Self-similar analysis was however so far applied either to byte or Packet count time series independently, while both signals are jointly collected and technically deeply related. The present contribution elaborates on a recently proposed multivariate self-similar model, Operator fractional Brownian Motion (OfBm), to analyze jointly self-similarity in bytes and packets. A non-linear regression procedure, based on an original Branch & Bound resolution procedure, is devised for the full identification of bivariate OfBm. The estimation performance is assessed by means of Monte Carlo simulations. Further, an Internet traffic anomaly detection procedure is proposed, that makes use of the vector of Hurst exponents underlying the OfBm based Internet data modeling. Applied to a large set of high quality and modern Internet data from the MAWI repository, proof-of-concept results in anomaly detection are detailed and discussed.
Anomaly detection, as a part of network security, is an important question, which has attracted much attention. The characteristics of data mining make it suitable for anomaly detection. Cluster analysis is a kind of data mining technology and it can divide records into different clusters, which is convenient for anomaly detection. Traditional K-manes is affected by the selection of initial centers, the number of clusters and isolated points. We combine information entropy and DD algorithm to improve K-means and use KDD CUP99 data set to analysis the performance. From twice experiences, we find that improved K-means has higher detection rate and lower false positive rate than traditional K-means.
The network security is one of the important factors that influence the development of computer network, under the background of the virtual experiment system is studied in this paper, on the Visual c + + 6.0 and Visual Studio2005 platform was designed and implemented a "network security" virtual experiment system, the system each function module in detail the design idea and implementation methods of description, also gives a design method of database.
In this paper, we concentrate on the problem of network security protection, and we proposed a novel pattern matching algorithm to tackle this problem. Particularly, we focus on the problem SQL Injection, which is a typical network security problem, and it means a type of web application security vulnerability. Afterwards, a typical pattern matching based network security system is illustrated. Additionally, we define that a pattern is a set of specific ternary strings in the traffic which illustrates the probability of potential intrusion or abnormality of the network. Furthermore, a novel pattern matching algorithm is given. In the end, to demonstrate the effectiveness of our method, experiments are conducted to maker performance.
As currently classical malware detection methods based on signatures fail to detect new malware, they are not always efficient with new obfuscation techniques. Besides, new malware is easily created and old malware can be recoded to produce new one. Therefore, classical Antivirus becomes consistently less effective in dealing with those new threats. Also malware gets hand tailored to bypass network security and Antivirus. But as analysts do not have enough time to dissect suspected malware by hand, automated approaches have been developed. To cope with the mass of new malware, statistical and machine learning methods proved to be a good approach classilying programs, especially when using multiple approaches together to provide a likelihood of software being malicious. In normal approach, some steps have been taken, mostly by analyzing the opcodes or mnemonics of disassembly and their distribution. In this paper, we focus on the control flow change (CFC) itself and finding out if it is significant to detect malware. In the scope of this work only relative control flow changes are contemplated, as these are easier to extract from the first chosen disassembler library and are within a range of 256 addresses. These features are analyzed as a raw feature, as n-grams of length 2, 4 and 6 and the even more abstract feature of the occurrences of the n-grams is used. Statistical methods were used as well as the Na??ve-Bayes algorithm to find out if there is significant data in CFC. We also test our approach with real-world datasets.
We investigate dynamic public good games in networks consisting of strategic users with interdependent network security. The strategic users can choose their investment strategies to contribute to the basic security of the network. Mimicking the behavior of infection propagation over multi-hop networks which depends on the average degree of the network, we propose a mean-field-type model to capture the effect of the others' control actions on the security state. Using linear-quadratic differential mean-field-type games we propose and analyze two different regimes, examining the equilibria and global optima of each to address. We show that, generically, each user has a unique best response strategy to invest into security. Closed-form expressions are obtained using the recent development of mean-field-type game theory.
Despite recent emerging development in intrusion detection or network monitoring, malicious attacks and misbehavior remain a high-risk issue within network traffic. In this paper, we present a proactive solution called MMT (Montimage1 Monitoring Tool) that allows facilitating network security and performance monitoring and operation troubleshooting. We demonstrate the improvements of MMT in comparison with other similar tools. Especially, we assess MMT to deal with a practical case-study in which we analyze the User-Agent field in HTTP headers to determine abnormal activities. Indeed, novel observations figure out the usefulness of the User-Agent field in HTTP requests as a good source to facilitate abnormal activities detection within an abundant traffic. There are eventually several researches alarming the vulnerabilities of the User-Agent field and proposing some manual solution including a combination of tools. However, existing countermeasures are rather passive and do not allow real-time detection. In the context of our research, MMT provides an automated detection of malicious traffic abusing vulnerable User-Agent field. Analyzing abnormal User-Agent strings is also useful to rapidly detect existing evil objects in the network (e.g., bots). The experimental results confirm the improvements of our implementation in comparison with other intrusion detection system (SNORT) and packet analyzing tool (TCPdump).
Hazard and Operability studies have been extensively used in chemical engineering and designing safety critical systems. Its rigorous analysis based on discovering deviations and hazard makes it ideal in the study of designs and experiments with confounding variables. In this paper, HAZOP methodology is applied to a case study of network security experiment to reliably measure the IP tracking behavior of malicious websites using a low interaction client honeypot. The experiment's design involves a large number of factors and components which could potentially introduce bias in the study and result in invalid analysis. We demonstrate that HAZOP can be applied to security experiments to create a proper experimental design and properly control potential bias of confounding variables.
Network security situational awareness is a new technology to solve the problem of single defense in recent years, and situation assessment is the most critical step in situational awareness. Because only in real-time and accurately evaluate the security situation of the current network, we can take more targeted defensive measures. This paper aims to improve timeliness and accuracy of the evaluation results. In the network security situation assessment method based on HMM, the establishment of time segment size to extract the observed value and the parameters of the model is an important factor, which affects the real-time performance and accuracy of the evaluation. Currently, in most cases time segment size is given by human at random, which cannot achieve equilibrium in efficient characterization of network security and real-time. Moreover, state transfer matrix and observation symbol matrix is often determined empirically, with a strong subjectivity. In order to solve the above problems, this article utilizes sliding time window mechanism to extract the observed value and hybrid multi-population genetic algorithm(MPGA) to train the HMM model parameters, so as to improve the reliability of parameters. Experiments show that this method can effectively and accurately reflect the current network safety status.
??? RF continues UltraSPARC CMT tradition of providing on-chip accelerators ??? RF includes Sun's 3rd generation on-chip security accelerator ??? RF's accelerator introduces > Additional ciphers, chaining modes and secure hashes > Non-priv fast-path to accelerators ??? Fast-path eliminates vast majority of overheads associated with offloads > Allows direct interaction between non-priv applications and the accelerators > Improves small object performance by up to 30X ??? RF provides additional non-priv crypto instructions to help accelerate authenticated-encryption operations ??? RF builds on the successes of the UltraSPARC T2 and significantly expands the application space which can benefit from the accelerators
In this paper, we present adaptive neural network tracking control of a robotic manipulator with input deadzone and output constraint. A barrier Lyapunov function is employed to deal with the output constraints. Adaptive neural networks are used to approximate the deadzone function and the unknown model of the robotic manipulator. Both full state feedback control and output feedback control are considered in this paper. For the output feedback control, the high gain observer is used to estimate unmeasurable states. With the proposed control, the output constraints are not violated, and all the signals of the closed loop system are semi-globally uniformly bounded. The performance of the proposed control is illustrated through simulations.
To predict network security situations better using expert knowledge and quantitative data, a new forecasting model known as cloud belief rule base (CBRB) model is proposed. The CBRB model utilizes the cloud model to describe the referential point of belief rule, which is more accurate for describing expert knowledge. Moreover, to achieve the optimal parameters of the proposed model, a constraint covariance matrix adaptation evolution strategy (CMA-ES) algorithm is presented in this letter. A case study for network security situation prediction is conducted with CBRB and CMA-ES. The experimental results demonstrate the effectiveness and practicality of the proposed CBRB model.
With the rapid development of mobile Internet, people pay increasing attention to the wireless network security problem. But due to the specificity of the wireless network, at present it is rare to see the research of wireless intrusion alerts clustering method for mobile Internet. This paper proposes a Wireless Intrusion Alert Clustering Method (WIACM) based on the information of the mobile terminal. The method includes alert formatting, alert reduction and alert classification. By introducing key information of the mobile terminal device, this method aggregates the original alerts into hyper alerts. The experimental results show that WIACM would be appropriate for real attack scenarios of mobile Internet, and reduce the amount of alerts with more accuracy of alert analysis.
In this paper, we propose a non-cooperative differential game theory based resource allocation approach for the network security risk assessment. For the risk assessment, the resource will be used for risk assess, including response cost and response negative cost. The whole assessment process is considered as a differential game for optimal resource control. The proposed scheme can be obtained through the Nash Equilibrium. It is proved that the game theory based algorithm is applicable and the optimal resource level can be achieved based on the proposed algorithm.
Energy storage can provide services to several sectors in electricity industry, including generation, transmission and distribution, where conflicts and synergies may arise when storage is used to manage network congestion and provide services in energy and balancing markets. In this context, this study proposes an optimisation model to coordinate multiple services delivered to various market participants that uses corrective actions to resolve conflicts between provision of distribution network services (e.g. congestion and security of supply) and other services. The model maximises storage profit by scheduling active and reactive power to provide portfolio of services including distribution network congestion management, energy price arbitrage, frequency response and reserve services remunerated at different prices. The authors demonstrate that adopting corrective security to provide network services and deal with network congestion in a post-fault fashion, is overall more beneficial despite the energy needed to be stored during pre-fault conditions for applying post-contingency actions right after a network fault occurs. Furthermore, the authors' analysis shows that application of corrective security can benefit both (i) storage owners through increased revenues in energy and balancing services markets and (ii) Distribution Network Operators through reduction in payments to storage owners and increased utilisation of network infrastructure.
Hackers have compromised the designs of numerous major US weapon systems. Safeguarding mission-critical systems requires effective network security and secure firmware and software. To achieve this, the US Defense Department should carefully screen contractors based on their past cybersecurity prowess and provide incentives for them to produce and maintain secure systems.
Nowadays, distributed denial of service (DDoS) becomes a major challenge in the network as it affects the network at multi-level. This leads to traffic overhead and wastage of bandwidth utilisation. In order to overcome these issues, ant-based DDoS detection technique using roaming virtual honeypots is proposed. In this technique, virtual roaming honeypot along with the multi-level secure architecture is used to collect the information about the various intruders at different levels in the network. Ant colony optimisation technique is used to detect the intruders based on the pheromone deposit on that considered area. A multi-level IP log table is used to detect the intruders at different levels of the network. Once the affected area is found, the information is sent to multi-level architecture to limit the spread of the affected area to the honeypot. This information is sent to the honeypot to make a defence system against the attackers. The advantage of the proposed technique is that it provides a full defence against DDoS at multi-level without creating any traffic overhead.
The primary quench detection system of the Korea Superconducting Tokamak Advanced Research (KSTAR) device has been operated in technical and plasma experiments for eight years. Cowound-type quench detectors have exhibited distinct performance of compensation for both self-induction and mutual induction;on the other hand, some cowound strips tended to break in the long-term operation. In contrast, Wheatstone bridge (WB)-type quench detectors had no wire breakage, and their signal wires are still accessible and repairable. Therefore, WB-type quench detectors are very important to be a backup method of quench detection for the coils. New WBs using a half-bridge at a fixed ratio of resistances of 1:1 were tested in the KSTAR operation. Upperlower comparison of the new WBs performed good compensation for induced voltage. The new WBs clearly demonstrated that the bridge circuits may become very simple, and in situ calibration may be unnecessary if the arrangement of voltage taps and quench detectors is optimized with respect to the magnetic field of the tokamak device.
Summary form only given. Use of internet based application in modern society is growing exponentially. Rapid development of service automation, social networking and connected devices continues to expand the human activities in cyber space resulting in vast quantities of security-critical data being accessible over internet. This has in turn caused an explosion of illegal activities such as unauthorized data access, data theft, data modification, fraud and various other intrusion activities. To mitigate such risks, intrusion detection and response techniques are becoming increasingly critical. We will explore the state of the art in advanced intrusion detection as well as intrusion response processes that aim to provide real time protection to current computer systems and networks.
Considering that the traditional gateway can't meet the requirements of the increasingly strict network security management, here we propose a design of security gateway based on firewall technology. We realize the real-time, dynamic and fine-grained control of the internal and external network of the gateway. This article introduces a method for gateway to be built, and it's based on the firewall in Linux system. I use both the rule priority and heap sort method to optimize the efficiency of the gateway. It can be proved that the gateway based on heap sort and rule priority evaluation method can effectively reduce the time to solve the exception rule and improve the efficiency of abnormal rules.
The value of software is proportional to the value of quality. Quality is one of the elements of security. There are two basic concepts that basically contains the security elements it contains. When tested in terms of security software are basically just white-box testing can be used to test the internal structure and may only be used in the user's black box testing. This concept is the concept of testing basic concepts contains many sub-types. This concept is also one of the sub-encryption package, gives an idea of the overall structure of the system's security over the information it contains. In this study, the protocols contained in encryption package was addressed to the differences in the protocols.
Computer network in work and life bring convenience for people at the same time, also inevitable has brought some safe hidden trouble. To bring the positive role of efficient use of computer network, you need to effectively safeguard the security of computer network information. This article mainly analyzes the present situation of computer network security problems, analyzes the main source of computer security threats, and puts forward some effective network security defense technology, to maximize the guarantee the safety of computer network information to provide reliable guarantee.
In this paper, we aim to solve the problem of neural network in computer network security assessment, which is very important for computer network's popularization. Our proposed computer network security assessment system contains client and server. The client module includes: 1) scanning configuration model, 2) assessment model, 3) scanning result database model and 4) output model. Furthermore, server is made up of 1) scanning engine, 2) vulnerability database, 3) rules database. To promote the performance of artificial neural network, we choose the back propagation neural network, and particle swarm optimization is utilized to optimize parameters. Finally, experimental results demonstrate the effectiveness of our proposed approach.
In symmetric key cryptography, there exists an issue of secure exchange of keys over the insecure network. Threats occur due to presence of adversary in the network or due to infiltration of personal security. The problem of network security is addressed by LHX-3PAKE protocol as it is proved to be free from various well-known attacks. But this protocol also poses a disadvantage of numerous complex and time-consuming modular exponential calculations. As the problem of network security is already taken into account by LHX-3PAKE, only the problem of personal security persists. If this issue can be solved, the session key can be reused, thus making the whole process of key establishment faster.
In this paper, we use GSM mobile network that is the most reliable and matured in this day to achieve community alarm system. And it will directly convey the alarm news in short message or telephone at manager's mobile phone. The design includes hardware and software parts. Its hardware uses MCU AT89C52 series as the core, and controls three alarm parts, they are infrared monitoring alarm system, alarm displayed on LCD, and GSM module. The system adopted passive infrared sensor to detect, and it turned the traditional network security and anti-theft window into the invisible network, so as to timely handle accidents. When the accident happened in the community, the infrared sensor module sends out the alarm signal to the micro-controller, single chip microcomputer receives the signal through the GSM module to send the message immediately to the manager, and then managers to take immediate measures, the site security alarm and remote alarm can be achieved simultaneously.
This paper analyzed the existing network security situation evaluation methods and discovered that they can't accurately reflect the features of large-scale, synergetic, multi-stage gradually shown by network attack behaviors. For this purpose, under deep analyzing the association between attack intention and network configuration information, a network security situation evaluation method based on attack intention recognition was proposed. Unlike traditional method, the evaluation method was based on intruder. This method firstly made causal analysis of attack event and discovered intrusion path to recognize every attack stages, then realized situation evaluation based on the attack stages. Lastly the method recognized the attack intention of next stage to forecast network security situation based on achieved attack stages, combined with vulnerability and network connectivity. A simulation experiments for the proposed network security situation evaluation model is performed by network examples. The experimental results show that this method is more accurate on reflecting the truth of attack. And the method does not need training on the historical sequence, so the method is more effective on situation forecasting.
In the field of network security, vulnerability evaluation is a very important method to assess the attack and defense means in many practical use, such as penetration testing and safety pre-warning. Up to now, there are a lot of vulnerability evaluate methods, such as CWE, CVSS, and there are a lot of basic evaluate methods for further improvement and optimization. This paper analyzes the existing vulnerability evaluate method and has found some insufficient changes in real-time environmental. This paper adds some new elements including topology environment factors, as well as log system information statistics, to make sure vulnerability evaluation can be used in a more flexible network security.
With the rapid development of the information technology, more and more high-speed networks came out. The 4G LTE network as a recently emerging network has gradually entered the mainstream of the communication network. This paper proposed an effective content-based information filtering based on the 4G LTE high-speed network by combing the content-based filter and traditional simple filter. Firstly, raw information is pre-processed by five-tuple filter. Secondly, we determine the topics and character of the source data by key nearest neighbor text classification after minimum-risk Bayesian classification. Finally, the improved AdaBoost algorithm achieves the four-level content-based information filtering. The experiments reveal that the effective information filtering method can be applied to the network security, big data analysis and other fields. It has high research value and market value.
Due to the growing advancement of crime ware services, the computer and network security becomes a crucial issue. Detecting sensitive data exfiltration is a principal component of each information protection strategy. In this research, a Multi-Level Data Exfiltration Detection (MLDED) system that can handle different types of insider data leakage threats with staircase difficulty levels and their implications for the organization environment has been proposed, implemented and tested. The proposed system detects exfiltration of data outside an organization information system, where the main goal is to use the detection results of a MLDED system for digital forensic purposes. MLDED system consists of three major levels Hashing, Keywords Extraction and Labeling. However, it is considered only for certain type of documents such as plain ASCII text and PDF files. In response to the challenging issue of identifying insider threats, a forensic readiness data exfiltration system is designed that is capable of detecting and identifying sensitive information leaks. The results show that the proposed system has an overall detection accuracy of 98.93%.
In this article the authors give a general idea of software-defined networking (SDN). The paper contains a description of Floodlight SDN controller and an issue of network security management. The authors present a set of techniques that provide more security in the controller. They are implemented as a network application and describe the basic mechanism.
This paper describes ongoing study and first results on the application of Neuro-Fuzzy (NF) to support large-scale forensics investigation in the domain of Network Forensics. In particular we focus on patterns of benign and malicious activity that can be find in network traffic dumps. We propose several improvements to the NF algorithm that results in proper handling of large-scale datasets, significantly reduces number of rules and yields a decreased complexity of the classification model. This includes better automated extraction of rules parameters as well as bootstrap aggregation for generalization. Experimental results show that such optimization gives a smaller number of rules, while the accuracy increases in comparison to existing approaches. In particular, it showed an accuracy of 98% when using only 39 rules. In our research we contribute to forensics science by increasing awareness and bringing more comprehensive fuzzy rules. During the last decade many cases related to network forensics resulted in data that can be related to Big Data due to its complexity. Application of Soft Computing methods, such that Neuro-Fuzzy may bring not only sufficient classification accuracy of normal and attack traffic, yet also facilitate in understanding traffic properties and developing a decision-support mechanism.
Over the past two decades Wireless Sensor Networks (WSNs) and their applications have been the topic of many studies. WSN is a network responsible for collecting, processing and distributing wireless data to the intended database storage center. Because these sensors are usually installed at remote sites, despite the recent advances in the WSN technology, its applications still face significant challenges. Out of these, network security threats, network architecture, data collection, deployment and network coverage rise as the major concerns. Therefore, these five issues have been well studied in the literature. The main purpose of this paper is to detail these issues and elaborate on possible solutions.
The security risks and threats that impact wired and wireless networks are now applicable to mobile telecommunication networks. Threat detection systems should be more intelligent because threats are becoming more dangerous. An intrusion detection system (IDS) is a potential network security solution for protecting the confidentiality, integrity, and availability of user data and information resources. A fast and effective IDS for mobile networks that does not violate the user's privacy or the network's QoS is required. This paper offers a set of flow-based features that can be utilized for mobile network traffic as a prerequisite for a privacy-aware and QoS-robust IDS. The principal component analysis (PCA) method was used for reduction of the features. Twelve features in six groups, which represent the user data in mobile traffic, were extracted and evaluated for IDSs. The evaluation process achieved a F-measure weighted average equal to 0.834, and the experimental time was equal to 12.9 seconds. The accomplished measurements have demonstrated the applicability of the proposed set of features.
Network structures and human behaviors are considered as two important factors in virus defense currently. However, due to ignorance of network security, normal users usually take simple activities, such as reinstalling computer system, or using the computer recovery system to clear virus. How system recovery influences virus spreading is not taken into consideration currently. In this paper, a new virus propagation model considering the system recovery is proposed first, and then in its steady-state analysis, the virus propagation steady time and steady states are deduced. Experiment results show that models considering system recovery can effectively restrain virus propagation. Furthermore, algorithm with system recovery in BA scale-free network is proposed. Simulation result turns out that target immunization strategy with system recovery works better than traditional ones in BA network.
A smart grid is delay sensitive and requires the techniques that can identify and react on the abnormal changes (i.e., system fault, attacker, shortcut, etc.) in a timely manner. In this paper, we propose a real-time detection scheme against false data injection attack in smart grid networks. Unlike the classical detection test, the proposed algorithm is able to tackle the unknown parameters with low complexity and process multiple measurements at once, leading to a shorter decision time and a better detection accuracy. The objective is to detect the adversary as quickly as possible while satisfying certain detection error constraints. A Markov-chain-based analytical model is constructed to systematically analyze the proposed scheme. With the analytical model, we are able to configure the system parameters for guaranteed performance in terms of false alarm rate, average detection delay, and missed detection ratio under a detection delay constraint. The simulations are conducted with MATPOWER 4.0 package for different IEEE test systems.
Mobile ad hoc networks (MANETs) are wireless networks that have a wide range of applications because of their dynamic topologies and ease of deployment. Owing to the independent and dynamic nature of mobile nodes, the topology of a MANET often changes and is prone to various attacks. Therefore, substantial research in the area of security is required. Certificate revocation is an effective mechanism for providing network security services. However, the existing schemes are not well suited to MANETs because of their considerable overhead or low accuracy with respect to certificate revocation. In this study, the authors investigate a distributed certificate revocation protocol. On the basis of the game-theoretic model, they design a new voting-based security scheme. Their game-based security paradigm can provide the ability to practically respond to the current system conditions and is suitable for real MANET operations. Simulation results demonstrate the effectiveness and the efficiency of their scheme with respect to certificate revocation. Finally, they discuss the results of an evaluation provide an outlook on the future work in this field.
Securing the networks of large organizations is technically challenging due to the complex configurations and constraints. Managing these networks requires rigorous and comprehensive analysis tools. A network administrator needs to identify vulnerable configurations, as well as tools for hardening the networks. Such networks usually have dynamic and fluidic structures, thus one may have incomplete information about the connectivity and availability of hosts. In this paper, we address the problem of statically performing a rigorous assessment of a set of network security defense strategies with the goal of reducing the probability of a successful large-scale attack in a dynamically changing and complex network architecture. We describe a probabilistic graph model and algorithms for analyzing the security of complex networks with the ultimate goal of reducing the probability of successful attacks. Our model naturally utilizes a scalable state-of-the-art optimization technique called sequential linear programming that is extensively applied and studied in various engineering problems. In comparison to related solutions on attack graphs, our probabilistic model provides mechanisms for expressing uncertainties in network configurations, which is not reported elsewhere. We have performed comprehensive experimental validation with real-world network configuration data of a sizable organization.
Intrusion response is a critical part of security protection. Compared with IT systems, industrial automation systems (IASs) have greater timeliness and availability demands. Real-time security policy enforcement of intrusion response is a challenge facing intrusion response for IASs. Inappropriate enforcement of the security policy can influence normal operation of the control system, and the loss caused by this security policy may even exceed that caused by cyberattacks. However, existing research about intrusion response focuses on security policy decisions and ignores security policy execution. This paper proposes a general, real-time control approach based on table-driven scheduling of intrusion response in IASs to address the problem of security policy execution. Security policy consists of a security service group, with each type of security service supported by a realization task set. Realization tasks from several task sets can be combined to form a response task set. In the proposed approach, first, a response task set is generated by a nondominated sorting genetic algorithm (GA) II with joint consideration of security performance and cost. Then, the system is reconfigured through an integrated scheduling scheme where system tasks and response tasks are mapped and scheduled together based on a GA. Furthermore, results from both numerical simulations and a real-application simulation show that the proposed method can implement the security policy in time with little effect on the system.
In the Software-defined networking (SDN), when multiple control domains are used in control services, the transient state problem can occur causing the service flow interruption when border switches are updated asynchronously. We analyze the uniqueness of this problem for SDN, and propose a light-weight protocol for safe reconfiguration of the border switches in order to improve the availability of the services running in SDN. Our solution is designed as a generic supervision layer added in the control plane to support different types of services. To demonstrate the benefits, we implement a prototype of Information-centric networks (ICN) with the protocol, and conduct experiments using PlanetLab. The results show the ICN service can continuously serve high volume requests for contents despite the congestions built by the heavy background traffic. The performance gains in terms of the mean and standard deviation of the content retrieve delay are 40.5% and 21.56%.
The rapid proliferation of personal wearable as well as embedded devices point to the emergence of networks of unprecedented size and complexity in the near future. Unfortunately, traditional network security solutions fall short of addressing the unique security requirements of the emerging environment given their general emphasis on administratively managed, preconfigured security context and strong physical security mechanisms. To cope with the security challenges of this emerging environment, novel cognitive-inspired security architectures have been proposed that emphasize dynamic, autonomous trust management. Cognitive security systems take advantage of sensing and computing capabilities of smart devices to analyze raw sensor data and apply machine learning techniques to make security decisions. In this article, we present a canonical representation of cognitive security architectures and examine the practicality of using these architectures to address the security challenges of rapidly growing networks of mobile/embedded autonomous devices including the ability to identify threats simply based on symptoms, without necessarily understanding attack methods. Using authentication as the main focus, we introduce our canonical representation and define various categories of contextual information commonly used by cognitive security architectures to handle authentication requirements, and highlight key advantages and disadvantages of each category. We then examine three grand challenges facing the cognitive security research including the tension between automation and security, the unintended consequences of using machine learning techniques as a basis for making security decisions, and the revocation problem in the context of cognitive security. We conclude by offering some insight into solution approaches to these challenges.
This article consists of a collection of slides from the author's conference presentation on hardware security. Some of the specific topics discussed include: examples of using Moving Target Defense for Secure Hardware design (DHS/AFRL project);system performance evaluations;network security considerations;secure processors;dynamic information flow tracking;software-hardware security verification;security measurement techniques;types of security attacks;supply chain security options;hardware trojans;security CAD tools;trust management issues;cyber-physical systems;and secure software design, new problems and solutions.
The storage of large amounts of network data is a challenging problem, in particular if it still needs to be actively consulted as for example in the case of network forensics. Here we propose a method to compress NetFlow data while simultaneously adding domain knowledge. Our method is based on a pattern classification scheme by considering all flows from a single source IP address simultaneously. Each pattern can be described by at most 19 attributes that give a good statistical description of the original NetFlow data, while minimising information loss. We estimate that on average a factor of about 300 in storage space can be gained. The process is explained using a real world dataset from a large, high-speed, network, and a formal rationale is provided.
The centralised control provided by Software- Defined Networking allows an increase in network security as all traffic can be vetted before leaving the attachment switch. Nevertheless, as in any complex system, there are implementation and policy compromises which lead to security vulnerabilities. This paper exploits such vulnerabilities to implement a suite of attacks, consisting of Address Resolution Protocol (ARP) cache poisoning, Man in the Middle, a firewall and access control bypassing port scan called a Phantom Host Scan, and a Distributed Denial of Service attack called a Phantom Storm which induces the participation of legitimate hosts. These attacks were successfully implemented in a Floodlight controlled network.
Software Defined Networking (SDN) is new networking architecture with the centralized control mechanism. SDN has been proven successful in improving not only network performance but also network security. However, the centralized control in SDN architecture incurs new security vulnerability. In particular, the UDP flooding attack can be easily launched and causes serious packet transmission delay, the performance loss on the controller and therefore the network shutdown. Hence, in this study, we consider the UDP flooding attack in SDN, propose a lightweight countermeasure, and confirm the performance and security of the proposed method via extensive experiments.
The nature of wireless network transmission and the emerging attacks are continuously creating or exploiting more vulnerabilities. Despite the fact that the security mechanisms and protocols are constantly upgraded and enhanced, the Small Office/Home Office (SOHO) environments that cannot afford a separate authentication system, and generally adopt the IEEE 802.11 Wi-Fi-Protected-Access-2/Pre-Shared-Key (WPA2-PSK) are still exposed to some attack categories such as de-authentication attacks that aim to push wireless client to re-authenticate to the Access Point (AP) and try to capture the keys exchanged during the handshake to compromise the network security. This kind of attack is impossible to detect or prevent in spite of having an Intrusion Detection and Prevention System (IDPS) installed on the client or on the AP, especially when the attack is not repetitive and is targeting only one client. This paper proposes a novel method which can mitigate and eliminate the risk of exposing the PSK to be captured during the re-authentication process by introducing a novel re-authentication protocol relying on an enhanced four-way handshake which does not require any hardware upgrade or heavy-weight cryptography affecting the network flexibility and performances.
Cognitive radio networks are a method for improving wireless communications with radios that can adapt their operating parameters to cope with changes in their environment. This flexibility enables radios to perform with greater throughput, robustness, and field updatability than traditional wireless communication devices;unfortunately, it also presents several security challenges unique to cognitive radio networks and new angles for traditional attacks. These threats can be classified into radio configurability and cognitive radio behavioral vulnerabilities. In this paper we explore specific threats to cognitive radio networks and their countermeasures from literature. With this paper, readers can have a thorough understanding of cognitive radio network security and the research trends in this area.
In this paper, we describe a process that has been developed to transfer network intrusion data captured by Fail2ban to an adaptive enterprise intrusion detection and prevention system. The process involves software agents that we have created that are interconnected to a central behavior analysis database service where each software agent records attack meta-information collected during previous intrusion attempts. These distributed agents are the first phase of an overall plan to create a smarter network defense system through the collection and analysis of network signatures generated by real security threats. The central database to which the agents report warehouses and analyzes the meta-information collected by the interconnected agents. The agents can then utilize both instantaneous and historical data by integrating rules derived from the data collection and analysis process into intrusion prevention policies. The final result will be a modular and scalable network defense system that should be more responsive and adaptable to imminent threats.
reCAPTCHA is a popular technique used for web security. It distinguishes humans from computer programs to protect websites from automated abuse by presenting a challenge to be solved. reCAPTCHA utilizes human computation power used in solving these challenges to aid in increasing the accuracy of digitizing printed manuscripts. Although reCAPTCHA has been developed in many languages, to date, there is no available reCAPTCHA for the Arabic language. There is an immense need for the development of an Arabic reCAPTCHA service to increase the accuracy that existing OCRs currently lacks in digitizing Arabic manuscripts, and to aid in securing millions of Arabic websites. In this paper, we present a cloud-based design and architecture for an Arabic reCAPTCHA service, and we detail and describe key design and system components which include word classification algorithms, database schema design, and technology selection. We also study the inadequacy of existing popular OCRs in recognizing correctly Arabic printed text.
Effective web security practices are key to the success of the Semantic Web. Security measures of authorization, integrity and privacy are to be catered for storage and maintenance of data on the web. Ontology is being highly recommended for security of web services. Many security parameters are being embedded in OWL-S. Security models mapped to ontology seem to be very effective. This paper demonstrates ontology based intrusion detection system for web application security. Context capture of information from links and scripts is the premise of the proposed system. The proposed IDS ontological model detects protocol specific attacks as well identifies malicious scripts. This model identifies types of attacks and vulnerabilities therein. A TCP dump of data on LAN was acquired and evaluated against KDD99 intrusion detection predictor model dataset. Ontology model was designed using Prot??g??. Our ontology model establishes semantic relationship between attacks and networks. The experimental results showed our model has improved detection rate and low rate of false positives.
The emerging trend now in network security is lightweight cryptography which is the need of the hour due to the growth of wireless technology. As the internet technology grows day by day identity authentication in wireless network is more important. Generally, these wireless systems are in demand of security and resource (power). In order to meet these constraints an important lightweight scheme called Remote user authentication with signcryption is proposed with security features such as confidentiality, integrity (originality of sender and receiver), message and user authentication, non-repudiation, forward secrecy and public verifiability. Signcryption is a new paradigm that fulfils the functions of signature and encryption in one logical step and therefore the cost in terms of computation is reduced compared to the scheme signature-then-encryption. Till today many Remote user authentication schemes based on Elliptic curve, El-Gamal and RSA have been proposed. This paper points out the limitations of existing scheme such as lack in security aspects for example, password guessing attack, impersonation attack, forward secrecy, computation overhead, larger memory requirements. The proposed scheme based on signcryption using hyper elliptic curve that fulfils all the gaps of existing system.
This paper address to the problem of web security and provide solution to enjoy secure web browsing facility. We have developed the browser extension ???ClickProtect??? which provide a generic solution to multiple Clickjacking attacks. ClickProtect will run on the browser while user is surfing on the internet. It is envisioned to detect clickjacking attacks by warning the user before proceeding with the unsafe actions. Here along with pop-up warning message the unsafe, hidden web components are made visible. This will help to reduce the chances of user information getting stolen, and thereby lead to secure browsing.
This paper discusses the usage of Deception as a strategy in network security and development of Java based honeypot that is named as ???Maya???. For network deception, intruders are deliberately provided with hosts having one or more vulnerabilities. Honeypot is such a network deception tool that provides illusion to the attackers. Basic purpose of Honeypot is to get compromised in order to gather information about intruders and their attack methods. It could also be used to lure attacker and divert him from the actual network. ???Maya??? honeypot is Java based deception tool having emulated services (FTP etc), a Rule & Anomaly based intrusion detection engine and a web based administration cum monitoring tool.
It is well known that not all intrusions can be prevented and additional lines of defense are needed to deal with intruders. However, most current approaches use honeynets relying on the assumption that simply attracting intruders into honeypots would thwart the attack. In this paper, we propose a different and more realistic approach, which aims at delaying intrusions, so as to control the probability that an intruder will reach a certain goal within a specified amount of time. Our method relies on analyzing a graphical representation of the computer network's logical layout and an associated probabilistic model of the adversary's behavior. We then artificially modify this representation by adding ???distraction clusters??? ??? collections of interconnected virtual machines ??? at key points of the network in order to increase complexity for the intruders and delay the intrusion. We study this problem formally, showing it to be NP-hard and then provide an approximation algorithm that exhibits several useful properties. Finally, we present experimental results obtained on a prototypal implementation of the proposed framework.
More and more networks and services are reachable via IPv6 and the interest for security monitoring of these IPv6 networks is increasing. Honeypots are valuable tools to monitor and analyse network attacks. HoneydV6 is a low-interaction honeypot which is well suited to deal with the large IPv6 address space, since it is capable of simulating a large number of virtual hosts on a single machine. This paper presents an extension for HoneydV6 which allows the detection, extraction and analyses of shellcode contained in IPv6 network attacks. The shellcode detection is based on the open source library libemu and combined with the online malware analysis tool Anubis. We compared the shellcode detection rate of HoneydV6 and Dionaea. While HoneydV6 is able to detect about 25 % of the malicious samples, the Dionaea honeypot detects only about 6 %.
Spam has been infesting our emails and Web experience for decades;distributing phishing scams, adult/dating scams, rogue security software, ransomware, money laundering and banking scams??? the list goes on. Fortunately, in the last few years, user awareness has increased and email spam filters have become more effective, catching over 99% of spam. The downside is that spammers are constantly changing their techniques as well as looking for new target platforms and means of delivery, and as the world is going mobile so too are the spammers. Indeed, mobile messaging spam has become a real problem and is steadily increasing year-over-year. We have been analyzing SMS spam data from a large US carrier for over six months, and we have observed all these threats, and more, indiscriminately targeting large numbers of subscribers. In this paper, we touch on such questions as what is driving SMS spam, how do the spammers operate, what are their activity patterns and how have they evolved over time. We also discuss what types of challenges SMS spam has created in terms of filtering, as well as security.
Many people have legitimate needs to avoid their online activities being tracked and linked to their real-world identities ??? from citizens of authoritarian regimes, to everyday victims of domestic abuse or law enforcement officers investigating organized crime. Current state-of-the-art anonymous communication systems are based on onion routing, an approach effective against localized adversaries with a limited ability to monitor or tamper with network traffic. In an environment of increasingly powerful and all-seeing state-level adversaries, however, onion routing is showing cracks, and may not offer reliable security for much longer. All current anonymity systems are vulnerable in varying degrees to five major classes of attacks: global passive traffic analysis, active attacks, ???denial-of-security??? or DoSec attacks, intersection attacks, and software exploits. Achieving tracking resistance in the future Internet will require solving the grand challenges presented by these classes of attacks. The Dissent project is prototyping a next-generation anonymity system representing a ground-up redesign of current approaches. Dissent is the first anonymity and pseudonymity architecture incorporating protection against the five major classes of known attacks. By switching from onion routing to alternate anonymity primitives offering provable resistance to traffic analysis, Dissent makes anonymity possible even against an adversary who can monitor most, or all, network communication. A collective control plane renders a group of participants in an online community indistinguishable even if an adversary interferes actively, such as by delaying messages or forcing users offline. Protocol-level accountability enables groups to identify and expel misbehaving nodes, preserving availability, and preventing adversaries from using denial-of-service attacks to weaken anonymity. The system computes anonymity metrics that give users realistic indicators of anonymity protection, even aga- nst adversaries capable of long-term intersection and statistical disclosure attacks, and gives users control over tradeoffs between anonymity loss and communication responsiveness. Finally, virtual machine isolation offers anonymity protection against browser software exploits of the kind recently employed to de-anonymize Tor users. Dissent is still an early proof-of-concept with many limitations and missing pieces, but we hope it serves to illustrate directions in which solutions to the grand challenges of online anonymity might be found.
Web applications become an important part for Communication now days. As the popularity of the web application increases like online transaction, net banking and many more, the role of web security has been increase as well. Web applications vulnerabilities let attackers to carry out malicious activities that range from gaining unauthorized access or stealing the sensitive data. Past research have shown a significant increase in the number of web application vulnerabilities which is still growing constantly. Vulnerability scanner is a tool used for verify whether web applications are vulnerable or not when they are subjected to improper input validation. Even though there are number of tools available for web application vulnerability still latest attacks (like attacks occur in HTML5) are hard to find. Web application scanner is a security tool designed to find out security holes in your web applications that an attacker can access to your whole system and data for malicious purpose. These tools used to find the multiple vulnerabilities including SQL injection, cross site scripting etc. This paper demonstrates how easy it is for attackers to automatically discover and exploit web application-level vulnerabilities in a large number of web applications. Using this research paper researcher can examine how vulnerability scanner work and components to implement any vulnerability scanner for improvement of web application security. This approach allows researcher/developer to develop an extensive good web application vulnerability scanner.
The fast retrieval in archival traffic data is essential for network security and forensic analysis. A bitmap index is a data structure enabling fast search over large data collections in a limited time, but the space consumption is always a problem. WAH, PLWAH and COMPAX are proposed for compressing bitmap indexes for less storage. In this paper, a new bitmap index encoding scheme, named MASC, is proposed to further improve the compression ratio without impairing the query performance. Instead of being limited to a fixed length (31 bits) in PLWAH and COMPAX, the stride size can be as long as possible to encode consecutive zero bits and nonzero bits in a more compact way. Instead of piggyback used in PLWAH, a new structure in MASC called carrier is introduced as piggyback in PLWAH only carries an individual nonzero bit. We also generalize the traditional literal word concept in PLWAH and COMPAX. The validity of MASC encoding scheme is demonstrated with the application in Internet Traffic Archival system. Based on experiments with real Internet traffic data set from CAIDA, MASC has a better compression ratio than PLWAH and COMPAX2 without the penalty in query performance.
Honeypots have been considered as one of the methods to ensure security for networks in the Internet of Things (IoT) realm. In this paper, we study the problem of defending against attacks in honeypot-enabled networks by looking at a game-theoretic model of deception involving an attacker and a defender. The attacker may try to deceive the defender by employing different types of attacks ranging from a suspicious to a seemingly normal activity, while the defender in turn can make use of honeypots as a tool of deception to trap attackers. The problem is modeled as a Bayesian game of incomplete information, where equilibria are identified for both the one-shot game and the repeated game versions. Our results showed that there is a threshold for the frequency of active attackers, above which both players will take deceptive actions and below which the defender can mix up his/her strategy while keeping the attackers success rate low.
Network reconfiguration is referred as operational schemes to alter the network topology by closing and opening the sectionalizing and tie switches of power distribution system and thus it allows to control the power flow from substation to power consumers with additional benefits such as load balancing, real power loss reducing, optimizing the load sharing between parallel circuits by directing power flow along contractual paths. Restructuring of the power network, however, may increase the tendency of overloading and thus congestion in certain areas. This congestion may lead to violation of system voltage or transmission capacity limits and threaten the power network security and reliability. On this view point, this paper presents a Genetic Algorithm (GA) based new reconfiguration algorithm of the network which will able to identify the most congested area of power network and fabricate the least loss condition after alleviating overload and overvoltage as well as ensuring efficient network operation. To establish the efficacy of the proposed algorithm, case studies have been carried out on modified IEEE-14 bus and IEEE-30 bus power network and the results are found to be encouraging.
An Application Layer Distributed Denial of Service Attack (DDoS) is one of the biggest concerns for web security. Many detection methods are designed to mitigate DDoS attack based on IP and TCP layer instead of the Application layer. These methods are not suitable for detection of Application layer DDoS attack since most of the IP and TCP layer DDoS attacks are based on request flooding attack. But Application layer DDoS attacks consist of request flooding, session flooding, and asymmetric attack. The solutions available to detect Application layer DDoS attack, detect only limited number of Application layer DDoS attacks. The solutions that detect all types of Application layer DDoS attacks have huge algorithm complexity. One of the major challenges in the detection of an Application layer DDoS attack is the non-availability of features to detect such attacks. Hence it is difficult to model normal user behavior from attack behavior. In this paper, Deep learning architecture is introduced to learn deep features of Application layer DDoS attack. Deep learning architecture consist of very deep neural network, typically more than three layers. In the proposed work the concept of AutoEncoder is applied, which is one of the deep learning based models that learns deep useful features in the Application layer DDoS attack dataset. The Stacked AutoEncoder deep learning architecture, is aimed to receive high level features. The performance of the proposed method was evaluated in terms of the metrics such as false positive rate and detection rate. Comparison of the proposed method with the existing methods reveals that the proposed method performs better than the existing methods.
The ever-increasing complexity of computer network and various new types of bugs make the network security become an ever-growing serious challenge. In the evaluation of network security risk, the cause-and-effect relationship between multiple attack steps can be described well in an attack graph model. However, its test result is uncertain. Focused on this issue, the method of fusing attack graph model and Hidden Markov model (HMM) was proposed. Firstly, the network environment and attacker's aggressive behavior were abstracted by the attack graph model;Secondly, the probabilistic mapping that was between network observation and attack status was established by the HMM;Finally, the Viterbi algorithm was used to calculate the maximum probability state transition sequence. Experimental results show that the maximum probability of the state transition sequence can be effectively calculated and then the attack intention can be accurately inferred by this dual model. This method provides a good configuration for network security administrators.
In this paper, we study the acceleration of applications that require searching for all occurrences of thousands of string-patterns in an input data-stream, using the Automata Processor (AP). For this purpose, we use two applications from two fields, namely, network security and bioinformatics. The first application, called Fast-SNAP (for Fast-SNort using AP), scans network data for 4312 signatures of intrusion derived from the popular open-source Snort database. Using the resources of a single AP board, Fast-SNAP can scan for all these signatures at 10.3 Gbps. The second application, called PROTOMATA (for PROTein autOMATA), looks for all occurrences of 1308 protein motifs from the PROSITE database in protein sequences. PROTOMATA is up to half a million times faster than its single-CPU-based counterpart. The techniques developed to program these applications may be useful in the design and development of similar applications using this new hardware accelerator.
While frequent reports on targeted attacks for Industrial Control Systems hit the news, the amount of untargeted attacks using standardized industrial protocols is still unclear, especially if devices are mistakenly or even knowingly connected to the Internet. To lay the foundation for a deeper insight into the interest of potential attackers, a large scale honeynet system that captures all interactions using industrial protocols is proposed. Special for the honeynet system architecture is the automated deployment on a cloud infrastructure and its modularisation of the industrial protocols. The centralized-but-redundant data collection allows correlating attacks that happen on multiple devices. A real-world experiment confirms the feasibility of the approach, and results of the observed interactions with the honeynet are presented.
In this paper, considering that the serious network security situation we are facing and the problem of an increasing amount of data generated by the network, we proposed an Intrusion Detection System based on Hadoop, due to the lack of the traditional K-Means algorithm exists at we propose an improved K-Means algorithm, we analyze the performance of the K-Means algorithm and the improved K-Means algorithm with KDD '99 data sets by using the Intrusion Detection System based on Hadoop. The experimental results show that the Accuracy Rate can reach 0.96, the Detection Rate can reach 0.89, the False Alarm rate the minimum is only 0.018. Three intrusion detection performance indicators are better than the traditional K-Means algorithm.
To address the issue of internal network security, Software-Defined Network technology has been introduced to large-scale cloud centers, as it could not only improve network performance but also deal with network attacks. In order to prevent man-in-the-middle (MITM) and denial of service (DoS) attack caused by Address Resolution Protocol (ARP) bug in the cloud center, this paper proposed an algorithm using SDN technology to calculate the probability of a host being an attacker, and further give out a detect model based on the algorithm. Experiments prove the validity of this method.
Software development and web applications have become fundamental in our lives. Millions of users access these applications to communicate, obtain information and perform transactions. However, these users are exposed to many risks;commonly due to the developer's lack of experience in security protocols. Although there are many researches about web security and hacking protection, there are plenty of vulnerable websites. This article focuses in analyzing 3 main hacking techniques: XSS, CSRF, and SQL Injection over a representative group of Colombian websites. Our goal is to obtain information about how Colombian companies and organizations give (or not) relevance to security;and how the final user could be affected.
The Cognitive Radio (CR) is a fully-reconfigurable wireless device that can intelligently sense, manage, and exploit temporarily-vacant licensed spectrum bands during the absence of incumbent users. Broadly, the IEEE 802.22 is the first complete Wireless Regional Area Network (WRAN) standard that utilizes CR technology in the opportunistic access of white spaces in the television (TV) bands. Intrinsically, the increased CR networks' vulnerabilities alongside the experienced growth of attackers' capabilities, creates a strain on CR network designers to act accordingly. However, most of the existing efforts solely examined the issues of Denial-of-Service (DoS) attacks of IEEE 802.22 networks, such as Primary User Emulation (PUE) attack and Spectrum Sensing Data Falsification (SSDF) attack, each treated in isolation. One main challenge in CR network security domain is to efficiently represent possible simultaneous multiple security threats, and assess their effects. Unlike the previous works, this paper addresses the aforementioned challenge through using the holistic approach of assessing the combined effect of simultaneous multiple DoS attacks. The Bayesian Attack Graph (BAG) model is utilized in this paper to capture the probabilistic dependencies among IEEE 802.22 DoS threat-environment and known vulnerabilities. The simulation results indicate up to 51.3% increase in the probability of DoS in IEEE 802.22 networks considering simultaneous multiple attacks in comparison to the most severe sole attack. Finally, the paper introduces the BAG model as a feasible CR vulnerability metric that can facilitate the creation of a security tightening plan.
Interventions such as vaccinations or installing anti-virus software are common strategies for controlling the spread of epidemics and malware on complex networks. Typically, nodes decide whether to implement such an intervention independently, depending on the costs they incur. A node can be protected by herd immunity, if enough other nodes implement such an intervention, making the problem of determining strategic decisions for vaccination a natural game-theoretical problem. There has been a lot of work on vaccination and network security game models, but all these models assume the vaccination decisions are made at the start of the game. However, in practice, a lot of individuals defer their vaccination decision, and the reasons for this behavior are not well understood, especially in network models. In this paper, we study a novel repeated game formulation, which considers vaccination decisions over time. We characterize Nash equilibria and the social optimum in such games, and find that a significant fraction of vaccinations might be deferred, in general. This depends crucially on the network structure, and the information and the vaccination delay. We show that finding Nash equilibria and the social optimum are NP-hard in general, and we develop an approximation algorithm for the social optimum whose approximation guarantee depends on the delay.
Honeypots, i.e. networked computer systems specially designed and crafted to mimic the normal operations of other systems while capturing and storing information about the interactions with the world outside, are a crucial technology into the study of cyber threats and attacks that propagate and occur through networks. Among them, high-interaction honeypots are considered the most efficient because the attacker (whether automated or not) perceives realistic interactions with the target machine. In the case of automated attacks, propagated by malware, currently available honeypots alone are not specialized enough to allow the analysis of their behaviors and effects on the target system. The research presented in this paper shows how high-interaction honeypots can be enhanced by powering them with specific features that improve the reverse engineering activities needed to effectively analyze captured malicious entities.
This paper presents a method for inferring interaction strength and structure amongst targets in multiple target tracking (MTT) applications. By making simple assumptions, it is shown how an efficient and well-mixing MCMC inference method can be developed to learn about the relationships between tracked targets, including leader-follower relationships, group relationships and the influence of targets on others. This network structure of influence between targets is inferred in a sparse way, setting many interaction terms to zero and allowing for more efficient inference and clearer structural conclusions to be drawn. The effectiveness of the method is demonstrated on both synthetic and real animal flocking data.
The earlier remote password authentication schemes required a service providing server to authenticate a legitimate user for remote login. However, the traditional schemes are not useful in multi-server architecture because of multiple user ids and passwords. In this paper, we present a remote password authentication scheme for multi-server architecture that can be robust and improved network security. This password authentication system is a trained classification system based on SVM. In this scheme, the users only remember own identity and password and own choices for the various server login which he filled during registration where the user can choose his password at will. Furthermore, this system does not require having any overhead of password or verification table and is very dynamic in nature and can also withstand the replay attack and masquerading.
The creation of a network for an organization or a firm can be a multifarious task especially if there a large number of nodes, middle-boxes and security nodes to be integrated in the network. This paper introduces the integration of the ABE algorithm into a virtual network security called the NETSECVISOR in order to achieve authentication of the users involved in the network and the confidentiality of data transmitted or received. A virtual network redeems the administrators and network designers from its complexity not only by reducing the number of devices required, to a specific workstation that will take as input the various policies pertaining to the packets to be sent but also by including various routing rules and response techniques for malicious attacks. The Attribute Based Encryption (ABE) algorithm is used to improve the security on an existing virtual network security called the NETSECVISOR. The ABE algorithm uses attributes that are known to the participants involved in the transmission and reception of packets. This helps ease the process of creation and retrieval of keys for encryption and decryption notwithstanding the performance of security that will be enhanced for the virtual network.
Many resource allocation problems can be formulated as a constrained maximization of a utility function. Network Utility Maximization (NUM) applies optimization techniques to achieve decomposition by duality or the primal-dual method. Several important problems, for example joint source rate control, routing, and scheduling design, can be optimized by using this framework. In this work, we introduce an important network security concept, &#x201C;trust&#x201D;, into the NUM formulation and we integrate nodes' trust values in the optimization framework. These trust values are based on the interaction history between network entities and community based monitoring. Our objective is to avoid routing packets though paths with large percentage of malicious nodes. We also add end-to-end delay constraints for each of the traffic flows. The delay constraints are introduced to capture the imposed quality of service (QoS) requirements for each traffic flow.
Network intrusion detection based on SVM is the hot topic of network security research, and the existing researches have low detection rate, high false positive rate and other issues. Optimizing particle swarm optimization parameters of SVM is an effective solution, but the PSO algorithm is easy to fall into local optimum and results premature convergence.We propose an improved particle swarm optimization algorithm ICPSO, which use chaos operator ergodicity, randomness, sensitivity to initial conditions and other characteristics and the ICPSO is used to make the chaos into the inertia weight factor parameters and The chaos is applied to the optimization of the RBF kernel function parameter g and the penalty factor C, and to improve the convergence speed and precision of the particle swarm optimization. The experimental results show that: relative to the PSO-SVM algorithm and GA-SVM algorithm, ICPSO-SVM improves the efficiency of intrusion detection, and is an effective intrusion detection model.
Micro-policies, originally proposed to implement hardware-level security monitors, constitute a flexible and general enforcement technique, based on assigning security tags to system components and taking security actions based on dynamic checks over these tags. In this paper, we present the first application of micro-policies to web security, by proposing a core browser model supporting them and studying its effectiveness at securing web sessions. In our view, web session security requirements are expressed in terms of a simple, declarative information flow policy, which is then automatically translated into a micro-policy enforcing it. This leads to a browser-side enforcement mechanism which is elegant, sound and flexible, while being accessible to web developers. We show how a large class of attacks against web sessions can be uniformly and effectively prevented by the adoption of this approach. We also develop a proof-of-concept implementation of a significant core of our proposal as a Google Chrome extension, Michrome: our experiments show that Michrome can be easily configured to enforce strong security policies without breaking the functionality of websites.
The paper is devoted to an analysis of a one-year-long period of operation of a honeynet composed of 6 Dionaea honeypots emulating Windows services. The analysis focused on the frequency of attacks according to the location of individual honeypots (sensors) as well as to the geographical location of attackers. From the statistical processing of the results, it was demonstrated that the most frequently attacking malware was well-known Conficker worm. Moreover, attacking OS were studied with the conclusion that Windows is the most frequent OS. Regarding the geographical location of the attackers, several non-western countries and autonomous systems were indicated as being the most frequent origin of the attacks.
Regular expressions are a very common tool for network security applications because they can match precisely and maintain high matching speed even for many simultaneous patterns. Their core feature is efficient representation as an automaton, where much of the interaction between patterns can be pre-computed and aggregated. Many algorithms have been devised to try and improve this pre-computation to not take exponential space while keeping high performance, but none has met all the requirements of fast, automated construction, small memory image, and high matching speed. We present Match Filtering, a technique for de-composing regular expressions into segments that can be matched independently, while a stateful post-processing engine filters these matches to eliminate those that do not correspond to matches of the original regular expression. Using standard CPU instructions, the post-processing engine can more efficiently represent constructs that would require a multiplicative increase in automaton states. Because the pre-processing is simple, automaton construction can be automated and fast, and because most on-line processing is done by a DFA, its matching speed is close to that of a DFA alone. We demonstrate experimentally 30&#x00D7;smaller, fast (seconds, not minutes) automaton construction and 43% faster matching speeds than state-of-the-art software algorithms.
Attempts to secure the enterprise network, even when using strong AAA (authentication, authorization and accounting) schemes, meet the user box spoofing and security middle boxes (firewalls and other filtering tools) bypassing problems. Seeking to strengthen the network security level, the names (users, addresses) and user machines must be bound tightly to the unambiguously defined network appliances and their ports. Using traditional network' architecture these solutions are difficult to realize. The SDN framework allows solving the aforementioned problems more precise and securely. One of the possible ways to gradually implement the controllability of traffic flow in standard hierarchical networks by applying OpenFlow driven SDN architecture and commodity access switches, is described in this paper. The performance impact of the solution is also assessed.
Current computer and information systems are now used in almost all aspects of our lives, especially in banking systems and network systems. Most of the times, many computer systems use the simple and common username/password or keystroke biometric scheme via a keyboard for authentication. However, those fixed secret information can be guessed easily using different methods such as network sniffer, social engineering, spyware, dictionary attack and brute force attacks, etc. Meanwhile, using the password-based solution suffers from many security flaws and usability limitations. Although the user adopts extreme measures such as changing of the password and using long and complex passwords to be efficient and secure, these are unfriendly and hard to memorize for the user. Therefore, a solution to this problem is the use of an alternative biometric authentication method called Keystroke Dynamics. It is one of the famous biometric technologies and can be yet provide ease of use and transparency to the user in addition to security robustness. Keystroke Dynamics allows to secure the authentication process by verifying the way of typing the static credentials such as typing behavior, typing period of time, etc. But the static behavior can be easy continuously monitoring the user's activities. There is always a golden opportunity for attacker who is physically close to the machine to have access to it. This paper investigates the use of keystroke dynamics that is combined user's typing pressure activities on keyboard. The proposed scheme utilizes the key press and release pressure to build random user's typing profile. The research done on Dynamic Keystroke Pressure-Based is ability to provide continual identity verification during the whole time and has a high accuracy level which was obtained under strictly controlled conditions.
The duration of handover in WiFi - 802.11 network is a limiting factor for multimedia applications. We propose a novel approach that rapidly decreases handover time, while leaving the current functionality of mobile stations unchanged. The proposal is based on a Remote MAC separator design and creates new network architecture taking into account client management, scalability issues and enhanced network security.
Network intrusion detection system (IDS) plays a major role in any security based architecture. Various IDS have been developed to detect the intrusions that occur in the real world. The most commonly used network security tool used is Snort IDS. Snort is a rule-based system that generates alerts for the matching network patterns. Most of the rules stored in the Snort database fail to generate alerts for real network traffic. It is necessary to create rules that detect the attacks efficiently. In this paper we have made an attempt to autonomously generate rules using the evolutionary approach. The rules produced were tested for Darpa 1999, ISCX 2012 and ICMP network packets and were able to detect attacks with a high detection rate.
Computer networks are becoming more and more important because of variety of services being applied through them, so is the trend of increase of threats to these networks. Traditional approaches of network security like packet filtering, IDS and more advanced IPS suffer from various problems. E.g. These mechanisms are not aware of the resources they are protecting, these mechanisms are independent of the context of their application, their working is common to every kind of environment, these approaches do not adapt to the changing environment (configuration of the network and changing scenarios) on the run, these approaches do not take the holistic view of security situation. The concept of Network Security Situational awareness has been proposed to tackle the network security in a holistic manner. For holistic view of security biggest challenge for computing community is an efficient interoperability among heterogeneous data sources Ontologies have proved to play important role in resolving semantic heterogeneity by providing formalization of knowledge in a particular domain. Ontology represents the concepts of interest in a domain and the relations among them in a machine processable format. Separate ontologies may be developed for related domains and various ontologies may be integrated. Individual ontologies allow the development of ontology for one information source independently, hence making possible the addition of new and removal of obsolete information sources. This feature makes the application of ontology in areas where configuration of a system is continuously changing like in a computer network. In this paper we have implemented a tool OntoSecure, a semantic web based tool for network security status prediction. The effectiveness of this tool has been proved by considering various test cases.
Botnets are one of the major threats to network security. A botnet can launch attacks by stealing information, phishing sites, sending spam mail and setting up distributed denial of service (DDoS). Some botnets called Domain Generation Algorithm (DGA) Botnets apply a domain generation algorithm to avoid being detected by the traditional blacklist detection scheme. Using a domain generation algorithm, a DGA bot periodically generates a huge list of candidate Command and Control server (C&amp;C) domains. The bot then attempts to connect to the C&amp;C server by querying DNS servers for the domains on the list one by one until it connects to an existing C&amp;C server. By doing this, DGA botnets become very elusive and difficult to detect by traditional defending systems and thus have high survivability. To resolve this issue, this study proposes a DGA botnet detection mechanism utilizing the feature-based characteristics of social networks. The effectiveness of this mechanism was measured by implementing it in a campus network environment and observing it over eighteen months. The most interesting finding of this experiment is a new class of DGA botnet with a query pattern that has not been detected before. The results show that the proposed mechanism has the ability to accurately and effectively detect both well-known and new malicious DGA botnets in real-world networks.
With the fast development of information and communication technologies over the past decade, Healthcare Information Technology (HIT) has been widely implemented for health stakeholders to access, modify, share Electronic Health Records (EHR) with a low cost of the facility, data and application maintenance. Due to the high value of healthcare data and lack of investment in cyber security, vulnerabilities of Healthcare Information Systems (HISs), especially data of EHR systems are exposed to attackers [1], [2]. This paper first introduces the network structure of the HIS and the communication standards for health data transmission among patients, hospitals, pharmacies, and insurance companies. After that, we introduce the Health Level 7 (HL7) standard in details and discuss the current security challenges of HISs. We also illustrate how to simulate attacks that exploit HL7 message vulnerabilities. An Autonomic Security Management (ASM) approach is designed for proactively self-protecting a HIS from internal and external attacks. The performance of a HIS can be monitored in real time, and potential attacks that may disrupt HIS services are predicted by the intrusion estimation module. The functionality and feasibility of intrusion detection systems for detecting known and unknown cyber attacks threatening the confidentiality and integrity of EHRs are presented. The intrusion response system of the ASM approach selects the most appropriate protection mechanisms to recover the compromised HIS back to normal with little or no human intervention.
Most of the existing network security situation prediction mechanisms are only directed by the prediction of the situation value, but these methods do not reveal the problem of the dynamics features of the network situational factors. For these issues, this paper proposes the network security situation prediction mechanism based on the complex network. By this mechanism, we can easily and intuitively trace the dynamics features in the value fluctuations of the network security situation prediction. After that, this paper proposes Markov prediction method based on the complex network, so that we can achieve an effective prediction of the Security Status. Through simulation analysis, the network security situation prediction mechanism based on the complex network can reflect the essence behavior of the system to some extent. At the same time this mechanism can precisely predict the Security Status in the complex network.
Security protocols are the key to ensure network security. In the context of the state of the art, so many methods have been developed to analyze the security properties of security protocols, such as Ban logic, theorem proving and model checking etc. This paper used model checking method to formally verify security protocols because of its high degree of automation, briefness and effectiveness. The model checker Spin with sound algorithm design has an extraordinary ability of checking and a good support for LTL. This paper studied the use of Spin on security protocols, and proposed a more effective intruder model to formally verify the security properties of security protocols, such as authentication. The method in this paper decreased the number of model states by a wide margin, and avoided the state space explosion effectively. This paper exampled NSPK protocol and DS protocol, and good experimental results were shown.
A wide variety of networks, ranging from biological to social, evolve, adapt and change over time. Recent methods employed in the assessment of temporal networks include tracking topological graph metrics, evolutionary clustering, tensor based anomaly methods and, more recently, graph to signal transformations. In this paper, we propose to assess the temporal evolution of networks by first transforming networks into signals through Classical Multidimensional Scaling based on the resistance distance and then constructing a tensor based on the spectra of each signal across time. The proposed method is first evaluated on simulated temporal networks with varying structural properties. Next, the method is applied to temporal functional connectivity networks constructed from multichannel electroencephalogram (EEG) data collected during a study of cognitive control. This analysis shows that the proposed method is more sensitive to changes in the network structure and more robust to variations in edge weights.
Optimization of power system restoration path is a key issue to the system restoration following a significant disruption, such as the Northeast Blackout of 2003 in the United States and Canada. The restoration path optimization problem (RPOP) is to calculate the shortest restoration path between specified nodes, while subject to network security constraints. The RPOP is normally modeled as a large-scale mixed integer nonlinear programming, including both routing components and the nonlinear steady-state power flow equations. Intelligent algorithm is widely used to solve this complicated problem due to its excellent optimization capability, but existing research didn't concern about the generate method of initial population. In this paper, an orthogonal genetic algorithm is adopted to achieve the optimal solutions. The orthogonal array method is used to generate an initial population of genetic algorithm. This method has been proven to be optimal to select representative samples from all the possible combinations, due to the selected samples scatter uniformly over the feasible solution space. Finally, the IEEE standard test systems are used to examine the applicability of proposed method. Simulation results demonstrate that the proposed method is more efficient than traditional method.
This paper presents a network security laboratory to teach data analysis for detecting TCP/IP covert channels. The laboratory is mainly designed for students of electrical engineering, but is open to students of other technical disciplines with similar background. Covert channels provide a method for leaking data from protected systems, which is a major concern for big enterprises and governments. The inclusion of covert channels in the curricula of network security students and network data analysts is therefore considered a valuable extension. In the lab exercises presented, students learn how covert channels in TCP/IP network traffic can be hidden and detected. Since the detection of covert channels requires an in-depth understanding of protocol standards and typical behavior of TCP/IP flows, the lab also provides a &#x201C;playground&#x201D;in which students can deepen their communication networks knowledge. Students learn how to use and interpret statistical analysis to discover abnormal patterns and footprints in network data. They are also trained to deal with noisy scenarios that increase ambiguity and uncertainty. The laboratory was first implemented during the winter semester 2014 with a class of 18 students at TU Wien, Austria. This experience showed that students consolidated the targeted skills as well as increased their interest in the topics explored. All exercises and datasets for the introduced &#x201C;Network Security Advanced&#x201D;lab are made publicly available.
Due to the broadcast nature of radio propagation, the wireless air interface is open and accessible to both authorized and illegitimate users. This completely differs from a wired network, where communicating devices are physically connected through cables and a node without direct association is unable to access the network for illicit activities. The open communications environment makes wireless transmissions more vulnerable than wired communications to malicious attacks, including both the passive eavesdropping for data interception and the active jamming for disrupting legitimate transmissions. Therefore, this paper is motivated to examine the security vulnerabilities and threats imposed by the inherent open nature of wireless communications and to devise efficient defense mechanisms for improving the wireless network security. We first summarize the security requirements of wireless networks, including their authenticity, confidentiality, integrity, and availability issues. Next, a comprehensive overview of security attacks encountered in wireless networks is presented in view of the network protocol architecture, where the potential security threats are discussed at each protocol layer. We also provide a survey of the existing security protocols and algorithms that are adopted in the existing wireless network standards, such as the Bluetooth, Wi-Fi, WiMAX, and the long-term evolution (LTE) systems. Then, we discuss the state of the art in physical-layer security, which is an emerging technique of securing the open communications environment against eavesdropping attacks at the physical layer. Several physical-layer security techniques are reviewed and compared, including information-theoretic security, artificial-noise-aided security, security-oriented beamforming, diversity-assisted security, and physical-layer key generation approaches. Since a jammer emitting radio signals can readily interfere with the legitimate wireless users, we also introduce the family of various jamming attacks and their countermeasures, including the constant jammer, intermittent jammer, reactive jammer, adaptive jammer, and intelligent jammer. Additionally, we discuss the integration of physical-layer security into existing authentication and cryptography mechanisms for further securing wireless networks. Finally, some technical challenges which remain unresolved at the time of writing are summarized and the future trends in wireless security are discussed.
Mobile <i>ad hoc</i> networks (MANETs) are wireless networks that have a wide range of applications because of their dynamic topologies and ease of deployment. Owing to the independent and dynamic nature of mobile nodes, the topology of a MANET often changes and is prone to various attacks. Therefore, substantial research in the area of security is required. Certificate revocation is an effective mechanism for providing network security services. However, the existing schemes are not well suited to MANETs because of their considerable overhead or low accuracy with respect to certificate revocation. In this study, the authors investigate a distributed certificate revocation protocol. On the basis of the game-theoretic model, they design a new voting-based security scheme. Their game-based security paradigm can provide the ability to practically respond to the current system conditions and is suitable for real MANET operations. Simulation results demonstrate the effectiveness and the efficiency of their scheme with respect to certificate revocation. Finally, they discuss the results of an evaluation provide an outlook on the future work in this field.
Network structures and human behaviors are considered as two important factors in virus defense currently. However, due to ignorance of network security, normal users usually take simple activities, such as reinstalling computer system, or using the computer recovery system to clear virus. How system recovery influences virus spreading is not taken into consideration currently. In this paper, a new virus propagation model considering the system recovery is proposed first, and then in its steady-state analysis, the virus propagation steady time and steady states are deduced. Experiment results show that models considering system recovery can effectively restrain virus propagation. Furthermore, algorithm with system recovery in BA scale-free network is proposed. Simulation result turns out that target immunization strategy with system recovery works better than traditional ones in BA network.
We are witnessing the evolution of optical networks toward highly heterogeneous, flexible networks with a widening area of application. As the bandwidth and reliability performance requirements of mission-critical applications tighten, and the amount of carried data grows, issues related to optical network security are becoming increasingly important. Optical networks are vulnerable to several types of attacks at the physical layer, typically aimed at disrupting the service or gaining unauthorized access to carried data. Such security breaches can induce financial losses to clients or loss of privacy, or cause network-wide service disruption, possibly leading to huge data and revenue losses. Awareness of system weaknesses and possible attack methods is a prerequisite for designing effective network security solutions. As optical networks evolve, new and changing vulnerabilities must be identified and dealt with efficiently. To this end, this article provides a comprehensive overview of potential physical-layer attack scenarios in current and future optical networks. It then proposes a general security framework, outlining possible strategies for dealing with such attacks, meant to aid in the development of efficient provisioning, monitoring, protection, and restoration schemes in the context of optical-layer security.
Two-way communication networks enable near realtime interactions in modern smart power systems. Also, computer and communication security systems have become one of the main factors of security in power systems. The need for methods to appropriately assess currently existing cyber risks and forecast possible future risks to a reasonable extent has become more important than ever before. This work establishes a surveillance architecture to monitor message transactions among nodes in communication networks. A security belief model is built to interpret surveillance observations as Dirichlet-distributed security events with certain probabilities. By taking the interaction between possibly suspicious nodes and the security operator as a transmitting-monitoring game, a game-theoretic risk assessment framework is presented to compute and forecast risk of network security impairment.
In the past decade, the use of wind turbines connected to distribution networks, in order to improve voltage profile, reduce losses and increase reliability and network security have been increasing. Meanwhile, the role of static converters and power electronic devices in connecting and controlling the wind turbine power is important. In order to generate the maximum power, decrease the mechanical stress and control the active and reactive power in the structure of the permanent magnet synchronous generator (PMSG) of 4th generation wind turbines, the full-power frequency converters have been used. Today, with the development of smart grids, the harmonic emission and immunity in the frequency range (2-150 KHz) have found particular importance. Switching in the static convertors of wind power plants causes the emission of the harmonics in the frequency range (2-150 KHz) in distribution networks. The frequency range (2-150 KHz) has not covered enough in power international standards. Majority of power line communication (PLC) systems have been operated in this frequency. However, no major report on the impact of these distribution networks has been provided yet, but these harmonics can cause disturbances in power line communication (PLC) system function. In this article, the emission of harmonics caused by switching of generator full-power frequency converter (PMSG) of a wind unit connected to the medium voltage (MV) network will be simulated and analyzed as real-time in Matlab software. The information of distribution network and wind power plant has been resulted from a real system. The simulation results show that despite the generation of a great spectrum of harmonics in the frequency range (2-150 KHz) by wind turbine frequency converter, due to the resonance of network elements and coupling transformer, the emission of these harmonics over the network is low. However, the harmonics in the range (2-150 KHz) were so large with the possibility to cause dysfunction in power line communications (PLC) systems.
All the safety standards, also if implemented with different philosophies and perspectives, highlight a program to protect against electric shocks, arc flashes and blasts. The European EN 50110 standard emphasizes equipment integrity &#x201C;giving collective protective measures priority over individual protective measures&#x201D; that remain such as additional means (Directive 89/391/EEC - OSH). Focusing on the specific hazard of arc flash, many prevention measures are available in the IEC approach of protection program. The main measures, essentially passive, are: the IP code; safety procedures for dead working; MV isolator switches that allow grounding of the working zone without the need of a manual equipment; the tested effectiveness of the switchboard design in protecting persons in case of an internal arc (internal arc class IAC); forms of separation for low voltage switchgear; the standardized method to calculate the minimum value of short-circuits. All these measures assist the risk reduction of the arc flash in the IEC/EN safety approach.
A new backup method, which achieves on-demand restore as part of file system backup to object storage, is proposed. On-demand restore is a function that restores only a certain directory or file according to the request from an end user. The proposed method backs up relationships between a file and an object on a directory basis in order to efficiently handle the relationships in the restore. It is experimentally shown that the proposed method reduces the response time of file access involving restore by over an order of magnitude.
Network traffic in the world wide is calculated to rise every year twice the times. To keep pace and profit from this increased amount of flows efficiently. And offer new services. Some efficient techniques needed. Day by day new applications are invented and they have heterogeneous nature in network environment and communication between these new devices also a critical part. improving the network performance, establish proper service policies in router, handling network security risks, management of network operations and provide Qos services to users in internet. To solve these issues classification techniques are used. In this survey different classification algorithms are discussed. K-means algorithm, classification using clustering algorithm, Classification based on Fuzzy Kernel K-means Clustering, Support vector machine algorithm, and self-learning classifier Bayesian classification, C5.0 and traffic classification using correlation information and robust network traffic algorithms are presented.
The driving forces for this project are demands for a hardware, reconfigurable, high performance, low cost and low power solution that could be used in a widest range of Internet of Things (IoT) devices. The solution should be able to secure computer networks against emerging threats and vulnerabilities, sustaining privacy and trust This paper presents an approach for developing a novel hardware platform for Ethernet-based network firewall security services for IP networks. The article highlights functional and structural levels of the proposed hardware architecture, performance estimations and a trade-off between performance and hardware cost. Some implementation details, including HDL used, testing approaches and design tools are provided as well.
HTTP automated software (auto-ware) are blooming for multiple purposes due to the fast growing of World Wide Web. Beside normal HTTP application are beneficial for users such as operating system or virus definition update software, in recent years, cyber criminals turn to fully exploit web as a medium of communication environment to lurk variety of forbidden or illicit activities through spreading malicious automated software such as adware, spyware or bot. In addition, auto-ware traffic is almost anonymity to users. Therefore, in a private network, due to early detection of internal threats, clustering of auto-ware communication is helpful to network security management. In this paper, based on analysis of the auto-ware communication behaviour, a network level approach in clustering of HTTP auto-ware communication is proposed. The experimentation with real outbound HTTP traffic data which collected through a proxy server of a private network gives a considerable result in clustering HTTP auto-ware traffic. The results can be used as a good resource for further security purposes such as malicious domain/URL detection or investigation of HTTP based malware.
Application layer attacks pose an ever serious threat to network security for years since it always comes after a technically legitimate connection has been established. In recent years, cyber criminals turn to fully exploit web as a medium of communication environment to lurk a variety of forbidden or illicit activities through spreading malicious automated software(auto-ware) such as adware, spyware or bot. When these malicious auto-ware infect into a user network, they will act like robot and mimic normal behaviour web access to bypass network firewall or IDS. Besides that, in a private and large network, with huge HTTP traffic generated each day, communication behaviour identification and also classification of auto-ware is really a great challenge. In this paper, based on the previous study and analysis of the auto-ware communication behaviour and addition new features, a method classification of HTTP autoware communication is proposed. In that, a NoSql database is applied to handle with large volume unstructured HTTP requests captured every day. The method is experimented with real HTTP traffic data collected through a proxy server of a private network, from which good results are archived in classification and detection of suspicious auto-ware web access.
In this paper, an epidemic model describing the transmission dynamics of a worm with partial immunization and nonlinear rate is investigated. Using this SVEIR model, we obtain the basic reproduction number for determining whether the worm dies out completely. The global stabilities of infection-free and endemic equilibriums are proven by using two different Lyapunov functions. The impact of different parameters of this model is studied. Simulation results show that the number of susceptible and infected hosts is consistent with theoretical analysis. The model provides a theoretical foundation for control and forecasting Internet worms.
Due to the immense popularity of the internet, usage of web application has expanded. Since extremely sensitive information are being exchanged via web applications every day, they have become a playground for cyber criminals to steal data and to use them for malicious purposes. In this paper, we present an efficient integrated penetration testing tool to detect five of the top ten web application vulnerabilities to date. New approaches have been proposed to detect two types of vulnerabilities and existing approaches have been modified to enhance the performance of the tool. The results indicate that the proposed tool has been effective and efficient in detecting the specific web app vulnerabilities with respect to other popular open source web vulnerability scanners.
Now a day's plenty of research work is going in the field of network security. Because of large data size and real time constraint. So most of the cryptographic algorithms are more suitable only for text. The main problem of image security is the encryption result is insensitive with respective to the plain text. In this paper trellis algorithm is combined with DNA sequence to provide more security to the image. In encryption side the trellis encoding process has performed as well as in decryption side trellis decoding has been used.
The threat landscape is changing significantly; complexity and rate of attacks is ever increasing, and the network defender does not have enough resources (people, technology, intelligence, context) to make informed decisions. The need for network defenders to develop and create proactive threat intelligence is on the rise. Network deception may provide analysts the ability to collect raw intelligence about threat actors as they reveal their Tools, Tactics and Procedures (TTP). This increased understanding of the latest cyber-attacks would enable cyber defenders to better support and defend the network, thereby increasing the cost to the adversary by making it more difficult to successfully attack an enterprise. Using a deception framework, we have created a live, unpredictable, and adaptable Deception Environment leveraging virtualization/cloud technology, software defined networking, introspection and analytics. The environment not only provides the means to identify and contain the threat, but also facilitates the ability to study, understand, and develop protections against sophisticated adversaries. By leveraging actionable data, in real-time or after a sustained engagement, the Deception Environment may be easily modified to interact with and change the perception of the adversary on-the-fly. This ability to change what and where the attacker is on the network, as well as change and modify the content of the adversary on exfiltration and infiltration, is the defining novelty of our Deception Environment.
Molecular biology provides the ability to implement forms of information and network security completely outside the bounds of legacy security protocols and algorithms. This paper addresses an approach which instantiates the power of gene expression for security. Molecular biology provides a rich source of gene expression and regulation mechanisms, which can be adopted to use in the information and electronic communication domains. Conventional security protocols are becoming increasingly vulnerable due to more intensive, highly capable attacks on the underlying mathematics of cryptography. Security protocols are being undermined by social engineering and substandard implementations by IT organizations. Molecular biology can provide countermeasures to these weak points with the current security approaches. Future advances in instruments for analyzing assays will also enable this protocol to advance from one of cryptographic algorithms to an integrated system of cryptographic algorithms and real-time expression and assay of gene expression products.
Denial of service attack permits the intruders to access the network services thereby preventing the legitimate users to access the services. To overcome the deficits of the DoS attack, it is very essential to design an intrusion detection system. Intrusion detection system (IDS) is software that operates as a network security mechanism to protect the computer network system from attacks. With increasing number of data being transmitted gradually from one network to another, the IDS identify the intrusions in such large datasets effectively. Data mining is an efficient tool applied to outline the intrusion detection system and prevent the massive network data from the intruders. Outliers are patterns in data that do not match to a well-defined notion of normal behavior. Outlier detection aims to find patterns in data that do not conform to expected behavior. It is widely used for developing intrusion detection in cyber security. This paper presents the study of outlier detection technique and how it is used to develop the intrusion detection system to overcome the DOS attack.
Industrial Control System (ICS) consists of large number of electronic devices connected to field devices to execute the physical processes. Communication network of ICS supports wide range of packet based applications. A growing issue with network security and its impact on ICS have highlighted some fundamental risks to critical infrastructure. To address network security issues for ICS a clear understanding of security specific defensive countermeasures is required. Reconnaissance of ICS network by deep packet inspection (DPI) consists analysis of the contents of the captured packets in order to get accurate measures of process that uses specific countermeasure to create an aggregated posture. In this paper we focus on novel approach by presenting a technique with captured network traffic. This technique is capable to identify the protocols and extract different features for classification of traffic based on network protocol, header information and payload to understand the whole architecture of complex system. Here we have segregated possible types of attacks on ICS.
While attackers have used deception to hide their identities, cause surprise, or mislead victims, defensive use of deception has been limited to honeypots and moving target defenses (MTDs). This has left unexplored a powerful defensive strategy namely, active manipulation of the adversary's decision loop. In contrast to the passive approach of honeypots and MTDs, this active approach deliberately interacts with the adversary to cause him to think he is succeeding and expend effort in an alternate reality. The work described in this paper took initial steps to realize active defensive deception in the context of distributed systems and built a prototype that creates an alternate reality in which to trap, learn about, and manipulate adversarial actors without affecting normal and legitimate operations. This prototype, called KAGE, employs Software Defined Networking (SDN), and virtualization to create a malleable substrate in which deception can occur. Deception is necessarily context dependent. In the case of KAGE, deception is tied to the mission purpose served by the distributed system being defended, specifically the services running, and the configuration, scale, and complexity of the environment. Consequently, there is no single deception strategy that will fit all system and mission contexts. KAGE therefore presents a framework through which a wide array of deceptions can be composed from component building blocks. This work-in-progress paper introduces the concept of active defensive cyber deception, discusses the early stage KAGE prototype, and introduces some of the challenges intrinsic to enabling defensive deception in distributed environments.
The large user base of mobile devices, such as smartphones and tablets equipped with high-speed Internet connectivity and abundance of apps, this portable yet powerful computing devices has become an interesting target for cyberattackers. Based from incident reports received by MyCERT, there is a rising trend of malware attacks on mobile platforms. The ubiquitous, fast response and lack of protection nature of these devices more or less has helped the spreading of cyberthreats on mobile devices. This paper review and investigates the utilization of low-interaction honeypots in detecting and learning about mobile malwares.
Attack graphs show possible paths that an attacker can use to intrude into a target network and gain privileges through series of vulnerability exploits. The computation of attack graphs suffers from the state explosion problem occurring most notably when the number of vulnerabilities in the target network grows large. Parallel computation of attack graphs can be utilized to attenuate this problem. When employed in online network security evaluation, the computation of attack graphs can be triggered with the correlated intrusion alerts received from sensors scattered throughout the target network. In such cases, distributed computation of attack graphs becomes valuable. This article introduces a parallel and distributed memory-based algorithm that builds vulnerability-based attack graphs on a distributed multi-agent platform. A virtual shared memory abstraction is proposed to be used over such a platform, whose memory pages are initialized by partitioning the network reachability information. We demonstrate the feasibility of parallel distributed computation of attack graphs and show that even a small degree of parallelism can effectively speed up the generation process as the problem size grows. We also introduce a rich attack template and network model in order to form chains of vulnerability exploits in attack graphs more precisely.
Power systems need to have sufficient generation capacity to support the demand at all times. In addition, dispatchable generation resources should be capable of adjusting their power output on a short term basis not only to alleviate uncertainties of nondispatchable generation and load fluctuations but also to correct for forecast errors. This paper presents a long-term planning approach to co-optimize capacities of energy storages and fast-ramping generation. We model and integrate the capability of the storage to provide multiple services for the system. Our formulation takes into account wind generation and demand forecast errors as well as short-term fluctuations. A stochastic optimization problem is formulated consisting of hourly and intrahour time scales. The approach determines the optimal size of newly deployed generation and storage resources to provide adequate generation capacity and ramping needed to support hourly demand. Additionally, our method ensures that the system is capable of following the net load in intrahour time intervals, as well as mitigating the impact of short-term wind power and load fluctuations. In this formulation, power balance, network security, and system ramping capability are stochastic constraints being modeled as chance constraints. A 3-bus and the IEEE 24-bus test systems are studied to show the effectiveness of the proposed approach.
After studying the routing and forwarding process of network stream and the implementation of SDN, we propose a retractable management model for flow table. A structure with parallel tables and synthesis processing is proposed according to the feature of SDN and traditional network. The parallel tables share the same storage resources. Thanks to the separation of data plane and control plane, control plane owns more computing resources than traditional device. It evaluates the role of nodes and the action of network flows, makes adjustment according to the historical and current information and streamlines flow tables by consolidating and simplifying old flow entries. Through simulation, it is proved that the realized method can defend offensive traffic while ensuring the safety of accessing and forwarding, especially existing blocking attack.
Detecting explicit user actions, i.e., requests for web pages such as hyper-link clicks, from passive traces is fundamental for many applications, such as network forensics or content popularity estimation. Every URL explicitly visited by a user usually triggers further automatic URL requests to obtain all objects that compose the web page. HTTP traces provide a summary of all URLs requested by users, but no information that could be used to separate explicit from automatic requests. Previous works have targeted this problem and ad-hoc heuristics have been proposed. Validation has been typically done using synthetic traces. This paper investigates whether an approach based solely on machine learning can successfully detect user actions from HTTP traces. A machine learning approach would come with many advantages - e.g., it minimizes manual tuning of parameters and can easily adapt to page structure changes. We build both real and synthetic traces to assess the performance and gain insights on the features that bring most advantages in classification. Our results show that machine learning reaches similar or better performance as previous heuristics. Furthermore, we show that models built with machine learning algorithms are robust, presenting consistent performance in different scenarios.
Current practices in network security deployment require multiple specialised devices as firewalls, traffic shapers, sensors or Intrusion Detection Systems (IDSs) to handle malicious traffic. This practice not only increases the overall operational costs but also makes network administration complicated. The high cost of Distributed Denial of Service (DDoS) mitigation devices empowers centralised services and network architectures as there is not a cost-effective model to deploy them at the &#x201C;true edge&#x201D; of the network. This paper describes the design and implementation of a multi-10 Gbit extensible network traffic analysis and policing system. It is composed of logical detection and enforcement functions built from reusable underlying primitives. As an example of such modular approach, we present an innovative DDoS scrubbing system composed of various attack detection primitives, combined with enforcement primitives that include traffic filtering, rate limiting, and proxying. Based on commodity hardware and open source software, such system is price, space, and power efficient enough to be practically deployable at the edge of the network. Performance measurements carried on 10Gbit networks, show that it can effectively provide both traffic visibility and enforcement of a wide range of network traffic policies.
With the constant development of network technology today, network not only brings us a convenient and efficient life, and is accompanied by a variety of network security problems. Firewall, as a main way to prevent network attacks, is often used to prevent illegal connection and separates the internal network from the insecure networks, to protect the safety of the Linux systems which used in small and medium-sized enterprise. In this paper, the main content is to complete the function of firewall which is based on the Linux operating system, using Netfliter as firewall architecture, and the IPtable as a user space module tool. Firstly, this paper briefly analyzes the Netfilter/IPtable architecture and princIPle and working process of state detection technology, then, configure the firewall. At the last, the firewall experiment verified the effectiveness and safety of the design of the firewall.
The wireless sensor network's nodes are mostly distributed in the wild, and frequently in a state of unattended, therefore, the network vulnerable to various security threats. Under the condition of serious limitations in resources, the traditional network security system which needs to spend a lot of resources is no longer applicable. So, how to ensure the security of wireless sensor network under limited resources has become a key problem. This paper starts from the security features and targets of wireless sensor network, Summarizes the various threats for the wireless sensor network, and the main key management and security routing technologies, then make outlook for the future development.
Network security technologies have different issues that is important in next generation networks because of the real-time nature of its applications (e.g. VoIP and IPTV). The main requirements of these types of applications is to handle the attack situations without quality degradation. There are many references for implementation of intrusion detection systems in VoIP infrastructures but there is little effort on intrusion response systems. We concentrate on response systems for SIP-based entities and present a cost sensitive response system which considers environmental dynamic conditions. We categorize the deployable responses into different groups based on their severity level by considering their side effects. We also propose a new quantitative metric for damage cost to compare it with response cost. Our proposed decision making process is done based on the comparison of these costs (response and damage costs), the environmental conditions (CPU, network and memory usages) and also the time of the detected attack. We verify our proposed framework by a real test-bed which is implemented by open-source tools such as OPENSIPS and SIPp. The implementation results show the effectiveness of our proposed SIP intrusion response system.
Attacks in network have caused a variety of serious problems, but intrusion detection in network is still an immature technology. And it is very important for network security to timely detect anomalies and rapidly response. Many intrusion detection methods have been proposed from simple to sophisticated techniques in the literature. Among them, the Context-Based Intrusion Detection (CBID) algorithm is an excellent. However, the CBID method just relying on personal experience but not a clear way to determine the context window size. In this paper, we propose a novel scheme to solve this problem by using chaos theory to select the parameter of the intrusion detection algorithm. We evaluate our scheme using traffic traces from a real network, namely from &#x201C;CAIDA DATASET&#x201D;. The experiment results show that our proposed method has a higher the true positive and a lower miss rate contrast to CBID.
The vast inter-connectivity of devices in the Internet of Things (IoT) under wireless environments facilitates information exchange, but challenges network security. In this paper, we propose a novel scheme for secure cooperative communication by hybrid forwarding and opportunistic selection in IOT. A closed form expression for the tight approximation of the secrecy outage probability (SOP) is derived. An asymptotic intercept probability analysis is also conducted to evaluate the diversity order performance of proposed relay selection scheme. The derivations are confirmed through Monte Carlo simulations. Simulation results also demonstrate that proposed hybrid forwarding scheme outperforms the AF and DF forwarding scheme in terms of secrecy outage performance.
With the advancement of internet, computer network security has brought serious concerns. Intrusion detection is an important topic in network security framework. To address the effectiveness and efficiency problem with traditional intrusion detection models, we present an intrusion detection method based on deep leaning. The deep belief network (DBN) constructed via the stacked Boltzmann machine model (RBM) is selected. Firstly, combining numeralization of symbolic features and numeric features normalization are used to processing network log features. In addition, extreme learning machine (ELM) was applied into the learning process of DBN. Compared with traditional DBN, the experimental results on the NSL KDD dataset demonstrate that intrusion detection based on IDBN has double training speed compared to DBN, while achieving a reliable detection rate.
In this paper we study network security that arises from the link structure of the network. We propose a hypothetical network in which members defend themselves through a mechanism that relies on the community structure of the network, the community structure provides a basis for mutualtrustworthiness. The goal of the attacker is to take control of the entire network by spreading an infectious virus among members of the network. When any member of a community is infected, the entire community is infected. Any infected individual has the ability to modify the link structure by establishing new links in the network, hence changing the community structure. Based on this mechanism, we define a metric of structural resilience of the network, which measures how much the network topology provides an inherent barrier to the infection process. We perform experiments on some standard graph classes and concludes that complete graphs enjoy the highest structural resilience, while star graphs have the lowest. We also perform experiments on several classes of synthesized random graphs and identify that graphs with a clear community structure tend to have high structural resilience.
Video streaming has become one of the most prevalent mobile applications, and takes a huge portion of the traffic on mobile networks today. YouTube is one of the most popular and volume-dominant video content providers. Understanding the user perception on the quality (i.e., Quality of Experience or QoE) of YouTube video streaming services is thus paramount for the content provider as well as its content delivery network (CDN) providers. Although various video QoE assessment approaches proposed to use different Key Performance Indicators (KPIs), they are all essentially related to a common parameter: Bitrate. However, after YouTube adopted HTTPS as its adaptive video streaming method to better protect user privacy and network security, bitrate cannot be obtained anymore from encrypted video traffic via typical deep packet inspection (DPI) method. In this paper, we tackle this challenge by proposing a machine learning based bitrate estimation (MBE) approach to parse bitrate information from IP packet level measurement. For evaluating the effectiveness of MBE, we have chosen video Mean Opinion Score (vMOS) proposed by a leading telecom vendor, as the QoE assessment framework, and have conducted comprehensive experiments to study the impact of bitrate estimation accuracy on its KPIs for HTTPS YouTube video streaming service. Experimental results show that MBE is a feasible and highly effective approach to obtain in real time the bitrate information from encrypted video streaming traffic.
Network monitoring is a difficult and demanding task that is a vital part of a network administrator's job. Network administrators are constantly striving to maintain smooth operation of their networks. If a network were to be down even for a small period of time, productivity within a company would decline, and in the case of public service departments the ability to provide essential services would be compromised. There are several approaches to network security monitoring. This paper provides the readers with a critical review of the prominent implementations of the current network monitoring approaches.
Over the past short time, network security facing a lot of challenges. Confidentiality, integrity, and availability are the major concerns of the data. To cope with this problem different systems have been developed and the systems are known as Intrusion detection systems. Intrusion detection system detects the violation of confidentiality, integrity, and availability of the data. Intrusion detection systems are developed on the bases of two different detection techniques, signature-based technique and anomaly-based technique. Classification approach has been widely adopted for the development of the anomaly detection model to classify the data into normal class and attack class. But irrelevant and redundant features are the obstacle for classification algorithm to build an efficient detection model. This paper proposes a detection model, ant system with support vector machine, which uses ant system, a variation of ant colony optimization, to filter out the redundant and irrelevant features for support vector machine classification algorithm. KDD99, which is a benchmark dataset used for anomaly detection, has been adopted here. Each instance in KDD99 has been represented by 41 features which also has some redundant or irrelevant features. Ant system has been used to remove those redundant and irrelevant features. The selected feature subset using ant system is then validated using support vector machine. The experimental results showed that the performance of the classification algorithm, when trained with the reduced feature set, has been improved. The performance measures used in this comparison are true positive rate, false positive rate, and precision.
In order to identify attacks and malicious behaviors, network packets must be intercepted and analyzed. However, the amount of data logged will be enormous. Trying to process manually this volume of traffic would be massively expensive. This research presents an integrated framework IWNetFAF that captures and analyzes the 802.11 wireless network traffic. IWNetFAF meets the common requirements of a wireless network forensics, namely: capturing wireless network traffic, analyzing the captured traffic according to the investigation's needs, and extracting and documenting digital evidence from the analyzed traffic.
Attacks of Ransomware are increasing, this form of malware bypasses many technical solutions by leveraging social engineering methods. This means established methods of perimeter defence need to be supplemented with additional systems. Honeypots are bogus computer resources deployed by network administrators to act as decoy computers and detect any illicit access. This study investigated whether a honeypot folder could be created and monitored for changes. The investigations determined a suitable method to detect changes to this area. This research investigated methods to implement a honeypot to detect ransomware activity, and selected two options, the File Screening service of the Microsoft File Server Resource Manager feature and EventSentry to manipulate the Windows Security logs. The research developed a staged response to attacks to the system along with thresholds when there were triggered. The research ascertained that witness tripwire files offer limited value as there is no way to influence the malware to access the area containing the monitored files.
The introduction of wireless communication technology into large Japanese hospitals is rapidly progressing, notably wireless LAN and wireless voice communication systems. However, many hospitals have problems, such as inadequate signal range, electromagnetic interference by waves generated inside/outside the hospital, or by devices carried in by the staff or patients. Many have problems with network security. We herein discuss the problem of &#x201C;network availability&#x201D;, which can be defined as &#x201C;Being able to communicate with the required transmission rate, wherever and whenever a user wants&#x201D;. We summarize the elements of &#x201C;network availability&#x201D; in hospitals and make suggestions for ensuring availability, especially focusing on wireless communications.
In order to enhance the ability of software defined network (SDN) proactive security protection, this paper puts forward a path hopping based SDN network defense (PH-SND) technology. PH-SND technology models the path hopping problem as a constraint solving problem, and utilizes satisfiability modulo theory solver to obtain multiple paths, which satisfy overlap and capacity constraints. According to the path hopping strategy and specific hopping slot, SDN controller installs corresponding flow entries into all OpenFlow switches along every path, and these switches can then use these flow entries to properly forward the corresponding protected flow, and randomly change the address and port information of this flow, which can not only realize random path hopping, and can also effectively hide original address and port information of both communication sides. Theoretical analysis and experimental results show that this proposed PH-SND technology can not only achieve transmission path hopping and address and port random hopping along every transmission path with a comparatively small communication delay and can also improve proactive security protection capability to resist network interception and analysis attack.
The ongoing transition towards a networked society requires reliable and secure network infrastructure and services. As networks evolve from simple point to point systems towards complex, software-defined, ultra-high capacity and reach, and distributed cloud environments, new security challenges emerge. The EU-funded RECODIS project aims at coordinating and fostering research collaboration in Europe on disaster resiliency in communication networks. One of the disaster types, considered by RECODIS Working Group (WG) 4, are deliberate human-made attacks aimed at gaining unauthorized access to the network or disrupting the service. In order to develop methods for increasing network security in the presence of attacks, it is crucial to first identify the security vulnerabilities and attack methods that exploit them, as well as the capabilities and shortcomings of existing security schemes. To this end, the members of RECODIS WG4 performed a comprehensive overview of attack methods and security approaches from the literature. This overview covers the security vulnerabilities inherent to the underlying physical layer, the implications of software-defined networking to security, and security challenges in cloud networks spanning geographically distributed data centers.
Regular expressions matching is commonly used in network security devices in order to detect malicious network traffic. New network attacks and other threats are emerging frequently. Therefore, the security device must be able to update the set of used regular expressions as soon as possible. The update operation must not disrupt normal operations of the security device. Therefore, the update must be done atomically. Current reconfigurable architectures are not suitable for highly integrated embedded network security devices because they require either additional external memory, ASICs or partial reconfiguration of the FPGA. Also, architectures based on deterministic finite automaton have an exponential time complexity even for real-word sets of regular expressions. Therefore, in this paper, we introduce a reconfigurable architecture with atomic updates suitable for real-world sets of regular expressions. Inspired by previous designs for both ASICs and FPGAs, we propose regular expressions matching architecture with significantly lower consumption of FPGA resources than previous dynamically reconfigurable FPGA design. The proposed architecture uses an interconnection matrix with a linear space complexity, while the previous one uses an interconnection matrix with a quadratic space complexity. The proposed architecture consumes from 6.9 to 48.9 times less LUTs than previous dynamically reconfigurable FPGA design. Single matched symbol utilizes between 4.35 and 32.2 LUTs.
Collaborative Anomaly Detection (CAD) is an emerging field of network security in both academia and industry. It has attracted a lot of attention, due to the limitations of traditional fortress-style defense modes. Even though a number of pioneer studies have been conducted in this area, few of them concern about the universality issue. This work focuses on two aspects of it. First, a unified collaborative detection framework is developed based on network virtualization technology. Its purpose is to provide a generic approach that can be applied to designing specific schemes for various application scenarios and objectives. Second, a general behavior perception model is proposed for the unified framework based on hidden Markov random field. Spatial Markovianity is introduced to model the spatial context of distributed network behavior and stochastic interaction among interconnected nodes. Algorithms are derived for parameter estimation, forward prediction, backward smooth, and the normality evaluation of both global network situation and local behavior. Numerical experiments using extensive simulations and several real datasets are presented to validate the proposed solution. Performance-related issues and comparison with related works are discussed.
Network operators rely on security services to protect their IT infrastructures. Different kinds of network security policies are defined globally and distributed among multiple security middleboxes deployed in networks. However, due to the complexity of security policy, it is inefficient to directly employ existing path-wise enforcement approaches. This paper models the enforcement of network security policy as the set-covering problem, and designs a computational-geometry-based policy space analysis (PSA) tool for set operations of security policy. Leveraging the PSA, this paper first investigates the topological characteristics of different types of policies. This heuristic information reveals intrinsic complexities of security policy and guides the design of our enforcement approach. Then the paper proposes a scope-wise policy enforcement algorithm that selects a modest number of enforcement network nodes to deploy multiple policy subsets in a greedy manner. This approach can be employed on network topologies of both datacenter and service provider. The efficiencies of the PSA tool and the enforcement algorithm are also evaluated. Compared with the header space analysis, the PSA achieves much better memory and time efficiencies on set operations of security policy. Additionally, the proposed enforcement algorithm is able to guarantee network security within a reasonable number of enforcement network nodes, without introducing many extra rules.
Spam messages propagate malware, disseminate phishing exploits, and advertise illegal products. Those messages generate costs for users and network operators, but it's difficult to measure the costs associated with spam traffic and determine who actually pays for it. Here, the authors provide a method to quantify the transit costs of spam traffic, identifying the routes traversed by spam messages collected at five honeypots. Combining the volume of spam traffic with traceroute measurements and a database of internetwork business relationships, they show that stub networks are systematically subject to high spam traffic costs. They also show that some networks profit from spam traffic and might not be interested in filtering it. Finally, a simple-but-effective algorithm is presented to identify the networks that would benefit from cooperating to filter spam traffic at the origin, to reduce transit costs.
Malware authors and operators typically collaborate to achieve the optimal profit. They also frequently change their behavior and resources to avoid detection. The authors propose a social similarity metrics that exploits these relationships to improve the effectiveness and stability of the threat propagation algorithm typically used to discover malicious collaboration. Furthermore, they propose behavioral modeling as a way to group similarly behaving servers, enabling extension of the ground truth that's so expensive to obtain in the field of network security. The authors also show that seeding the threat propagation algorithm from a set of coherently behaving servers (instead of from a single known malicious server identified by threat intelligence) makes the algorithm far more effective and significantly more robust, without compromising the precision of findings.
The increasing complexity of securing modern computer networks makes decision support systems an important tool for administrators. A challenge many existing tools fail to address is that attackers react strategically to new security measures, adapting their behaviors in response. Game theory provides a methodology for making decisions that takes into account these reactions, rather than assuming static attackers. The authors present an overview of how game theory can be used to inform one type of security decision: how to optimally place honeypots in a network. They demonstrate this approach on a realistic case study and present initial validation results based on a study comparing their approach with human decision makers.
